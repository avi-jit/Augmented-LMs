{"papers":[{"url":"https://www.semanticscholar.org/paper/2029349c55c1dba3493c5b3bd25152f18ba21ae2","title":"Augmented Language Models: a Survey","venue":"","year":2023,"referenceCount":169,"citationCount":1,"influentialCitationCount":1,"publicationDate":"15/02/2023","authors":"Grégoire Mialon,Roberto Dessì,M. Lomeli,Christoforos Nalmpantis,Ramakanth Pasunuru,Roberta Raileanu,Baptiste Rozière,Timo Schick,Jane Dwivedi-Yu,Asli Celikyilmaz,Edouard Grave,Yann LeCun,Thomas Scialom","id":"2029349c55c1dba3493c5b3bd25152f18ba21ae2","summary":"The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks.","score":136},{"url":"https://www.semanticscholar.org/paper/9325c4bee1de9dc9b91cfb43fa65320817d2eea4","title":"Complex QA and language models hybrid architectures, Survey","venue":"","year":2023,"referenceCount":346,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/02/2023","authors":"Xavier Daull,P. Bellot,Emmanuel Bruno,Vincent Martin,Elisabeth Murisasco","id":"9325c4bee1de9dc9b91cfb43fa65320817d2eea4","summary":"Current solutions and promising strategies are reviewed, using elements such as hybrid LLM architectures, human-in-the-loop reinforcement learning, prompting adaptation, neuro-symbolic and structured knowledge grounding, program synthesis, and others, and an overview of the current research and trends in the area of complex QA.","score":50},{"url":"https://www.semanticscholar.org/paper/d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning","venue":"ArXiv","year":2022,"referenceCount":216,"citationCount":1,"influentialCitationCount":1,"publicationDate":"20/12/2022","authors":"Pan Lu,Liang Qiu,Wenhao Yu,S. Welleck,Kai-Wei Chang","id":"d3a7a4543d83f568f79d1febe8379465ff0140c9","summary":"This survey paper reviews the key tasks, datasets, and methods at the intersec-tion of mathematical reasoning and deep learning over the past decade, and evaluates existing benchmarks and methods and discusses future research directions.","score":25},{"url":"https://www.semanticscholar.org/paper/873a581320d928249609d3c07229d5af182a379c","title":"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?","venue":"","year":2023,"referenceCount":86,"citationCount":2,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Chengwei Qin,Aston Zhang,Zhuosheng Zhang,Jiaao Chen,Michihiro Yasunaga,Diyi Yang","id":"873a581320d928249609d3c07229d5af182a379c","summary":"It is found that ChatGPT performs well on many tasks favoring reasoning capabilities while it still faces challenges when solving specific tasks such as sequence tagging, and with extensive empirical studies, both the effectiveness and limitations of the current version of ChatG PT are demonstrated.","score":24},{"url":"https://www.semanticscholar.org/paper/5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models","venue":"ArXiv","year":2022,"referenceCount":104,"citationCount":41,"influentialCitationCount":11,"publicationDate":"20/10/2022","authors":"Hyung Won Chung,Le Hou,S. Longpre,Barret Zoph,Yi Tay,W. Fedus,Eric Li,Xuezhi Wang,M. Dehghani,Siddhartha Brahma,Albert Webson,S. Gu,Zhuyun Dai,Mirac Suzgun,Xinyun Chen,Aakanksha Chowdhery,Dasha Valter,Sharan Narang,Gaurav Mishra,A. Yu,Vincent Zhao,Yanping Huang,Andrew M. Dai,Hongkun Yu,Slav Petrov,E. Chi,J. Dean,Jacob Devlin,Adam Roberts,Denny Zhou,Quoc Le,Jason Wei","id":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","summary":"This result shows that instruction and UL2 continued pre-training are complementary compute-eﬃcient methods to improve the performance of language models without increasing model scale.","score":23},{"url":"https://www.semanticscholar.org/paper/f2b0017ddd77fa38760a18145e63553105a1a236","title":"The Flan Collection: Designing Data and Methods for Effective Instruction Tuning","venue":"ArXiv","year":2023,"referenceCount":91,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"S. Longpre,Le Hou,Tu Vu,Albert Webson,Hyung Won Chung,Yi Tay,Denny Zhou,Quoc V. Le,Barret Zoph,Jason Wei,Adam Roberts","id":"f2b0017ddd77fa38760a18145e63553105a1a236","summary":"It is found task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings actually yields stronger performance in all settings.","score":21},{"url":"https://www.semanticscholar.org/paper/6d7b8a478801bd9d21df82d5f33ae6eced90da5e","title":"Solving math word problems with process- and outcome-based feedback","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":3,"influentialCitationCount":2,"publicationDate":"25/11/2022","authors":"J. Uesato,Nate Kushman,Ramana Kumar,Francis Song,Noah Siegel,L. Wang,Antonia Creswell,Geoffrey Irving,I. Higgins","id":"6d7b8a478801bd9d21df82d5f33ae6eced90da5e","summary":"It is found that pure outcome-based supervision produces similar final-answer error rates with less label supervision, but for correct reasoning steps it is necessary to use processbased supervision or supervision from learned reward models that emulate process-based feedback.","score":20},{"url":"https://www.semanticscholar.org/paper/e002bb8dae5a18a5ea1e7e1aafa16e19ad545662","title":"Solving Math Word Problems with Process-based and Outcome-based Feedback","venue":"","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"J. Uesato,Nate Kushman,Ramana Kumar,Francis Song,Noah Siegel,L. Wang,Antonia Creswell,Geoffery Irving,I. Higgins","id":"e002bb8dae5a18a5ea1e7e1aafa16e19ad545662","summary":"This work runs the first comprehensive comparison between process- and outcome- based approaches trained on a natural language task, GSM8K, and finds that pure outcome-based supervision produces similar final-answer error rates with less label supervision.","score":19},{"url":"https://www.semanticscholar.org/paper/e8db669c8cb1c07557ede15e2771968f9370330b","title":"Large language models are not zero-shot communicators","venue":"ArXiv","year":2022,"referenceCount":100,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Laura Ruis,Akbir Khan,Stella Rose Biderman,Sara Hooker,Tim Rocktaschel,Edward Grefenstette","id":"e8db669c8cb1c07557ede15e2771968f9370330b","summary":"A simple task is designed and widely used state-of-the-art models are evaluated, finding that, despite only evaluating on utterances that require a binary inference (yes or no), most perform close to random.","score":19},{"url":"https://www.semanticscholar.org/paper/5791c2b41dd23310c53d6738a4c0d587107c2dc8","title":"MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation","venue":"ArXiv","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Swarnadeep Saha,Xinyan Velocity Yu,Mohit Bansal,Ramakanth Pasunuru,Asli Celikyilmaz","id":"5791c2b41dd23310c53d6738a4c0d587107c2dc8","summary":"MURMUR is a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning that generates highly faithful and correct reasoning paths that lead to 26% more logically consistent summaries on LogicNLG, compared to direct prompting.","score":19},{"url":"https://www.semanticscholar.org/paper/4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning","venue":"ArXiv","year":2022,"referenceCount":70,"citationCount":7,"influentialCitationCount":3,"publicationDate":"15/11/2022","authors":"Hattie Zhou,Azade Nova,H. Larochelle,Aaron C. Courville,Behnam Neyshabur,Hanie Sedghi","id":"4d17732d90440682b0500f4e209c6cc4fac20e0e","summary":"This work shows that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which it is referred to as algorithmic prompting, and evaluates the approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrates boosts in performance over existing prompting techniques.","score":17},{"url":"https://www.semanticscholar.org/paper/ed99a2572fb5f4240aa6068e3bf274832e831306","title":"Recitation-Augmented Language Models","venue":"ArXiv","year":2022,"referenceCount":71,"citationCount":2,"influentialCitationCount":1,"publicationDate":"04/10/2022","authors":"Zhiqing Sun,Xuezhi Wang,Yi Tay,Yiming Yang,Denny Zhou","id":"ed99a2572fb5f4240aa6068e3bf274832e831306","summary":"It is shown that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks.","score":17},{"url":"https://www.semanticscholar.org/paper/663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":26,"influentialCitationCount":5,"publicationDate":"17/10/2022","authors":"Mirac Suzgun,Nathan Scales,Nathanael Scharli,Sebastian Gehrmann,Yi Tay,Hyung Won Chung,Aakanksha Chowdhery,Quoc V. Le,E. Chi,Denny Zhou,Jason Wei","id":"663a41c866d49ce052801fbc88947d39764cad29","summary":"It is found that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex to surpass it on 17 of the23 tasks.","score":17},{"url":"https://www.semanticscholar.org/paper/1bb6d5761903c7ac978188ae36e2648905e95dc5","title":"Transcending Scaling Laws with 0.1% Extra Compute","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":8,"influentialCitationCount":1,"publicationDate":"20/10/2022","authors":"Yi Tay,Jason Wei,Hyung Won Chung,V. Tran,David R. So,Siamak Shakeri,Xavier García,Huaixiu Zheng,J. Rao,Aakanksha Chowdhery,Denny Zhou,Donald Metzler,Slav Petrov,N. Houlsby,Quoc V. Le,M. Dehghani","id":"1bb6d5761903c7ac978188ae36e2648905e95dc5","summary":"U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks, reasoning tasks with chain-of-thought, multilingual tasks, MMLU and challenging BIG-Bench tasks, and is able to substantially improve the scaling properties of large language models on downstream metrics.","score":17},{"url":"https://www.semanticscholar.org/paper/03fca0b32fa443ea5a343a2e8859f1e221d03d9c","title":"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning","venue":"ArXiv","year":2023,"referenceCount":53,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Yunhu Ye,Binyuan Hui,Min Yang,Binhua Li,Fei Huang,Yongbin Li","id":"03fca0b32fa443ea5a343a2e8859f1e221d03d9c","summary":"This work exploits large language models (LLMs) as decomposers for effective table-based reasoning and proposes a “parsing-execution-ﬁlling” strategy to alleviate the hallucination dilemma of the chain of thought by decoupling logic and numerical computation in each step.","score":17},{"url":"https://www.semanticscholar.org/paper/8bfc22de7fe66286ad9ae705d677246757fbf8a8","title":"Large Language Models Can Be Easily Distracted by Irrelevant Context","venue":"ArXiv","year":2023,"referenceCount":57,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Freda Shi,Xinyun Chen,Kanishka Misra,Nathan Scales,David Dohan,E. Chi,Nathanael Scharli,Denny Zhou","id":"8bfc22de7fe66286ad9ae705d677246757fbf8a8","summary":"This work investigates the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context, and introduces Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description.","score":17},{"url":"https://www.semanticscholar.org/paper/03532123ccffae8d411264320e8a5ae2b6eddea0","title":"Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":4,"influentialCitationCount":0,"publicationDate":"28/12/2022","authors":"O. Khattab,Keshav Santhanam,Xiang Lisa Li,D. Hall,Percy Liang,Christopher Potts,M. Zaharia","id":"03532123ccffae8d411264320e8a5ae2b6eddea0","summary":"This work proposes D EMONSTRATE – S EARCH –P REDICT (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM, and can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions.","score":16},{"url":"https://www.semanticscholar.org/paper/ad5573cb25fd403f7620332f363ae87327c69a49","title":"The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":2,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Hanlin Zhang,Yi-Fan Zhang,Li Erran Li,Eric Xing","id":"ad5573cb25fd403f7620332f363ae87327c69a49","summary":"Comprehensive experiments are included to systematically compare LMLP with CoT in deductive reasoning settings, showing that L MLP enjoys more than 25% higher accuracy than COT on length generalization bench-marks even with fewer parameters.","score":16},{"url":"https://www.semanticscholar.org/paper/f7da57d40e68831fb1818b38d608c1332fd39359","title":"Transformer models: an introduction and catalog","venue":"","year":2023,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/02/2023","authors":"X. Amatriain","id":"f7da57d40e68831fb1818b38d608c1332fd39359","summary":"","score":16},{"url":"https://www.semanticscholar.org/paper/d697b440dd0e65a05fe027e4c0ea85f62dcba033","title":"Can large language models reason about medical questions?","venue":"ArXiv","year":2022,"referenceCount":52,"citationCount":12,"influentialCitationCount":2,"publicationDate":"17/07/2022","authors":"Valentin Li'evin,C. Hother,O. Winther","id":"d697b440dd0e65a05fe027e4c0ea85f62dcba033","summary":"It is speculated that scaling model and data, enhancing prompt alignment and allowing for better contextualization of the completions will be sufﬁcient for LLMs to reach human-level performance on this type of task.","score":16},{"url":"https://www.semanticscholar.org/paper/cef330bacf014d60daabbd489647b2006af130ca","title":"Discovering Language Model Behaviors with Model-Written Evaluations","venue":"ArXiv","year":2022,"referenceCount":86,"citationCount":4,"influentialCitationCount":1,"publicationDate":"19/12/2022","authors":"Ethan Perez,Sam Ringer,Kamilė Lukošiūtė,Karina Nguyen,Edwin Chen,Scott Heiner,Craig Pettit,Catherine Olsson,Sandipan Kundu,Saurav Kadavath,Andy Jones,Anna Chen,Benjamin Mann,Brian Israel,Bryan Seethor,C. McKinnon,C. Olah,Daisong Yan,Daniela Amodei,Dario Amodei,Dawn Drain,Dustin Li,Eli Tran-Johnson,G. Khundadze,John Kernion,J. Landis,Jamie Kerr,J. Mueller,Jeeyoon Hyun,J. Landau,Kamal Ndousse,L. Goldberg,Liane Lovitt,Martin Lucas,Michael Sellitto,Miranda Zhang,Neerav Kingsland,Nelson Elhage,Nicholas Joseph,Noem'i Mercado,Nova DasSarma,Oliver Rausch,Robin Larson,Sam McCandlish,Scott Johnston,S. Kravec,Sheer El Showk,Tamera Lanham,Timothy Telleen-Lawton,Tom B. Brown,T. Henighan,Tristan Hume,Yuntao Bai,Zac Hatfield-Dodds,Jack Clark,Sam Bowman,Amanda Askell,Roger C. Grosse,Danny Hernandez,Deep Ganguli,Evan Hubinger,Nicholas Schiefer,Jared Kaplan","id":"cef330bacf014d60daabbd489647b2006af130ca","summary":"","score":16},{"url":"https://www.semanticscholar.org/paper/3eed4de25636ac90f39f6e1ef70e3507ed61a2a6","title":"Talking About Large Language Models","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":5,"influentialCitationCount":0,"publicationDate":"07/12/2022","authors":"M. Shanahan","id":"3eed4de25636ac90f39f6e1ef70e3507ed61a2a6","summary":"The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.","score":16},{"url":"https://www.semanticscholar.org/paper/2cd72e71299c5d62d5cdb1164df5236172d418c4","title":"Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits","venue":"ArXiv","year":2023,"referenceCount":90,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Ruibo Liu,Chenyan Jia,Ge Zhang,Ziyu Zhuang,Tony X. Liu,Soroush Vosoughi","id":"2cd72e71299c5d62d5cdb1164df5236172d418c4","summary":"A new learning paradigm that enables language models (LMs) to re -align with human values, S ECOND T HOUGHTS achieves superior performance in three value alignment benchmark datasets but also shows strong human-value transfer learning ability in few-shot scenarios.","score":15},{"url":"https://www.semanticscholar.org/paper/54a4517022703a27e1670d3b84214521882f0108","title":"Contrastive Distillation Is a Sample-Efficient Self-Supervised Loss Policy for Transfer Learning","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Christopher T. Lengerich,Gabriel Synnaeve,Amy Zhang,H. Leather,Kurt Shuster,Franccois Charton,Charysse Redwood","id":"54a4517022703a27e1670d3b84214521882f0108","summary":"A self-supervised loss policy is proposed called contrastive distillation which manifests latent variables with high mutual information with both source and target tasks from weights to tokens which outperforms common methods of transfer learning and suggests a useful design axis of trading for generalizability for online transfer.","score":15},{"url":"https://www.semanticscholar.org/paper/69619a2a47faee7a29ec596db13172e2a42ff921","title":"Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models","venue":"ArXiv","year":2023,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Zhihong Shao,Yeyun Gong,Yelong Shen,Minlie Huang,Nan Duan,Weizhu Chen","id":"69619a2a47faee7a29ec596db13172e2a42ff921","summary":"Synthetic prompting is introduced, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning.","score":15},{"url":"https://www.semanticscholar.org/paper/79b88230fabb59a1d368641bbc822af0f09bf262","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models","venue":"ArXiv","year":2022,"referenceCount":75,"citationCount":118,"influentialCitationCount":32,"publicationDate":"21/03/2022","authors":"Xuezhi Wang,Jason Wei,D. Schuurmans,Quoc Le,E. Chi,Denny Zhou","id":"79b88230fabb59a1d368641bbc822af0f09bf262","summary":"A simple ensemble strategy, self-consistency, that robustly improves accuracy across a variety of language models and model scales without the need for additional training or auxiliary models is explored.","score":14},{"url":"https://www.semanticscholar.org/paper/4bea09d4c897fb201c032b9eb605a943b1e70435","title":"CORRPUS: Detecting Story Inconsistencies via Codex-Bootstrapped Neurosymbolic Reasoning","venue":"ArXiv","year":2022,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Yi Dong,Lara J. Martin,Chris Callison-Burch","id":"4bea09d4c897fb201c032b9eb605a943b1e70435","summary":"It is shown that the CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI task 2 and Re 3 ) with minimal hand engineering.","score":14},{"url":"https://www.semanticscholar.org/paper/9a9e68d400069f023f7dc9b982226c95159a509d","title":"Dissociating language and thought in large language models: a cognitive perspective","venue":"ArXiv","year":2023,"referenceCount":412,"citationCount":8,"influentialCitationCount":0,"publicationDate":"16/01/2023","authors":"Kyle Mahowald,Anna A. Ivanova,I. Blank,N. Kanwisher,J. Tenenbaum,Evelina Fedorenko","id":"9a9e68d400069f023f7dc9b982226c95159a509d","summary":"","score":14},{"url":"https://www.semanticscholar.org/paper/b2542a738b75ee9b7ce1a13d8b78f9095d212412","title":"Generate rather than Retrieve: Large Language Models are Strong Context Generators","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":11,"influentialCitationCount":3,"publicationDate":"21/09/2022","authors":"W. Yu,Dan Iter,Shuohang Wang,Yichong Xu,Mingxuan Ju,Soumya Sanyal,Chenguang Zhu,Michael Zeng,Meng Jiang","id":"b2542a738b75ee9b7ce1a13d8b78f9095d212412","summary":"The proposed method is evaluated on three different knowledge-intensive tasks and its effectiveness on both zero-shot and supervised settings is demonstrated.","score":14},{"url":"https://www.semanticscholar.org/paper/766709f901c660863d3e7f7fa11a4e2f696da438","title":"Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks","venue":"ArXiv","year":2022,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/10/2022","authors":"Zhenhailong Wang,Xiaoman Pan,Dian Yu,Dong Yu,Jianshu Chen,Heng Ji","id":"766709f901c660863d3e7f7fa11a4e2f696da438","summary":"The checkpoint that achieved the best overall performance across all tasks was the one with the lowest encode.rate of 1e-4.","score":14},{"url":"https://www.semanticscholar.org/paper/90350aa626bed47b02d0c162462e5b0ca82be6b2","title":"Automatic Chain of Thought Prompting in Large Language Models","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":18,"influentialCitationCount":4,"publicationDate":"07/10/2022","authors":"Zhuosheng Zhang,Aston Zhang,Mu Li,Alexander J. Smola","id":"90350aa626bed47b02d0c162462e5b0ca82be6b2","summary":"An automatic CoT prompting method that samples questions with diversity and generates reasoning chains to construct demonstrations and consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations.","score":14},{"url":"https://www.semanticscholar.org/paper/2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75","title":"Grounding Language Models to Images for Multimodal Generation","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Jing Yu Koh,R. Salakhutdinov,Daniel Fried","id":"2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75","summary":"An ef-fective, general solution for leveraging pretrained language models in visually grounded settings, enabling them to process and generate arbitrarily interleaved image-and-text data.","score":14},{"url":"https://www.semanticscholar.org/paper/d2170504c4ad9403bea118ae8debdfda95978546","title":"The Wisdom of Hindsight Makes Language Models Better Instruction Followers","venue":"ArXiv","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"Tianjun Zhang,Fangchen Liu,Justin Wong,P. Abbeel,Joseph Gonzalez","id":"d2170504c4ad9403bea118ae8debdfda95978546","summary":"HIR is proposed, a novel algorithm for aligning language models with instructions by converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner and it outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning.","score":13},{"url":"https://www.semanticscholar.org/paper/04bc67f263a247ab49c19a3712be8c9c77297f08","title":"Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models","venue":"ArXiv","year":2022,"referenceCount":91,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/10/2022","authors":"Xiaoman Pan,Wenlin Yao,Hongming Zhang,Dian Yu,Dong Yu,Jianshu Chen","id":"04bc67f263a247ab49c19a3712be8c9c77297f08","summary":"A novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory and finds that KiC can be identified as a special mixture-of-experts (MoE) model, where the knowledge selector plays the role of a router that is used to determine the sequence- to-expert assignment in MoE.","score":13},{"url":"https://www.semanticscholar.org/paper/22d314c347a6c5c0bbf528a102455229ce0b36b5","title":"Harnessing Knowledge and Reasoning for Human-Like Natural Language Generation: A Brief Review","venue":"ArXiv","year":2022,"referenceCount":146,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/12/2022","authors":"Jiangjie Chen,Yanghua Xiao","id":"22d314c347a6c5c0bbf528a102455229ce0b36b5","summary":"The importance of NLG being guided by knowledge, in order to convey human-like reasoning through language generation, is explored, and ten goals for intelligent NLG systems to pursue are proposed.","score":13},{"url":"https://www.semanticscholar.org/paper/fb49e88c6bd676516898e911e42b4f8479e6f1bf","title":"Ask Me Anything: A simple strategy for prompting language models","venue":"ArXiv","year":2022,"referenceCount":71,"citationCount":10,"influentialCitationCount":0,"publicationDate":"05/10/2022","authors":"Simran Arora,A. Narayan,Mayee F. Chen,Laurel J. Orr,Neel Guha,Kush S Bhatia,Ines Chami,Frederic Sala,Christopher R'e","id":"fb49e88c6bd676516898e911e42b4f8479e6f1bf","summary":"This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks.","score":13},{"url":"https://www.semanticscholar.org/paper/236bba82c1f048b3aa1f661460ec462f2dc9257f","title":"REPLUG: Retrieval-Augmented Black-Box Language Models","venue":"ArXiv","year":2023,"referenceCount":44,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Weijia Shi,Sewon Min,Michihiro Yasunaga,Minjoon Seo,Rich James,M. Lewis,Luke Zettlemoyer,Wen-tau Yih","id":"236bba82c1f048b3aa1f661460ec462f2dc9257f","summary":"R E P LUG is introduced, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model and can be easily applied to any existing retrieval and language models.","score":13},{"url":"https://www.semanticscholar.org/paper/85996f9fc312777f487dd51bf9e96bb3704c2fb7","title":"On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)","venue":"","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Karthik Valmeekam,S. Sreedharan,Matthew Marquez,Alberto Olmo,Subbarao Kambhampati","id":"85996f9fc312777f487dd51bf9e96bb3704c2fb7","summary":"The results show that LLM's ability to autonomously generate executable plans is quite meager, averaging only about 3% success rate, and the heuristic and human-in-the-loop modes show slightly more promise.","score":13},{"url":"https://www.semanticscholar.org/paper/1853a05df0ac03f8e4445ec65f7599359e89fb2b","title":"ThoughtSource: A central hub for large language model reasoning data","venue":"ArXiv","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Simon Ott,Konstantin Hebenstreit,Valentin Li'evin,C. Hother,M. Moradi,Maximilian Mayrhauser,Robert Praas,O. Winther,M. Samwald","id":"1853a05df0ac03f8e4445ec65f7599359e89fb2b","summary":"The goal of ThoughtSource is to improve future artiﬁcial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data.","score":13},{"url":"https://www.semanticscholar.org/paper/13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models","venue":"ArXiv","year":2022,"referenceCount":115,"citationCount":275,"influentialCitationCount":50,"publicationDate":"02/05/2022","authors":"Susan Zhang,Stephen Roller,Naman Goyal,Mikel Artetxe,Moya Chen,Shuohui Chen,Christopher Dewan,Mona Diab,Xian Li,Xi Victoria Lin,Todor Mihaylov,Myle Ott,Sam Shleifer,Kurt Shuster,Daniel Simig,Punit Singh Koura,Anjali Sridhar,Tianlu Wang,Luke Zettlemoyer","id":"13a0d8bb38f739990c8cd65a44061c6534f17221","summary":"This work presents Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which they aim to fully and responsibly share with interested researchers.","score":13},{"url":"https://www.semanticscholar.org/paper/5922f437512158970c417f4413bface021df5f78","title":"A Generalist Agent","venue":"ArXiv","year":2022,"referenceCount":102,"citationCount":134,"influentialCitationCount":20,"publicationDate":"12/05/2022","authors":"S. Reed,Konrad Zolna,Emilio Parisotto,Sergio Gomez Colmenarejo,Alexander Novikov,Gabriel Barth-Maron,Mai Gimenez,Yury Sulsky,Jackie Kay,J. T. Springenberg,Tom Eccles,Jake Bruce,Ali Razavi,Ashley D. Edwards,N. Heess,Yutian Chen,R. Hadsell,Oriol Vinyals,Mahyar Bordbar,N. D. Freitas","id":"5922f437512158970c417f4413bface021df5f78","summary":"","score":13},{"url":"https://www.semanticscholar.org/paper/559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review","venue":"ArXiv","year":2022,"referenceCount":697,"citationCount":4,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"D. Hupkes,Mario Giulianelli,Verna Dankers,Mikel Artetxe,Yanai Elazar,Tiago Pimentel,Christos Christodoulopoulos,Karim Lasri,Naomi Saphra,Arabella J. Sinclair,Dennis Ulmer,Florian Schottmann,Khuyagbaatar Batsuren,Kaiser Sun,Koustuv Sinha,Leila Khalatbari,Maria Ryskina,Rita Frieske,Ryan Cotterell,Zhijing Jin","id":"559bfba3bee31f6061a5d5c7061f22794de47e39","summary":"A taxonomy for characterising and understanding generalisation research in NLP is presented, a taxonomy is used to present a comprehensive map of published generalisation studies, and recommendations for which areas might deserve attention in the future are made.","score":13},{"url":"https://www.semanticscholar.org/paper/075f83cac2742dbb36ee49d30f7aee2a322f3127","title":"Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models","venue":"","year":2022,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/10/2022","authors":"Hao Liu,Xinyang Geng,Lisa Lee,Igor Mordatch,S. Levine,Sharan Narang,P. Abbeel","id":"075f83cac2742dbb36ee49d30f7aee2a322f3127","summary":"This work proposes a simple technique, Forgetful Causal Masking (FCM), which improves both few-shot and netuning performance of PaLM and considers a simple extension, T-FCM, which introduces bidirectional context to causal language model without altering the sequence order.","score":13},{"url":"https://www.semanticscholar.org/paper/3a07a87090a061ca41dd30ac8398a9a5d9d39826","title":"Dense Text Retrieval based on Pretrained Language Models: A Survey","venue":"ArXiv","year":2022,"referenceCount":344,"citationCount":4,"influentialCitationCount":0,"publicationDate":"27/11/2022","authors":"Wayne Xin Zhao,Jing Liu,Ruiyang Ren,Ji-rong Wen","id":"3a07a87090a061ca41dd30ac8398a9a5d9d39826","summary":"This survey aims to provide a comprehensive, practical reference focused on the major progress for dense text retrieval, and takes a new perspective to organize the related work by four major aspects, including architecture, training, indexing and integration, and summarize the mainstream techniques for each aspect.","score":13},{"url":"https://www.semanticscholar.org/paper/89c3bd70ad33c4f8832f00ab98872b77861ee0ec","title":"Discovering Latent Knowledge in Language Models Without Supervision","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":4,"influentialCitationCount":1,"publicationDate":"07/12/2022","authors":"Collin Burns,Hao-Tong Ye,D. Klein,J. Steinhardt","id":"89c3bd70ad33c4f8832f00ab98872b77861ee0ec","summary":"It is shown that despite using no supervision and no model outputs, the method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4% on average.","score":13},{"url":"https://www.semanticscholar.org/paper/0ca25741b79f8780614a7cd730a6170fccdc5abb","title":"A Survey for In-context Learning","venue":"ArXiv","year":2022,"referenceCount":74,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/12/2022","authors":"Qingxiu Dong,Lei Li,Damai Dai,Ce Zheng,Zhiyong Wu,Baobao Chang,Xu Sun,Jingjing Xu,Zhifang Sui","id":"0ca25741b79f8780614a7cd730a6170fccdc5abb","summary":"This paper presents a formal definition of ICL and clarify its correlation to related studies, and organizes and discusses advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis.","score":13},{"url":"https://www.semanticscholar.org/paper/2f21201ac9fcb88a72c56471402388ec2fc365a8","title":"Inferring Implicit Relations in Complex Questions with Language Models","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Uri Katz,Mor Geva,Jonathan Berant","id":"2f21201ac9fcb88a72c56471402388ec2fc365a8","summary":"This work investigates why current models struggle with implicit reasoning question answering (QA) tasks, by decoupling inference of reasoning steps from their execution, and evaluates models from the GPT-3 family, finding that while these models struggle on the implicit reasoning QA task, they often succeed at inferring implicit relations.","score":12},{"url":"https://www.semanticscholar.org/paper/598d9b235f5ab148fc757240d9bc39a47b8eaf72","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":20,"influentialCitationCount":5,"publicationDate":"22/11/2022","authors":"Wenhu Chen,Xueguang Ma,Xinyi Wang,William W. Cohen","id":"598d9b235f5ab148fc757240d9bc39a47b8eaf72","summary":"Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12% across all the evaluated datasets, and by combining PoT with self-consistency decoding, can achieve SoT performance on all math problem datasets and near-SoTA performance on ﬁnancial datasets.","score":12},{"url":"https://www.semanticscholar.org/paper/9431181f8115a2360621df5ed76e1a23b88e3b2f","title":"Evaluating Human-Language Model Interaction","venue":"ArXiv","year":2022,"referenceCount":168,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Mina Lee,Megha Srivastava,Amelia Hardy,John Thickstun,Esin Durmus,Ashwin Paranjape,Ines Gerard-Ursin,Xiang Lisa Li,Faisal Ladhak,Frieda Rong,Rose E. Wang,Minae Kwon,Joon Sung Park,Hancheng Cao,Tony Lee,Rishi Bommasani,Michael Bernstein,Percy Liang","id":"9431181f8115a2360621df5ed76e1a23b88e3b2f","summary":"A framework, Human-AI Language-based Interaction Evaluation (H-LINE), is developed that expands non-interactive evaluation along three dimensions, capturing the interactive process, not only the output of the system, and notions of preference beyond quality.","score":12},{"url":"https://www.semanticscholar.org/paper/f84728f28cd3f234ecce8fe899d6cb001b18a263","title":"Nonparametric Masked Language Modeling","venue":"ArXiv","year":2022,"referenceCount":85,"citationCount":2,"influentialCitationCount":1,"publicationDate":"02/12/2022","authors":"Sewon Min,Weijia Shi,M. Lewis,Xilun Chen,Wen-tau Yih,Hanna Hajishirzi,Luke Zettlemoyer","id":"f84728f28cd3f234ecce8fe899d6cb001b18a263","summary":"It is shown that N P M can be efﬁciently trained with a contrastive objective and an in-batch approximation to full corpus retrieval, and is particularly better on dealing with rare patterns (word senses or facts), and predicting rare or nearly unseen words.","score":12},{"url":"https://www.semanticscholar.org/paper/645b10b7802a035e034488e3640fc0bc415de34c","title":"UL2: Unifying Language Learning Paradigms","venue":"","year":2022,"referenceCount":125,"citationCount":1,"influentialCitationCount":1,"publicationDate":"10/05/2022","authors":"Yi Tay,M. Dehghani,V. Tran,Xavier García,Jason Wei,Xuezhi Wang,Hyung Won Chung,Dara Bahri,Tal Schuster,Huaixiu Zheng,Denny Zhou,N. Houlsby,Donald Metzler","id":"645b10b7802a035e034488e3640fc0bc415de34c","summary":"By scaling the model up to 20B parameters, this paper achieves SOTA performance on 50 well-established supervised NLP tasks ranging from language generation, language understanding, text classiﬁcation, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval.","score":12},{"url":"https://www.semanticscholar.org/paper/e3bdf0e84ca06ad456dcd2b073cc72fe81c5b46e","title":"ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning","venue":"ArXiv","year":2022,"referenceCount":52,"citationCount":1,"influentialCitationCount":1,"publicationDate":"15/12/2022","authors":"O. Yu. Golovneva,Moya Chen,Spencer Poff,Martin Corredor,Luke Zettlemoyer,M. Fazel-Zarandi,Asli Celikyilmaz","id":"e3bdf0e84ca06ad456dcd2b073cc72fe81c5b46e","summary":"ROSCOE is presented, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics and can measure semantic consistency, logicality, informativeness, fluency, and factuality — among other traits — by leveraging properties of step-by-step rationales.","score":12},{"url":"https://www.semanticscholar.org/paper/e66f0f822d4c4853b39b27daaafa2993005fd55e","title":"Large Language Models are few(1)-shot Table Reasoners","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":8,"influentialCitationCount":1,"publicationDate":"13/10/2022","authors":"Wenhu Chen","id":"e66f0f822d4c4853b39b27daaafa2993005fd55e","summary":"This paper evaluated LLMs on popular table QA and fact veriﬁcation datasets like WikiTableQuestion, FetaQA, TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning over table structures, though these models are not pre-trained on any table corpus.","score":12},{"url":"https://www.semanticscholar.org/paper/e9f5c4bb0db632eaf48c4d4ce83a29753d2d861b","title":"Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":13,"influentialCitationCount":3,"publicationDate":"21/06/2022","authors":"Karthik Valmeekam,Alberto Olmo,S. Sreedharan,Subbarao Kambhampati","id":"e9f5c4bb0db632eaf48c4d4ce83a29753d2d861b","summary":"This work proposes an extensible assessment framework to test the capabilities of LLMs on reasoning about actions and change, a central aspect of human intelligence and provides multiple test cases that are more involved than any of the previously established benchmarks.","score":12},{"url":"https://www.semanticscholar.org/paper/ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":14,"influentialCitationCount":1,"publicationDate":"28/07/2022","authors":"Mohammad Bavarian,Heewoo Jun,N. Tezak,J. Schulman,C. McLeavey,Jerry Tworek,Mark Chen","id":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","summary":"There is extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales.","score":12},{"url":"https://www.semanticscholar.org/paper/c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd","title":"Internet-augmented language models through few-shot prompting for open-domain question answering","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":12,"influentialCitationCount":3,"publicationDate":"10/03/2022","authors":"Angeliki Lazaridou,E. Gribovskaya,Wojciech Stokowiec,N. Grigorev","id":"c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd","summary":"It is suggested that it might be crucial to slow down the race towards the biggest model and instead shift attention towards more effective ways to use models, including but not limited to, better prompting or increasing inference-time compute.","score":12},{"url":"https://www.semanticscholar.org/paper/e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","venue":"BIGSCIENCE","year":2022,"referenceCount":141,"citationCount":76,"influentialCitationCount":10,"publicationDate":"14/04/2022","authors":"Sid Black,Stella Rose Biderman,Eric Hallahan,Quentin G. Anthony,Leo Gao,Laurence Golding,Horace He,Connor Leahy,Kyle McDonell,Jason Phang,M. Pieler,Usvsn Sai Prashanth,Shivanshu Purohit,Laria Reynolds,J. Tow,Benqi Wang,Samuel Weinbach","id":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","summary":"GPT-NeoX-20B is introduced, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.","score":12},{"url":"https://www.semanticscholar.org/paper/1d26c947406173145a4665dd7ab255e03494ea28","title":"GLM-130B: An Open Bilingual Pre-trained Model","venue":"ArXiv","year":2022,"referenceCount":118,"citationCount":16,"influentialCitationCount":5,"publicationDate":"05/10/2022","authors":"Aohan Zeng,Xiao Liu,Zhengxiao Du,Zihan Wang,Hanyu Lai,Ming Ding,Zhuoyi Yang,Yifan Xu,Wendi Zheng,Xiao Xia,W. Tam,Zixuan Ma,Yufei Xue,Jidong Zhai,Wenguang Chen,P. Zhang,Yuxiao Dong,Jie Tang","id":"1d26c947406173145a4665dd7ab255e03494ea28","summary":"An attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained, including its design choices, training strategies for both efficiency and stability, and engineering efforts is introduced.","score":12},{"url":"https://www.semanticscholar.org/paper/2303ee0de927266c296287202519f17bdea9e4e8","title":"Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization","venue":"ArXiv","year":2022,"referenceCount":76,"citationCount":3,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"Seonghyeon Ye,Joel Jang,Doyoung Kim,Yongrae Jo,Minjoon Seo","id":"2303ee0de927266c296287202519f17bdea9e4e8","summary":"This paper explores how the retrieval of soft prompts obtained through prompt tuning can assist hard prompts in zero-shot task generalization and finds that retrieving source embeddings trained on similar answer choice formats is more important than those on similar task types.","score":12},{"url":"https://www.semanticscholar.org/paper/ec7324a15009a9bd0b676f6b17762f759cf5dd9a","title":"Large Language Models Are Human-Level Prompt Engineers","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":13,"influentialCitationCount":2,"publicationDate":"03/11/2022","authors":"Yongchao Zhou,Andrei Ioan Muresanu,Ziwen Han,Keiran Paster,Silviu Pitis,Harris Chan,Jimmy Ba","id":"ec7324a15009a9bd0b676f6b17762f759cf5dd9a","summary":"It is shown that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts.","score":12},{"url":"https://www.semanticscholar.org/paper/9cc5c25517c3a78183a052e8e93a44e85bb17432","title":"Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/12/2022","authors":"Jifan Chen,Yuhao Zhang,Lan Liu,Rui Dong,Xinchi Chen,Patrick Ng,William Yang Wang,Zhiheng Huang","id":"9cc5c25517c3a78183a052e8e93a44e85bb17432","summary":"This paper designs the task conﬁgurations to explicitly specify the task type, as well as its input and output types, and shows that this not only allows the model to better learn shared knowledge across different tasks at training, but also allows us to control the model by composing new conﰂgurations that apply novel input-output combinations in a zero-shot manner.","score":12},{"url":"https://www.semanticscholar.org/paper/ea0688f9e7dfb0d3c2249486af65209c25809544","title":"Faithful Chain-of-Thought Reasoning","venue":"ArXiv","year":2023,"referenceCount":31,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"QING LYU,Shreya Havaldar,Adam Stein,Li Zhang,D. Rao,Eric Wong,Marianna Apidianaki,Chris Callison-Burch","id":"ea0688f9e7dfb0d3c2249486af65209c25809544","summary":"Faithful CoT is proposed, a faithful-by-construction framework that decomposes a reasoning task into two stages: Translation (Natural Language query $\\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\\ rightarrow$ answer), using an LM and a deterministic solver respectively.","score":12},{"url":"https://www.semanticscholar.org/paper/fbd49b25bdab98c171af49962a41139c73dacbde","title":"Specializing Smaller Language Models towards Multi-Step Reasoning","venue":"ArXiv","year":2023,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Yao Fu,Hao-Chun Peng,Litu Ou,Ashish Sabharwal,Tushar Khot","id":"fbd49b25bdab98c171af49962a41139c73dacbde","summary":"This work shows two important aspects of model abilities: there exists a very complex balance/ tradeoff between language models’ multi-dimensional abilities and by paying the price of decreased generic ability, it can clearly lift up the scaling curve of models smaller than 10B towards a specialized multi-step math reasoning ability.","score":12},{"url":"https://www.semanticscholar.org/paper/4ef5410ec4b546eda642fe786cc1bdbb5a7251e1","title":"Attributed Text Generation via Post-hoc Research and Revision","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":6,"influentialCitationCount":1,"publicationDate":2022,"authors":"Luyu Gao,Zhuyun Dai,Panupong Pasupat,Anthony Chen,Arun Tejasvi Chaganty,Yicheng Fan,Vincent Zhao,N. Lao,Hongrae Lee,Da-Cheng Juan,Kelvin Guu","id":"4ef5410ec4b546eda642fe786cc1bdbb5a7251e1","summary":"RARR is a system that automatically attribution for the output of any text generation model and post-edits the output to unsupported content while preserving the original output as much as possible and improves attribution while otherwise preserving theOriginal input to a much greater degree than previously explored edit models.","score":11},{"url":"https://www.semanticscholar.org/paper/239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"E. Zelikman,Qian Huang,Gabriel Poesia,Noah D. Goodman,N. Haber","id":"239b5649b12f28fd610de036afba41b9246db6c9","summary":"This work introduces Parsel 2, a framework enabling automatic implementation and validation of complex algorithms with code LLMs, based on hierarchical function descriptions in natural language, which can be used across domains requiring hierarchical reasoning, e.g. code synthesis, theorem proving, and robotic planning.","score":11},{"url":"https://www.semanticscholar.org/paper/edcd520e553dc58c728eceb8433e3d155955a89a","title":"Complementary Explanations for Effective In-Context Learning","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":4,"influentialCitationCount":1,"publicationDate":"25/11/2022","authors":"Xi Ye,Srini Iyer,Asli Celikyilmaz,V. Stoyanov,Greg Durrett,Ramakanth Pasunuru","id":"edcd520e553dc58c728eceb8433e3d155955a89a","summary":"This work proposes a maximal-marginal-relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs.","score":11},{"url":"https://www.semanticscholar.org/paper/29be9045fb09f0c947fb24c76bd1136d47880d96","title":"Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":3,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"H. Trivedi,Niranjan Balasubramanian,Tushar Khot,Ashish Sabharwal","id":"29be9045fb09f0c947fb24c76bd1136d47880d96","summary":"IRCoT is proposed, a new approach that interleaves retrieval with CoT for multi-step QA, guiding the retrieval with coT and in turn using retrieved results to improve CoT.","score":11},{"url":"https://www.semanticscholar.org/paper/1e9e1da1097d6e9d3849ea6539b170893c325bdb","title":"ThinkSum: Probabilistic reasoning over sets using large language models","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":2,"influentialCitationCount":0,"publicationDate":"04/10/2022","authors":"Batu Mehmet Ozturkler,Nikolay Malkin,Zhen Wang,N. Jojic","id":"1e9e1da1097d6e9d3849ea6539b170893c325bdb","summary":"It is argued that because the probabilistic inference in T HINK S UM is performed outside of calls to the LLM, it is less sensitive to prompt design, yields more interpretable predictions, and can be ﬂexibly combined with latent variable models to extract structured knowledge from LLMs.","score":11},{"url":"https://www.semanticscholar.org/paper/c43a4a7b7ea4f4889de051321cb0073fd577f843","title":"Causal Reasoning of Entities and Events in Procedural Texts","venue":"ArXiv","year":2023,"referenceCount":85,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/01/2023","authors":"Li Zhang,Hai Xu,Yue Yang,Shuyan Zhou,Weiqiu You,Manni Arora,Chris Callison-Burch","id":"c43a4a7b7ea4f4889de051321cb0073fd577f843","summary":"This work proposes CREPE, the first benchmark on causal reasoning of event plausibility and entity states, and boosts model performance to .59 F1 by creatively representing events as programming languages while prompting language models pretrained on code.","score":11},{"url":"https://www.semanticscholar.org/paper/0fbcb934048a0da9c12ccffe2417e48793884992","title":"Unifying Molecular and Textual Representations via Multi-task Language Modelling","venue":"ArXiv","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/01/2023","authors":"Dimitrios Christofidellis,Giorgio Giannone,Jannis Born,O. Winther,T. Laino,M. Manica","id":"0fbcb934048a0da9c12ccffe2417e48793884992","summary":"This work proposes a multi-domain, multi-task language model that can handle chemical and natural language concurrently, without requiring expensive pre-training on single domains or task-speciﬁc models, and suggests that such models can robustly and e-ciently accelerate discovery in physical sciences by superseding problem-speciation and enhancing human-model interactions.","score":11},{"url":"https://www.semanticscholar.org/paper/de1c7ae2818aa26fc86a0ea8ed70014cffc8b20a","title":"Fine-tuning language models to find agreement among humans with diverse preferences","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":5,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"Michiel A. Bakker,Martin Chadwick,H. Sheahan,Michael Henry Tessler,Lucy Campbell-Gillingham,Jan Balaguer,Nathan McAleese,A. Glaese,J. Aslanides,M. Botvinick,C. Summerfield","id":"de1c7ae2818aa26fc86a0ea8ed70014cffc8b20a","summary":"A large language modeling model is tuned to generate statements that maximize the expected approval for a group of people with potentially diverse opinions and produces consensus statements that are preferred by human users over those from prompted LLMs and outperforms a tight baseline that lacks the ranking step.","score":11},{"url":"https://www.semanticscholar.org/paper/79d083a742fe2c20f543f442f5324e63a4d4ae2d","title":"Auditing Large Language Models: A Three-Layered Approach","venue":"SSRN Electronic Journal","year":2023,"referenceCount":240,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Jakob Mokander,Jonas Schuett,Hannah Rose Kirk,Luciano Floridi","id":"79d083a742fe2c20f543f442f5324e63a4d4ae2d","summary":"","score":11},{"url":"https://www.semanticscholar.org/paper/205eab69e430b4da93ecf3fd9115f0919b448040","title":"WeLM: A Well-Read Pre-trained Language Model for Chinese","venue":"ArXiv","year":2022,"referenceCount":85,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/09/2022","authors":"Hui Su,Xiao Zhou,Houjin Yu,Yuwen Chen,Zilin Zhu,Yang Yu,Jie Zhou","id":"205eab69e430b4da93ecf3fd9115f0919b448040","summary":"A well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations, and has basic skills at explaining and calibrating the decisions from itself, which can be promising directions for future research.","score":11},{"url":"https://www.semanticscholar.org/paper/711d5e8ddbb840ad31a9ffa3d38590603ba69a92","title":"Prompting GPT-3 To Be Reliable","venue":"ArXiv","year":2022,"referenceCount":100,"citationCount":8,"influentialCitationCount":1,"publicationDate":"17/10/2022","authors":"Chenglei Si,Zhe Gan,Zhengyuan Yang,Shuohang Wang,Jianfeng Wang,Jordan L. Boyd-Graber,Lijuan Wang","id":"711d5e8ddbb840ad31a9ffa3d38590603ba69a92","summary":"This systematic empirical study sheds new insights on the reliability of prompting LLMs, but more importantly, its prompting strategies can help practitioners more reliably use LLMs like GPT-3.","score":11},{"url":"https://www.semanticscholar.org/paper/ff21066b94e9e09ffda6acfbc1c550b05a09b333","title":"Leveraging Large Language Models for Multiple Choice Question Answering","venue":"ArXiv","year":2022,"referenceCount":71,"citationCount":1,"influentialCitationCount":0,"publicationDate":"22/10/2022","authors":"Joshua Robinson,C. Rytting,D. Wingate","id":"ff21066b94e9e09ffda6acfbc1c550b05a09b333","summary":"It is shown that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.","score":11},{"url":"https://www.semanticscholar.org/paper/7b0f98f51040700aae3cd9f0e3432dedcd69fb30","title":"When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Alex Mallen,Akari Asai,Victor Zhong,R. Das,Hannaneh Hajishirzi,Daniel Khashabi","id":"7b0f98f51040700aae3cd9f0e3432dedcd69fb30","summary":"It is found that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the tail, and a simple, yet effective, method for powerful andcient retrieval-augmented LMs, which retrieves non-parametric memories only when necessary is devised.","score":11},{"url":"https://www.semanticscholar.org/paper/cd5fd34d6446f3965b2760a81c49b24b13477d5b","title":"Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":7,"influentialCitationCount":1,"publicationDate":"03/10/2022","authors":"Abulhair Saparov,He He","id":"cd5fd34d6446f3965b2760a81c49b24b13477d5b","summary":"To enable systematic exploration of the reasoning ability of LLMs, a new synthetic question-answering dataset is presented, where each example is generated from a synthetic world model represented in ﬁrst-order logic, which allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis.","score":11},{"url":"https://www.semanticscholar.org/paper/43d3dbabea106b59e1ec248457c88b19636e4f47","title":"Can language models handle recursively nested grammatical structures? A case study on comparing models and humans","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":3,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Andrew Kyle Lampinen","id":"43d3dbabea106b59e1ec248457c88b19636e4f47","summary":"","score":11},{"url":"https://www.semanticscholar.org/paper/a8fd9c1625011741f74401ff9bdc1c584e25c86d","title":"Language Models are General-Purpose Interfaces","venue":"ArXiv","year":2022,"referenceCount":133,"citationCount":17,"influentialCitationCount":0,"publicationDate":"13/06/2022","authors":"Y. Hao,Haoyu Song,Li Dong,Shaohan Huang,Zewen Chi,Wenhui Wang,Shuming Ma,Furu Wei","id":"a8fd9c1625011741f74401ff9bdc1c584e25c86d","summary":"This work proposes a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders, and subsume the advantages and capabilities from both causal and non-causing modeling, thereby combining the best of two worlds.","score":11},{"url":"https://www.semanticscholar.org/paper/15bacb240e2598457af4ded3039b6988aa9706f0","title":"Few-shot Adaptation Works with UnpredicTable Data","venue":"ArXiv","year":2022,"referenceCount":144,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/08/2022","authors":"Jun Shern Chan,M. Pieler,Jonathan Jao,J. Scheurer,Ethan Perez","id":"15bacb240e2598457af4ded3039b6988aa9706f0","summary":"This work automatically extracting 413,299 tasks from internet tables - orders of magnitude more than the next-largest public datasets - and finds that narrow subsets of the authors' dataset sometimes outperform more diverse datasets.","score":11},{"url":"https://www.semanticscholar.org/paper/07ec0d4cc6a2be39def51139d228292c6a0dc627","title":"Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners","venue":"ArXiv","year":2022,"referenceCount":71,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"Seonghyeon Ye,Doyoung Kim,Joel Jang,Joongbo Shin,Minjoon Seo","id":"07ec0d4cc6a2be39def51139d228292c6a0dc627","summary":"FLIPPED gives particularly large improvements on unseen labels, outperforming T0-11B by up to +20% average F1 score, indicating that the strong task generalization of FLIPPED comes from improved generalization to novel labels.","score":11},{"url":"https://www.semanticscholar.org/paper/3fa70115248377c3d1517c9f978791a296fbc1dd","title":"Large Language Models Can Self-Improve","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":22,"influentialCitationCount":3,"publicationDate":"20/10/2022","authors":"Jiaxin Huang,S. Gu,Le Hou,Yuexin Wu,Xuezhi Wang,Hongkun Yu,Jiawei Han","id":"3fa70115248377c3d1517c9f978791a296fbc1dd","summary":"This work uses a pre-trained LLM to generate “high-conﬁdence” rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and conducts ablation studies and shows that ablation on reasoning is critical for self-improvement.","score":11},{"url":"https://www.semanticscholar.org/paper/6ae3e52ae55578c10722db3c2f898442f20e336c","title":"LMentry: A Language Model Benchmark of Elementary Language Tasks","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/11/2022","authors":"Avia Efrat,Or Honovich,Omer Levy","id":"6ae3e52ae55578c10722db3c2f898442f20e336c","summary":"LMentry is speciﬁcally designed to provide quick and interpretable insights into the capabilities and robustness of large language models, providing a quick, automatic, and easy-to-run “unit test”, without resorting to large benchmark suites of complex tasks.","score":11},{"url":"https://www.semanticscholar.org/paper/327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","venue":"ArXiv","year":2022,"referenceCount":157,"citationCount":56,"influentialCitationCount":11,"publicationDate":"09/11/2022","authors":"Teven Le Scao,Angela Fan,Christopher Akiki,Elizabeth-Jane Pavlick,Suzana Ili'c,Daniel Hesslow,Roman Castagn'e,A. Luccioni,Franccois Yvon,Matthias Gallé,J. Tow,Alexander M. Rush,Stella Rose Biderman,Albert Webson,Pawan Sasanka Ammanamanchi,Thomas Wang,Benoît Sagot,Niklas Muennighoff,Albert Villanova del Moral,Olatunji Ruwase,Rachel Bawden,Stas Bekman,Angelina McMillan-Major,Iz Beltagy,Huu Nguyen,Lucile Saulnier,Samson Tan,Pedro Ortiz Suarez,Victor Sanh,Hugo Laurenccon,Yacine Jernite,Julien Launay,Margaret Mitchell,Colin Raffel,Aaron Gokaslan,Adi Simhi,Aitor Soroa Etxabe,Alham Fikri Aji,Amit Alfassy,Anna Rogers,Ariel Kreisberg Nitzav,Canwen Xu,Chenghao Mou,Chris C. Emezue,Christopher Klamm,Colin Leong,Daniel Alexander van Strien,David Ifeoluwa Adelani,Dragomir R. Radev,Eduardo G. Ponferrada,Efrat Levkovizh,Ethan Kim,E. Natan,F. Toni,Gérard Dupont,Germán Kruszewski,Giada Pistilli,Hady ElSahar,Hamza Benyamina,Hieu Tran,Ian Yu,Idris Abdulmumin,Isaac Johnson,Itziar Gonzalez-Dios,Javier de la Rosa,Jenny Chim,Jesse Dodge,Jian Zhu,Jonathan Chang,Jorg Frohberg,Josephine L. Tobing,J. Bhattacharjee,Khalid Almubarak,Kimbo Chen,Kyle Lo,Leandro von Werra,Leon Weber,Long Phan,Loubna Ben Allal,Ludovic Tanguy,Manan Dey,M. Muñoz,Maraim Masoud,Mar'ia Grandury,Mario vSavsko,Max Huang,Maximin Coavoux,Mayank Singh,Mike Tian-Jian Jiang,Minh Chien Vu,M. A. Jauhar,Mustafa Ghaleb,Nishant Subramani,Nora Kassner,Nurulaqilla Khamis,Olivier Nguyen,Omar Espejel,Ona de Gibert,Paulo Villegas,Peter Henderson,Pierre Colombo,Priscilla Amuok,Quentin Lhoest,Rheza Harliman,Rishi Bommasani,R. L'opez,Rui Ribeiro,Salomey Osei,Sampo Pyysalo,Sebastian Nagel,Shamik Bose,Shamsuddeen Hassan Muhammad,Shanya Sharma,S. Longpre,Somaieh Nikpoor,Stanislav Silberberg,S. Pai,S. Zink,Tiago Timponi Torrent,Timo Schick,Tristan Thrush,V. Danchev,Vassilina Nikoulina,Veronika Laippala,Violette Lepercq,V. Prabhu,Zaid Alyafeai,Zeerak Talat,Arun Raja,Benjamin Heinzerling,Chenglei Si,Elizabeth Salesky,Sabrina J. Mielke,Wilson Y. Lee,Abheesht Sharma,Andrea Santilli,Antoine Chaffin,Arnaud Stiegler,Debajyoti Datta,Eliza Szczechla,Gunjan Chhablani,Han Wang,Harshit Pandey,Hendrik Strobelt,Jason Alan Fries,Jos Rozen,Leo Gao,Lintang Sutawika,M Saiful Bari,Maged S. Al-shaibani,Matteo Manica,Nihal V. Nayak,Ryan Teehan,Samuel Albanie,Sheng Shen,Srulik Ben-David,Stephen H. Bach,Taewoon Kim,T. Bers,Thibault Févry,Trishala Neeraj,Urmish Thakker,Vikas Raunak,Xiang Tang,Zheng Xin Yong,Zhiqing Sun,Shaked Brody,Y. Uri,Hadar Tojarieh,Adam Roberts,Hyung Won Chung,Jaesung Tae,Jason Phang,Ofir Press,Conglong Li,D. Narayanan,Hatim Bourfoune,J. Casper,Jeff Rasley,Max Ryabinin,Mayank Mishra,Minjia Zhang,M. Shoeybi,Myriam Peyrounette,N. Patry,Nouamane Tazi,Omar Sanseviero,Patrick von Platen,Pierre Cornette,Pierre Franccois Lavall'ee,R. Lacroix,Samyam Rajbhandari,Sanchit Gandhi,Shaden Smith,S. Requena,Suraj Patil,Tim Dettmers,Ahmed Baruwa,Amanpreet Singh,Anastasia Cheveleva,Anne-Laure Ligozat,Arjun Subramonian,Aur'elie N'ev'eol,Charles Lovering,Daniel H Garrette,D. Tunuguntla,Ehud Reiter,Ekaterina Taktasheva,E. Voloshina,Eli Bogdanov,Genta Indra Winata,Hailey Schoelkopf,Jan-Christoph Kalo,Jekaterina Novikova,J. Forde,Jordan Clive,Jungo Kasai,Ken Kawamura,Liam Hazan,Marine Carpuat,Miruna Clinciu,Najoung Kim,Newton Cheng,Oleg Serikov,Omer Antverg,Oskar van der Wal,Rui Zhang,Ruochen Zhang,Sebastian Gehrmann,S. Pais,Tatiana Shavrina,Thomas Scialom,Tian Yun,Tomasz Limisiewicz,Verena Rieser,Vitaly Protasov,V. Mikhailov,Yada Pruksachatkun,Yonatan Belinkov,Zachary Bamberger,Zdenvek Kasner,A. Rueda,A. Pestana,A. Feizpour,Ammar Khan,Amy Faranak,A. Santos,A. Hevia,Antigona Unldreaj,Arash Aghagol,Arezoo Abdollahi,A. Tammour,Azadeh HajiHosseini,Bahareh Behroozi,B. Ajibade,B. Saxena,Carlos Muñoz Ferrandis,Danish Contractor,D. Lansky,Davis David,Douwe Kiela,D. A. Nguyen,Edward Tan,Emily Baylor,Ezinwanne Ozoani,Fatim T Mirza,Frankline Ononiwu,Habib Rezanejad,H.A. Jones,Indrani Bhattacharya,Irene Solaiman,Irina Sedenko,I. Nejadgholi,J. Passmore,Joshua Seltzer,Julio Bonis Sanz,Karen Fort,L. Dutra,Mairon Samagaio,Maraim Elbadri,M. Mieskes,Marissa Gerchick,Martha Akinlolu,Michael McKenna,Mike Qiu,M. Ghauri,Mykola Burynok,Nafis Abrar,Nazneen Rajani,Nour Elkott,N. Fahmy,O. Samuel,Ran An,R. Kromann,Ryan Hao,S. Alizadeh,Sarmad Shubber,Silas L. Wang,Sourav Roy,S. Viguier,Thanh-Cong Le,Tobi Oyebade,T. Le,Yoyo Yang,Z. Nguyen,Abhinav Ramesh Kashyap,Alfredo Palasciano,A. Callahan,Anima Shukla,Antonio Miranda-Escalada,A. Singh,Benjamin Beilharz,Bo Wang,C. Brito,Chenxi Zhou,Chirag Jain,Chuxin Xu,Clémentine Fourrier,Daniel Le'on Perin'an,Daniel Molano,Dian Yu,Enrique Manjavacas,Fabio Barth,Florian Fuhrimann,Gabriel Altay,Giyaseddin Bayrak,Gully A. Burns,Helena U. Vrabec,I. Bello,Isha Dash,J. Kang,John Giorgi,J. Golde,J. Posada,Karthi Sivaraman,Lokesh Bulchandani,Lu Liu,Luisa Shinzato,Madeleine Hahn de Bykhovetz,Maiko Takeuchi,Marc Pàmies,M. A. Castillo,Marianna Nezhurina,Mario Sanger,M. Samwald,Michael Cullan,Michael Weinberg,M. Wolf,Mina Mihaljcic,Minna Liu,M. Freidank,Myungsun Kang,Natasha Seelam,N. Dahlberg,N. Broad,N. Muellner,Pascale Fung,Patricia Haller,R. Chandrasekhar,R. Eisenberg,Robert Martin,Rodrigo L. Canalli,Rosaline Su,Ruisi Su,Samuel Cahyawijaya,Samuele Garda,Shlok S Deshmukh,Shubhanshu Mishra,Sid Kiblawi,Simon Ott,Sinee Sang-aroonsiri,Srishti Kumar,Stefan Schweter,S. Bharati,T. A. Laud,Th'eo Gigant,Tomoya Kainuma,Wojciech Kusa,Yanis Labrak,Yashasvi Bajaj,Y. Venkatraman,Yifan Xu,Ying Xu,Yun-chao Xu,Z. Tan,Zhongli Xie,Zifan Ye,M. Bras,Younes Belkada,Thomas Wolf","id":"327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","summary":"BLOOM is a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers and achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning.","score":11},{"url":"https://www.semanticscholar.org/paper/05d77715d49714506a920f26c5432b92078cd37c","title":"Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"Bernd Bohnet,V. Tran,Pat Verga,Roee Aharoni,D. Andor,Livio Baldini Soares,Jacob Eisenstein,Kuzman Ganchev,Jonathan Herzig,Kai Hui,T. Kwiatkowski,Ji Ma,Jianmo Ni,Tal Schuster,William W. Cohen,Michael Collins,Dipanjan Das,Donald Metzler,Slav Petrov,Kellie Webster","id":"05d77715d49714506a920f26c5432b92078cd37c","summary":"This work formulate and study Attributed QA as a key first step in the development of attributed LLMs, and proposes a reproducible evaluation framework for the task and benchmark a broad set of architectures.","score":11},{"url":"https://www.semanticscholar.org/paper/1e122149779c644855d1cccca5d96135db0482cb","title":"Self-Prompting Large Language Models for Open-Domain QA","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Junlong Li,Zhuosheng Zhang,Hai Zhao","id":"1e122149779c644855d1cccca5d96135db0482cb","summary":"This paper shows that the ODQA architecture can be dramatically simplified by treating Large Language Models (LLMs) as a knowledge corpus and proposes a Self-Prompting framework for LLMs to perform ODZA so as to eliminate the need for training data and external knowledge corpus.","score":11},{"url":"https://www.semanticscholar.org/paper/b7c4677cc6950d556f8d58084f87b2cb9cfe29b8","title":"Are Language Models Worse than Humans at Following Prompts? It's Complicated","venue":"ArXiv","year":2023,"referenceCount":34,"citationCount":2,"influentialCitationCount":0,"publicationDate":"17/01/2023","authors":"Albert Webson,Alyssa Marie Loo,Qinan Yu,Elizabeth-Jane Pavlick","id":"b7c4677cc6950d556f8d58084f87b2cb9cfe29b8","summary":"","score":11},{"url":"https://www.semanticscholar.org/paper/5882dd04d95c9c88cdec389059fcf44d56cbb789","title":"Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation","venue":"ArXiv","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Jessica Huynh,Cathy Jiao,Prakhar Gupta,Shikib Mehri,Payal Bajaj,Vishrav Chaudhary,M. Eskénazi","id":"5882dd04d95c9c88cdec389059fcf44d56cbb789","summary":"The paper shows that the choice of datasets used for training a model contributes to how well it performs on a task as well as on how the prompt should be structured, and investigates how the number of examples in the prompt and the type of example selection affect the model's performance.","score":11},{"url":"https://www.semanticscholar.org/paper/86d03160e6f05deb17d0169e515f5a55d6361f7c","title":"Exploring the Benefits of Training Expert Language Models over Instruction Tuning","venue":"ArXiv","year":2023,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Joel Jang,Seung-bin Kim,Seonghyeon Ye,Doyoung Kim,L. Logeswaran,Moontae Lee,Kyungjae Lee,Minjoon Seo","id":"86d03160e6f05deb17d0169e515f5a55d6361f7c","summary":"An unexpected finding is reported that an expert LM fine-tuned on just a single task can outperform an MT LM trained with 300+ different tasks on 11 different unseen datasets and on 13 datasets of the BIG-bench benchmark by a mean accuracy of 3.20% and 1.29%, respectively.","score":11},{"url":"https://www.semanticscholar.org/paper/ffc8a0e3779ce0d772c46b55e78a2c51bd8bcc23","title":"On Improving Summarization Factual Consistency from Natural Language Feedback","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yixin Liu,Budhaditya Deb,Milagro Teruel,Aaron L Halfaker,Dragomir R. Radev,A. Awadallah","id":"ffc8a0e3779ce0d772c46b55e78a2c51bd8bcc23","summary":"This work collects a high-quality dataset, DeFacto, containing human demonstrations and informational feedback in natural language consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary, and evaluates if models can automatically correct factual inconsistencies in generated summaries.","score":11},{"url":"https://www.semanticscholar.org/paper/9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey","venue":"ArXiv","year":2022,"referenceCount":156,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Daoguang Zan,Bei Chen,Fengji Zhang,Di Lu,Bingchao Wu,Bei Guan,Yongji Wang,Jian-Guang Lou","id":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","summary":"This survey focuses on how does neural network (NN) solves NL2Code and proposes a comprehensive framework, which is able to cover all studies in this task, and in-depth parse the existing studies into this framework.","score":11},{"url":"https://www.semanticscholar.org/paper/ec6a9627807c0b4035040b95ff1f2767c88b26bc","title":"RARR: Researching and Revising What Language Models Say, Using Language Models","venue":"","year":2022,"referenceCount":58,"citationCount":2,"influentialCitationCount":1,"publicationDate":"17/10/2022","authors":"Luyu Gao,Zhuyun Dai,Panupong Pasupat,Anthony Chen,Arun Tejasvi Chaganty,Yicheng Fan,Vincent Zhao,N. Lao,Hongrae Lee,Da-Cheng Juan,Kelvin Guu","id":"ec6a9627807c0b4035040b95ff1f2767c88b26bc","summary":"It is shown that when applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, that RARR improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models.","score":10},{"url":"https://www.semanticscholar.org/paper/6d934171cb679bb804ff73a945746e2ed70e1a80","title":"Visconde: Multi-document QA with GPT-3 and Neural Reranking","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Jayr Alencar Pereira,R. Fidalgo,R. Lotufo,Rodrigo Nogueira","id":"6d934171cb679bb804ff73a945746e2ed70e1a80","summary":"A question-answering system that can answer questions whose supporting evidence is spread over multiple (potentially long) documents, called Visconde, uses a three-step pipeline to perform the task: decompose, retrieve, and aggregate.","score":10},{"url":"https://www.semanticscholar.org/paper/4f4e98cc9133e1814ac2eee9fc4693bf80d1d0d4","title":"Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":2,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Josh Abramson,Arun Ahuja,Federico Carnevale,Petko Georgiev,Alex Goldin,Alden Hung,Jessica Landon,Jirka Lhotka,T. Lillicrap,Alistair Muldal,George Powell,Adam Santoro,Guy Scully,Sanjana Srivastava,Tamara von Glehn,Greg Wayne,Nathaniel Wong,Chen Yan,Rui Zhu","id":"4f4e98cc9133e1814ac2eee9fc4693bf80d1d0d4","summary":"How to use reinforcement learning from human feedback (RLHF) to improve upon simulated, embodied agents trained to a base level of com-petency with imitation learning is demonstrated.","score":10},{"url":"https://www.semanticscholar.org/paper/4288d44c2b8e6a89607780caf1272061028f6f97","title":"On the Advance of Making Language Models Better Reasoners","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":34,"influentialCitationCount":10,"publicationDate":"06/06/2022","authors":"Yifei Li,Zeqi Lin,Shizhuo Zhang,Qiang Fu,Bei Chen,Jian-Guang Lou,Weizhu Chen","id":"4288d44c2b8e6a89607780caf1272061028f6f97","summary":"This paper conducts extensive experiments using the latest language model code-davinci-002 and demonstrates that D I V E RS E can achieve new state-of-the-art performance on six out of eight reasoning benchmarks, out-performing the PaLM model with 540B parameters.","score":10},{"url":"https://www.semanticscholar.org/paper/0f4ab3fe492ececbfd38be9682047371e2e9b8c6","title":"Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":4,"influentialCitationCount":1,"publicationDate":"05/10/2022","authors":"Jacob Eisenstein,D. Andor,Bernd Bohnet,Michael Collins,David Mimno","id":"0f4ab3fe492ececbfd38be9682047371e2e9b8c6","summary":"A new way to build trustworthy pipeline systems from a combination of end-task annotations and frozen pretrained language models, called markup-and-mask, which combines aspects of extractive and free-text explanations.","score":10},{"url":"https://www.semanticscholar.org/paper/b4b37f87e0357f2e4cec70af67b2f088f6efce70","title":"A Survey of Knowledge-Enhanced Pre-trained Language Models","venue":"ArXiv","year":2022,"referenceCount":247,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/11/2022","authors":"Linmei Hu,Zeyi Liu,Ziwang Zhao,Lei Hou,Liqiang Nie,Juanzi Li","id":"b4b37f87e0357f2e4cec70af67b2f088f6efce70","summary":"A comprehensive review of Knowledge-Enhanced Pre-trained Language Models (KE-PLMs) is presented to provide a clear insight into this thriving industry and introduces appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight the focus of these two kinds of tasks.","score":10},{"url":"https://www.semanticscholar.org/paper/c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","title":"Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey","venue":"ArXiv","year":2021,"referenceCount":322,"citationCount":39,"influentialCitationCount":2,"publicationDate":"01/11/2021","authors":"Bonan Min,Hayley H. Ross,Elior Sulem,Amir Pouran Ben Veyseh,Thien Huu Nguyen,Oscar Sainz,Eneko Agirre,Ilana Heinz,D. Roth","id":"c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","summary":"A survey of recent work that uses large, pre-trained transformer-based language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches.","score":10},{"url":"https://www.semanticscholar.org/paper/b17cc18e4130505b939f7d527082eb6be2a7fd5b","title":"Rationale-Augmented Ensembles in Language Models","venue":"ArXiv","year":2022,"referenceCount":52,"citationCount":24,"influentialCitationCount":11,"publicationDate":"02/07/2022","authors":"Xuezhi Wang,Jason Wei,D. Schuurmans,Quoc Le,E. Chi,Denny Zhou","id":"b17cc18e4130505b939f7d527082eb6be2a7fd5b","summary":"It is demonstrated that rationale-augmented ensembles achieve more accurate and interpretable results than existing prompting approaches—including standard prompting without rationales and rationale-based chain-of-thought prompting—while simultaneously improving interpretability of model predictions through the associated rationales.","score":10},{"url":"https://www.semanticscholar.org/paper/196cc546041cb6db167784f632037f0a1dcf4a79","title":"Generating Natural Language Proofs with Verifier-Guided Search","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":70,"citationCount":8,"influentialCitationCount":2,"publicationDate":"25/05/2022","authors":"Kaiyu Yang,Jia Deng,Danqi Chen","id":"196cc546041cb6db167784f632037f0a1dcf4a79","summary":"A novel stepwise method, NLProofS (Natural Language Proof Search), which learns to generate relevant steps conditioning on the hypothesis, which improves the correctness of predicted proofs from 27.7% to 33.3% in the distractor setting of EntailmentBank, demonstrating the effectiveness of NL proofS in generating challenging human-authored proofs.","score":10},{"url":"https://www.semanticscholar.org/paper/55e3fe05598be7c3dd357d51166869f6571b824f","title":"Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/10/2022","authors":"Mohammad Reza Taesiri,F. Macklon,Yihe Wang,Hengshuo Shen,C. Bezemer","id":"55e3fe05598be7c3dd357d51166869f6571b824f","summary":"This study explores the possibil-ity of leveraging the zero-shot capabilities of large language models for video game bug detection by formulating the bug detection problem as a question-answering task, and shows thatLarge language models can identify which event is buggy in a sequence of textual descriptions of events from a game.","score":10},{"url":"https://www.semanticscholar.org/paper/b7c09e2d5ac1268205ebd7c54f11ad5c935beced","title":"LAMBADA: Backward Chaining for Automated Reasoning in Natural Language","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Seyed Mehran Kazemi,Najoung Kim,Deepti Bhatia,Xinyuan Xu,Deepak Ramachandran","id":"b7c09e2d5ac1268205ebd7c54f11ad5c935beced","summary":"A Backward Chaining algorithm is developed, which is called L AM - BADA, that decomposes reasoning into four sub-modules, each of which can be simply implemented by few-shot prompted LM inference and achieves massive accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets.","score":10},{"url":"https://www.semanticscholar.org/paper/df24c0f317fc73b893c852a3fce9536ba8607dfa","title":"BioReader: a Retrieval-Enhanced Text-to-Text Transformer for Biomedical Literature","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":102,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Giacomo Frisoni,M. Mizutani,G. Moro,Lorenzo Valgimigli","id":"df24c0f317fc73b893c852a3fce9536ba8607dfa","summary":"This work introduces BioReader, the first retrieval-enhanced text-to-text model for biomedical natural language processing and shows that domain knowledge can be easily altered or supplemented to make the model generate correct predictions bypassing the retraining step and thus addressing the literature overload issue.","score":10},{"url":"https://www.semanticscholar.org/paper/12f4eff1b991d4953b563544242d106e4c751c65","title":"In-Context Retrieval-Augmented Language Models","venue":"ArXiv","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Ori Ram,Yoav Levine,Itay Dalmedigos,Dor Muhlgay,A. Shashua,K. Leyton-Brown,Y. Shoham","id":"12f4eff1b991d4953b563544242d106e4c751c65","summary":"It is shown that in-context RALM which uses off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora and has considerable potential to increase the prevalence of LM grounding.","score":10},{"url":"https://www.semanticscholar.org/paper/31d05bdc1ad24c4c3f5416ea542ad2e76ad580d9","title":"Pretraining Language Models with Human Preferences","venue":"","year":2023,"referenceCount":81,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Tomasz Korbak,Kejian Shi,Angelica Chen,Rasika Bhalerao,C. L. Buckley,Jason Phang,Sam Bowman,Ethan Perez","id":"31d05bdc1ad24c4c3f5416ea542ad2e76ad580d9","summary":"The results suggest that the authors should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training, i.e., learning and then unlearning undesirable behavior.","score":10},{"url":"https://www.semanticscholar.org/paper/c88cafa3e980765a64febe369ceb7c2aa7261d2a","title":"Complexity-Based Prompting for Multi-Step Reasoning","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":15,"influentialCitationCount":5,"publicationDate":"03/10/2022","authors":"Yao Fu,Hao-Chun Peng,Ashish Sabharwal,Peter Clark,Tushar Khot","id":"c88cafa3e980765a64febe369ceb7c2aa7261d2a","summary":"This work proposes complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning that achieves substantially better performance on math word reasoning tasks over strong baselines and demonstrates the robustness of the methods under format perturbation and distribution shift.","score":10},{"url":"https://www.semanticscholar.org/paper/7d5175db1b99552491063d2d9581b0b51e1d2932","title":"Despite \"super-human\" performance, current LLMs are unsuited for decisions about ethics and safety","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"Joshua Albrecht,Ellie Kitanidis,Abraham J. Fetterman","id":"7d5175db1b99552491063d2d9581b0b51e1d2932","summary":"This work provides a simple new prompting strategy that leads to yet another supposedly “super-human” result, this time out-performing humans at common sense ethical reasoning (as measured by accuracy on a subset of the ETHICS dataset).","score":10},{"url":"https://www.semanticscholar.org/paper/f02e8f1c9b5ab12ddfb1977570f9f5445a99a973","title":"Large Language Models are reasoners with Self-Verification","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Yixuan Weng,Minjun Zhu,Shizhu He,Kang Liu,Jun Zhao","id":"f02e8f1c9b5ab12ddfb1977570f9f5445a99a973","summary":"This work proposes a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked, and calculates an explainable verification score based on the accuracy.","score":10},{"url":"https://www.semanticscholar.org/paper/655d34b590e8911e072013ad21582c0382447600","title":"Unsupervised Explanation Generation via Correct Instantiations","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Sijie Cheng,Zhiyong Wu,Jiangjie Chen,Zhixing Li,Yang Liu,Lingpeng Kong","id":"655d34b590e8911e072013ad21582c0382447600","summary":"N EON is proposed, a two-phrase, unsupervised explanation generation framework that generates corrected instantia- tions of the statement, then uses them to prompt large PLMs to complete the explanation and demonstrate that N EON remains effective when generalizing to different scenarios.","score":10},{"url":"https://www.semanticscholar.org/paper/6c67d6046c01d61e89bc11ccd260e20603fe2398","title":"Semi-Parametric Video-Grounded Text Generation","venue":"ArXiv","year":2023,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Sungdong Kim,Jin-Hwa Kim,Jiyoung Lee,Minjoon Seo","id":"6c67d6046c01d61e89bc11ccd260e20603fe2398","summary":"A semi-parametric video-grounded text generation model, SeViT, is proposed, offering a novel perspective on scalable video-language modeling toward long untrimmed videos and has a signiﬁcant advantage in longer videos and causal video understanding.","score":10},{"url":"https://www.semanticscholar.org/paper/40c318400809abf5e50aba5a5a80c8012a7715d5","title":"GPTScore: Evaluate as You Desire","venue":"ArXiv","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Jinlan Fu,See-Kiong Ng,Zhengbao Jiang,Pengfei Liu","id":"40c318400809abf5e50aba5a5a80c8012a7715d5","summary":"A novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction) of generative pre-trained models to score generated texts to overcome several long-standing challenges in text evaluation.","score":10},{"url":"https://www.semanticscholar.org/paper/074f9767bf81f04448ce6d997b7e29dc92b10fe7","title":"General Intelligence Requires Rethinking Exploration","venue":"ArXiv","year":2022,"referenceCount":190,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Minqi Jiang,Tim Rocktaschel,Edward Grefenstette","id":"074f9767bf81f04448ce6d997b7e29dc92b10fe7","summary":"This work proposes the problem of generalized exploration to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing it to highlight key similarities across learning settings and open research challenges.","score":10},{"url":"https://www.semanticscholar.org/paper/7547589d925ce4782159b73d017b27d55b5f41c8","title":"Is GPT-3 a Good Data Annotator?","venue":"ArXiv","year":2022,"referenceCount":52,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Bosheng Ding,Chengwei Qin,Linlin Liu,Lidong Bing,Shafiq R. Joty,Boyang Li","id":"7547589d925ce4782159b73d017b27d55b5f41c8","summary":"This analysis aims to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP by comparing it with traditional data annotation methods and analyzing its output on a range of tasks.","score":10},{"url":"https://www.semanticscholar.org/paper/cad43496b7196db48a806723ee4e2c502128f316","title":"Inclusive Artificial Intelligence","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/12/2022","authors":"Dilip Arumugam,Shi Dong,B. V. Roy","id":"cad43496b7196db48a806723ee4e2c502128f316","summary":"","score":10},{"url":"https://www.semanticscholar.org/paper/3994eb8e237a94dae1efc6e767a09044b8550ace","title":"FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Hao Liu,Xinyang Geng,Lisa Lee,Igor Mordatch,S. Levine,Sharan Narang,P. Abbeel","id":"3994eb8e237a94dae1efc6e767a09044b8550ace","summary":"Experimental results show that the proposed technique improves PaLM’s zero and few-shot performance on a diverse suite of tasks, including commonsense reasoning, natural language inference and cloze completion, and also helps representation learning.","score":10},{"url":"https://www.semanticscholar.org/paper/4e5f7cd537a1bbcd090f9887b1b59f39a3715dba","title":"Instruction Induction: From Few Examples to Natural Language Task Descriptions","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":7,"influentialCitationCount":3,"publicationDate":"22/05/2022","authors":"Or Honovich,Uri Shaham,Samuel R. Bowman,Omer Levy","id":"4e5f7cd537a1bbcd090f9887b1b59f39a3715dba","summary":"It is discovered that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; this surprising result suggests that instruction induction might be a viable learning paradigm in and of itself.","score":10},{"url":"https://www.semanticscholar.org/paper/5c70d4f334b7da45b156d223afcb256dbabd9b2f","title":"On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"Omar Shaikh,Hongxin Zhang,William B. Held,Michael Bernstein,Diyi Yang","id":"5c70d4f334b7da45b156d223afcb256dbabd9b2f","summary":"It is found that using zero-shot CoT reasoning in a prompt can signiﬁcantly increase a model’s likelihood to produce undesirable output and should be avoided on tasks where models can make inferences about marginalized groups or harmful topics.","score":10},{"url":"https://www.semanticscholar.org/paper/5c32c653735b43a0a8923ca65ac191bd4bf15311","title":"Precise Zero-Shot Dense Retrieval without Relevance Labels","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":1,"publicationDate":"20/12/2022","authors":"Luyu Gao,Xueguang Ma,Jimmy Lin,Jamie Callan","id":"5c32c653735b43a0a8923ca65ac191bd4bf15311","summary":"The experiments show that HyDE outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to ﬁne-tuned retrievers, across various tasks and languages.","score":10},{"url":"https://www.semanticscholar.org/paper/060cee8411181e8151ab1e3212b81528accd9b8b","title":"On Transforming Reinforcement Learning by Transformer: The Development Trajectory","venue":"ArXiv","year":2022,"referenceCount":269,"citationCount":2,"influentialCitationCount":0,"publicationDate":"29/12/2022","authors":"Shengchao Hu,Li Shen,Ya Zhang,Yixin Chen,Dacheng Tao","id":"060cee8411181e8151ab1e3212b81528accd9b8b","summary":"This survey collects and dissect recent advances concerning the transformation of RL with transformers (transformer-based RL (TRL), and examines the main applications of TRL in robotic manipulation, text-based games (TBGs), navigation and autonomous driving.","score":10},{"url":"https://www.semanticscholar.org/paper/6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc","title":"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning","venue":"ArXiv","year":2023,"referenceCount":76,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2023","authors":"Zhenfang Chen,Qinhong Zhou,Yikang Shen,Yining Hong,Hao Zhang,Chuang Gan","id":"6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc","summary":"A novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge-based visual reasoning, which achieves better performance than the previous few- shot learning baselines and enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step.","score":10},{"url":"https://www.semanticscholar.org/paper/795736777f08e92a80c95dab7f205d1d7c28a10b","title":"The CRINGE Loss: Learning what language not to model","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Leonard Adolphs,Tianyu Gao,Jing Xu,Kurt Shuster,Sainbayar Sukhbaatar,J. Weston","id":"795736777f08e92a80c95dab7f205d1d7c28a10b","summary":"This work proposes a novel procedure to train with negative data called the C RINGE loss (ContRastive Iterative Negative GEneration), and shows the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue.","score":10},{"url":"https://www.semanticscholar.org/paper/a8afd12bcd51488ba69bd838ef6dbf2728d5121a","title":"Prompting Is Programming: A Query Language For Large Language Models","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":4,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Luca Beurer-Kellner,Marc Fischer,Martin T. Vechev","id":"a8afd12bcd51488ba69bd838ef6dbf2728d5121a","summary":"LMQL is implemented, which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model.","score":10},{"url":"https://www.semanticscholar.org/paper/e659fa1e79a2a151be331125c14339988542aac3","title":"Batch Prompting: Efficient Inference with Large Language Model APIs","venue":"ArXiv","year":2023,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/01/2023","authors":"Zhoujun Cheng,Jungo Kasai,Tao Yu","id":"e659fa1e79a2a151be331125c14339988542aac3","summary":"Batch prompting, a simple alternative prompting approach that enables the LLM to run inference in batches, instead of one sample at a time, is proposed, which reduces both token and time costs while retaining downstream performance.","score":10},{"url":"https://www.semanticscholar.org/paper/a732b89acf14bf7a2a7282b4f7fbb863d79a4347","title":"Learning, Fast and Slow: A Goal-Directed Memory-Based Approach for Dynamic Environments","venue":"ArXiv","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Tan Chong Min John,M. Motani","id":"a732b89acf14bf7a2a7282b4f7fbb863d79a4347","summary":"It is posited that the future of Reinforcement Learning (RL) will be to model goals and sub-goals for various tasks, and plan it out in a goal-directed memory-based approach.","score":9},{"url":"https://www.semanticscholar.org/paper/0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension","venue":"ACM Computing Surveys","year":2021,"referenceCount":343,"citationCount":54,"influentialCitationCount":4,"publicationDate":"27/07/2021","authors":"Anna Rogers,Matt Gardner,Isabelle Augenstein","id":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","summary":"This study is the largest survey of the deep learning models in NLP field to date, providing an overview of the various formats and domains of the current resources, and highlighting the current lacunae for future work.","score":9},{"url":"https://www.semanticscholar.org/paper/798abf86efae9e37b9b6a694ef87b6c1dbaab263","title":"Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"Victor S. Bursztyn,David Demeter,Doug Downey,Larry Birnbaum","id":"798abf86efae9e37b9b6a694ef87b6c1dbaab263","summary":"This work presents compositional fine-tuning (CFT): an approach based on explicitly decomposing a target task into component tasks, and then fine- Tuning smaller LMs on a curriculum of such component tasks.","score":9},{"url":"https://www.semanticscholar.org/paper/02ab15d5a21715724889cf2a34242a330972fbb5","title":"Decoding a Neural Retriever’s Latent Space for Query Suggestion","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":53,"citationCount":2,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Leonard Adolphs,Michelle Chen Huebscher,C. Buck,Sertan Girgin,Olivier Bachem,Massimiliano Ciaramita,Thomas Hofmann","id":"02ab15d5a21715724889cf2a34242a330972fbb5","summary":"It is shown that it is possible to decode a meaningful query from its latent representation and, when moving in the right direction in latent space, to decode an query that retrieves the relevant paragraph from the collection.","score":9},{"url":"https://www.semanticscholar.org/paper/108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models","venue":"MATHNLP","year":2022,"referenceCount":65,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"M. Bueno,Carlos Gemmel,Jeffrey Stephen Dalton,R. Lotufo,Rodrigo Nogueira","id":"108c25905be36b2a7a0fc7256ac314985ecd9699","summary":"This work demonstrates that large language models can succeed in extrapolation without modifying their architecture or training procedure, and shows how generating step-by-step rationales and introducing marker tokens are both required for effective extrapolation.","score":9},{"url":"https://www.semanticscholar.org/paper/6d5555348f453bac901c5b57e8a4eeb3074b4071","title":"Learning to Reason With Relational Abstractions","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"A. Nam,Mengye Ren,Chelsea Finn,James L. McClelland","id":"6d5555348f453bac901c5b57e8a4eeb3074b4071","summary":"This work introduces new types of sequences that more explicitly provide an abstract characterization of the transitions through intermediate solution steps to the goal state and shows that models that are supplied with such sequences as prompts can solve tasks with a signiﬁcantly higher accuracy, and models that is trained to produce such sequences solve problems better than those that are trained with previously used human-generated sequences and other baselines.","score":9},{"url":"https://www.semanticscholar.org/paper/44d6c201a4056e260e9844bdbb01461ea9b1a011","title":"A data-driven approach for learning to control computers","venue":"International Conference on Machine Learning","year":2022,"referenceCount":31,"citationCount":11,"influentialCitationCount":1,"publicationDate":"16/02/2022","authors":"P. Humphreys,David Raposo,Tobias Pohlen,Gregory Thornton,Rachita Chhaparia,Alistair Muldal,Josh Abramson,Petko Georgiev,Alex Goldin,Adam Santoro,T. Lillicrap","id":"44d6c201a4056e260e9844bdbb01461ea9b1a011","summary":"This workigate the setting of computer control using keyboard and mouse, with goals specified via natural language, and suggests a formula for achieving competency beyond MiniWob ++ and towards controlling computers, in general, as a human would.","score":9},{"url":"https://www.semanticscholar.org/paper/9aed848be4e9e401b0e61a2e5d60dbdafa0c6cc1","title":"A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams","venue":"ArXiv","year":2022,"referenceCount":15,"citationCount":2,"influentialCitationCount":1,"publicationDate":"11/06/2022","authors":"Sarah Zhang,Reece Shuttleworth,Derek Austin,Yann Hicke,Leonard Tang,Sathwik Karnik,Darnell Granberry,Iddo Drori","id":"9aed848be4e9e401b0e61a2e5d60dbdafa0c6cc1","summary":"A student survey comparing the quality, appropriateness, andulty of machine- generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated Questions and are suitable for ﬁnal exams.","score":9},{"url":"https://www.semanticscholar.org/paper/717d52007edbb7ddb3f70621636eed36521ba7e0","title":"Reward Gaming in Conditional Text Generation","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/11/2022","authors":"Richard Yuanzhe Pang,Vishakh Padmakumar,Thibault Sellam,Ankur P. Parikh,He He","id":"717d52007edbb7ddb3f70621636eed36521ba7e0","summary":"This discussion piece would like to highlight reward gaming in the natural language generation (NLG) community using concrete conditional text generation examples and discuss potential fixes and areas for future work.","score":9},{"url":"https://www.semanticscholar.org/paper/f5d93cf5d81575aeee61faeddde4437fbcb74cd0","title":"The Programmer's Assistant: Conversational Interaction with a Large Language Model for Software Development","venue":"","year":2023,"referenceCount":111,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/02/2023","authors":"Steven I. Ross,Fernando Martinez,Stephanie Houde,Michael J. Muller,Justin D. Weisz","id":"f5d93cf5d81575aeee61faeddde4437fbcb74cd0","summary":"This work developed a prototype system -- the Programmer's Assistant -- in order to explore the utility of conversational interactions grounded in code, as well as software engineers' receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM.","score":9},{"url":"https://www.semanticscholar.org/paper/e63c0f7d2ff2939ccec5c2461a22b97fff3b0a86","title":"The Capacity for Moral Self-Correction in Large Language Models","venue":"","year":2023,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Deep Ganguli,Amanda Askell,Nicholas Schiefer,Thomas Liao,Kamil.e Lukovsiut.e,Anna Chen,Anna Goldie,Azalia Mirhoseini,Catherine Olsson,Danny Hernandez,Dawn Drain,Dustin Li,Eli Tran-Johnson,Ethan Perez,John Kernion,Jamie Kerr,J. Mueller,J. Landau,Kamal Ndousse,Karina Nguyen,Liane Lovitt,Michael Sellitto,Nelson Elhage,Noem'i Mercado,Nova DasSarma,R. Lasenby,Robin Larson,Sam Ringer,Sandipan Kundu,Saurav Kadavath,Scott Johnston,S. Kravec,Sheer El Showk,Tamera Lanham,Timothy Telleen-Lawton,T. Henighan,Tristan Hume,Yuntao Bai,Zac Hatfield-Dodds,Benjamin Mann,Dario Amodei,Nicholas Joseph,Sam McCandlish,Tom B. Brown,C. Olah,Jack Clark,Sam Bowman,Jared Kaplan","id":"e63c0f7d2ff2939ccec5c2461a22b97fff3b0a86","summary":"","score":9},{"url":"https://www.semanticscholar.org/paper/62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":18,"influentialCitationCount":1,"publicationDate":"06/10/2022","authors":"Freda Shi,Mirac Suzgun,Markus Freitag,Xuezhi Wang,Suraj Srivats,Soroush Vosoughi,Hyung Won Chung,Yi Tay,Sebastian Ruder,Denny Zhou,Dipanjan Das,Jason Wei","id":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","summary":"It is shown that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment, and that models have strikingly strong mult bilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili.","score":9},{"url":"https://www.semanticscholar.org/paper/712f21411526e8450036d7199637808590be3579","title":"Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":1,"publicationDate":"11/10/2022","authors":"Fan Zhou,Haoyu Dong,Qian Liu,Zhoujun Cheng,Shi Han,Dongmei Zhang","id":"712f21411526e8450036d7199637808590be3579","summary":"","score":9},{"url":"https://www.semanticscholar.org/paper/965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Ippei Fujisawa,R. Kanai","id":"965e409a3e7b5670d609837fac9823b160d6639c","summary":"This work describes and characterize logical tasks and discusses system requirements for their solution, and discusses the relevance of logical tasks to concepts such as extrapolation, explainability, and inductive bias.","score":9},{"url":"https://www.semanticscholar.org/paper/a4a41319d5805a29316f24ed9519f09db77d4c29","title":"Benchmarking Large Language Models for News Summarization","venue":"ArXiv","year":2023,"referenceCount":42,"citationCount":2,"influentialCitationCount":1,"publicationDate":"31/01/2023","authors":"Tianyi Zhang,Faisal Ladhak,Esin Durmus,Percy Liang,K. McKeown,Tatsunori Hashimoto","id":"a4a41319d5805a29316f24ed9519f09db77d4c29","summary":"It is found that LMM summaries are judged to be on par with human written summaries, and instruction tuning, and not model size, is the key to the LLM’s zero-shot summarization capability.","score":9},{"url":"https://www.semanticscholar.org/paper/5697a0ede5425954d48daa6e1893dc87bd7d8be7","title":"Contrastive Search Is What You Need For Neural Text Generation","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":5,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Yixuan Su,N. Collier","id":"5697a0ede5425954d48daa6e1893dc87bd7d8be7","summary":"Surprisingly, the anisotropic problem only exists in the two specific English GPT-2-small/medium models, and all other evaluated LMs are naturally isotropic which is in contrast to the conclusion drawn by previous studies.","score":9},{"url":"https://www.semanticscholar.org/paper/e6745fb621481ccb0ed53c267a37292e499c1b42","title":"Automatic Generation of Socratic Subquestions for Teaching Math Word Problems","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":64,"citationCount":2,"influentialCitationCount":0,"publicationDate":"23/11/2022","authors":"K. Shridhar,Jakub Macina,Mennatallah El-Assady,Tanmay Sinha,Manu Kapur,Mrinmaya Sachan","id":"e6745fb621481ccb0ed53c267a37292e499c1b42","summary":"This work explores the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving and proposes various guided question generation schemes based on input conditioning and reinforcement learning.","score":9},{"url":"https://www.semanticscholar.org/paper/68edfd62d2619fb4af7c2469edb95b9e2fe4544a","title":"Execution-based Code Generation using Deep Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Parshin Shojaee,Aneesh Jain,S. Tipirneni,C. Reddy","id":"68edfd62d2619fb4af7c2469edb95b9e2fe4544a","summary":"PPOCoder is a new framework for code generation that combines pretrained PL models with Proximal Policy Optimization (PPO) deep reinforcement learning and employs execution feedback as the external source of knowledge into the model optimization, which is transferable across different code generation tasks and PLs.","score":9},{"url":"https://www.semanticscholar.org/paper/6e6983417939dbcb04a50a46a489ce6bbfe8aa9d","title":"Using In-Context Learning to Improve Dialogue Safety","venue":"ArXiv","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Nicholas Meade,Spandana Gella,Devamanyu Hazarika,Prakhar Gupta,Di Jin,Siva Reddy,Yang Liu,Dilek Z. Hakkani-Tür","id":"6e6983417939dbcb04a50a46a489ce6bbfe8aa9d","summary":"This work investigates a retrieval-based framework for reducing bias and toxicity in responses generated from neural-based chatbots, which uses in-context learning to steer a model towards safer generations.","score":9},{"url":"https://www.semanticscholar.org/paper/782f3d43b37790a83c98d5fd3ef142b296f20616","title":"CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models","venue":"ArXiv","year":2023,"referenceCount":76,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Changan Niu,Chuanyi Li,Vincent Ng,Bin Luo","id":"782f3d43b37790a83c98d5fd3ef142b296f20616","summary":"A large-scale benchmark that includes 216 existing code-related tasks is proposed and it is demonstrated that the cross-task generalization of models can be largely improved by in-context learning methods such as few-shot learning and learning from task instructions, which shows the promising prospects of conducting cross- task learning research on this benchmark.","score":9},{"url":"https://www.semanticscholar.org/paper/eb9b60db6832b00b9932584663bec104d6d415dd","title":"Tree-Based Representation and Generation of Natural and Mathematical Language","venue":"","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Alexander Scarlatos,Andrew S. Lan","id":"eb9b60db6832b00b9932584663bec104d6d415dd","summary":"A series of modifications to existing language models are proposed to jointly represent and generate text and math: representing mathematical expressions as sequences of node tokens in their operator tree format, using math symbol and tree position embeddings to preserve the semantic and structural properties of mathematical expressions, and using a constrained decoding method to generate mathematically valid expressions.","score":9},{"url":"https://www.semanticscholar.org/paper/ae10c4b220a0bc0999bf169d5c219086d1f1aeed","title":"Edinburgh Research Explorer Taxonomy of risks posed by language models","venue":"","year":2022,"referenceCount":218,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Laura Weidinger,J. Uesato,M. Rauh,C. Griffin,P. Huang,John F. J. Mellor,A. Glaese,M. Cheng,B. Balle,A. Kasirzadeh,C. Biles,S. Brown,Z. Kenton,W. Hawkins,T. Stepleton,A. Birhane,L. Hendricks,Rimell,Laura Weidinger,J. Uesato,M. Rauh,C. Griffin,John F. J. Mellor,A. Glaese,M. Cheng,B. Balle,A. Kasirzadeh,C. Biles,S. Brown,Z. Kenton,Tom,Stepleton,A. Birhane,Lisa Anne Hendricks,Laura Rimell","id":"ae10c4b220a0bc0999bf169d5c219086d1f1aeed","summary":"A comprehensive taxonomy of ethical and social risks associated with LMs is developed, drawing on expertise and literature from computer science, linguistics, and the social sciences to ensure that language models are developed responsibly.","score":9},{"url":"https://www.semanticscholar.org/paper/f2c17758e74707d379b87372528221656d14b697","title":"Taxonomy of Risks posed by Language Models","venue":"Conference on Fairness, Accountability and Transparency","year":2022,"referenceCount":209,"citationCount":23,"influentialCitationCount":5,"publicationDate":"20/06/2022","authors":"Laura Weidinger,J. Uesato,M. Rauh,C. Griffin,Po-Sen Huang,John F. J. Mellor,A. Glaese,M. Cheng,B. Balle,A. Kasirzadeh,C. Biles,S. Brown,Z. Kenton,W. Hawkins,T. Stepleton,A. Birhane,Lisa Anne Hendricks,Laura Rimell,William S. Isaac,Julia Haas,Sean Legassick,Geoffrey Irving,Iason Gabriel","id":"f2c17758e74707d379b87372528221656d14b697","summary":"A comprehensive taxonomy of ethical and social risks associated with LMs is developed, drawing on expertise and literature from computer science, linguistics, and the social sciences to ensure that language models are developed responsibly.","score":9},{"url":"https://www.semanticscholar.org/paper/e23c5bf7362988624ec67bd4fe405b71e8b5c0ad","title":"Variational Open-Domain Question Answering","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/09/2022","authors":"Valentin Li'evin,Andreas Geert Motzfeldt,Ida Riis Jensen,O. Winther","id":"e23c5bf7362988624ec67bd4fe405b71e8b5c0ad","summary":"The Rényi variational bound, a lower bound to the task marginal likelihood, can be exploited to aid optimization and use importance sampling to estimate the task log-likelihood lower bound and its gradients using samples drawn from an auxiliary retriever.","score":9},{"url":"https://www.semanticscholar.org/paper/f5b3cb14e0947c62b470d2072483481f14258738","title":"A Solvable Model of Neural Scaling Laws","venue":"ArXiv","year":2022,"referenceCount":74,"citationCount":4,"influentialCitationCount":1,"publicationDate":"30/10/2022","authors":"A. Maloney,Daniel A. Roberts,J. Sully","id":"f5b3cb14e0947c62b470d2072483481f14258738","summary":"Key findings are the manner in which the power laws that occur in the statistics of natural datasets are extended by nonlinear random feature maps and then translated into power-law scalings of the test loss and how the finite extent of the data’s spectral power law causes the model’'s performance to plateau.","score":9},{"url":"https://www.semanticscholar.org/paper/dee23912cf0586b690b1c0eb386bc92d38ce8c16","title":"Fine-Tuning Language Models via Epistemic Neural Networks","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/11/2022","authors":"Ian Osband,S. Asghari,B. V. Roy,Nathan McAleese,J. Aslanides,Geoffrey Irving","id":"dee23912cf0586b690b1c0eb386bc92d38ce8c16","summary":"It is shown that, using an epinet to prioritize uncertain data, BERT on GLUE tasks to the same performance while using 2x less data and performance in synthetic neural network generative models designed to build understanding is investigated.","score":9},{"url":"https://www.semanticscholar.org/paper/99ca5162211a895a5dfbff9d7e36e21e09ca646e","title":"Measuring Progress on Scalable Oversight for Large Language Models","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":5,"influentialCitationCount":1,"publicationDate":"04/11/2022","authors":"Sam Bowman,Jeeyoon Hyun,Ethan Perez,Edwin Chen,Craig Pettit,Scott Heiner,Kamilė Lukošiūtė,Amanda Askell,Andy Jones,Anna Chen,Anna Goldie,Azalia Mirhoseini,C. McKinnon,C. Olah,Daniela Amodei,Dario Amodei,Dawn Drain,Dustin Li,Eli Tran-Johnson,John Kernion,Jamie Kerr,J. Mueller,Jeff Ladish,J. Landau,Kamal Ndousse,Liane Lovitt,Nelson Elhage,Nicholas Schiefer,Nicholas Joseph,Noem'i Mercado,Nova DasSarma,Robin Larson,Sam McCandlish,S. Kundu,Scott Johnston,S. Kravec,Sheer El Showk,Stanislav Fort,Timothy Telleen-Lawton,Tom B. Brown,T. Henighan,Tristan Hume,Yuntao Bai,Zac Hatfield-Dodds,Benjamin Mann,Jared Kaplan","id":"99ca5162211a895a5dfbff9d7e36e21e09ca646e","summary":"It is found that human participants who interact with an unreliable large-language-model dialog assistant through chat—a trivial baseline strategy for scalable oversight—substantially outperform both the model alone and their own unaided performance, an encouraging sign that scalable oversight will be tractable to study with present models.","score":9},{"url":"https://www.semanticscholar.org/paper/557a5147d88ad361b807d4c93decac6d57d2d5e9","title":"Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans","venue":"SSRN Electronic Journal","year":2022,"referenceCount":318,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/09/2022","authors":"John J. Nay","id":"557a5147d88ad361b807d4c93decac6d57d2d5e9","summary":"Law Informs Code describes how the data generated by legal processes and the theoretical constructs and practices of law can facilitate the robust specification of inherently vague human goals for AI.","score":9},{"url":"https://www.semanticscholar.org/paper/3ad346ae7af5c30964c4916dbcee798f72e1bdb7","title":"Translating Natural Language to Planning Goals with Large-Language Models","venue":"ArXiv","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"Yaqi Xie,Chenyao Yu,Tongyao Zhu,Jinbin Bai,Ze Gong,Harold Soh","id":"3ad346ae7af5c30964c4916dbcee798f72e1bdb7","summary":"The empirical results on GPT 3.5 variants show that LLMs are much better suited towards translation rather than planning, and these models are promising for translation to structured planning languages, but care should be taken in their use.","score":9},{"url":"https://www.semanticscholar.org/paper/7ce0c89a452e3c2917b63847495533865697c79c","title":"Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":6,"influentialCitationCount":0,"publicationDate":"26/09/2022","authors":"Joel Jang,Seonghyeon Ye,Minjoon Seo","id":"7ce0c89a452e3c2917b63847495533865697c79c","summary":"This work shows that there exists a scaling law between the size of Language Models and their zero-shot performance on different downstream NLP tasks but this phenomenon does not hold when evaluating large LMs on tasks with negated prompts, but instead shows an inverse scaling law.","score":9},{"url":"https://www.semanticscholar.org/paper/1450c89fd9bd4b9978a1c85f9ac70ae379c9d36d","title":"Query Refinement Prompts for Closed-Book Long-Form Question Answering","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Reinald Kim Amplayo,Kellie Webster,Michael Collins,Dipanjan Das,Shashi Narayan","id":"1450c89fd9bd4b9978a1c85f9ac70ae379c9d36d","summary":"Query refinement prompts are defined that encourage LLMs to explicitly express the multifacetedness in questions and generate long-form answers covering multiple facets of the question to outperform fully finetuned models in the closed book setting, as well as achieve results comparable to retrieve-then-generate open-book models.","score":9},{"url":"https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096","title":"Pretraining in Deep Reinforcement Learning: A Survey","venue":"ArXiv","year":2022,"referenceCount":213,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/11/2022","authors":"Zhihui Xie,Zichuan Lin,Junyou Li,Shuai Li,Deheng Ye","id":"c90a33f1f0049d524e9b5b3174d35611fd9a8096","summary":"This survey seeks to systematically review existing works in pretraining for deep reinforcement learning, provide a taxonomy of these methods, discuss each sub-ﬁeld, and bring attention to open problems and future directions.","score":9},{"url":"https://www.semanticscholar.org/paper/a4bdc300db297756f36bedee2859b62df8e268c2","title":"Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding","venue":"ArXiv","year":2022,"referenceCount":83,"citationCount":3,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Mirac Suzgun,Luke Melas-Kyriazi,Dan Jurafsky","id":"a4bdc300db297756f36bedee2859b62df8e268c2","summary":"This work presents crowd sampling, a family of decoding methods based on Bayesian risk minimization, to ad-dress this diversity-quality trade-off in open-ended natural-language generation.","score":9},{"url":"https://www.semanticscholar.org/paper/8cf05ed2b7cd3b0f601c454914a678c24d393de3","title":"Task-aware Retrieval with Instructions","venue":"ArXiv","year":2022,"referenceCount":88,"citationCount":4,"influentialCitationCount":0,"publicationDate":"16/11/2022","authors":"Akari Asai,Timo Schick,Patrick Lewis,Xilun Chen,Gautier Izacard,Sebastian Riedel,Hannaneh Hajishirzi,Wen-tau Yih","id":"8cf05ed2b7cd3b0f601c454914a678c24d393de3","summary":"TART shows strong capabilities to adapt to a new retrieval task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger.","score":9},{"url":"https://www.semanticscholar.org/paper/fd8c1b8741163d8737652fbcd3507bcd7d6225c7","title":"Multitask Vision-Language Prompt Tuning","venue":"ArXiv","year":2022,"referenceCount":104,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Sheng Shen,Shijia Yang,Tianjun Zhang,Bohan Zhai,Joseph Gonzalez,K. Keutzer,Trevor Darrell","id":"fd8c1b8741163d8737652fbcd3507bcd7d6225c7","summary":"This paper demonstrates the effectiveness of learning a single transferable prompt from multiple source tasks to initialize the prompt for each target task and shows many target tasks can beneﬁt each other from sharing prompt vectors and thus can be jointly learned via multitask prompt tuning.","score":9},{"url":"https://www.semanticscholar.org/paper/943afe24f79b42a763fd422ed2e2297a52b7389e","title":"Editing Models with Task Arithmetic","venue":"ArXiv","year":2022,"referenceCount":102,"citationCount":6,"influentialCitationCount":1,"publicationDate":"08/12/2022","authors":"Gabriel Ilharco,Marco Tulio Ribeiro,Mitchell Wortsman,Suchin Gururangan,Ludwig Schmidt,Hannaneh Hajishirzi,Ali Farhadi","id":"943afe24f79b42a763fd422ed2e2297a52b7389e","summary":"This work proposes a new paradigm for steering the behavior of neural networks, centered around task vectors, and shows that task arithmetic is a simple, efﬁcient and effective way of editing models.","score":9},{"url":"https://www.semanticscholar.org/paper/b674db9649f1c21c670ea98a0e91e095c2d0a4cf","title":"Teaching Small Language Models to Reason","venue":"ArXiv","year":2022,"referenceCount":19,"citationCount":7,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Lucie Charlotte Magister,Jonathan Mallinson,Jakub Adamek,Eric Malmi,Aliaksei Severyn","id":"b674db9649f1c21c670ea98a0e91e095c2d0a4cf","summary":"The proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets and transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation is explored.","score":9},{"url":"https://www.semanticscholar.org/paper/6f4cc536f9ed83d0dbf7e919dc609be12aa0848a","title":"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":3,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Or Honovich,Thomas Scialom,Omer Levy,Timo Schick","id":"6f4cc536f9ed83d0dbf7e919dc609be12aa0848a","summary":"Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, sur-passing the performance of models such as T0++ and Tk-Instruct across various benchmarks.","score":9},{"url":"https://www.semanticscholar.org/paper/bc36e46688725d574fb3e4e74808435c85bd61cd","title":"JASMINE: Arabic GPT Models for Few-Shot Learning","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"E. Nagoudi,M. Abdul-Mageed,AbdelRahim Elmadany,Alcides Alcoba Inciarte,Md. Tawkat Islam Khondaker","id":"bc36e46688725d574fb3e4e74808435c85bd61cd","summary":"JASMINE is introduced, a suite of powerful Arabic autoregressive Transformer language models ranging in size between 300 million- 13 billion parameters, and a novel benchmark for both automated and human evaluation of these models focused at investigating potential social biases, harms, and toxicity in these models.","score":9},{"url":"https://www.semanticscholar.org/paper/26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning","venue":"ArXiv","year":2022,"referenceCount":173,"citationCount":243,"influentialCitationCount":29,"publicationDate":"29/04/2022","authors":"Jean-Baptiste Alayrac,Jeff Donahue,Pauline Luc,Antoine Miech,Iain Barr,Yana Hasson,Karel Lenc,A. Mensch,Katie Millican,Malcolm Reynolds,Roman Ring,Eliza Rutherford,Serkan Cabi,Tengda Han,Zhitao Gong,S. Samangooei,Marianne Monteiro,Jacob Menick,Sebastian Borgeaud,Andy Brock,Aida Nematzadeh,Sahand Sharifzadeh,Mikolaj Binkowski,Ricardo Barreira,Oriol Vinyals,A. Zisserman,K. Simonyan","id":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","summary":"It is demonstrated that a single Flamingo model can achieve a new state of the art for few-shot learning, simply by prompting the model with task-speciﬁc examples.","score":9},{"url":"https://www.semanticscholar.org/paper/7ed237af793f43c442b3e8e1bc9ace906a276b2a","title":"Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?","venue":"ArXiv","year":2023,"referenceCount":95,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/01/2023","authors":"Jielin Qiu,W. Han,Jiacheng Zhu,Mengdi Xu,Michael Rosenberg,Emerson Liu,Douglas Weber,Ding Zhao","id":"7ed237af793f43c442b3e8e1bc9ace906a276b2a","summary":"This work proposes an approach for cardiovascular disease diagnosis and automatic ECG diagnosis report generation and introduces an additional loss function by Optimal Transport to align the distribution between ECG and language embedding and proves the feasibility of transferring knowledge from LLMs to the cardiac domain.","score":9},{"url":"https://www.semanticscholar.org/paper/9595f62731d88406b9f1a1aca8f372ba57b70a6c","title":"Representation Learning for Continuous Action Spaces is Beneficial for Efficient Policy Learning","venue":"Neural Networks","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/11/2022","authors":"Tingting Zhao,Ying Wang,Weidong Sun,Yarui Chen,Gang Niu,M. Sugiyama","id":"9595f62731d88406b9f1a1aca8f372ba57b70a6c","summary":"An efficient policy learning method in latent state and action spaces is proposed that extends the idea of state representations to action representations for better policy generalization capability and is demonstrated by MountainCar, CarRacing and Cheetah experiments.","score":8},{"url":"https://www.semanticscholar.org/paper/71b676fdf3ee3eec9c11089a957e55749a8334d5","title":"A Survey of Machine Learning for Computer Architecture and Systems","venue":"ACM Computing Surveys","year":2021,"referenceCount":343,"citationCount":12,"influentialCitationCount":0,"publicationDate":"16/02/2021","authors":"Nan Wu,Yuan Xie","id":"71b676fdf3ee3eec9c11089a957e55749a8334d5","summary":"This article presents a comprehensive review of the work that applies ML for computer architecture and system design, and summarizes the common problems in computer architecture/system design that can be solved by ML techniques and the typical ML techniques employed to resolve each of them.","score":8},{"url":"https://www.semanticscholar.org/paper/eac5e5ef2bb1d158645ee8ae8f3e167767316b46","title":"Understanding and Improving Zero-shot Multi-hop Reasoning in Generative Question Answering","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/10/2022","authors":"Zhengbao Jiang,J. Araki,Haibo Ding,Graham Neubig","id":"eac5e5ef2bb1d158645ee8ae8f3e167767316b46","summary":"It is demonstrated that multi-hop reasoning does not emerge naturally in generative QA models, but can be encouraged by advances in training or modeling techniques.","score":8},{"url":"https://www.semanticscholar.org/paper/2c953a3c378b40dadf2e3fb486713c8608b8e282","title":"Pretrained Transformers for Text Ranking: BERT and Beyond","venue":"North American Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":483,"citationCount":282,"influentialCitationCount":28,"publicationDate":"13/10/2020","authors":"Jimmy J. Lin,Rodrigo Nogueira,Andrew Yates","id":"2c953a3c378b40dadf2e3fb486713c8608b8e282","summary":"This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known example, and covers a wide range of techniques.","score":8},{"url":"https://www.semanticscholar.org/paper/7087dd676c98c3358571b1daa5778b0095a30d9a","title":"A Survey on Causal Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":190,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"Yan Zeng,Ruichu Cai,Fuchun Sun,Libo Huang,Z. Hao","id":"7087dd676c98c3358571b1daa5778b0095a30d9a","summary":"This work divides existing CRL approaches into two categories according to whether their causality-based information is given in advance or not, and analyzes each category in terms of the formalization of different models, ranging from the Markov Decision Process (MDP), Partially Observed Markov decision process (POMDP), Multi-Arm Bandits (MAB), and Dynamic Treatment Regime (DTR).","score":8},{"url":"https://www.semanticscholar.org/paper/28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","venue":"ACM Computing Surveys","year":2021,"referenceCount":175,"citationCount":405,"influentialCitationCount":61,"publicationDate":"28/07/2021","authors":"Pengfei Liu,Weizhe Yuan,Jinlan Fu,Zhengbao Jiang,Hiroaki Hayashi,Graham Neubig","id":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","summary":"The basics of this promising paradigm in natural language processing are introduced, a unified set of mathematical notations that can cover a wide variety of existing work are described, and existing work is organized along several dimensions.","score":8},{"url":"https://www.semanticscholar.org/paper/f403082b101821f5377ae82fe4ed7d02f6abd3ad","title":"A Survey of Meta-Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":278,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/01/2023","authors":"Jacob Beck,Risto Vuorio,E. Liu,Zheng Xiong,L. Zintgraf,Chelsea Finn,Shimon Whiteson","id":"f403082b101821f5377ae82fe4ed7d02f6abd3ad","summary":"This survey describes the meta-RL problem setting in detail as well as its major variations and discusses howMeta-RL research can be clustered based on the presence of a task distribution and the learning budget available for each individual task.","score":8},{"url":"https://www.semanticscholar.org/paper/0581387bf8411046a562994040f5edd0ad549293","title":"QR-CLIP: Introducing Explicit Open-World Knowledge for Location and Time Reasoning","venue":"ArXiv","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Weimin Shi,Mingchen Zhuge,Zhong Zhou,D. Gao,Deng-Ping Fan","id":"0581387bf8411046a562994040f5edd0ad549293","summary":"This study lays a technical foundation for location and time reasoning and suggests that effectively introducing open-world knowledge is one of the panaceas for the tasks.","score":8},{"url":"https://www.semanticscholar.org/paper/9adec13eb929b374d3e2866908fd46dbdb504902","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":9,"influentialCitationCount":2,"publicationDate":"29/09/2022","authors":"Pan Lu,Liang Qiu,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Tanmay Rajpurohit,Peter Clark,A. Kalyan","id":"9adec13eb929b374d3e2866908fd46dbdb504902","summary":"A novel approach is proposed, P ROMPT PG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example, which outperforms the best baseline on the accuracy metric and reduces the prediction variance.","score":8},{"url":"https://www.semanticscholar.org/paper/5704b6ccdccba5645421daef67e1651dbc373b9b","title":"Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals","venue":"ArXiv","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/02/2023","authors":"Yue Wu,Yewen Fan,Paul Pu Liang,A. Azaria,Yuan-Fang Li,Tom Michael Mitchell","id":"5704b6ccdccba5645421daef67e1651dbc373b9b","summary":"Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers, and improves on 4 games in the Atari environment with sparse rewards, and requires 1000x less training frames compared to the previous SOTA Agent 57 on Skiing.","score":8},{"url":"https://www.semanticscholar.org/paper/4e114c28b589cb5ec4b567b87becd2e2e718fa65","title":"Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model","venue":"ArXiv","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Parishad BehnamGhader,Santiago Miret,Siva Reddy","id":"4e114c28b589cb5ec4b567b87becd2e2e718fa65","summary":"The strengths and weaknesses of different retriever-augmented language models such as REALM, k NN-LM, FiD, ATLAS, and Flan-T5 in reasoning over the selected documents in different tasks are studied.","score":8},{"url":"https://www.semanticscholar.org/paper/9a258f42e333ed5ff79037724eb01747ede0bb49","title":"Few-Shot Self-Rationalization with Natural Language Prompts","venue":"NAACL-HLT","year":2021,"referenceCount":73,"citationCount":28,"influentialCitationCount":5,"publicationDate":"16/11/2021","authors":"Ana Marasović,Iz Beltagy,Doug Downey,Matthew E. Peters","id":"9a258f42e333ed5ff79037724eb01747ede0bb49","summary":"This work identifies the right prompting approach by extensively exploring natural language prompts on FEB and demonstrates that making progress on few-shot self-rationalization is possible, and presents FEB—a stan-dardized collection of four existing English-language datasets and associated metrics.","score":8},{"url":"https://www.semanticscholar.org/paper/76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":7,"influentialCitationCount":0,"publicationDate":"24/02/2022","authors":"Erik Jones,J. Steinhardt","id":"76f023c3a819fc58989a064a1b50825b11fce95d","summary":"The results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave, and draw inspiration from human cognitive biases as motivation to generate hypotheses for problems that models may have and develop experiments that elicit these problems.","score":8},{"url":"https://www.semanticscholar.org/paper/316206a2f89eb94ce02a81fba1dc304586f21b39","title":"Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":43,"citationCount":10,"influentialCitationCount":1,"publicationDate":"25/05/2022","authors":"Junyeob Kim,Hyuhng Joon Kim,Hyunsoo Cho,Hwiyeol Jo,Sang-Woo Lee,Sang-goo Lee,Kang Min Yoo,Taeuk Kim","id":"316206a2f89eb94ce02a81fba1dc304586f21b39","summary":"Through extensive analyses, it is found that the correct input-label mappings can have varying impacts on the downstream in-context learning performances, depending on the experimental configuration.","score":8},{"url":"https://www.semanticscholar.org/paper/5b70e69b65b29d231d37bea354b25c05daec07e2","title":"Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":44,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Hung-Ting Chen,Michael J.Q. Zhang,Eunsol Choi","id":"5b70e69b65b29d231d37bea354b25c05daec07e2","summary":"A new calibration study is presented, where models are discouraged from presenting any single answer when presented with multiple conflicting answer candidates in retrieved evidences, and it is discovered that contradictions among knowledge sources affect model confidence only marginally.","score":8},{"url":"https://www.semanticscholar.org/paper/0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":6,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"S. Welleck,Jiacheng Liu,Ximing Lu,Hannaneh Hajishirzi,Yejin Choi","id":"0efa0441da820b1905572666ba1974a06a9663fb","summary":"N ATURAL P ROVER is capable of proving some theorems that require short (2-6 step) proofs, and providing next-step suggestions that are rated as correct and useful over 40% of the time, which is to the authors' knowledge the first demonstration of these capabilities using neural language models.","score":8},{"url":"https://www.semanticscholar.org/paper/4d81c33b295c092016ac236cfd32020a5bb70b97","title":"Optimizing Prompts for Text-to-Image Generation","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Y. Hao,Zewen Chi,Li Dong,Furu Wei","id":"4d81c33b295c092016ac236cfd32020a5bb70b97","summary":"This work proposes prompt adaptation, a general framework that automatically adapts original user input to model-preferred prompts, and proposes a reward function that encourages the policy to generate more aesthetically pleasing images while preserving the original user intentions.","score":8},{"url":"https://www.semanticscholar.org/paper/7c48d59f7be422f6e68b8cddd53b17fbab19599f","title":"Critic-Guided Decoding for Controlled Text Generation","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Minbeom Kim,Hwanhee Lee,Kang Min Yoo,Joonsuk Park,Hwaran Lee,Kyomin Jung","id":"7c48d59f7be422f6e68b8cddd53b17fbab19599f","summary":"Evaluation of the novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding shows that it generates more coherent and well-controlled texts than previous methods.","score":8},{"url":"https://www.semanticscholar.org/paper/52b79321a4862d44db09065e4b40021e2ec1eb0c","title":"On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective","venue":"ArXiv","year":2022,"referenceCount":95,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/12/2022","authors":"Ying Wen,Ziyu Wan,M. Zhou,Shufang Hou,Zhe Cao,Chenyang Le,Jingxiao Chen,Zheng Tian,Weinan Zhang,J. Wang","id":"52b79321a4862d44db09065e4b40021e2ec1eb0c","summary":"It is argued that a foundation decision model (FDM) can be established by formu-lating various decision-making tasks as a sequence decoding task using the Transformer architecture; this would be a promising solution to advance the applications of IDM in more complex real world tasks.","score":8},{"url":"https://www.semanticscholar.org/paper/23cae400cfd1a7c455c721256b838e98a307d5e6","title":"ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":3,"influentialCitationCount":0,"publicationDate":"30/12/2022","authors":"Katharina Jeblick,B. Schachtner,Jakob Dexl,Andreas Mittermeier,Anna Theresa Stüber,Johanna Topalis,Tobias Weber,P. Wesp,B. Sabel,J. Ricke,M. Ingrisch","id":"23cae400cfd1a7c455c721256b838e98a307d5e6","summary":"The initial insights of this study indicate a great potential in using large language models like ChatGPT to improve patient-centered care in radiology and other medical domains.","score":8},{"url":"https://www.semanticscholar.org/paper/8973ad5fc1264594a1fda3bd9e04258074cea9cc","title":"Neural Architecture Search: Insights from 1000 Papers","venue":"ArXiv","year":2023,"referenceCount":322,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/01/2023","authors":"Colin White,Mahmoud Safari,R. Sukthanker,Binxin Ru,T. Elsken,Arber Zela,Debadeepta Dey,F. Hutter","id":"8973ad5fc1264594a1fda3bd9e04258074cea9cc","summary":"This survey provides an organized and comprehensive guide to neural architecture search, giving a taxonomy of search spaces, algorithms, and speedup techniques, and discusses resources such as benchmarks, best practices, other surveys, and open-source libraries.","score":8},{"url":"https://www.semanticscholar.org/paper/ab4467bd55ddfe7b575aad37df11720ec93965d6","title":"Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning","venue":"ArXiv","year":2023,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Xinyi Wang,Wanrong Zhu,William Yang Wang","id":"ab4467bd55ddfe7b575aad37df11720ec93965d6","summary":"This study proposes an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrates a 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classiﬁcation datasets, supporting the hypothesis that large language models implicitly infer a latent concept variable.","score":8},{"url":"https://www.semanticscholar.org/paper/2b6fe1d6a34c2b4a3113209cb30111247d78585d","title":"Towards Agile Text Classifiers for Everyone","venue":"","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Maximilian Mozes,Jessica Hoffmann,Katrin Tomanek,Muhamed Kouate,Nithum Thain,Ann Yuan,Tolga Bolukbasi,Lucas Dixon","id":"2b6fe1d6a34c2b4a3113209cb30111247d78585d","summary":"It is argued that this enables a paradigm shift for text classification, especially for models supporting safer online discourse, whereby classifiers are trained using small, targeted datasets that can be quickly developed for a particular policy.","score":8},{"url":"https://www.semanticscholar.org/paper/390c71025beb7e7613640ecd331fa9a1179ca568","title":"Guiding Pretraining in Reinforcement Learning with Large Language Models","venue":"","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Yuqing Du,Olivia Watkins,Zihan Wang,Cédric Colas,Trevor Darrell,P. Abbeel,Abhishek Gupta,Jacob Andreas","id":"390c71025beb7e7613640ecd331fa9a1179ca568","summary":"A method that uses background knowledge from text corpora to shape exploration and rewards an agent for achieving goals suggested by a language model prompted with a description of the agent's current state, called ELLM (Exploring with LLMs).","score":8},{"url":"https://www.semanticscholar.org/paper/5d4e29995fba13ed87df00d894cbd96f20f2196a","title":"ANSEL Photobot: A Robot Event Photographer with Semantic Intelligence","venue":"","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"D. Rivkin,Gregory Dudek,Nikhil Kakodkar,D. Meger,Oliver Limoyo,Xue Liu,F. Hogan","id":"5d4e29995fba13ed87df00d894cbd96f20f2196a","summary":"This work illustrates how to produce a photo-taking robot with an exceptional level of semantic awareness by leveraging recent advances in general purpose language (LM) and vision-language (VLM) models.","score":8},{"url":"https://www.semanticscholar.org/paper/2ac93ba3f4cce13d31546c7c8d6ac6fa79ed34bf","title":"Enabling Conversational Interaction with Mobile UI using Large Language Models","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":2,"influentialCitationCount":0,"publicationDate":"18/09/2022","authors":"Bryan Wang,Gang Li,Yang Li","id":"2ac93ba3f4cce13d31546c7c8d6ac6fa79ed34bf","summary":"This paper proposes a design space to categorize conversations between the user and the agent when collaboratively accomplishing mobile tasks, and designs prompting techniques to adapt an LLM to conversational tasks on mobile UIs.","score":8},{"url":"https://www.semanticscholar.org/paper/262d6a0f6eb3f4c81e47fecd7e14be004295a7cf","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":2,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Alessandro Stolfo,Zhijing Jin,K. Shridhar,B. Schölkopf,Mrinmaya Sachan","id":"262d6a0f6eb3f4c81e47fecd7e14be004295a7cf","summary":"This work proposes a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands and math operators on the output solution, and shows that robustness does not appear to continuously improve as a function of scale, but that the recent GPT-3-Instruct achieves a dra-matic improvement in both robustness and sensitivity, compared to all other GPT variants.","score":8},{"url":"https://www.semanticscholar.org/paper/01da025a208ff4f35ba036ce882b21aa505243f3","title":"Self-adaptive In-context Learning","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Zhiyong Wu,Yaoxiang Wang,Jiacheng Ye,Lingpeng Kong","id":"01da025a208ff4f35ba036ce882b21aa505243f3","summary":"This paper advocates a new principle for ICL: self-adaptive in-context learning, and proposes a general select-then-rank framework and instantiate it with new selection and ranking algorithms.","score":8},{"url":"https://www.semanticscholar.org/paper/eb70971db1248785528667deddba68088944a375","title":"Little Red Riding Hood Goes Around the Globe: Crosslingual Story Planning and Generation with Large Language Models","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"E. Razumovskaia,Joshua Maynez,Annie Louis,Mirella Lapata,Shashi Narayan","id":"eb70971db1248785528667deddba68088944a375","summary":"","score":8},{"url":"https://www.semanticscholar.org/paper/ee805f55c98920f74d0182aaf136330a97b4123f","title":"ScatterShot: Interactive In-context Example Curation for Text Transformation","venue":"","year":2023,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/02/2023","authors":"Tongshuang Sherry Wu,Hua Shen,Daniel S. Weld,Jeffrey Heer,Marco Tulio Ribeiro","id":"ee805f55c98920f74d0182aaf136330a97b4123f","summary":"ScatterShot iteratively slices unlabeled data into task-specific patterns, samples informative inputs from underexplored or not-yet-saturated slices in an active learning manner, and helps users label more efficiently with the help of an LLM and the current example set.","score":8},{"url":"https://www.semanticscholar.org/paper/f59f27ae53bc860818cd3363d799a4b85fe6b4f9","title":"Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models","venue":"ArXiv","year":2022,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Zdenvek Kasner,Ioannis Konstas,Ondrej Dusek","id":"f59f27ae53bc860818cd3363d799a4b85fe6b4f9","summary":"It is argued that using data with a diverse set of clear and meaningful labels is key to training D2T generation systems capable of generalizing to novel domains.","score":8},{"url":"https://www.semanticscholar.org/paper/2fa6dbd2f147b6fa34d22c80aa225cc2cad0d96e","title":"Complex Reading Comprehension Through Question Decomposition","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":2,"influentialCitationCount":1,"publicationDate":"07/11/2022","authors":"Xiao-Yu Guo,Yuan-Fang Li,Gholamreza Haffari","id":"2fa6dbd2f147b6fa34d22c80aa225cc2cad0d96e","summary":"A novel learning approach is proposed that helps language models better understand multi-hop questions and perform “com-plex, compositional” reasoning.","score":8},{"url":"https://www.semanticscholar.org/paper/9dab4c20648cd4c7c6830e6274a95294b014aac9","title":"QAmeleon: Multilingual QA with Only 5 Examples","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":3,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Priyanka Agrawal,Chris Alberti,Fantine Huot,Joshua Maynez,Ji Ma,Sebastian Ruder,Kuzman Ganchev,Dipanjan Das,Mirella Lapata","id":"9dab4c20648cd4c7c6830e6274a95294b014aac9","summary":"This approach uses a PLM to automatically generate multilingual data upon which QA models are trained, thus avoiding costly annotation and shows that few-shot prompt tuning for data synthesis scales across languages and is a viable alternative to large-scale annotation.","score":8},{"url":"https://www.semanticscholar.org/paper/d164fc8d71ca304bbd7833de5d03ad0a5ca32afa","title":"Multi-document Summarization via Deep Learning Techniques: A Survey","venue":"ACM Computing Surveys","year":2020,"referenceCount":209,"citationCount":34,"influentialCitationCount":2,"publicationDate":"10/11/2020","authors":"Congbo Ma,W. Zhang,Mingyu Guo,Hu Wang,Quan Z. Sheng","id":"d164fc8d71ca304bbd7833de5d03ad0a5ca32afa","summary":"This survey, the first of its kind, systematically overviews the recent deep-learning-based MDS models and proposes a novel taxonomy to summarize the design strategies of neural networks and conduct a comprehensive summary of the state of the art.","score":8},{"url":"https://www.semanticscholar.org/paper/998592c0f3aaa6d64a1a3a79aa9e9caa3c7a5633","title":"Dynamic Scheduled Sampling with Imitation Loss for Neural Text Generation","venue":"ArXiv","year":2023,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Xiang Lin,Prathyusha Jwalapuram,Shafiq R. Joty","id":"998592c0f3aaa6d64a1a3a79aa9e9caa3c7a5633","summary":"D Y SI is introduced, which maintains the schedule based solely on the training time accuracy, while enhancing the curriculum learning by introducing an imitation loss, which attempts to make the behavior of the decoder in-distinguishable from the behavior from a teacher-forced decoder.","score":8},{"url":"https://www.semanticscholar.org/paper/c710d9a8c30b3130caaf3ea45e786de6b5a3eed8","title":"Adversarial Training for High-Stakes Reliability","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":7,"influentialCitationCount":0,"publicationDate":"03/05/2022","authors":"Daniel M. Ziegler,Seraphina Nix,Lawrence Chan,Tim Bauman,Peter Schmidt-Nielsen,Tao Lin,Adam Scherlis,Noa Nabeshima,Ben Weinstein-Raun,D. Haas,Buck Shlegeris,Nate Thomas","id":"c710d9a8c30b3130caaf3ea45e786de6b5a3eed8","summary":"This work created a series of adversarial training techniques—including a tool that assists human adversaries—to create and eliminate failures in a classiﬁer that ﬁlters text completions suggested by a generator, and found that adversarialTraining increased robustness to the adversarial attacks that it trained on, without affecting in-distribution performance.","score":8},{"url":"https://www.semanticscholar.org/paper/e10ed48cceca216d8ac43113c0562cf340dbdce3","title":"Unveiling Transformers with LEGO: a synthetic reasoning task","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":11,"influentialCitationCount":3,"publicationDate":"09/06/2022","authors":"Yi Zhang,A. Backurs,Sébastien Bubeck,Ronen Eldan,Suriya Gunasekar,Tal Wagner","id":"e10ed48cceca216d8ac43113c0562cf340dbdce3","summary":"It is found that in some data regime the trained transformer finds “shortcut” solutions to follow the chain of reasoning, which impedes the model’s ability to generalize to simple variants of the main task, and moreover one can prevent such shortcut with appropriate architecture modification or careful data preparation.","score":8},{"url":"https://www.semanticscholar.org/paper/2bd7ba07f9ae9102cefbf650ae24f7990de541ad","title":"Regulating ChatGPT and other Large Generative AI Models","venue":"ArXiv","year":2023,"referenceCount":145,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/02/2023","authors":"P. Hacker,A. Engel,M. Mauer","id":"2bd7ba07f9ae9102cefbf650ae24f7990de541ad","summary":"This paper sitsuate these new generative models in the current debate on trustworthy AI regulation, and suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between L GAIM developers, deployers, professional and non-professional users, as well as recipients of LGA IM output.","score":8},{"url":"https://www.semanticscholar.org/paper/292da1c4640c105aa5d7919a1ded9a1225c07d4d","title":"Large Language Models and the Reverse Turing Test","venue":"Neural Computation","year":2022,"referenceCount":82,"citationCount":3,"influentialCitationCount":0,"publicationDate":"28/07/2022","authors":"T. Sejnowski","id":"292da1c4640c105aa5d7919a1ded9a1225c07d4d","summary":"","score":8},{"url":"https://www.semanticscholar.org/paper/bc6650c553e341062569c0c39ec36d8a5063bf67","title":"S TANDING ON THE S HOULDERS OF G IANT F ROZEN L ANGUAGE M ODELS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"bc6650c553e341062569c0c39ec36d8a5063bf67","summary":"Condense relevant information from 100+ retrieved documents into the input sequence length of the frozen LM reader to reach and surpass leading tuning approaches on Natural Questions, a open-domain question answering benchmark.","score":8},{"url":"https://www.semanticscholar.org/paper/5d49c7401c5f2337c4cc88d243ae39ed659afe64","title":"Red Teaming Language Models with Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":100,"citationCount":58,"influentialCitationCount":2,"publicationDate":"07/02/2022","authors":"Ethan Perez,Saffron Huang,Francis Song,Trevor Cai,Roman Ring,J. Aslanides,A. Glaese,Nathan McAleese,Geoffrey Irving","id":"5d49c7401c5f2337c4cc88d243ae39ed659afe64","summary":"This work automatically finds cases where a target LM behaves in a harmful way, by generating test cases (“red teaming”) using another LM, and evaluates the target LM’s replies to generated test questions using a classifier trained to detect offensive content.","score":8},{"url":"https://www.semanticscholar.org/paper/1e7fcebce44eb35cb7dd48645af76378155faea8","title":"Improving Passage Retrieval with Zero-Shot Question Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":46,"citationCount":15,"influentialCitationCount":4,"publicationDate":"15/04/2022","authors":"Devendra Singh Sachan,M. Lewis,Mandar Joshi,Armen Aghajanyan,Wen-tau Yih,J. Pineau,Luke Zettlemoyer","id":"1e7fcebce44eb35cb7dd48645af76378155faea8","summary":"This work proposes a simple and effective re-ranking method for improving passage retrieval in open question answering that improves strong unsupervised retrieval models by 6%-18% absolute and strong supervised models by up to 12% in terms of top-20 passage retrieval accuracy.","score":8},{"url":"https://www.semanticscholar.org/paper/fb30166c218bef3597b0d9789ad340defc3989ca","title":"In-BoXBART: Get Instructions into Biomedical Multi-Task Learning","venue":"NAACL-HLT","year":2022,"referenceCount":60,"citationCount":4,"influentialCitationCount":0,"publicationDate":"15/04/2022","authors":"Mihir Parmar,Swaroop Mishra,Mirali Purohit,Man Luo,M. H. Murad,Chitta Baral","id":"fb30166c218bef3597b0d9789ad340defc3989ca","summary":"This is the first attempt to propose a uniﬁed model in the biomedical domain and use instructions to achieve generalization across several biomedical tasks, and indicates that there is room for improvement across tasks in the BoX, implying the scope for future research direction.","score":8},{"url":"https://www.semanticscholar.org/paper/3b8b2d43e38eb4d48d97b14a8f981e96a6c60df0","title":"Standing on the Shoulders of Giant Frozen Language Models","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":15,"influentialCitationCount":6,"publicationDate":"21/04/2022","authors":"Yoav Levine,Itay Dalmedigos,Ori Ram,Yoel Zeldes,Daniel Jannai,Dor Muhlgay,Yoni Osin,Opher Lieber,Barak Lenz,S. Shalev-Shwartz,A. Shashua,Kevin Leyton-Brown,Y. Shoham","id":"3b8b2d43e38eb4d48d97b14a8f981e96a6c60df0","summary":"Condense relevant information from 100+ retrieved documents into the input sequence length of the frozen LM reader to reach and surpass leading tuning approaches on Natural Questions, a open-domain question answering benchmark.","score":8},{"url":"https://www.semanticscholar.org/paper/47dc00cf21a0e7452b6f61207f119a9cc01c52b8","title":"HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":4,"influentialCitationCount":0,"publicationDate":"17/08/2022","authors":"Swaroop Mishra,E. Nouri","id":"47dc00cf21a0e7452b6f61207f119a9cc01c52b8","summary":"This paper proposes a simple prompting strategy HELP ME THINK where GPT3 is encouraged to help non-expert users by asking a set of relevant questions and leveraging user answers to execute the task.","score":8},{"url":"https://www.semanticscholar.org/paper/51a00995498ec296e5f19d5e7e59fe901cbc08b4","title":"Using Large Language Models to Simulate Multiple Humans","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":7,"influentialCitationCount":1,"publicationDate":"18/08/2022","authors":"Gati Aher,RosaI. Arriaga,A. Kalai","id":"51a00995498ec296e5f19d5e7e59fe901cbc08b4","summary":"A new type of test, called a Turing Experiment (TE), for evaluating how well a language model, such as GPT-3, can simulate different aspects of human behavior, and reveals a\"hyper-accuracy distortion\"present in some language models.","score":8},{"url":"https://www.semanticscholar.org/paper/d3135733aa39dec20ce72aa138589dda27c8406d","title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering","venue":"ArXiv","year":2022,"referenceCount":70,"citationCount":14,"influentialCitationCount":2,"publicationDate":"20/09/2022","authors":"Pan Lu,Swaroop Mishra,Tony Xia,Liang Qiu,Kai-Wei Chang,Song-Chun Zhu,Oyvind Tafjord,Peter Clark,A. Kalyan","id":"d3135733aa39dec20ce72aa138589dda27c8406d","summary":"This work designs language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering S CIENCE QA questions and explores the upper bound of GPT-3 and shows that CoT helps language models learn from fewer data.","score":8},{"url":"https://www.semanticscholar.org/paper/e86009d9f9b1cdf083a48d087552bc4153784451","title":"Promptagator: Few-shot Dense Retrieval From 8 Examples","venue":"ArXiv","year":2022,"referenceCount":70,"citationCount":14,"influentialCitationCount":3,"publicationDate":"23/09/2022","authors":"Zhuyun Dai,Vincent Zhao,Ji Ma,Yi Luan,Jianmo Ni,Jing Lu,A. Bakalov,Kelvin Guu,Keith B. Hall,Ming-Wei Chang","id":"e86009d9f9b1cdf083a48d087552bc4153784451","summary":"This paper proposes Promptbase Query Generation for Retriever (PROMPTAGATOR), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data.","score":8},{"url":"https://www.semanticscholar.org/paper/8f0bbd0920cc13dbd3fec5f254facca7991875a4","title":"News Summarization and Evaluation in the Era of GPT-3","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":17,"influentialCitationCount":2,"publicationDate":"26/09/2022","authors":"Tanya Goyal,Junyi Jessy Li,Greg Durrett","id":"8f0bbd0920cc13dbd3fec5f254facca7991875a4","summary":"It is shown that not only do humans overwhelmingly prefer GPT-3 summaries, but these also do not suffer from common dataset-speciﬁc issues such as poor factuality, and both reference-based and reference-free automatic metrics cannot reliably evaluate zero-shot summaries.","score":8},{"url":"https://www.semanticscholar.org/paper/8fbd7ddf1ea30c991f3b1152a245df77caa18e16","title":"Learning by Distilling Context","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":6,"influentialCitationCount":0,"publicationDate":"30/09/2022","authors":"Charles Burton Snell,D. Klein,Ruiqi Zhong","id":"8fbd7ddf1ea30c991f3b1152a245df77caa18e16","summary":"This work shows that context distillation is a general method to train language models, and it can effectively internalize 3 types of training signals, and can internalize step-by-step reasoning for complex tasks.","score":8},{"url":"https://www.semanticscholar.org/paper/7d29a84a589aa5655e5d3fed8d725ea472816599","title":"Explanations from Large Language Models Make Small Reasoners Better","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":10,"influentialCitationCount":1,"publicationDate":"13/10/2022","authors":"SHIYANG LI,Jianshu Chen,Yelong Shen,Zhiyu Chen,Xinlu Zhang,Zekun Li,Hong Wang,Jingu Qian,Baolin Peng,Yi Mao,Wenhu Chen,Xifeng Yan","id":"7d29a84a589aa5655e5d3fed8d725ea472816599","summary":"This work systematically explore three explanation generation approaches from LLM and utilizes a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities.","score":8},{"url":"https://www.semanticscholar.org/paper/9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","title":"\"John is 50 years old, can his son be 65?\" Evaluating NLP Models' Understanding of Feasibility","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Himanshu Gupta,Neeraj Varshney,Swaroop Mishra,Kuntal Kumar Pal,Saurabh Arjun Sawant,Kevin Scaria,Siddharth Goyal,Chitta Baral","id":"9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","summary":"This work introduces FeasibilityQA, a question-answering dataset involving binary classification (BCQ) and multi-choice multi-correct questions (MCQ) that test understanding of feasibility, and shows that even state-of-the-art models such as GPT-3, G PT-2, and T5 struggle to answer the feasibility questions correctly.","score":8},{"url":"https://www.semanticscholar.org/paper/8b24e92e70a498c6a8d16e00e6ab5d5c529ab183","title":"A Universal Discriminator for Zero-Shot Generalization","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Haike Xu,Zongyu Lin,Jing Zhou,Yanan Zheng,Zhilin Yang","id":"8b24e92e70a498c6a8d16e00e6ab5d5c529ab183","summary":"This work challenges this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks, and jointly train a generalized UD in combination with generative tasks, which maintains its advantage on discrim inative tasks and simultaneously works onGenerative tasks.","score":8},{"url":"https://www.semanticscholar.org/paper/d0c67804d7f8cc8d79c7346ec8a011f43a21c553","title":"Data-Efficient Finetuning Using Cross-Task Nearest Neighbors","venue":"ArXiv","year":2022,"referenceCount":52,"citationCount":1,"influentialCitationCount":1,"publicationDate":"01/12/2022","authors":"Hamish Ivison,Noah A. Smith,Hannaneh Hajishirzi,Pradeep Dasigi","id":"d0c67804d7f8cc8d79c7346ec8a011f43a21c553","summary":"It is shown that training on a carefully chosen subset of instances can outperform training on all available data on a variety of datasets, and small subsets of P3 and T5 models that outperform the 3-billion parameter variant of T0 by 3–30% on 12 out of 14 evaluation datasets while using at most 2% of the data used to train T0-3B.","score":8},{"url":"https://www.semanticscholar.org/paper/2e6fa3095df1d1ed041dfb4f5a18e31d4b7bd7bb","title":"In-Context Learning with Many Demonstration Examples","venue":"ArXiv","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/02/2023","authors":"Mukai Li,Shansan Gong,Jiangtao Feng,Yiheng Xu,Jinchao Zhang,Zhiyong Wu,Lingpeng Kong","id":"2e6fa3095df1d1ed041dfb4f5a18e31d4b7bd7bb","summary":"A long-range language model EVALM based on an efficient transformer mechanism that can achieve higher performance with more demonstrations under many-shot instruction tuning, and further extending the length of instructions can further improve the upper bound of scaling in-context learning.","score":8},{"url":"https://www.semanticscholar.org/paper/e295c65efbec7754cc194af0999d489292a3c8b4","title":"Residual Learning of Neural Text Generation with n-gram Language Model","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":50,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Huayang Li,Deng Cai,J. Xu,Taro Watanabe","id":"e295c65efbec7754cc194af0999d489292a3c8b4","summary":"Experimental results on three typical language tasks demonstrate that the proposed neural LM attains additional performance gains over popular standalone neural models consistently and allows for effective domain adaptation by simply switching to a domain-specific $n$-gram model, without any extra training.","score":8},{"url":"https://www.semanticscholar.org/paper/60964d20095f7008d4285632cc991e63099a3d77","title":"Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey","venue":"ArXiv","year":2022,"referenceCount":161,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/12/2022","authors":"Yuxin Wang,Jieru Lin,Zhiwei Yu,Wei Hu,Börje F. Karlsson","id":"60964d20095f7008d4285632cc991e63099a3d77","summary":"A systematical taxonomy regarding how existing methods integrate structured knowledge into story generation is presented and multidimensional insights into the challenges of knowledge-enhanced story generation are given and light is cast on promising directions for future study.","score":8},{"url":"https://www.semanticscholar.org/paper/c042d0169ea4f4e4490d406befb973ebab135e7e","title":"Vision Encoders in Visual Question Answering","venue":"","year":2022,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Ryan R. Anderson","id":"c042d0169ea4f4e4490d406befb973ebab135e7e","summary":"This work formats the inputs used to prompt a VLM using a modified text-only template from a closed-book question answering task that the language-model component of the VLM was pretrained on, enabling the V LM to leverage the similarities between the tasks, such as the answer-length distribution, when generating answers to the visual questions.","score":8},{"url":"https://www.semanticscholar.org/paper/d68c8db0e1b8b7f1e6c44393e0a425daa44a16c7","title":"VIMA: General Robot Manipulation with Multimodal Prompts","venue":"ArXiv","year":2022,"referenceCount":125,"citationCount":9,"influentialCitationCount":1,"publicationDate":"06/10/2022","authors":"Yunfan Jiang,Agrim Gupta,Zichen Zhang,Guanzhi Wang,Yongqiang Dou,Yanjun Chen,Li Fei-Fei,Anima Anandkumar,Yuke Zhu,Linxi (Jim) Fan","id":"d68c8db0e1b8b7f1e6c44393e0a425daa44a16c7","summary":"This work designs a transformer-based generalist robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively and achieves strong scalability in both model capacity and data size.","score":8},{"url":"https://www.semanticscholar.org/paper/4294e8a4a1ea06a3cfc55bbacc03299a72260473","title":"AI in Human-computer Gaming: Techniques, Challenges and Opportunities","venue":"Machine Intelligence Research","year":2021,"referenceCount":68,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/11/2021","authors":"Qiyue Yin,Jun Yang,Kaiqi Huang,Meijing Zhao,Wancheng Ni,Bin Liang,Yan Huang,Shu Wu,Liangsheng Wang","id":"4294e8a4a1ea06a3cfc55bbacc03299a72260473","summary":"A survey of recent successful game AIs, covering board game AIS, card game A is, first-person shooting game Ais, and real-time strategy game AI, to compare the main difficulties among different kinds of games and the corresponding techniques utilized for achieving professional human-level AIs.","score":7},{"url":"https://www.semanticscholar.org/paper/7dfd7b37267f5c290d107de529c428413ba297ad","title":"Reinforce-lib: A Reinforcement Learning Library for Scientific Research","venue":"Proceedings of International Symposium on Grids &amp; Clouds 2022 — PoS(ISGC2022)","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/09/2022","authors":"Luca Anzalone,D. Bonacorsi","id":"7dfd7b37267f5c290d107de529c428413ba297ad","summary":"This paper proposes a modern, modular, simple and understandable Python RL library called reinforce-lib, aimed at enabling newcomers, practitioners, and researchers to easily employ RL to solve new scientific problems.","score":7},{"url":"https://www.semanticscholar.org/paper/de5ebf2ed4433eac7bceb16484d33f66e6b5b580","title":"Cooperative and competitive multi-agent deep reinforcement learning","venue":"Other Conferences","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Ruizhi Chen","id":"de5ebf2ed4433eac7bceb16484d33f66e6b5b580","summary":"This article mostly focuses on recent papers on Multi-agent deep reinforcement learning (MADRL), and key ideas and main techniques in each work are discussed.","score":7},{"url":"https://www.semanticscholar.org/paper/af502f64a48b1551c4beb0649a887f98776fca2c","title":"A Critical Review of Traffic Signal Control and A Novel Unified View of Reinforcement Learning and Model Predictive Control Approaches for Adaptive Traffic Signal Control","venue":"ArXiv","year":2022,"referenceCount":107,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/11/2022","authors":"Xiaoyu Wang,S. Sanner,B. Abdulhai","id":"af502f64a48b1551c4beb0649a887f98776fca2c","summary":"A novel unified view of modern ATSCs is proposed to identify common ground as well as differences and shortcomings of existing methodologies with the ultimate goal to facilitate cross-fertilization and advance the state-of-the-art.","score":7},{"url":"https://www.semanticscholar.org/paper/f18c3f40f62596337ce79d3d103160d3236498f2","title":"Reinforcement Learning for Quantitative Trading","venue":"ACM Transactions on Intelligent Systems and Technology","year":2021,"referenceCount":166,"citationCount":8,"influentialCitationCount":0,"publicationDate":"28/09/2021","authors":"Shuo Sun,R. Wang,Bo An","id":"f18c3f40f62596337ce79d3d103160d3236498f2","summary":"A taxonomy of RL-based QT models is devised, along with a comprehensive summary of the state of the art, and current challenges and proposed future research directions in this exciting field are discussed.","score":7},{"url":"https://www.semanticscholar.org/paper/6d4a9f1c41b078846901362ba0dce8295dd6a2a8","title":"End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering","venue":"Neural Information Processing Systems","year":2021,"referenceCount":56,"citationCount":48,"influentialCitationCount":12,"publicationDate":"09/06/2021","authors":"Devendra Singh Sachan,Siva Reddy,William Hamilton,Chris Dyer,Dani Yogatama","id":"6d4a9f1c41b078846901362ba0dce8295dd6a2a8","summary":"An end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers and demonstrates the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.","score":7},{"url":"https://www.semanticscholar.org/paper/25fa3de9227cce01568fe7273169195df9342d4b","title":"Mutually improved dense retriever and GNN-based reader for arbitrary-hop open-domain question answering","venue":"Neural computing & applications (Print)","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/03/2022","authors":"Ronghan Li,Lifang Wang,Zejun Jiang,Zhongtian Hu,Meng Zhao,Xinyu Lu","id":"25fa3de9227cce01568fe7273169195df9342d4b","summary":"This paper investigates the mutual promotion of dense retrievers and Graph Neural Network-based readers to improve OpenQA and introduces an alternate training strategy where the scores of the dense retriever and the GNN-based reader are used as correction weights to enhance the performance of each other.","score":7},{"url":"https://www.semanticscholar.org/paper/63ddd417f4bb3b5b736542494ec8e99d5d4227a0","title":"Detecting Frozen Phrases in Open-Domain Question Answering","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/07/2022","authors":"Mostafa Yadegari,Ehsan Kamalloo,Davood Rafiei","id":"63ddd417f4bb3b5b736542494ec8e99d5d4227a0","summary":"The experiments reveal that detecting frozen phrases whose presence in answer documents are highly plausible yields significant improvements in retrievals as well as in the end-to-end accuracy of open-domain QA models.","score":7},{"url":"https://www.semanticscholar.org/paper/c00448c8097b043004bb8010186215e2c15b70c0","title":"Towards Intrinsic Interactive Reinforcement Learning","venue":"","year":2021,"referenceCount":131,"citationCount":2,"influentialCitationCount":1,"publicationDate":"02/12/2021","authors":"Ben Poole,Minwoo Lee","id":"c00448c8097b043004bb8010186215e2c15b70c0","summary":"A tutorial and review of intrinsic IRL so far with an emphasis on its parent field of feedback-driven IRL along with discussions concerning validity, challenges, and open problems is provided.","score":7},{"url":"https://www.semanticscholar.org/paper/c512d35fd20fbe4612f2bce2b6f5409c8b0a73e1","title":"Automated Reinforcement Learning (AutoRL): A Survey and Open Problems","venue":"Journal of Artificial Intelligence Research","year":2022,"referenceCount":284,"citationCount":28,"influentialCitationCount":2,"publicationDate":"11/01/2022","authors":"Jack Parker-Holder,Raghunandan Rajan,Xingyou Song,André Biedenkapp,Yingjie Miao,Theresa Eimer,Baohe Zhang,V. Nguyen,R. Calandra,Aleksandra Faust,F. Hutter,M. Lindauer","id":"c512d35fd20fbe4612f2bce2b6f5409c8b0a73e1","summary":"This survey seeks to unify the field of AutoRL, provide a common taxonomy, discuss each area in detail and pose open problems of interest to researchers going forward.","score":7},{"url":"https://www.semanticscholar.org/paper/f7374cc79d56dd683b965a91dcff7a7b2d5ceaeb","title":"Optimizing communication in deep reinforcement learning with XingTian","venue":"Proceedings of the 23rd ACM/IFIP International Middleware Conference","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Lichen Pan,Jun Qian,Wei Xia,Hangyu Mao,Jun Yao,Pengze Li,Zhen Xiao","id":"f7374cc79d56dd683b965a91dcff7a7b2d5ceaeb","summary":"XingTian is a novel DRL framework that co-designs the management of communication and computation in DRL algorithms in a decentralized way and provides an asynchronous communication channel that maintains high communication efficiency under different scale deployments.","score":7},{"url":"https://www.semanticscholar.org/paper/8632fc7c21c7f2cac630f2c989cb4f3d2e6cc86b","title":"High-Accuracy Model-Based Reinforcement Learning, a Survey","venue":"Artificial Intelligence Review","year":2021,"referenceCount":117,"citationCount":3,"influentialCitationCount":0,"publicationDate":"17/07/2021","authors":"A. Plaat,W. Kosters,M. Preuss","id":"8632fc7c21c7f2cac630f2c989cb4f3d2e6cc86b","summary":"This paper surveys model-based reinforcement learning methods, explaining in detail how they work and what their strengths and weaknesses are, and concludes with a research agenda for future work to make the methods more robust and more widely applicable to other applications.","score":7},{"url":"https://www.semanticscholar.org/paper/e7688af964a3e7eba2d11afa1fe0aab14afe3c15","title":"PrimeQA: The Prime Repository for State-of-the-Art Multilingual Question Answering Research and Development","venue":"ArXiv","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2023","authors":"Avirup Sil,Jaydeep Sen,Bhavani Iyer,M. Franz,Kshitij P. Fadnis,Mihaela A. Bornea,Sara Rosenthal,Scott McCarley,Rong Zhang,Vishwajeet Kumar,Yulong Li,Md Arafat Sultan,Riyaz Ahmad Bhat,Radu Florian,S. Roukos","id":"e7688af964a3e7eba2d11afa1fe0aab14afe3c15","summary":"P RIME QA is introduced: a one-stop and open-source QA repository with an aim to democratize QA research and facilitate easy replication of state-of-the-art (SOTA) QA methods.","score":7},{"url":"https://www.semanticscholar.org/paper/894188a2fa6825722f946eece9490d70998aa0b7","title":"Towards automating Codenames spymasters with deep reinforcement learning","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/12/2022","authors":"Sherman Siu","id":"894188a2fa6825722f946eece9490d70998aa0b7","summary":"Codenames is a good benchmark for both humanAI co-operation and text-based reinforcement learning, which are both several important areas of AI research.","score":7},{"url":"https://www.semanticscholar.org/paper/b18f32e6cdef86680ebd9a6cc82826127e2cff1a","title":"A Survey on Transformers in Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":90,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/01/2023","authors":"Wenzhe Li,Hao Luo,Zichuan Lin,Chongjie Zhang,Zongqing Lu,Deheng Ye","id":"b18f32e6cdef86680ebd9a6cc82826127e2cff1a","summary":"This paper seeks to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-ﬁeld, and summarize future prospects.","score":7},{"url":"https://www.semanticscholar.org/paper/05a2c3bdd1eeaeef226f56be4545bfadc9eb68f5","title":"Rewards Encoding Environment Dynamics Improves Preference-based Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/11/2022","authors":"Katherine Metcalf,Miguel Sarabia,B. Theobald","id":"05a2c3bdd1eeaeef226f56be4545bfadc9eb68f5","summary":"It is demonstrated that encoding environment dynamics in the reward function (REED) dramatically reduces the number of preference labels required in state-of-the-art preference-based RL frameworks, and it is hypothesized that REED-based methods better partition the state- action space and facilitate generalization to state-action pairs not included in the preference dataset.","score":7},{"url":"https://www.semanticscholar.org/paper/27ad3c82903fbf78491d707f97235ef3e8b4580f","title":"(QA)2: Question Answering with Questionable Assumptions","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Najoung Kim,Phu Mon Htut,Sam Bowman,Jackson Owen Petty","id":"27ad3c82903fbf78491d707f97235ef3e8b4580f","summary":"(QA) 2 (Question Answering with Questionable Assumptions), an open-domain evaluation dataset consisting of naturally-occurring search engine queries that may or may not contain questionable assumptions, is proposed.","score":7},{"url":"https://www.semanticscholar.org/paper/7e38476342ce1fcc8ef0dcd23686539395961769","title":"Inductive biases for deep learning of higher-level cognition","venue":"Proceedings of the Royal Society A","year":2020,"referenceCount":260,"citationCount":119,"influentialCitationCount":7,"publicationDate":"30/11/2020","authors":"Anirudh Goyal,Yoshua Bengio","id":"7e38476342ce1fcc8ef0dcd23686539395961769","summary":"This work considers a larger list of inductive biases that humans and animals exploit, focusing on those which concern mostly higher-level and sequential conscious processing, to help build AI systems benefiting from humans’ abilities in terms of flexible out-of-distribution and systematic generalization.","score":7},{"url":"https://www.semanticscholar.org/paper/a81a09b2a4ce36ae5c847fc4e3558c523d301179","title":"Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":66,"citationCount":7,"influentialCitationCount":1,"publicationDate":"16/12/2021","authors":"Akari Asai,Matt Gardner,Hannaneh Hajishirzi","id":"a81a09b2a4ce36ae5c847fc4e3558c523d301179","summary":"This work introduces a method to incorporate evidentiality of passages—whether a passage contains correct evidence to support the output—into training the generator, and introduces a multi-task learning framework to jointly generate the final output and predict the evidentialsity of each passage.","score":7},{"url":"https://www.semanticscholar.org/paper/032dc764cac41dea93680131d9a49284f2ae479f","title":"You can't pick your neighbors, or can you? When and how to rely on retrieval in the $k$NN-LM","venue":"","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/10/2022","authors":"Andrew Drozdov,Shufan Wang,Razieh Rahimi,A. McCallum,Hamed Zamani,Mohit Iyyer","id":"032dc764cac41dea93680131d9a49284f2ae479f","summary":"A new formulation of the k NN-LM that uses retrieval quality to assign the interpolation coefﬁcient and empirically measures the effectiveness of the approach on two English language modeling datasets, Wikitext-103 and PG-19.","score":7},{"url":"https://www.semanticscholar.org/paper/4ce987d4f8ae0f4680808c318980d42a82b9aa89","title":"Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"K. Choromanski,Shanda Li,Valerii Likhosherstov,Kumar Avinava Dubey,Shengjie Luo,Di He,Yiming Yang,Tamás Sarlós,Thomas Weingarten,Adrian Weller","id":"4ce987d4f8ae0f4680808c318980d42a82b9aa89","summary":"FLTs are the first Transformers architectures providing RPE-enhanced linear attention and provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling.","score":7},{"url":"https://www.semanticscholar.org/paper/cd471b5ef162906ef3d9a84398b3f98e9ee4bf56","title":"A Review on Language Models as Knowledge Bases","venue":"ArXiv","year":2022,"referenceCount":167,"citationCount":15,"influentialCitationCount":5,"publicationDate":"12/04/2022","authors":"Badr AlKhamissi,Millicent Li,Asli Celikyilmaz,Mona T. Diab,Marjan Ghazvininejad","id":"cd471b5ef162906ef3d9a84398b3f98e9ee4bf56","summary":"This paper presents a set of aspects that it is deemed an LM should have to fully act as a KB, and reviews the recent literature with respect to those aspects.","score":7},{"url":"https://www.semanticscholar.org/paper/1c1ca2392155ddf30408a442e6b504b5d60d4f2a","title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment","venue":"ArXiv","year":2022,"referenceCount":78,"citationCount":1,"influentialCitationCount":0,"publicationDate":"04/10/2022","authors":"Zhijing Jin,Sydney Levine,Fernando Gonzalez,Ojasv Kamal,Maarten Sap,Mrinmaya Sachan,Rada Mihalcea,J. Tenenbaum,B. Schölkopf","id":"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","summary":"This paper presents a novel challenge set consisting of rule-breaking question answering (RBQA) of cases that involve potentially permissible rule- Breaking – inspired by recent moral psychology studies and proposes a novel moral chain of thought prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments.","score":7},{"url":"https://www.semanticscholar.org/paper/dda0f7f086fc875d583604f8b0cf4a8678bc4de4","title":"Bootstrapping Multilingual Semantic Parsers using Large Language Models","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Abhijeet Awasthi,Nitish Gupta,Bidisha Samanta,Shachi Dave,Sunita Sarawagi,P. Talukdar","id":"dda0f7f086fc875d583604f8b0cf4a8678bc4de4","summary":"The effectiveness and flexibility offered by large language models (LLMs) for translating English datasets into several languages via few-shot prompting are demonstrated and it is shown that the method of translating data using LLMs outperforms a strong translate-train baseline on 41 out of 50 languages.","score":7},{"url":"https://www.semanticscholar.org/paper/4c0f95df93ac75f9ab06b0104e4196a6e8fda25e","title":"Can GPT-3 Perform Statutory Reasoning?","venue":"ArXiv","year":2023,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Andrew Blair-Stanek,Nils Holzenberger,Benjamin Van Durme","id":"4c0f95df93ac75f9ab06b0104e4196a6e8fda25e","summary":"This paper explores the capabilities of the most capable GPT-3 model, text-davinci-003, on an established statutory-reasoning dataset called SARA, and considers a variety of approaches, including dynamic few-shot prompting, chain-of-thought prompting, and zero- shot prompting.","score":7},{"url":"https://www.semanticscholar.org/paper/0180d35b85dd4daead90e0652b64b1339e754684","title":"Assistance with large language models","venue":"","year":2022,"referenceCount":30,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Dmitrii Krasheninnikov","id":"0180d35b85dd4daead90e0652b64b1339e754684","summary":"A behavioral cloning approach is applied to GPT-3 such that it can respond to clear input questions directly, clarify the intent behind vague input questions, and respond based on the clarification it receives, and this approach leads to quantitative improvements in answer accuracy compared to a baseline that cannot ask for clarifications.","score":7},{"url":"https://www.semanticscholar.org/paper/58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","title":"Can Foundation Models Wrangle Your Data?","venue":"Proceedings of the VLDB Endowment","year":2022,"referenceCount":91,"citationCount":7,"influentialCitationCount":1,"publicationDate":"20/05/2022","authors":"A. Narayan,Ines Chami,Laurel J. Orr,Christopher R'e","id":"58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","summary":"It is found that large FMs generalize and achieve SoTA performance on data cleaning and integration tasks, even though they are not trained for these data tasks.","score":7},{"url":"https://www.semanticscholar.org/paper/815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Pengcheng Yin,Wen-Ding Li,Kefan Xiao,A. Rao,Yeming Wen,Kensen Shi,Joshua Howland,Paige Bailey,Michele Catasta,H. Michalewski,Alex Polozov,Charles Sutton","id":"815c6ca281536d18ec0eb408b6e46e72a0826163","summary":"P A C H - I NC O, a 62B code language model for Python computational notebooks, which outperforms public code LMs and explores few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions.","score":7},{"url":"https://www.semanticscholar.org/paper/49293522fe3818769f5067e360c971bd7a82e0aa","title":"Explanation Selection Using Unlabeled Data for In-Context Learning","venue":"ArXiv","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/02/2023","authors":"Xi Ye,Greg Durrett","id":"49293522fe3818769f5067e360c971bd7a82e0aa","summary":"Across four textual reasoning tasks spanning question answering, mathematical reasoning, and natural language inference, results show that the proxy metrics correlate with ground truth accuracy and the overall method can effectively improve prompts over crowdworker annotations and naive search strategies.","score":7},{"url":"https://www.semanticscholar.org/paper/bd78dab66b19958950d56262a257a1041628aaed","title":"ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation","venue":"ArXiv","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"KAI-QING Zhou,Kai Zheng,Connor Pryor,Yilin Shen,Hongxia Jin,L. Getoor,X. Wang","id":"bd78dab66b19958950d56262a257a1041628aaed","summary":"This work presents a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments.","score":7},{"url":"https://www.semanticscholar.org/paper/87126a964ed14d0d2207747fc732b197e2fc9493","title":"Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Zhengbao Jiang,Luyu Gao,J. Araki,Haibo Ding,Zhiruo Wang,Jamie Callan,Graham Neubig","id":"87126a964ed14d0d2207747fc732b197e2fc9493","summary":"It is demonstrated for the first time that an end-to-end trained single Transformer can achieve both competitive retrieval and QA performance on in-domain datasets, matching or even slightly outperforming state-of-the-art dense retrievers and readers.","score":7},{"url":"https://www.semanticscholar.org/paper/a3a241e9397fe29b37f96cb5e8f4b8bebed3d3da","title":"STREET: A Multi-Task Structured Reasoning and Explanation Benchmark","venue":"","year":2023,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"D. Ribeiro,Shen Wang,Xiaofei Ma,He Zhu,Rui Dong,Deguang Kong,Juliette Burger,Anjelica Ramos,William Yang Wang,Zhiheng Huang,G. Karypis,Bing Xiang,D. Roth","id":"a3a241e9397fe29b37f96cb5e8f4b8bebed3d3da","summary":"STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark, is introduced to provide a way for the community to better train and test systems on multi-step reasoning and explanations in natural language.","score":7},{"url":"https://www.semanticscholar.org/paper/f9838a3be5c94bb2674a0e224de349b50e18f3c4","title":"Learning To Retrieve Prompts for In-Context Learning","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":59,"citationCount":75,"influentialCitationCount":10,"publicationDate":"16/12/2021","authors":"Ohad Rubin,Jonathan Herzig,Jonathan Berant","id":"f9838a3be5c94bb2674a0e224de349b50e18f3c4","summary":"This work proposes an efficient method for retrieving prompts for in-context learning using annotated data and an LM, and trains an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time.","score":7},{"url":"https://www.semanticscholar.org/paper/6d86f08a5d936780a4785acfad92f5f3e82004ad","title":"Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":3,"influentialCitationCount":0,"publicationDate":"06/04/2022","authors":"Noam Wies,Yoav Levine,A. Shashua","id":"6d86f08a5d936780a4785acfad92f5f3e82004ad","summary":"It is shown that when concatenating intermediate supervision to the input and training a sequence-to-sequence model on this modified input, unlearnable composite problems can become learnable.","score":7},{"url":"https://www.semanticscholar.org/paper/452cf9f8b756995363baa659976b30cb09893c89","title":"Decoupled Context Processing for Context Augmented Language Modeling","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":5,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Zonglin Li,Ruiqi Guo,Surinder Kumar","id":"452cf9f8b756995363baa659976b30cb09893c89","summary":"It is shown that such a simple architecture for incorporating external context into language models based on decoupled Encoder-Decoder architecture achieves competitive results on auto-regressive language modeling and open domain question answering tasks.","score":7},{"url":"https://www.semanticscholar.org/paper/0ef6fc10b3283e2730843e8a7cbaf342d46237e0","title":"Context Generation Improves Open Domain Question Answering","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Dan Su,M. Patwary,Shrimai Prabhumoye,Peng Xu,R. Prenger,M. Shoeybi,Pascale Fung,Anima Anandkumar,Bryan Catanzaro","id":"0ef6fc10b3283e2730843e8a7cbaf342d46237e0","summary":"This work proposes a two-stage, closed-book QA framework which employs a coarse-to-ﬁne approach to extract relevant knowledge and answer a question and is able to better exploit the stored knowledge in pretrained LMs without adding extra learnable parameters or needing ﬁnetuning.","score":7},{"url":"https://www.semanticscholar.org/paper/a8394722079c251d360fbbc53137753849a64c63","title":"Careful Data Curation Stabilizes In-context Learning","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Ting-Yun Chang,Robin Jia","id":"a8394722079c251d360fbbc53137753849a64c63","summary":"It is shown that curating a carefully chosen subset of training data greatly stabilizes ICL performance, and that stable subset examples are no more diverse than average, and are not outliers in terms of sequence length and perplexity.","score":7},{"url":"https://www.semanticscholar.org/paper/01404a50a3c0c065da3391c60dda49a0cab36251","title":"A Review and a Taxonomy of Edge Machine Learning: Requirements, Paradigms, and Techniques","venue":"","year":2023,"referenceCount":246,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Wenbin Li,Hakim Hacid,Ebtesam Almazrouei,M. Debbah","id":"01404a50a3c0c065da3391c60dda49a0cab36251","summary":"A comprehensive taxonomy and a systematic review of Edge ML techniques are provided, identifying the Edge ML requirements driven by the joint constraints and analyzing how each technique fits into Edge ML by meeting a subset of the identified requirements.","score":7},{"url":"https://www.semanticscholar.org/paper/64da659c0687762359226b4cf455520c78acd165","title":"Neural Language Generation: Formulation, Methods, and Evaluation","venue":"ArXiv","year":2020,"referenceCount":504,"citationCount":17,"influentialCitationCount":2,"publicationDate":"31/07/2020","authors":"Cristina Garbacea,Q. Mei","id":"64da659c0687762359226b4cf455520c78acd165","summary":"There is no standard way to assess the quality of text produced by these generative models, which constitutes a serious bottleneck towards the progress of the field, so this survey will provide an informative overview of formulations, methods, and assessments of neural natural language generation.","score":7},{"url":"https://www.semanticscholar.org/paper/9b5fb07df99b0dd65f3058701d7f017c3a70c144","title":"Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data","venue":"ArXiv","year":2023,"referenceCount":102,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/01/2023","authors":"Jing Wei,Sungdong Kim,Hyunhoon Jung,Young-Ho Kim","id":"9b5fb07df99b0dd65f3058701d7f017c3a70c144","summary":"This work formulated four prompt designs with different structures and personas to explore what design factors of prompts can help steer chatbots to talk naturally and collect data reliably and discusses the opportunities and challenges of building chatbots with LLMs.","score":7},{"url":"https://www.semanticscholar.org/paper/66c52c6ef45cdd77c086996bcaaf01470e82dbd2","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Jean-Baptiste Döderlein,M. Acher,D. Khelladi,B. Combemale","id":"66c52c6ef45cdd77c086996bcaaf01470e82dbd2","summary":"The results showed that varying the input parameters can significantly improve the performance of language models, however, there is a tight dependency when varying the temperature, the prompt and the number of generated solutions, making potentially hard for developers to properly control the parameters to obtain an optimal result.","score":7},{"url":"https://www.semanticscholar.org/paper/538288d24bdad73d831dfed44b706958287ed318","title":"Generating Sequences by Learning to Self-Correct","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":6,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"S. Welleck,Ximing Lu,Peter West,Faeze Brahman,T. Shen,Daniel Khashabi,Yejin Choi","id":"538288d24bdad73d831dfed44b706958287ed318","summary":"SELF - CORRECTION is presented, an approach that decouples an imperfect base generator from a separate corrector that learns to iteratively correct imperfect generations and improves upon the base generator in three diverse generation tasks– mathematical program synthesis, lexically-constrained generation, and toxicity control.","score":7},{"url":"https://www.semanticscholar.org/paper/048ed70192de0232086eb32a95ffb3be8d336c76","title":"Metaphors We Learn By","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/11/2022","authors":"R. Memisevic","id":"048ed70192de0232086eb32a95ffb3be8d336c76","summary":"This essay relates parameter sharing (“weight sharing”) to analogy making and the school of thought of cognitive metaphor, and discusses how recurrent and auto-regressive models can be thought of as extending analogy making from static features to dynamic skills and procedures.","score":7},{"url":"https://www.semanticscholar.org/paper/c66704e04b0ca1f3664a85ad0a505c5d8f933ecd","title":"What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Ori Ram,L. Bezalel,Adi Zicher,Yonatan Belinkov,Jonathan Berant,A. Globerson","id":"c66704e04b0ca1f3664a85ad0a505c5d8f933ecd","summary":"This work proposes to interpret the vector representations produced by dual encoders by projecting them into the model’s vocabulary space, and shows that the resulting distributions over vocabulary tokens are intuitive and contain rich semantic information.","score":7},{"url":"https://www.semanticscholar.org/paper/ab2aa46bbe305627113499ee57958e2e1f55bc25","title":"Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":1,"influentialCitationCount":1,"publicationDate":"20/12/2022","authors":"Weijia Shi,Xiaochuang Han,Hila Gonen,Ari Holtzman,Yulia Tsvetkov,Luke Zettlemoyer","id":"ab2aa46bbe305627113499ee57958e2e1f55bc25","summary":"Analysis of common attributes shared by effective prompts reveals that effective prompts are topically related to the task domain and calibrate the prior probability of label words, and proposes a human readable prompt tuning method based on Langevin dynamics.","score":7},{"url":"https://www.semanticscholar.org/paper/224b8cd8c31cfa86c2a84bec3a65d9ba44f38280","title":"iPrompt: Explaining Data Patterns in Natural Language via Interpretable Autoprompting","venue":"","year":2023,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Chandan Singh,John X. Morris,J. Aneja,Alexander M. Rush,Jianfeng Gao","id":"224b8cd8c31cfa86c2a84bec3a65d9ba44f38280","summary":"Inter interpretable autoprompting ( iPrompt ), an algorithm that generates a natural language string explaining the data that is human-interpretable and able to aid in scientiﬁc discovery.","score":7},{"url":"https://www.semanticscholar.org/paper/259b7a01700c39d5669e88d1434873ea38a13528","title":"In-Context Policy Iteration","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":1,"influentialCitationCount":0,"publicationDate":"07/10/2022","authors":"Ethan Brooks,Logan Walls,Richard L. Lewis,Satinder Singh","id":"259b7a01700c39d5669e88d1434873ea38a13528","summary":"An algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients, and is presented as a policy-iteration method in which the prompt content is the entire locus of learning.","score":7},{"url":"https://www.semanticscholar.org/paper/478f85aa1731c3929f020a39cff8922071834bf7","title":"Broken Neural Scaling Laws","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Ethan Caballero,Kshitij Gupta,I. Rish,David Krueger","id":"478f85aa1731c3929f020a39cff8922071834bf7","summary":"A smoothly broken power law functional form that accurately models and extrapolates the scaling behaviors of deep neural networks for a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings is presented.","score":7},{"url":"https://www.semanticscholar.org/paper/ebd4bec684808aff360b5f255d15c0d112ba13d3","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Zhangir Azerbayev,Ansong Ni,Hailey Schoelkopf,Dragomir R. Radev","id":"ebd4bec684808aff360b5f255d15c0d112ba13d3","summary":"This paper proposes explicit knowledge transfer (EKT), which uses the few-shot capabilities of a teacher LLM to create NL-code pairs that are filter for correctness and fine-tune the student on, and finds that EKT not only yields better performance than training with expert iteration, but also outperforms knowledge distillation, another form of knowledge transfer.","score":7},{"url":"https://www.semanticscholar.org/paper/70feb009bc1e8b1cb8dff64bf9fd67789636438b","title":"From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models","venue":"ArXiv","year":2022,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Jiaxian Guo,Junnan Li,Dongxu Li,A. M. H. Tiong,Boyang Li,Dacheng Tao,Steven Hoi","id":"70feb009bc1e8b1cb8dff64bf9fd67789636438b","summary":"Img2Prompt is a plug-and-play module that provides the prompts that can bridge the aforementioned modality and task disconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end training.","score":7},{"url":"https://www.semanticscholar.org/paper/f2a68932d9238209cf870058071be6903ae945a4","title":"Compositional Exemplars for In-context Learning","venue":"ArXiv","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/02/2023","authors":"Jiacheng Ye,Zhiyong Wu,Jiangtao Feng,Tao Yu,Lingpeng Kong","id":"f2a68932d9238209cf870058071be6903ae945a4","summary":"This work proposes CEIL (Compositional Exemplars for In-context Learning), which is instantiated by Determinantal Point Processes (DPPs) to model the interaction between the given input and in-context examples, and optimized through a carefully-designed contrastive learning objective to obtain preference from LMs.","score":7},{"url":"https://www.semanticscholar.org/paper/914254fac74a2da051cccf6ca16afcaad416a079","title":"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model","venue":"ArXiv","year":2022,"referenceCount":81,"citationCount":10,"influentialCitationCount":1,"publicationDate":"02/08/2022","authors":"Saleh Soltan,Shankar Ananthakrishnan,Jack G. M. FitzGerald,Rahul Gupta,Wael Hamza,Haidar Khan,Charith S. Peris,Stephen Rawls,Andrew Rosenbaum,Anna Rumshisky,Chandan Prakash,Mukund Sridhar,Fabian Triefenbach,Apurv Verma,G. Tur,Premkumar Natarajan","id":"914254fac74a2da051cccf6ca16afcaad416a079","summary":"It is demonstrated that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more eﬃcient few-shot learners than decoder-only models on various tasks.","score":7},{"url":"https://www.semanticscholar.org/paper/8345d757e9127eff382d5285fef99312eaf283cd","title":"WikiWhy: Answering and Explaining Cause-and-Effect Questions","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Matthew Ho,Aditya Sharma,Justin Chang,Michael Stephen Saxon,Sharon Levy,Yujie Lu,William Yang Wang","id":"8345d757e9127eff382d5285fef99312eaf283cd","summary":"WIKIWHY is introduced, a QA dataset built around a novel auxiliary task: explaining why an answer is true in natural language, which demands rigorous explicit rationales for each answer to demonstrate the acquisition of implicit commonsense knowledge, which is unlikely to be easily memorized.","score":7},{"url":"https://www.semanticscholar.org/paper/12f0560f2973a56f70e5503ba260725161fc46a8","title":"Reinforced Question Rewriting for Conversational Question Answering","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Zhiyu Chen,Jie Zhao,Anjie Fang,B. Fetahu,O. Rokhlenko,S. Malmasi","id":"12f0560f2973a56f70e5503ba260725161fc46a8","summary":"Experiments show that the proposed approach can effectively improve QA performance over baselines for both extractive and retrieval QA, and human evaluation shows that the method can generate more accurate and detailed rewrites when compared to human annotations.","score":7},{"url":"https://www.semanticscholar.org/paper/fba0b0817dbc8200b41a1de22654b54b778a11e9","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models","venue":"ArXiv","year":2023,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Toufique Ahmed,Supriyo Ghosh,Chetan Bansal,T. Zimmermann,Xuchao Zhang,S. Rajmohan","id":"fba0b0817dbc8200b41a1de22654b54b778a11e9","summary":"This work does the first large-scale study to evaluate the effectiveness of GPT-3.x models for helping engineers root cause and mitigate production incidents and compares several large language models in zero-shot, fine-tuned and multi-task setting using semantic and lexical metrics.","score":7},{"url":"https://www.semanticscholar.org/paper/9b9b292bd7814fb9c62b8609e8c28c8693a078f5","title":"EditEval: An Instruction-Based Benchmark for Text Improvements","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/09/2022","authors":"Jane Dwivedi-Yu,Timo Schick,Zhengbao Jiang,M. Lomeli,Patrick Lewis,Gautier Izacard,Edouard Grave,Sebastian Riedel,Fabio Petroni","id":"9b9b292bd7814fb9c62b8609e8c28c8693a078f5","summary":"An instruction-based, benchmark and evaluation suite that leverages high-quality existing and new datasets for automatic evaluation of editing capabilities such as making text more cohesive and paraphrasing and un-lock future research in developing models capable of iterative and more controllable editing.","score":7},{"url":"https://www.semanticscholar.org/paper/d9d12205007ac48b03d921225f9cdaf90f7c3fdd","title":"Model Criticism for Long-Form Text Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":84,"citationCount":2,"influentialCitationCount":0,"publicationDate":"16/10/2022","authors":"Yuntian Deng,Volodymyr Kuleshov,Alexander M. Rush","id":"d9d12205007ac48b03d921225f9cdaf90f7c3fdd","summary":"This work proposes to apply a statistical tool, model criticism in latent space, to evaluate the high-level structure of the generated text and finds that transformer-based language models are able to capture topical structures but have a harder time maintaining structural coherence or modeling coreference.","score":7},{"url":"https://www.semanticscholar.org/paper/7ebdc4ed93e37e7d3691085f4d08c495557ba71b","title":"A survey on knowledge-enhanced multimodal learning","venue":"ArXiv","year":2022,"referenceCount":259,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/11/2022","authors":"Maria Lymperaiou,G. Stamou","id":"7ebdc4ed93e37e7d3691085f4d08c495557ba71b","summary":"The current survey aims to unify the fields of VL representation learning and knowledge graphs, and provides a taxonomy and analysis of knowledge-enhanced VL models.","score":7},{"url":"https://www.semanticscholar.org/paper/46d0a832fada6147bceb0bd4e39928e482733246","title":"How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective","venue":"ArXiv","year":2022,"referenceCount":75,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"Guang Yang,Yu Zhou,Wenhua Yang,Tao Yue,Xiang Chen,Taolue Chen","id":"46d0a832fada6147bceb0bd4e39928e482733246","summary":"The potential of benefiting from method names to enhance the performance of PCGMs, from a model robustness perspective, is studied and a novel approach is proposed, named RADAR (neuRAl coDe generAtor Robustifier).","score":7},{"url":"https://www.semanticscholar.org/paper/ef52ba73fbf41eed320a479f5736e127d3a06049","title":"KPT: Keyword-guided Pre-training for Grounded Dialog Generation","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/12/2022","authors":"Qi Zhu,Fei Mi,Zheng Zhang,Yasheng Wang,Yitong Li,Xin Jiang,Qun Liu,Xiaoyan Zhu,Minlie Huang","id":"ef52ba73fbf41eed320a479f5736e127d3a06049","summary":"This work proposes KPT (Keyword-guided Pre-Training), a novel self-supervised pre-training method for grounded dialog generation without relying on extra knowledge annotation, and demonstrates that KPT consistently outperforms state-of-the-art methods on these tasks with diverse grounding knowledge.","score":7},{"url":"https://www.semanticscholar.org/paper/6d4e540e1bed26679097139bf90c8652919e4e5c","title":"Explanation Regeneration via Information Bottleneck","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Qintong Li,Zhiyong Wu,Lingpeng Kong,Wei Bi","id":"6d4e540e1bed26679097139bf90c8652919e4e5c","summary":"This work develops an information bottleneck method EIB that regenerates the free-text explanation by polishing the single-pass output from the pretrained language model but retaining the information that supports the contents being explained.","score":7},{"url":"https://www.semanticscholar.org/paper/d7ea898cc97754e06d209df0fd55ab60250601f2","title":"Contrastive Learning Reduces Hallucination in Conversations","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Weiwei Sun,Zhengliang Shi,Shen Gao,Pengjie Ren,M. de Rijke,Z. Ren","id":"d7ea898cc97754e06d209df0fd55ab60250601f2","summary":"A novel mixed contrastive objective is proposed to explicitly optimize the implicit knowledge elicitation process of LMs, and thus reduce their hallucination in conversations, and it is shown that MixCL achieves comparable performance to state-of-the-art KB-based approaches while enjoying notable advantages in terms of efficiency and scalability.","score":7},{"url":"https://www.semanticscholar.org/paper/9a0ffb6156763be319a1ea39c9fac4080bc61182","title":"Reinforced Clarification Question Generation with Defeasibility Rewards for Disambiguating Social and Moral Situations","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Valentina Pyatkin,Jena D. Hwang,Vivek Srikumar,Ximing Lu,Liwei Jiang,Yejin Choi,Chandra Bhagavatula","id":"9a0ffb6156763be319a1ea39c9fac4080bc61182","summary":"","score":7},{"url":"https://www.semanticscholar.org/paper/8cd67f8671a9469037933c00f983491d1712f18e","title":"A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding","venue":"ArXiv","year":2023,"referenceCount":346,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/01/2023","authors":"D. O. Cajueiro,A. G. Nery,Igor Tavares,M. K. D. Melo,Silvia A. dos Reis,L. Weigang,V. R. R. Celestino","id":"8cd67f8671a9469037933c00f983491d1712f18e","summary":"This work presents the diverse approaches to ATS guided by the mechanisms they use to generate a summary and presents an empirical exploration of these methods using the CNN Corpus dataset that provides golden summaries for extractive and abstractive methods.","score":7},{"url":"https://www.semanticscholar.org/paper/1cdfa7c3465943a295f8df2d2097c4bb3e222426","title":"Rationalization for Explainable NLP: A Survey","venue":"ArXiv","year":2023,"referenceCount":97,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/01/2023","authors":"Sai Gurrapu,Ajay Kulkarni,Lifu Huang,Ismini Lourentzou,Laura J. Freeman,Feras A. Batarseh","id":"1cdfa7c3465943a295f8df2d2097c4bb3e222426","summary":"This survey presents available methods, explainable evaluations, code, and datasets used across various NLP tasks that use rationalization, and a new subfield in Explainable AI (XAI), namely, Rational AI (RAI), is introduced to advance the current state of rationalization.","score":7},{"url":"https://www.semanticscholar.org/paper/572e82dfdde0f72f9448caf72fdc68a233da6659","title":"Open Problems in Applied Deep Learning","venue":"ArXiv","year":2023,"referenceCount":280,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/01/2023","authors":"M. Raissi","id":"572e82dfdde0f72f9448caf72fdc68a233da6659","summary":"This work formulates the machine learning mechanism as a bi-level optimization problem, and considers case specific and complex cases while increasing the level of complexity from supervised learning to semi-supervised, self-super supervised, un supervised, few-shot, federated, reinforcement, and physics-informed learning.","score":7},{"url":"https://www.semanticscholar.org/paper/6a4901e3bc255dd9c14d1e0d9e76e46cdd0f0ac0","title":"Retrieval-augmented Image Captioning","venue":"","year":2023,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"R. Ramos,Desmond Elliott,Bruno Martins","id":"6a4901e3bc255dd9c14d1e0d9e76e46cdd0f0ac0","summary":"A new approach to image captioning that generates sentences given the input image and a set of captions retrieved from a datastore, as opposed to the image alone, is presented.","score":7},{"url":"https://www.semanticscholar.org/paper/6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":84,"citationCount":10,"influentialCitationCount":2,"publicationDate":"24/05/2022","authors":"Linlu Qiu,Peter Shaw,Panupong Pasupat,Tianze Shi,Jonathan Herzig,Emily Pitler,Fei Sha,Kristina Toutanova","id":"6e10343767ab09dde83cf99ea3442907402a9810","summary":"Limits of current techniques for effectively leveraging model scale for compositional generalization are highlighted, while the analysis also suggests promising directions for future work.","score":7},{"url":"https://www.semanticscholar.org/paper/1be5064847c7a5e9a7e516e4c21aa8f4f8ab2eb5","title":"Multi-Game Decision Transformers","venue":"ArXiv","year":2022,"referenceCount":84,"citationCount":37,"influentialCitationCount":10,"publicationDate":"30/05/2022","authors":"Kuang-Huei Lee,Ofir Nachum,Mengjiao Yang,L. Lee,Daniel Freeman,Winnie Xu,S. Guadarrama,Ian S. Fischer,Eric Jang,H. Michalewski,Igor Mordatch","id":"1be5064847c7a5e9a7e516e4c21aa8f4f8ab2eb5","summary":"It is shown that a single transformer-based model – with a single set of weights – trained purely ofﬂine can play a suite of up to 46 Atari games simultaneously at close-to-human performance.","score":7},{"url":"https://www.semanticscholar.org/paper/7cf4f8cb8b4a373d869e785b79160dda7a49a250","title":"Exploring The Landscape of Distributional Robustness for Question Answering Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":78,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/10/2022","authors":"Anas Awadalla,Mitchell Wortsman,Gabriel Ilharco,Sewon Min,Ian H. Magnusson,Hannaneh Hajishirzi,Ludwig Schmidt","id":"7cf4f8cb8b4a373d869e785b79160dda7a49a250","summary":"This investigation spans over 350 models and 16 question answering datasets, including a diverse set of architectures, model sizes, and adaptation methods, and indicates that zero-shot and in-context learning methods are more robust to distribution shifts than fully fine-tuned models.","score":7},{"url":"https://www.semanticscholar.org/paper/d3ec78e4eeb88d7d292a8085d5cd18b254379816","title":"Inverse scaling can become U-shaped","venue":"ArXiv","year":2022,"referenceCount":15,"citationCount":4,"influentialCitationCount":1,"publicationDate":"03/11/2022","authors":"Jason Wei,Yi Tay,Quoc V. Le","id":"d3ec78e4eeb88d7d292a8085d5cd18b254379816","summary":"","score":7},{"url":"https://www.semanticscholar.org/paper/b3598a2f6c4e8a9e6d89de10b46bfad8e016ab2e","title":"Astronomia ex machina: a history, primer, and outlook on neural networks in astronomy","venue":"ArXiv","year":2022,"referenceCount":266,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Michael J. Smith,J. Geach","id":"b3598a2f6c4e8a9e6d89de10b46bfad8e016ab2e","summary":"","score":7},{"url":"https://www.semanticscholar.org/paper/094a2e887a983b4891f3bb5b4cafd68500508d92","title":"'Rarely' a problem? Language models exhibit inverse scaling in their predictions following 'few'-type quantifiers","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"J. Michaelov,B. Bergen","id":"094a2e887a983b4891f3bb5b4cafd68500508d92","summary":"Not only do the models perform poorly on few -type quantiﬁers, but overall the larger the model, the worse its performance, and it is argued that decreasing performance of larger models may challenge uses of Language Models as the basis for Natural Language Systems.","score":7},{"url":"https://www.semanticscholar.org/paper/ac9bd7da3331d699008444925ef1121cc7178dc3","title":"Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Hritik Bansal,Karthik Gopalakrishnan,Saket Dingliwal,S. Bodapati,Katrin Kirchhoff,D. Roth","id":"ac9bd7da3331d699008444925ef1121cc7178dc3","summary":"The hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components is investigated and a small set of attention heads score highly on their ability to perform primitive induction operations associated with in- context learning, namely, preﬁx matching and copying.","score":7},{"url":"https://www.semanticscholar.org/paper/65043331280851b767df238fabca32f6c52f1148","title":"Training Trajectories of Language Models Across Scales","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"M. Xia,Mikel Artetxe,Chunting Zhou,Xi Victoria Lin,Ramakanth Pasunuru,Danqi Chen,Luke Zettlemoyer,V. Stoyanov","id":"65043331280851b767df238fabca32f6c52f1148","summary":"Analyzing the intermediate training checkpoints of differently sized OPT models shows that perplexity is more predictive of model behaviors than model size or training computation.","score":7},{"url":"https://www.semanticscholar.org/paper/3d849136e0070f6d038dd96985ed67ead5aedb69","title":"Locally Typical Sampling","venue":"Transactions of the Association for Computational Linguistics","year":2022,"referenceCount":61,"citationCount":10,"influentialCitationCount":6,"publicationDate":"01/02/2022","authors":"Clara Meister,Tiago Pimentel,Gian Wiher,Ryan Cotterell","id":"3d849136e0070f6d038dd96985ed67ead5aedb69","summary":"This work posit that the abstraction of natural language generation as a discrete stochastic process—which allows for an information-theoretic analysis—can provide new insights into the behavior of probabilistic language generators, for example, why high-probability texts can be dull or repetitive.","score":7},{"url":"https://www.semanticscholar.org/paper/3b16a709a5b18e52b0b6741cbc3c0e68a03ecd8e","title":"The unreasonable effectiveness of few-shot learning for machine translation","venue":"ArXiv","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Xavier García,Yamini Bansal,Colin Cherry,George F. Foster,M. Krikun,Fan Feng,Melvin Johnson,Orhan Firat","id":"3b16a709a5b18e52b0b6741cbc3c0e68a03ecd8e","summary":"It is shown that with only 5 examples of high-quality translation data shown at inference, a transformer decoder-only model trained solely with self-supervised learning, is able to match specialized supervised state-of-the-art models as well as more general commercial translation systems.","score":7},{"url":"https://www.semanticscholar.org/paper/ccf15b75d3ed3287c0ac524666578ed785bff1a3","title":"Big Little Transformer Decoder","venue":"","year":2023,"referenceCount":78,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Sehoon Kim,Karttikeya Mangalam,J. Malik,Michael W. Mahoney,A. Gholami,K. Keutzer","id":"ccf15b75d3ed3287c0ac524666578ed785bff1a3","summary":"Big Little Decoder (BiLD) is proposed, a framework that can improve inference efficiency and latency for a wide range of text generation applications and is fully plug-and-play as it does not require any training or modifications to model architectures.","score":7},{"url":"https://www.semanticscholar.org/paper/9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","title":"20Q: Overlap-Free World Knowledge Benchmark for Language Models","venue":"IEEE Games Entertainment Media Conference","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans","id":"9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","summary":"20Q is introduced, a novel benchmark using the Twenty Questions game to evaluate world knowledge and common sense of language models and shows that in-context learning is inefficient for evaluating language models’ world knowledge — fine-tuning is necessary to show their true capabilities.","score":7},{"url":"https://www.semanticscholar.org/paper/374dd173491a59a10bbb2b3519ebcfe3649f529d","title":"Teaching Models to Express Their Uncertainty in Words","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":9,"influentialCitationCount":3,"publicationDate":"28/05/2022","authors":"Stephanie C. Lin,Jacob Hilton,Owain Evans","id":"374dd173491a59a10bbb2b3519ebcfe3649f529d","summary":"It is shown that a GPT-3 model can learn to express uncertainty about its own answers in natural language – without use of model logits – and is sensitive to uncertainty in its own Answers, rather than imitating human examples.","score":7},{"url":"https://www.semanticscholar.org/paper/6fb0b072c4fcdc0c78218bfd1b181fd562f07cd2","title":"COMPS: Conceptual Minimal Pair Sentences for testing Robust Property Knowledge and its Inheritance in Pre-trained Language Models","venue":"","year":2022,"referenceCount":87,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/10/2022","authors":"Kanishka Misra,J. Rayz,Allyson Ettinger","id":"6fb0b072c4fcdc0c78218bfd1b181fd562f07cd2","summary":"It is found that PLMs can demonstrate behavior consistent with property inheritance to a great extent, but fail in the presence of distracting information, which decreases the performance of many models, sometimes even below chance.","score":7},{"url":"https://www.semanticscholar.org/paper/be09ed6cd73654a23f78416433a1b23ea623ea79","title":"Symbolic Behaviour in Artificial Intelligence","venue":"ArXiv","year":2021,"referenceCount":139,"citationCount":19,"influentialCitationCount":0,"publicationDate":"05/02/2021","authors":"Adam Santoro,Andrew Kyle Lampinen,K. Mathewson,T. Lillicrap,David Raposo","id":"be09ed6cd73654a23f78416433a1b23ea623ea79","summary":"This work argues that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them, and suggests that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge.","score":7},{"url":"https://www.semanticscholar.org/paper/7fa273f450251523e6b7fcc2eb3fdbdfd4a30493","title":"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":172,"citationCount":68,"influentialCitationCount":18,"publicationDate":"18/04/2021","authors":"Qinyuan Ye,Bill Yuchen Lin,Xiang Ren","id":"7fa273f450251523e6b7fcc2eb3fdbdfd4a30493","summary":"This paper presents the NLP Few-shot Gym, a repository of 160 diverse few-shot NLP tasks created from open-access NLP datasets and converted to a unified text-to-text format, and reveals that the few- shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks.","score":7},{"url":"https://www.semanticscholar.org/paper/d83c7aa12420d5d035f43d7737bfa6893e9e2c61","title":"Exploring Universal Intrinsic Task Subspace via Prompt Tuning","venue":"","year":2021,"referenceCount":125,"citationCount":3,"influentialCitationCount":0,"publicationDate":"15/10/2021","authors":"Yujia Qin,Xiaozhi Wang,Yusheng Su,Yankai Lin,Ning Ding,Jing Yi,Weize Chen,Zhiyuan Liu,Juanzi Li,Lei Hou,Peng Li,Maosong Sun,Jie Zhou","id":"d83c7aa12420d5d035f43d7737bfa6893e9e2c61","summary":"Evidence is shown indicating that the adaptations of PLMs to various few-shot tasks can be reparameterized as optimizing only a few free parameters in a low-dimensional intrinsic task subspace, which may help to understand why PLMs could easily adapt to various NLP tasks with small-scale data.","score":7},{"url":"https://www.semanticscholar.org/paper/3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e","title":"A General Language Assistant as a Laboratory for Alignment","venue":"ArXiv","year":2021,"referenceCount":52,"citationCount":57,"influentialCitationCount":3,"publicationDate":"01/12/2021","authors":"Amanda Askell,Yuntao Bai,Anna Chen,Dawn Drain,Deep Ganguli,T. Henighan,Andy Jones,Nicholas Joseph,Benjamin Mann,Nova DasSarma,Nelson Elhage,Zac Hatfield-Dodds,Danny Hernandez,John Kernion,Kamal Ndousse,Catherine Olsson,Dario Amodei,Tom B. Brown,Jack Clark,Sam McCandlish,C. Olah,Jared Kaplan","id":"3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e","summary":"A ‘preference model pre-training’ stage of training is studied, with the goal of improving sample efﬁciency when ﬁnetuning on human preferences, and investigating scaling trends for several training objectives relevant to alignment.","score":7},{"url":"https://www.semanticscholar.org/paper/2a83a92b08e0f3873d07162c73c67e533321112e","title":"Aligning Generative Language Models with Human Values","venue":"NAACL-HLT","year":2022,"referenceCount":58,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Ruibo Liu,Ge Zhang,Xinyu Feng,Soroush Vosoughi","id":"2a83a92b08e0f3873d07162c73c67e533321112e","summary":"S ENSEI is a new reinforcement learning based method that can embed human values judgements into each step of language generation and achieves higher alignment performance in terms of both automatic and human evaluations, but also shows improvements on robustness and transfer learning on unseen human values.","score":7},{"url":"https://www.semanticscholar.org/paper/47df3fd32d00220c85c2c51a571254fd99b2ecc7","title":"MetaICL: Learning to Learn In Context","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":136,"citationCount":77,"influentialCitationCount":18,"publicationDate":"29/10/2021","authors":"Sewon Min,M. Lewis,Luke Zettlemoyer,Hannaneh Hajishirzi","id":"47df3fd32d00220c85c2c51a571254fd99b2ecc7","summary":"This work introduces MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks.","score":7},{"url":"https://www.semanticscholar.org/paper/605c32428861eb26b8631617b8f6c97a850d6a04","title":"True Few-Shot Learning with Prompts—A Real-World Perspective","venue":"Transactions of the Association for Computational Linguistics","year":2021,"referenceCount":86,"citationCount":13,"influentialCitationCount":2,"publicationDate":"26/11/2021","authors":"Timo Schick,Hinrich Schütze","id":"605c32428861eb26b8631617b8f6c97a850d6a04","summary":"An extensive study of Pet, a method that combines textual instructions with example-based finetuning, shows that, if correctly configured, Pet performs strongly in true few-shot settings without a dev set and underpin the belief that learning from instructions will play an important role on the path towards human-like few- shot learning capabilities.","score":7},{"url":"https://www.semanticscholar.org/paper/9cbc044e315cdefe9a255119037ac7c23e9abdd5","title":"Predictability and Surprise in Large Generative Models","venue":"Conference on Fairness, Accountability and Transparency","year":2022,"referenceCount":92,"citationCount":29,"influentialCitationCount":2,"publicationDate":"15/02/2022","authors":"Deep Ganguli,Danny Hernandez,Liane Lovitt,Nova DasSarma,T. Henighan,Andy Jones,Nicholas Joseph,John Kernion,Benjamin Mann,Amanda Askell,Yuntao Bai,Anna Chen,Tom Conerly,Dawn Drain,Nelson Elhage,Sheer El Showk,Stanislav Fort,Zac Hatfield-Dodds,Scott Johnston,S. Kravec,Neel Nanda,Kamal Ndousse,Catherine Olsson,Daniela Amodei,Dario Amodei,Tom B. Brown,Jared Kaplan,Sam McCandlish,C. Olah,Jack Clark","id":"9cbc044e315cdefe9a255119037ac7c23e9abdd5","summary":"This paper highlights a counterintuitive property of large-scale generative models, which have a paradoxical combination of predictable loss on a broad training distribution, and unpredictable specific capabilities, inputs, and outputs, and analyzed how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment.","score":7},{"url":"https://www.semanticscholar.org/paper/2145fcceeb69385e108bf1796d52f974854d4c0b","title":"ZeroGen: Efficient Zero-shot Learning via Dataset Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":82,"citationCount":18,"influentialCitationCount":1,"publicationDate":"16/02/2022","authors":"Jiacheng Ye,Jiahui Gao,Qintong Li,Hang Xu,Jiangtao Feng,Zhiyong Wu,Tao Yu,Lingpeng Kong","id":"2145fcceeb69385e108bf1796d52f974854d4c0b","summary":"It is argued that ZeroGen can also provide useful insights from the perspective of data-free model-agnostic knowledge distillation, and unreferenced text generation evaluation.","score":7},{"url":"https://www.semanticscholar.org/paper/1fafaccebc4a74898a74c606f846318c4c2c7536","title":"On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":31,"citationCount":13,"influentialCitationCount":1,"publicationDate":"28/04/2022","authors":"Seongjin Shin,Sang-Woo Lee,Hwijeen Ahn,Sungdong Kim,Hyoungseok Kim,Boseop Kim,Kyunghyun Cho,Gichang Lee,W. Park,Jung-Woo Ha,Nako Sung","id":"1fafaccebc4a74898a74c606f846318c4c2c7536","summary":"This in-depth investigation of the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model finds that in- context learning performance heavily depends on the corpus domain source.","score":7},{"url":"https://www.semanticscholar.org/paper/08c43a7b09cead543bca41d2f4f64260f3b9d574","title":"Language Models in the Loop: Incorporating Prompting into Weak Supervision","venue":"ArXiv","year":2022,"referenceCount":71,"citationCount":6,"influentialCitationCount":0,"publicationDate":"04/05/2022","authors":"Ryan Smith,Jason Alan Fries,Braden Hancock,Stephen H. Bach","id":"08c43a7b09cead543bca41d2f4f64260f3b9d574","summary":"The experimental evaluation shows that prompting large language models within a weak supervision framework can provide gains in accuracy, and that this approach produces classiﬁers with comparable or superior accuracy to those trained from hand-engineered rules.","score":7},{"url":"https://www.semanticscholar.org/paper/99752e255a866484291866a5ff5cf94e96d6bdc4","title":"Is a Question Decomposition Unit All We Need?","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":40,"citationCount":8,"influentialCitationCount":1,"publicationDate":"25/05/2022","authors":"Pruthvi H. Patel,Swaroop Mishra,Mihir Parmar,Chitta Baral","id":"99752e255a866484291866a5ff5cf94e96d6bdc4","summary":"The findings indicate that Human-in-the-loop Question Decomposition (HQD) can potentially provide an alternate path to building large LMs and provides a viable option to involve people in NLP research in a meaningful way.","score":7},{"url":"https://www.semanticscholar.org/paper/096c2791c3dd4b123333e324ce88cd97661ffd3f","title":"Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning","venue":"ArXiv","year":2022,"referenceCount":76,"citationCount":7,"influentialCitationCount":0,"publicationDate":"29/05/2022","authors":"Xiang Chen,Lei Li,Ningyu Zhang,Xiaozhuan Liang,Shumin Deng,Chuanqi Tan,Fei Huang,Luo Si,Huajun Chen","id":"096c2791c3dd4b123333e324ce88cd97661ffd3f","summary":"This work develops R ETRO P ROMPT with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization, which can reduce the reliance of language models on memorization and improve generalization for downstream tasks.","score":7},{"url":"https://www.semanticscholar.org/paper/4bc51cb3ba793de7c06bb77770f2f9a91ff809f7","title":"MVP: Multi-task Supervised Pre-training for Natural Language Generation","venue":"ArXiv","year":2022,"referenceCount":187,"citationCount":3,"influentialCitationCount":0,"publicationDate":"24/06/2022","authors":"Tianyi Tang,Junyi Li,Wayne Xin Zhao,Ji-rong Wen","id":"4bc51cb3ba793de7c06bb77770f2f9a91ff809f7","summary":"This work collects a large-scale natural language generation corpus, MVPCorpus, from 77 datasets over 11 diverse NLG tasks, and unifies these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner.","score":7},{"url":"https://www.semanticscholar.org/paper/2c676ecdc954ec24e1907c76accb1e8ac06deec0","title":"Data-Efficiency with a Single GPU: An Exploration of Transfer Methods for Small Language Models","venue":"ArXiv","year":2022,"referenceCount":16,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/10/2022","authors":"Alon Albalak,Akshat Shrivastava,Chinnadhurai Sankar,Adithya Sagar,Mike Ross","id":"2c676ecdc954ec24e1907c76accb1e8ac06deec0","summary":"It is demonstrated that instruction tuning provides a modest 2% performance improvement for small models, contrary to prior works on large models.","score":7},{"url":"https://www.semanticscholar.org/paper/6494c6149e5036c09ee92da9fd67cbecc998a52f","title":"PCFG-based Natural Language Interface Improves Generalization for Controlled Text Generation","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Jingyu Zhang,James R. Glass,Tianxing He","id":"6494c6149e5036c09ee92da9fd67cbecc998a52f","summary":"This work proposes a natural language (NL) interface, where a PCFG isCrafted to embed the control attributes into natural language commands, and proposes variants of existing CTG models that take commands as input.","score":7},{"url":"https://www.semanticscholar.org/paper/bb15f3727f827a3cb88b5d3ca48415c09b40a88f","title":"What Language Model to Train if You Have One Million GPU Hours?","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":80,"citationCount":15,"influentialCitationCount":1,"publicationDate":"27/10/2022","authors":"Teven Le Scao,Thomas Wang,Daniel Hesslow,Lucile Saulnier,Stas Bekman,Saiful Bari,Stella Rose Biderman,Hady ElSahar,Niklas Muennighoff,Jason Phang,Ofir Press,Colin Raffel,Victor Sanh,Sheng Shen,Lintang Sutawika,Jaesung Tae,Zheng Xin Yong,Julien Launay,Iz Beltagy","id":"bb15f3727f827a3cb88b5d3ca48415c09b40a88f","summary":"An ablation study at the billion-parameter scale comparing different modeling practices and their impact on zero-shot generalization is performed, and the scaling behaviour of Transformers is considered to choose the target model size, shape, and training setup.","score":7},{"url":"https://www.semanticscholar.org/paper/68e401c6f90ce421d46b0899458f3c103b4aa29a","title":"Solving Math Word Problem via Cooperative Reasoning induced Language Models","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":1,"influentialCitationCount":0,"publicationDate":"28/10/2022","authors":"Xinyu Zhu,Junjie Wang,Lin Zhang,Yuxiang Zhang,Ruyi Gan,Jiaxing Zhang,Yujiu Yang","id":"68e401c6f90ce421d46b0899458f3c103b4aa29a","summary":"A cooperative reasoning-induced PLM for solving MWPs is developed, resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the veriﬁer, and decent improvement over state-of-the-art methods, up to 9.8% increase over best baselines.","score":7},{"url":"https://www.semanticscholar.org/paper/5de4860323ffaba9b7f5aefeedc2d8db2a529a96","title":"Zero-Label Prompt Selection","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"Chonghua Liao,Yanan Zheng,Zhilin Yang","id":"5de4860323ffaba9b7f5aefeedc2d8db2a529a96","summary":"A Zero-Label Prompt Selection (ZPS) method that selects prompts without any labeled data or gradient update is proposed that improves over prior methods by a sizeable margin in zero-label performance.","score":7},{"url":"https://www.semanticscholar.org/paper/2a0953e6aa8a8c4b88928957338e93f8636ebe84","title":"Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Saurabh Kulshreshtha,Anna Rumshisky","id":"2a0953e6aa8a8c4b88928957338e93f8636ebe84","summary":"A new framework for applying chain-of-thought inspired structured rationale generation to multi-hop question generation under a very low supervision regime is introduced, treating each reasoning step as a separate task to be performed by a generative language model.","score":7},{"url":"https://www.semanticscholar.org/paper/075129a9380730f985beedd5fe7c78ff8e35cb9b","title":"Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/12/2022","authors":"Aran Komatsuzaki,J. Puigcerver,J. Lee-Thorp,Carlos Riquelme Ruiz,Basil Mustafa,J. Ainslie,Yi Tay,M. Dehghani,N. Houlsby","id":"075129a9380730f985beedd5fe7c78ff8e35cb9b","summary":"This work proposes sparse upcycling -- a simple way to reuse sunk training costs by initializing a sparsely activated Mixture-of-Experts model from a dense checkpoint, and shows that sparsely upcycled T5 Base, Large, and XL language models and Vision Transformer Base and Large models, respectively, significantly outperform their dense counterparts on SuperGLUE and ImageNet.","score":7},{"url":"https://www.semanticscholar.org/paper/9f61d366b9d00becb25f7823997c626c6b1d5c16","title":"One Embedder, Any Task: Instruction-Finetuned Text Embeddings","venue":"ArXiv","year":2022,"referenceCount":78,"citationCount":2,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Hongjin Su,Weijia Shi,Jungo Kasai,Yizhong Wang,Yushi Hu,Mari Ostendorf,Wen-tau Yih,Noah A. Smith,Luke Zettlemoyer,Tao Yu","id":"9f61d366b9d00becb25f7823997c626c6b1d5c16","summary":"I NSTRUCT OR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training, and achieves state-of-the-art performance on diverse datasets.","score":7},{"url":"https://www.semanticscholar.org/paper/397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e","title":"Layer Norm Attention Layer Norm Router from scratch MLP 1 MLP 2 MLP","venue":"","year":2023,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"C. Riquelme,Basil Mustafa,J. Ainslie","id":"397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e","summary":"This work proposes sparse upcycling – a simple way to reuse sunk training costs by initializing a sparsely activated Mixture-of-Experts model from a dense checkpoint, and shows that sparsely upcycled T5 Base, Large, and XL language models and Vision Transformer Base and Large models, respectively, significantly outperform their dense counterparts on SuperGLUE and ImageNet.","score":7},{"url":"https://www.semanticscholar.org/paper/c879413103f8950bdd414c7f60a39bd7748c9be8","title":"Prompting Large Language Model for Machine Translation: A Case Study","venue":"ArXiv","year":2023,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/01/2023","authors":"Biao Zhang,B. Haddow,Alexandra Birch","id":"c879413103f8950bdd414c7f60a39bd7748c9be8","summary":"A systematic study on prompting strategies for translation, examining various factors for prompt template and demonstration example selection, and exploring the use of monolingual data and the feasibility of cross-lingual, cross-domain, and sentence-to-document transfer learning in prompting.","score":7},{"url":"https://www.semanticscholar.org/paper/0e3d1457a66e442fae46c8f96886dc76aef3b085","title":"Offsite-Tuning: Transfer Learning without Full Model","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/02/2023","authors":"Guangxuan Xiao,Ji Lin,Song Han","id":"0e3d1457a66e442fae46c8f96886dc76aef3b085","summary":"Offsite-tuning can achieve comparable accuracy as full model fine- Tuning while being privacy-preserving and efficient, achieving 6.5x speedup and 5.6x memory reduction.","score":7},{"url":"https://www.semanticscholar.org/paper/0cf694b8f85ab2e11d45595de211a15cfbadcd22","title":"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks","venue":"ArXiv","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/02/2023","authors":"Daniel Kang,Xuechen Li,I. Stoica,Carlos Guestrin,M. Zaharia,Tatsunori Hashimoto","id":"0cf694b8f85ab2e11d45595de211a15cfbadcd22","summary":"It is shown that instruction-following large language models can produce targeted malicious content, including hate speech and scams, bypassing in-the-wild defenses implemented by LLM API vendors.","score":7},{"url":"https://www.semanticscholar.org/paper/04407b388432e957031cebd1859e868c96006522","title":"Artefact Retrieval: Overview of NLP Models with Knowledge Base Access","venue":"ArXiv","year":2022,"referenceCount":88,"citationCount":3,"influentialCitationCount":0,"publicationDate":"24/01/2022","authors":"Vilém Zouhar,Marius Mosbach,Debanjali Biswas,D. Klakow","id":"04407b388432e957031cebd1859e868c96006522","summary":"This paper systematically describes the typology of artefacts, retrieval mechanisms and the way these artefacts are fused into the model to uncover combinations of design decisions that had not yet been tried in NLP systems.","score":7},{"url":"https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6","title":"MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge","venue":"ArXiv","year":2022,"referenceCount":131,"citationCount":28,"influentialCitationCount":6,"publicationDate":"17/06/2022","authors":"Linxi (Jim) Fan,Guanzhi Wang,Yunfan Jiang,Ajay Mandlekar,Yuncong Yang,Haoyi Zhu,Andrew Tang,De-An Huang,Yuke Zhu,Anima Anandkumar","id":"32c9b3859086d15184989454eb878638659e64c6","summary":"This work introduces M INE D OJO, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions and proposes a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function.","score":7},{"url":"https://www.semanticscholar.org/paper/e3a9af420cd2c0c8241856da92374027fefb87be","title":"Language Model Cascades","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":27,"influentialCitationCount":3,"publicationDate":"21/07/2022","authors":"David Dohan,Winnie Xu,Aitor Lewkowycz,Jacob Austin,David Bieber,Raphael Gontijo Lopes,Yuhuai Wu,H. Michalewski,R. Saurous,Jascha Narain Sohl-Dickstein,Kevin Murphy,Charles Sutton","id":"e3a9af420cd2c0c8241856da92374027fefb87be","summary":"Existing techniques from probabilistic programming, including scratchpads / chain of thought, veriﬁers, STaR, selection-inference, and tool use are formalized and referred to as language model cascades.","score":7},{"url":"https://www.semanticscholar.org/paper/35f5483fa6c1816739b604a5bb57719fadd79249","title":"Affective Faces for Goal-Driven Dyadic Communication","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/01/2023","authors":"Scott Geng,Revant Teotia,Purva Tendulkar,Sachit Menon,Carl Vondrick","id":"35f5483fa6c1816739b604a5bb57719fadd79249","summary":"A video framework for modeling the association between verbal and non-verbal communication during dyadic conversation and is able to output listeners that are significantly more socially appropriate than baselines is introduced.","score":7},{"url":"https://www.semanticscholar.org/paper/5a87204ba586f81c7a0d9f02a3af572a710ae24e","title":"IC3: Image Captioning by Committee Consensus","venue":"ArXiv","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"David Chan,Austin Myers,Sudheendra Vijayanarasimhan,David A. Ross,J. Canny","id":"5a87204ba586f81c7a0d9f02a3af572a710ae24e","summary":"This work introduces a simple, yet novel, method, designed to generate a single caption that captures high-level details from several viewpoints, that can improve the performance of SOTA automated recall systems by up to 84%, indicating significant material improvements over existing SOTA approaches for visual description.","score":7},{"url":"https://www.semanticscholar.org/paper/4168325b82e9d296f11e71c8f56b3107ee297cc2","title":"Distilling Internet-Scale Vision-Language Models into Embodied Agents","venue":"ArXiv","year":2023,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/01/2023","authors":"T. Sumers,Kenneth Marino,Arun Ahuja,R. Fergus,I. Dasgupta","id":"4168325b82e9d296f11e71c8f56b3107ee297cc2","summary":"This work outlines a new and effective way to use internet-scale VLMs, repur-posing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents.","score":7},{"url":"https://www.semanticscholar.org/paper/62ddfa4d7e85ef390052aced081514c78a0a42b3","title":"DePlot: One-shot visual language reasoning by plot-to-table translation","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Fangyu Liu,Julian Martin Eisenschlos,Francesco Piccinno,Syrine Krichene,Chenxi Pang,Kenton Lee,Mandar Joshi,Wenhu Chen,N. Collier,Y. Altun","id":"62ddfa4d7e85ef390052aced081514c78a0a42b3","summary":"The key in this method is a modality conversion module, named as D E P LOT, which translates the image of a plot or chart to a linearized table, and can be used off-the-shelf to-gether with LLMs in a plug-and-play fashion.","score":7},{"url":"https://www.semanticscholar.org/paper/6d269364de402d4a72ac30b0c8d81324f6849807","title":"LEVER: Learning to Verify Language-to-Code Generation with Execution","venue":"","year":2023,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Ansong Ni,Srini Iyer,Dragomir R. Radev,V. Stoyanov,Wen-tau Yih,Sida I. Wang,Xi Victoria Lin","id":"6d269364de402d4a72ac30b0c8d81324f6849807","summary":"This work proposes LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results, which consistently improves over the base CodeLMs and achieves new state-of-the-art results on all of them.","score":7},{"url":"https://www.semanticscholar.org/paper/7de648a460947ba0516eb88f83c33c50bbee2f15","title":"Distributed Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox","venue":"ArXiv","year":2022,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Qiyue Yin,Tongtong Yu,Shengqi Shen,Jun Yang,Meijing Zhao,Kaiqi Huang,Bin Liang,Liangsheng Wang","id":"7de648a460947ba0516eb88f83c33c50bbee2f15","summary":"A multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, and validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributedDeep reinforcement learning under complex games.","score":6},{"url":"https://www.semanticscholar.org/paper/a087581d07ff739e4ee0fed06f40385a4b6a69c8","title":"Toward the third generation artificial intelligence","venue":"Science China Information Sciences","year":2020,"referenceCount":93,"citationCount":14,"influentialCitationCount":2,"publicationDate":"01/09/2020","authors":"Jun Zhu,Hang Su,Bo Zhang","id":"a087581d07ff739e4ee0fed06f40385a4b6a69c8","summary":"This paper looks toward developing a third generation artificial intelligence by combining the current paradigms, and considers symbolism as the first generation of AI and connectionism as the second generation.","score":6},{"url":"https://www.semanticscholar.org/paper/269b4349158ace9b7e411a67670a99928fdd3a00","title":"Open Problems and Modern Solutions for Deep Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/02/2023","authors":"Weiqin Chen","id":"269b4349158ace9b7e411a67670a99928fdd3a00","summary":"Two publications that investigate the issues of DRL and propose effective solutions are reviewed, one designs the reward for human-robot collaboration by combining the manually designed extrinsic reward with a parameterized intrinsic reward function via the deterministic policy gradient, which improves the task performance and guarantees a stronger obstacle avoidance.","score":6},{"url":"https://www.semanticscholar.org/paper/3684491d62db5c3e5602375271e4b339bbf416ee","title":"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval","venue":"International Conference on Learning Representations","year":2020,"referenceCount":63,"citationCount":93,"influentialCitationCount":28,"publicationDate":"27/09/2020","authors":"Wenhan Xiong,Xiang Lorraine Li,Srini Iyer,Jingfei Du,Patrick Lewis,William Yang Wang,Yashar Mehdad,Wen-tau Yih,Sebastian Riedel,Douwe Kiela,Barlas Oğuz","id":"3684491d62db5c3e5602375271e4b339bbf416ee","summary":"This work proposes a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on twoMulti-hop datasets, HotpotQA and multi-evidence FEVER, and can be applied to any unstructured text corpus.","score":6},{"url":"https://www.semanticscholar.org/paper/3912c7224da9b0a999d57a0df33c44a24c6f751b","title":"Phrase Retrieval Learns Passage Retrieval, Too","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":41,"citationCount":23,"influentialCitationCount":3,"publicationDate":"16/09/2021","authors":"Jinhyuk Lee,Alexander Wettig,Danqi Chen","id":"3912c7224da9b0a999d57a0df33c44a24c6f751b","summary":"This work follows the intuition that retrieving phrases naturally entails retrieving larger text blocks and study whether phrase retrieval can serve as the basis for coarse-level retrieval including passages and documents, and demonstrates how phrase filtering and vector quantization can reduce the size of the index by 4-10x, making dense phrase retrieval a practical and versatile solution in multi-granularity retrieval.","score":6},{"url":"https://www.semanticscholar.org/paper/4f4a409f701f7552d45c46a5b0fea69dca6f8e84","title":"Unsupervised Dense Information Retrieval with Contrastive Learning","venue":"","year":2021,"referenceCount":68,"citationCount":37,"influentialCitationCount":13,"publicationDate":"16/12/2021","authors":"Gautier Izacard,Mathilde Caron,Lucas Hosseini,Sebastian Riedel,Piotr Bojanowski,Armand Joulin,Edouard Grave","id":"4f4a409f701f7552d45c46a5b0fea69dca6f8e84","summary":"This work explores the limits of contrastive learning as a way to train unsupervised dense retrievers and shows that it leads to strong performance in various retrieval settings and performs cross-lingual retrieval between scripts, which would not be possible with term matching methods.","score":6},{"url":"https://www.semanticscholar.org/paper/00ef52092ef3f109a09b66037707cd3227accb42","title":"Challenges in Generalization in Open Domain Question Answering","venue":"NAACL-HLT","year":2021,"referenceCount":67,"citationCount":21,"influentialCitationCount":2,"publicationDate":"02/09/2021","authors":"Linqing Liu,Patrick Lewis,S. Riedel,Pontus Stenetorp","id":"00ef52092ef3f109a09b66037707cd3227accb42","summary":"This work introduces and annotates questions according to three categories that measure different lev- 010 els and kinds of generalization and shows that key questionulty factors are cascading errors from the retrieval, frequency of question pattern, and fre- 028 quency of the entity.","score":6},{"url":"https://www.semanticscholar.org/paper/e4a052055cc293753983c0f0b74f82a73b3002d3","title":"Dynamic Collaborative Multi-Agent Reinforcement Learning Communication for Autonomous Drone Reforestation","venue":"ArXiv","year":2022,"referenceCount":83,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"P. D. Siedler","id":"e4a052055cc293753983c0f0b74f82a73b3002d3","summary":"Results show how communication enables collaboration and increases collective performance, planting precision and the risk-taking propensity of individual agents.","score":6},{"url":"https://www.semanticscholar.org/paper/c7b4b6d0d95c4e42e8573d2e46047df5e7d70a22","title":"Towards Intrinsic Interactive Reinforcement Learning: A Survey","venue":"ArXiv","year":2021,"referenceCount":132,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"Ben Poole,Minwoo Lee","id":"c7b4b6d0d95c4e42e8573d2e46047df5e7d70a22","summary":"A review of intrinsic IRL with an emphasis on its parent field of feedback-driven IRL while also providing discussions concerning the validity, challenges, and future research directions is provided.","score":6},{"url":"https://www.semanticscholar.org/paper/599595541456ef35dd0ed4c9e1325a2173f0ea75","title":"Monte Carlo Tree Search based Hybrid Optimization of Variational Quantum Circuits","venue":"ArXiv","year":2022,"referenceCount":94,"citationCount":3,"influentialCitationCount":0,"publicationDate":"30/03/2022","authors":"Jiahao Yao,Haoya Li,M. Bukov,Lin Lin,Lexing Ying","id":"599595541456ef35dd0ed4c9e1325a2173f0ea75","summary":"A new algorithm called MCTS-QAOA is proposed, which combines a Monte Carlo tree search method with an improved natural policy gradient solver to optimize the discrete and continuous variables in the quantum circuit, respectively.","score":6},{"url":"https://www.semanticscholar.org/paper/8c1729752f45181b25f4ec9f2afad6b66224fc95","title":"MSRL: Distributed Reinforcement Learning with Dataflow Fragments","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/10/2022","authors":"Huanzhou Zhu,Bo Zhao,Gang Chen,Weifeng Chen,Yijie Chen,Liang Shi,Yaodong Yang,P. Pietzuch,Lei Chen","id":"8c1729752f45181b25f4ec9f2afad6b66224fc95","summary":"MindSpore Reinforcement Learning is described, a distributed RL training system that supports distribution policies that govern how RL training computation is parallelised and distributed on cluster resources, without requiring changes to the algorithm implementation.","score":6},{"url":"https://www.semanticscholar.org/paper/45eeeacc0ca2b193a9762ebeedba28fb8a8e8513","title":"Maneuver Decision-Making for Autonomous Air Combat Based on FRE-PPO","venue":"Applied Sciences","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Hongpeng Zhang,Yujie Wei,Huan Zhou,Changqiang Huang","id":"45eeeacc0ca2b193a9762ebeedba28fb8a8e8513","summary":"","score":6},{"url":"https://www.semanticscholar.org/paper/b5dbf1e8065480bc0c57829f1dfd5e29987bf6d4","title":"Probing Transfer in Deep Reinforcement Learning without Task Engineering","venue":"CoLLAs","year":2022,"referenceCount":63,"citationCount":1,"influentialCitationCount":1,"publicationDate":"22/10/2022","authors":"Andrei A. Rusu,Sebastian Flennerhag,Dushyant Rao,Razvan Pascanu,R. Hadsell","id":"b5dbf1e8065480bc0c57829f1dfd5e29987bf6d4","summary":"It is argued that Atari game curricula offer a challenging benchmark for transfer learning in RL, that can help the community better understand the generalisation capabilities of RL agents along dimensions which meaningfully impact human generalisation performance.","score":6},{"url":"https://www.semanticscholar.org/paper/da22226379401577bb76ac2582f2a43b89e1f8a0","title":"Learning to design without prior data: Discovering generalizable design strategies using deep learning and tree search","venue":"Journal of Mechanical Design","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/11/2022","authors":"A. Raina,J. Cagan,Christopher McComb","id":"da22226379401577bb76ac2582f2a43b89e1f8a0","summary":"This paper presents a methodology to self-learn high-performing and generalizable problem-solving behavior in an arbitrary problem space, circumventing the needs for expert data, existing solutions, and problem-specific learning.","score":6},{"url":"https://www.semanticscholar.org/paper/3659a48a7c7d2e9a3285d8c47206a1c7f7c783dd","title":"Importance of prefrontal meta control in human-like reinforcement learning","venue":"Frontiers in Computational Neuroscience","year":2022,"referenceCount":142,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"J. Lee,Joel Z. Leibo,S. An,Sang Wan Lee","id":"3659a48a7c7d2e9a3285d8c47206a1c7f7c783dd","summary":"","score":6},{"url":"https://www.semanticscholar.org/paper/a8755cbdaca0ea9723fb81a06589a7837ea44619","title":"A Survey of Zero-shot Generalisation in Deep Reinforcement Learning","venue":"Journal of Artificial Intelligence Research","year":2021,"referenceCount":211,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/11/2021","authors":"Robert Kirk,Amy Zhang,Edward Grefenstette,Tim Rocktäschel","id":"a8755cbdaca0ea9723fb81a06589a7837ea44619","summary":"It is argued that taking a purely procedural content generation approach to benchmark design is not conducive to progress in ZSG, and it is suggested fast online adaptation and tackling RL-specific problems as some areas for future work on methods for ZSG.","score":6},{"url":"https://www.semanticscholar.org/paper/01516fcd4220b195242dda3cd74dfae165aef25e","title":"Replay and compositional computation","venue":"Neuron","year":2022,"referenceCount":169,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/09/2022","authors":"Z. Kurth-Nelson,T. Behrens,Greg Wayne,Kevin J. Miller,Lennart Luettgau,Raymond Dolan,Yunzhe Liu,P. Schwartenbeck","id":"01516fcd4220b195242dda3cd74dfae165aef25e","summary":"A new hypothesis is proposed: that replay is able to implement a form of compositional computation where entities are assembled into relationally bound structures to derive qualitatively new knowledge.","score":6},{"url":"https://www.semanticscholar.org/paper/f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e","title":"Mastering Diverse Domains through World Models","venue":"ArXiv","year":2023,"referenceCount":68,"citationCount":7,"influentialCitationCount":2,"publicationDate":"10/01/2023","authors":"Danijar Hafner,J. Pašukonis,Jimmy Ba,T. Lillicrap","id":"f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e","summary":"This work presents DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with hyperparameters.","score":6},{"url":"https://www.semanticscholar.org/paper/6f8ffdf8493323baadb2eb4b8c70f2d7084474f8","title":"Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in OpenQA","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":56,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Junjie Huang,Wanjun Zhong,Qianchu Liu,Ming Gong,Daxin Jiang,Nan Duan","id":"6f8ffdf8493323baadb2eb4b8c70f2d7084474f8","summary":"This work introduces an optimized OpenQA Table-Text Retriever (OTTeR) to jointly retrieve tabular and textual evidences and proposes to enhance mixed-modality representation learning via two mechanisms: modality-enhanced representation and mixed- modality negative sampling strategy.","score":6},{"url":"https://www.semanticscholar.org/paper/4d1e6c441a0b6aa2b9ec500a5064dc307d797b0c","title":"Relational Memory-Augmented Language Models","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2022,"referenceCount":92,"citationCount":8,"influentialCitationCount":1,"publicationDate":"24/01/2022","authors":"Qi Liu,Dani Yogatama,P. Blunsom","id":"4d1e6c441a0b6aa2b9ec500a5064dc307d797b0c","summary":"A memory-augmented approach to condition an autoregressive language model on a knowledge graph that represents the graph as a collection of relation triples and retrieve relevant relations for a given context to improve text generation.","score":6},{"url":"https://www.semanticscholar.org/paper/58ed1fbaabe027345f7bb3a6312d41c5aac63e22","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks","venue":"Neural Information Processing Systems","year":2020,"referenceCount":65,"citationCount":532,"influentialCitationCount":94,"publicationDate":"22/05/2020","authors":"Patrick Lewis,Ethan Perez,Aleksandara Piktus,Fabio Petroni,Vladimir Karpukhin,Naman Goyal,Heinrich Kuttler,M. Lewis,Wen-tau Yih,Tim Rocktäschel,Sebastian Riedel,Douwe Kiela","id":"58ed1fbaabe027345f7bb3a6312d41c5aac63e22","summary":"A general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation, and finds that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.","score":6},{"url":"https://www.semanticscholar.org/paper/db9296eaa252231e24d066e8413bf29fb058ee45","title":"Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering","venue":"ArXiv","year":2021,"referenceCount":156,"citationCount":86,"influentialCitationCount":5,"publicationDate":"04/01/2021","authors":"Fengbin Zhu,Wenqiang Lei,Chao Wang,Jianming Zheng,Soujanya Poria,Tat-Seng Chua","id":"db9296eaa252231e24d066e8413bf29fb058ee45","summary":"This work reviews the latest research trends in OpenQA, with particular attention to systems that incorporate neural MRC techniques, and revisiting the origin and development of Open QA systems.","score":6},{"url":"https://www.semanticscholar.org/paper/76224711646c27576e5f1e71249f7dd2a3450df0","title":"CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks","venue":"International Conference on Information and Knowledge Management","year":2022,"referenceCount":53,"citationCount":2,"influentialCitationCount":0,"publicationDate":"16/08/2022","authors":"Jiangui Chen,Ruqing Zhang,J. Guo,Y. Liu,Yixing Fan,Xueqi Cheng","id":"76224711646c27576e5f1e71249f7dd2a3450df0","summary":"This work proposes to replace the traditional multi-step search pipeline with a novel single-step generative model, which can dramatically simplify the search process and be optimized in an end-to-end manner, and names the pre-trained generative retrieval model as CorpusBrain as all information about the corpus is encoded in its parameters without the need of constructing additional index.","score":6},{"url":"https://www.semanticscholar.org/paper/f61b04277662741222cf8af74fc8660f4c82c8e3","title":"Time-Efficient Reward Learning via Visually Assisted Cluster Ranking","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"David W. Zhang,Micah Carroll,Andreea Bobu,A. Dragan","id":"f61b04277662741222cf8af74fc8660f4c82c8e3","summary":"Across some simple Mujoco tasks, it is shown that this high-level approach to batching comparisons together holds promise and is able to greatly increase the performance of the resulting agents, provided the same amount of human labeling time.","score":6},{"url":"https://www.semanticscholar.org/paper/535708777863633f7cc7fd6f4a716b1483c7100f","title":"Reinforcement Learning from Diverse Human Preferences","venue":"ArXiv","year":2023,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Wanqi Xue,Bo An,Shuicheng Yan,Zhongwen Xu","id":"535708777863633f7cc7fd6f4a716b1483c7100f","summary":"The proposed method is tested on a variety of tasks in DMcontrol and Meta-world and has shown consistent and signiﬁcant improvements over existing preference-based RL algorithms when learning from diverse feedback, paving the way for real-world applications of RL methods.","score":6},{"url":"https://www.semanticscholar.org/paper/3a2aa950971a46167b6da9431098b02facffe342","title":"Questions Are All You Need to Train a Dense Passage Retriever","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":6,"influentialCitationCount":2,"publicationDate":"21/06/2022","authors":"Devendra Singh Sachan,M. Lewis,Dani Yogatama,Luke Zettlemoyer,J. Pineau,M. Zaheer","id":"3a2aa950971a46167b6da9431098b02facffe342","summary":"ART is introduced, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data and removes the need for labeled data and task-speciﬁc losses.","score":6},{"url":"https://www.semanticscholar.org/paper/4596139b28c3ceacbd7e3c34dc0df079dbf4e96b","title":"Language Models as Agent Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":41,"citationCount":5,"influentialCitationCount":0,"publicationDate":"03/12/2022","authors":"Jacob Andreas","id":"4596139b28c3ceacbd7e3c34dc0df079dbf4e96b","summary":"","score":6},{"url":"https://www.semanticscholar.org/paper/d3bc7ba19e274bb6fb5e055a3f1b62924c731432","title":"Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment","venue":"ArXiv","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Hao Liu,Wilson Yan,P. Abbeel","id":"d3bc7ba19e274bb6fb5e055a3f1b62924c731432","summary":"This work is the first work that uses unaligned images for multimodal tasks by leveraging the power of pretrained language models, and enables few-shot image classification with large language models (e.g., GPT-3) as well as linear classification of images based on BERT text features.","score":6},{"url":"https://www.semanticscholar.org/paper/cc99dc93dcbc61f2c17698ce446a60ab1fd22ae8","title":"MPI: Evaluating and Inducing Personality in Pre-trained Language Models","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":3,"influentialCitationCount":0,"publicationDate":"20/05/2022","authors":"Guangyuan Jiang,Manjie Xu,Song-Chun Zhu,Wenjuan Han,Chi Zhang,Yixin Zhu","id":"cc99dc93dcbc61f2c17698ce446a60ab1fd22ae8","summary":"","score":6},{"url":"https://www.semanticscholar.org/paper/adbac4afd1e788753be72f455259a7b6c7d69d69","title":"Generating Executable Action Plans with Environmentally-Aware Language Models","venue":"ArXiv","year":2022,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Maitrey Gramopadhye,D. Szafir","id":"adbac4afd1e788753be72f455259a7b6c7d69d69","summary":"This paper proposes an approach to generate environmentally-aware action plans that can be directly mapped to executable agent actions, integrating environmental objects and object relations as additional inputs into LLM action plan generation to provide the system with an awareness of its surroundings.","score":6},{"url":"https://www.semanticscholar.org/paper/51965de80f86432d42749427db1e5bb0fa1e204c","title":"B-Pref: Benchmarking Preference-Based Reinforcement Learning","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":88,"citationCount":19,"influentialCitationCount":5,"publicationDate":"04/11/2021","authors":"Kimin Lee,Laura Smith,A. Dragan,P. Abbeel","id":"51965de80f86432d42749427db1e5bb0fa1e204c","summary":"This paper introduces B-Pref: a benchmark specially designed for preference-based RL, and showcases the utility of the benchmark by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preferencebased RL algorithms.","score":6},{"url":"https://www.semanticscholar.org/paper/b0bd491191e57bc4059e4fee4c2d4ed41a61f470","title":"Conditioning Predictive Models: Risks and Strategies","venue":"ArXiv","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Evan Hubinger,A. Jermyn,Johannes Treutlein,Rubi Hudson,Kate Woolverton","id":"b0bd491191e57bc4059e4fee4c2d4ed41a61f470","summary":"It is thought that conditioning approaches for predictive models represent the safest known way of eliciting human-level and slightly superhuman capabilities from large language models and other similar future models.","score":6},{"url":"https://www.semanticscholar.org/paper/9fc3a0e96f2fa6bc7a92a96d61c68f7cb09f0a8f","title":"Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":10,"influentialCitationCount":1,"publicationDate":"10/04/2022","authors":"Wenhu Chen,Pat Verga,Michiel de Jong,J. Wieting,W. Cohen","id":"9fc3a0e96f2fa6bc7a92a96d61c68f7cb09f0a8f","summary":"A new QA system which aug-ments a text-to-text model with a large memory of question-answer pairs, and a new pre-training task for the latent step of question retrieval, which greatly improves performance on smaller QA benchmarks.","score":6},{"url":"https://www.semanticscholar.org/paper/d267ffcf1b0720d156aa2d9b80c921eddb70113e","title":"Retrieval-Enhanced Machine Learning","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2022,"referenceCount":75,"citationCount":6,"influentialCitationCount":0,"publicationDate":"02/05/2022","authors":"Hamed Zamani,Fernando Diaz,M. Dehghani,Donald Metzler,Michael Bendersky","id":"d267ffcf1b0720d156aa2d9b80c921eddb70113e","summary":"A generic retrieval-enhanced machine learning (REML) framework is described, which includes a number of existing models as special cases and lays a foundation for a new style of information access research and paves a path towards advancing machine learning and artificial intelligence.","score":6},{"url":"https://www.semanticscholar.org/paper/6fcdad7b8d6b60b23bc51859e736c29f913b249a","title":"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering","venue":"Transactions of the Association for Computational Linguistics","year":2022,"referenceCount":44,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"Shamane Siriwardhana,Rivindu Weerasekera,Elliott Wen,Tharindu Kaluarachchi,R. Rana,Suranga Nanayakkara","id":"6fcdad7b8d6b60b23bc51859e736c29f913b249a","summary":"This work proposes RAG-end2end, an extension to RAG that can adapt to a domain- specific knowledge base by updating all components of the external knowledge base during training and introduces an auxiliary training signal to inject more domain-specific knowledge.","score":6},{"url":"https://www.semanticscholar.org/paper/421c19183033288e0bfd5df1451083d8b3cb8007","title":"Language Modeling with Latent Situations","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Belinda Z. Li,Maxwell Nye,Jacob Andreas","id":"421c19183033288e0bfd5df1451083d8b3cb8007","summary":"A family of approaches for improving coherence in LMs by training them to construct and condition on explicit representations of entities and their states, showing that standard LMs can be sample-efﬁciently trained to model not just language but the situations it describes.","score":6},{"url":"https://www.semanticscholar.org/paper/3cb4a3ebaf027852e5d185ccedc70990781bff72","title":"Empirical Investigation of Neural Symbolic Reasoning Strategies","venue":"","year":2023,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Y. Aoki,Keito Kudo,Tatsuki Kuribayashi,Ana Brassard,Masashi Yoshikawa,Keisuke Sakaguchi,Kentaro Inui","id":"3cb4a3ebaf027852e5d185ccedc70990781bff72","summary":"This work decomposes the reasoning strategy w.r.t. step granularity and chaining strategy and finds that certain configurations lead to nearly perfect performance, even in the case of length extrapolation.","score":6},{"url":"https://www.semanticscholar.org/paper/9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Yu Gu,Xiang Deng,Yu Su","id":"9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","summary":"Pangu is proposed, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability, and enables, for the first time, effective few-shot in-context learning for KBQA with large LMs such as Codex.","score":6},{"url":"https://www.semanticscholar.org/paper/7195ed3c7f11220f29634cecb68b1d39db2e36d9","title":"Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/01/2023","authors":"Shuaichen Chang,J. Wang,Mingwen Dong,Lin Pan,Henghui Zhu,A. Li,Wuwei Lan,Shenmin Zhang,Jiarong Jiang,Joseph Lilien,Stephen M. Ash,William Yang Wang,Zhiguo Wang,Vittorio Castelli,Patrick Ng,Bing Xiang","id":"7195ed3c7f11220f29634cecb68b1d39db2e36d9","summary":"A comprehensive robustness benchmark 1 based on Spider, a cross-domain text-to-SQL benchmark, is proposed to diagnose the model robustness and a diagnostic study of the state-of-the-art models on the robustness set is conducted.","score":6},{"url":"https://www.semanticscholar.org/paper/1d34b6cffe67077cdd4df41950b1195f09ae0cb8","title":"Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs","venue":"ArXiv","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Guanhui. Liu,En-Pei Hu,Pu-Jen Cheng,Hung-yi Lee,Shao-Hua Sun","id":"1d34b6cffe67077cdd4df41950b1195f09ae0cb8","summary":"This work proposes to learn a meta-policy that composes a series of programs sampled from the learned program embedding space that can produce program policies that describe out-of-distributionally complex behaviors and directly assign credits to programs that induce desired behaviors.","score":6},{"url":"https://www.semanticscholar.org/paper/ca35e54176d0c0e540fc99d6aabf69a72ed64412","title":"FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":2,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"Michiel de Jong,Yury Zemlyanskiy,J. Ainslie,Nicholas FitzGerald,Sumit K. Sanghai,Fei Sha,W. Cohen","id":"ca35e54176d0c0e540fc99d6aabf69a72ed64412","summary":"This work shows that the majority of inference time results from memory bandwidth constraints in the decoder, and proposes two simple changes to the FiD architecture to speed up inference by 7x, and indicates that the faster decoder inference then allows for a much larger decoder.","score":6},{"url":"https://www.semanticscholar.org/paper/655e0fdfe5f92c3a6de43f9beab8a745ee5d7653","title":"Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute","venue":"ArXiv","year":2023,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/01/2023","authors":"Michiel de Jong,Yury Zemlyanskiy,Nicholas FitzGerald,J. Ainslie,Sumit K. Sanghai,Fei Sha,W. Cohen","id":"655e0fdfe5f92c3a6de43f9beab8a745ee5d7653","summary":"It is shown that LUMEN outperforms pure memory on multiple question-answering tasks while being much cheaper than FiD, and outperforms both for any given compute budget, and the advantage of LUMen over FiD increases with model size.","score":6},{"url":"https://www.semanticscholar.org/paper/4b516216d7d150a081fd74993bddf36b6b22c118","title":"Chain of Thought Imitation with Procedure Cloning","venue":"ArXiv","year":2022,"referenceCount":92,"citationCount":2,"influentialCitationCount":0,"publicationDate":"22/05/2022","authors":"Mengjiao Yang,D. Schuurmans,P. Abbeel,Ofir Nachum","id":"4b516216d7d150a081fd74993bddf36b6b22c118","summary":"Through empirical analysis on navigation, simulated robotic manipulation, and game-playing environments, it is shown that imitating the intermediate computations of an expert’s behavior enables procedure cloning to learn policies exhibiting generalization to unseen environment conﬁgurations, including those for which running the expert's procedure directly is infeasible.","score":6},{"url":"https://www.semanticscholar.org/paper/9574d10f55e77d432f923ee71e3205fc64a3104a","title":"Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?","venue":"","year":2023,"referenceCount":161,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Chengwei Qin,Shafiq R. Joty,Q. Li,Ruochen Zhao","id":"9574d10f55e77d432f923ee71e3205fc64a3104a","summary":"Meta prompt tuning is studied to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks and an in-depth analysis from the perspective of task similarity.","score":6},{"url":"https://www.semanticscholar.org/paper/706c6b3781374b0b11f98f204a4ddd05b26ed009","title":"Knowledge Infused Decoding","venue":"International Conference on Learning Representations","year":2022,"referenceCount":98,"citationCount":6,"influentialCitationCount":1,"publicationDate":"06/04/2022","authors":"Ruibo Liu,Guoqing Zheng,Shashank Gupta,Radhika Gaonkar,Chongyang Gao,Soroush Vosoughi,Milad Shokouhi,A. Awadallah","id":"706c6b3781374b0b11f98f204a4ddd05b26ed009","summary":"Knowledge Infused Decoding (KID)—a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding, which maintains a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning.","score":6},{"url":"https://www.semanticscholar.org/paper/49b499598a8864eee55ab264fc16a5bf8d2f87ef","title":"Social Simulacra: Creating Populated Prototypes for Social Computing Systems","venue":"ACM Symposium on User Interface Software and Technology","year":2022,"referenceCount":87,"citationCount":5,"influentialCitationCount":0,"publicationDate":"08/08/2022","authors":"J. Park,Lindsay Popowski,Carrie J. Cai,M. Morris,Percy Liang,Michael S. Bernstein","id":"49b499598a8864eee55ab264fc16a5bf8d2f87ef","summary":"It is demonstrated that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of “what if?” scenarios where community members or moderators intervene.","score":6},{"url":"https://www.semanticscholar.org/paper/a609b4142feb8a00e0e9ae94c48d999f46ed80dd","title":"Argumentative Reward Learning: Reasoning About Human Preferences","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":1,"influentialCitationCount":0,"publicationDate":"28/09/2022","authors":"Francis Rhys Ward,F. Belardinelli,F. Toni","id":"a609b4142feb8a00e0e9ae94c48d999f46ed80dd","summary":"A novel neuro-symbolic framework, argumentative reward learning, is presented, which combines preference-based argumentation with existing approaches to reinforcement learning from human feedback and improves prior work by generalising human preferences, reducing the bur-den on the user and increasing the robustness of the reward model.","score":6},{"url":"https://www.semanticscholar.org/paper/7d6f17706cbcfcca55f08485bcbf8c82e00c9279","title":"Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":2,"influentialCitationCount":0,"publicationDate":"04/10/2022","authors":"Rohin Shah,Vikrant Varma,Ramana Kumar,Mary Phuong,Victoria Krakovna,J. Uesato,Z. Kenton","id":"7d6f17706cbcfcca55f08485bcbf8c82e00c9279","summary":"It is demonstrated that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains and suggesting several research directions that could reduce the risk of goal mis generalization for future systems.","score":6},{"url":"https://www.semanticscholar.org/paper/eecb45aa040064cbc0b37fd100706c02e7dc880e","title":"Structured Prompting: Scaling In-Context Learning to 1, 000 Examples","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":2,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"Y. Hao,Yutao Sun,Li Dong,Zhixiong Han,Yuxian Gu,Furu Wei","id":"eecb45aa040064cbc0b37fd100706c02e7dc880e","summary":"Experimental results on a diverse set of tasks show that the structured prompting approach improves end-task performance and reduces evaluation variance over conventional in-context learning as the number of demonstration examples increases.","score":6},{"url":"https://www.semanticscholar.org/paper/da345b189e4faaaa489f7319640868a37a3932a1","title":"Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?","venue":"","year":2023,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Keito Kudo,Y. Aoki,Tatsuki Kuribayashi,Ana Brassard,Masashi Yoshikawa,Keisuke Sakaguchi,Kentaro Inui","id":"da345b189e4faaaa489f7319640868a37a3932a1","summary":"A skill tree on compositionality in arithmetic symbolic reasoning that defines the hierarchical levels of complexity along with three compositionality dimensions: systematicity, productivity, and substitutivity is introduced.","score":6},{"url":"https://www.semanticscholar.org/paper/023edab4738690444e3924e224c2641017a0d794","title":"Quark: Controllable Text Generation with Reinforced Unlearning","venue":"ArXiv","year":2022,"referenceCount":90,"citationCount":13,"influentialCitationCount":4,"publicationDate":"26/05/2022","authors":"Ximing Lu,S. Welleck,Liwei Jiang,Jack Hessel,Lianhui Qin,Peter West,Prithviraj Ammanabrolu,Yejin Choi","id":"023edab4738690444e3924e224c2641017a0d794","summary":"Quantized Reward Konditioning ( Quark) is introduced, an algorithm for optimizing a reward function that quantiﬁes an (un)wanted property, while not straying too far from the original model.","score":6},{"url":"https://www.semanticscholar.org/paper/4c2534f9b03ac2f3810c07abc398a11bcf47258e","title":"Transformers Learn Shortcuts to Automata","venue":"ArXiv","year":2022,"referenceCount":104,"citationCount":4,"influentialCitationCount":0,"publicationDate":"19/10/2022","authors":"Bingbin Liu,J. Ash,Surbhi Goel,A. Krishnamurthy,Cyril Zhang","id":"4c2534f9b03ac2f3810c07abc398a11bcf47258e","summary":"The theoretical results completely characterize shortcut solutions, whereby a shallow Transformer with only o(T ) layers can exactly replicate the computation of an automaton on an input sequence of length T .","score":6},{"url":"https://www.semanticscholar.org/paper/fff24425c8eaa3af4422261b9e108374ded678f0","title":"Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"22/10/2022","authors":"Kaixin Ma,Hao Cheng,Xiaodong Liu,Eric Nyberg,Jianfeng Gao","id":"fff24425c8eaa3af4422261b9e108374ded678f0","summary":"This work proposes a novel open-domain question answering (ODQA) framework for answering single/multi-hop questions across heterogeneous knowledge sources and substantially outperforms the previous state-of-the-art on OTT-QA with an exact match score of 47.3.","score":6},{"url":"https://www.semanticscholar.org/paper/0040dac7a1bf7a1eeb01c86ddb993f331f35b158","title":"Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Swarnadeep Saha,Peter Hase,Nazneen Rajani,Mohit Bansal","id":"0040dac7a1bf7a1eeb01c86ddb993f331f35b158","summary":"This work collects human-written explanations in the form of generalizable commonsense rules and finds that for hard examples, human explanations are significantly better than GPT-3 explanations both in terms of label-supportiveness and generalizability judgements.","score":6},{"url":"https://www.semanticscholar.org/paper/8745c5b9522c11818418f64fdc880894faeaed16","title":"A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation","venue":"ArXiv","year":2022,"referenceCount":115,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/11/2022","authors":"Hiroki Furuta,Yusuke Iwasawa,Yutaka Matsuo,S. Gu","id":"8745c5b9522c11818418f64fdc880894faeaed16","summary":"This work explores a method for learning a single policy that manipulates various forms of agents to solve various tasks by distilling a large amount of proficient behavioral data and suggests large diverse offline datasets, unified IO representation, and policy representation and architecture selection through supervised learning form a promising approach for studying and advancing morphology-task generalization.","score":6},{"url":"https://www.semanticscholar.org/paper/ae441f7305dc2cd58c708528b3ecee3501cc5c46","title":"Plansformer: Generating Symbolic Plans using Transformers","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Vishal Pallagani,Bharath Muppasani,K. Murugesan,F. Rossi,L. Horesh,Biplav Srivastava,F. Fabiano,Andrea Loreggia","id":"ae441f7305dc2cd58c708528b3ecee3501cc5c46","summary":"The use of LLMs for automated planning - a branch of AI concerned with the realization of action sequences (plans) to achieve a goal, typically executed by intelligent agents, autonomous robots, and unmanned vehicles are explored.","score":6},{"url":"https://www.semanticscholar.org/paper/9cffc161896ce2b8d1a3083ab4f293bc166134ce","title":"Language model acceptability judgements are not always robust to context","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Koustuv Sinha,Jon Gauthier,Aaron Mueller,Kanishka Misra,Keren Fuentes,R. Levy,Adina Williams","id":"9cffc161896ce2b8d1a3083ab4f293bc166134ce","summary":"This paper investigates the stability of language models’ performance on targeted syntactic evaluations as they vary properties of the input context: the length of the context, the types of syntactic phenomena it contains, and whether or not there are violations of grammaticality.","score":6},{"url":"https://www.semanticscholar.org/paper/7897e693726b3ddf6efab786fcf731fca8bd72ca","title":"Does unsupervised grammar induction need pixels?","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Boyi Li,Rodolfo Corona,Karttikeya Mangalam,Catherine Chen,Daniel Flaherty,S. Belongie,Kilian Q. Weinberger,J. Malik,Trevor Darrell,D. Klein","id":"7897e693726b3ddf6efab786fcf731fca8bd72ca","summary":"The results challenge the notion that extralinguistic signals such as image pixels are needed for unsupervised grammar induction, and point to the need for better text-only baselines in evaluating the need of multi-modality for the task.","score":6},{"url":"https://www.semanticscholar.org/paper/434250ed6b9fb5f1ad871709fb3cf0a6635ff5ce","title":"ORCA: A Challenging Benchmark for Arabic Language Understanding","venue":"ArXiv","year":2022,"referenceCount":111,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"AbdelRahim Elmadany,E. Nagoudi,M. Abdul-Mageed","id":"434250ed6b9fb5f1ad871709fb3cf0a6635ff5ce","summary":"To measure current progress in Arabic NLU, ORCA is used to offer a comprehensive comparison between 18 multilingual and Arabic language models and to provide a public leaderboard with a single-number evaluation metric ( ORCA score) to facilitate future research.","score":6},{"url":"https://www.semanticscholar.org/paper/5278b81db686b4d36143941bff1c683bea963a63","title":"SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient","venue":"ArXiv","year":2023,"referenceCount":113,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Max Ryabinin,Tim Dettmers,Michael Diskin,Alexander Borzunov","id":"5278b81db686b4d36143941bff1c683bea963a63","summary":"This work considers alternative setups for training large models: using cheap “preemptible” instances or pooling existing resources from multiple regions, and proposes SWARM parallelism, a model-parallel training algorithm designed for poorly connected, heterogeneous and unreliable devices.","score":6},{"url":"https://www.semanticscholar.org/paper/a2bcacc8fefb859c94c69d524b2368bb4792f9b1","title":"Adversarial Prompting for Black Box Foundation Models","venue":"ArXiv","year":2023,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"N. Maus,Patrick Chao,Eric Wong,Jacob R. Gardner","id":"a2bcacc8fefb859c94c69d524b2368bb4792f9b1","summary":"A black-box framework for generating adversarial prompts for unstructured image and text generation and induce specific behaviors into the generative process, such as generating images of a particular object or biasing the frequency of specific letters in the generated text.","score":6},{"url":"https://www.semanticscholar.org/paper/3f4d11971f2c64be9125a7fe99c019588bbebf16","title":"Iteratively Prompt Pre-trained Language Models for Chain of Thought","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":48,"citationCount":5,"influentialCitationCount":3,"publicationDate":"16/03/2022","authors":"Boshi Wang,Xiang Deng,Huan Sun","id":"3f4d11971f2c64be9125a7fe99c019588bbebf16","summary":"An iterative prompting framework is explored, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference by learning to dynamically synthesize prompts conditioned on the current step’s contexts.","score":6},{"url":"https://www.semanticscholar.org/paper/b8bd29a6104d26a16687400049a4e7e026ae6258","title":"Active Example Selection for In-Context Learning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":43,"citationCount":4,"influentialCitationCount":1,"publicationDate":"08/11/2022","authors":"Yiming Zhang,Shi Feng,Chenhao Tan","id":"b8bd29a6104d26a16687400049a4e7e026ae6258","summary":"It is demonstrated that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information.","score":6},{"url":"https://www.semanticscholar.org/paper/52136f813243ac3de8e277906112a41590a376d4","title":"What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis","venue":"ArXiv","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Xiang Deng,Vasilisa Bashlovkina,Feng Han,Simon Baumgartner,Michael Bendersky","id":"52136f813243ac3de8e277906112a41590a376d4","summary":"This model generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production and finds that prompting the LLM to produce Chain-of-Thought summaries and forcing it through sev-eral reasoning paths helps generate more stable and accurate labels.","score":6},{"url":"https://www.semanticscholar.org/paper/c9ad9d69d7568110dd5527598a92c7f8b335eef4","title":"Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations","venue":"ArXiv","year":2023,"referenceCount":200,"citationCount":4,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Josh A. Goldstein,Girish Sastry,Micah Musser,Renée DiResta,Matthew Gentzel,Katerina Sedova","id":"c9ad9d69d7568110dd5527598a92c7f8b335eef4","summary":"This report lays out possible changes to the actors, behaviors, and content of online influence operations, and provides a framework for stages of the language model-to-influence operations pipeline that mitigations could target.","score":6},{"url":"https://www.semanticscholar.org/paper/feea0452e03b78f7c85f40e5daa1bd08b61bb44b","title":"Revisiting Offline Compression: Going Beyond Factorization-based Methods for Transformer Language Models","venue":"ArXiv","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"M. Banaei,Klaudia Balazy,A. Kasymov,R. Lebret,J. Tabor,K. Aberer","id":"feea0452e03b78f7c85f40e5daa1bd08b61bb44b","summary":"This paper explores offline compression methods, meaning computationally-cheap approaches that do not require further fine-tuning of the compressed model, and challenges the classical matrix factorization methods by proposing a novel, better-performing autoencoder-based framework.","score":6},{"url":"https://www.semanticscholar.org/paper/6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":12,"influentialCitationCount":2,"publicationDate":"05/07/2022","authors":"Hung Le,Yue Wang,Akhilesh Deepak Gotmare,S. Savarese,S. Hoi","id":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","summary":"This work proposes “CodeRL”, a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL), and treats the code-generating LM as an actor network, and introduces a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor.","score":6},{"url":"https://www.semanticscholar.org/paper/a328907b45724b61aafbad746d490f00fe0fd761","title":"Summarization Programs: Interpretable Abstractive Summarization with Neural Modular Trees","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":3,"influentialCitationCount":1,"publicationDate":"21/09/2022","authors":"Swarnadeep Saha,Shiyue Zhang,Peter Hase,Mohit Bansal","id":"a328907b45724b61aafbad746d490f00fe0fd761","summary":"An interpretable modular framework consisting of an (ordered) list of binary trees, each encoding the step-by-step generative process of an abstractive summary sentence from the source document, and demonstrates that SP-S EARCH effectively represents theGenerative process behind human summaries using modules that are typically faithful to their intended behavior.","score":6},{"url":"https://www.semanticscholar.org/paper/f4913994ce936cc79ebcbeea51a63f6d2005e978","title":"Document-Level Abstractive Summarization","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Gonçalo Raposo,Afonso Raposo,Ana Sofia Carmo","id":"f4913994ce936cc79ebcbeea51a63f6d2005e978","summary":"This work proposes a novel retrieval-enhanced approach based on the architecture which reduces the cost of generating a summary of the entire document by processing smaller chunks, and suggests a more efﬁcient memory a consumption and truthfulness.","score":6},{"url":"https://www.semanticscholar.org/paper/533fe3da3518244d0e21b7bec7fa35647b5fa345","title":"Teacher Forcing Recovers Reward Functions for Text Generation","venue":"ArXiv","year":2022,"referenceCount":69,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/10/2022","authors":"Yongchang Hao,Yuxin Liu,Lili Mou","id":"533fe3da3518244d0e21b7bec7fa35647b5fa345","summary":"This work proposes a task-agnostic approach that derives a step-wise reward function directly from a model trained with teacher forcing, and proposes a simple modi-cation to stabilize the RL training on non-parallel datasets with the authors' induced reward function.","score":6},{"url":"https://www.semanticscholar.org/paper/d4f77cdb04d7ae02860415877cc4c463e93595a1","title":"Retrieval Augmentation for Commonsense Reasoning: A Unified Approach","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":68,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"W. Yu,Chenguang Zhu,Zhihan Zhang,Shuohang Wang,Zhuosheng Zhang,Yuwei Fang,Meng Jiang","id":"d4f77cdb04d7ae02860415877cc4c463e93595a1","summary":"A unified framework of retrieval-augmented commonsense reasoning (called RACo), including a newly constructed commonsense corpus with over 20 million documents and novel strategies for training a commonsense retriever is proposed.","score":6},{"url":"https://www.semanticscholar.org/paper/bfdbe107413a02a1ce6f233848b6d934979638cb","title":"Revision for Concision: A Constrained Paraphrase Generation Task","venue":"TSAR","year":2022,"referenceCount":69,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Wenchuan Mu,Kwanin Lim","id":"bfdbe107413a02a1ce6f233848b6d934979638cb","summary":"This work introduces and formulate revising for concision as a natural language processing task at the sentence level, which requires algorithms to use only necessary words to rewrite a sentence while preserving its meaning.","score":6},{"url":"https://www.semanticscholar.org/paper/7633c8b7cd4209222d02c453184b480533a0e139","title":"Analyzing Multi-Task Learning for Abstractive Text Summarization","venue":"IEEE Games Entertainment Media Conference","year":2022,"referenceCount":103,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Frederic Kirstein,Jan Philip Wahle,Terry Ruas,Bela Gipp","id":"7633c8b7cd4209222d02c453184b480533a0e139","summary":"This work analyzes the influence of multi-task learning strategies using task families for the English abstractive text summarization task, and finds that certain combinations of task families positively impact downstream performance.","score":6},{"url":"https://www.semanticscholar.org/paper/9b32f1b56da9b31816e237e17324beca35c15cce","title":"ED-FAITH: Evaluating Dialogue Summarization on Faithfulness","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Sicong Huang,Asli Celikyilmaz,Haoran Li","id":"9b32f1b56da9b31816e237e17324beca35c15cce","summary":"Inspired by the strong zero-shot performance of the T0 language model, T0-Score is proposed – a new metric for faithfulness evaluation, which shows consistent improvement against baseline metrics across multiple domains.","score":6},{"url":"https://www.semanticscholar.org/paper/a9a53c28f3b964cf561c05bf204b4c06f6454eec","title":"Bipartite-play Dialogue Collection for Practical Automatic Evaluation of Dialogue Systems","venue":"AACL","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/11/2022","authors":"Shiki Sato,Yosuke Kishinami,Hiroaki Sugiyama,Reina Akama,Ryoko Tokuhisa,Jun Suzuki","id":"a9a53c28f3b964cf561c05bf204b4c06f6454eec","summary":"Experimental results show that the automatic evaluation using the bipartite-play method mitigates these two drawbacks and correlates as strongly with human subjectivity as existing methods.","score":6},{"url":"https://www.semanticscholar.org/paper/aa4005a80379db297b81c01122a4cff969677b5f","title":"CE-BART: Cause-and-Effect BART for Visual Commonsense Generation","venue":"Italian National Conference on Sensors","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Junyeong Kim,Jiajing Hong,Sunjae Yoon,Chang D. Yoo","id":"aa4005a80379db297b81c01122a4cff969677b5f","summary":"Cause-and-Effect BART (CE-BART) is proposed, which is based on a structured graph reasoner that captures intra- and inter-modality relationships among visual and textual representations and a cause-And-effect generator that generates cause- and-effect captions by considering the causal relations among inferences.","score":6},{"url":"https://www.semanticscholar.org/paper/9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Qingfu Zhu,Xianzhen Luo,Fang Liu,Cuiyun Gao,Wanxiang Che","id":"9b4055674cd9849f8595240695bed69cd02492bc","summary":"This paper comprehensively investigates existing work in natural language processing for programming, rang-ing from early deductive models to the latest competition-level models.","score":6},{"url":"https://www.semanticscholar.org/paper/2bff7efafc7a07cc6c8402c595cb469bb90fea3d","title":"Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation","venue":"ArXiv","year":2022,"referenceCount":90,"citationCount":4,"influentialCitationCount":1,"publicationDate":"15/12/2022","authors":"Yixin Liu,Alexander R. Fabbri,Pengfei Liu,Yilun Zhao,Linyong Nan,Ruilin Han,Simeng Han,Shafiq R. Joty,Chien-Sheng Wu,Caiming Xiong,Dragomir R. Radev","id":"2bff7efafc7a07cc6c8402c595cb469bb90fea3d","summary":"A modiﬁed summarization salience protocol, Atomic Content Units (ACUs), which relies onained semantic units and al-lows for high inter-annotator agreement is proposed, which has important implications for evaluating large language models (LLMs), as it shows that LLMs adjusted by human feedback may over-strained human evaluation.","score":6},{"url":"https://www.semanticscholar.org/paper/690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Zhiruo Wang,Shuyan Zhou,Daniel Fried,Graham Neubig","id":"690c210564226c9307b3bab977cdc07a6a45863a","summary":"ODEX corroborates the mer-its of execution-based evaluation over metrics without execution but also unveils their complementary effects, and is released to facilitate research into open-domain problems for the code generation community.","score":6},{"url":"https://www.semanticscholar.org/paper/b1a3527466abbebe7d2baa75f3923c6fdc85d3d6","title":"On the Blind Spots of Model-Based Evaluation Metrics for Text Generation","venue":"ArXiv","year":2022,"referenceCount":70,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Tianxing He,Jingyu Zhang,Tianle Wang,Sachin Kumar,Kyunghyun Cho,James R. Glass,Yulia Tsvetkov","id":"b1a3527466abbebe7d2baa75f3923c6fdc85d3d6","summary":"This work design and synthesize a wide range of potential errors and check whether they result in a drop in the metric scores, and investigates the reasons behind these blind spots and suggests practical workarounds for a more reliable evaluation of text generation.","score":6},{"url":"https://www.semanticscholar.org/paper/c4b0ce9321c0c0ac8f8221aefda3281cbb566058","title":"A Survey of Natural Language Generation","venue":"ACM Computing Surveys","year":2021,"referenceCount":165,"citationCount":4,"influentialCitationCount":0,"publicationDate":"22/12/2021","authors":"Chenhe Dong,Yinghui Li,Haifan Gong,M. Chen,Junxin Li,Ying Shen,Min Yang","id":"c4b0ce9321c0c0ac8f8221aefda3281cbb566058","summary":"This survey aims to give the latest synthesis of deep learning research on the NLG core tasks, as well as the architectures adopted in the field, and detail meticulously and comprehensively various NLG tasks and datasets.","score":6},{"url":"https://www.semanticscholar.org/paper/25c1dd5c0276fdb3c46868f91b604049f5182723","title":"A Cognitive Evaluation of Instruction Generation Agents tl;dr They Need Better Theory-of-Mind Capabilities","venue":"ArXiv","year":2022,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Lingjun Zhao,Khanh Nguyen,Hal Daum'e","id":"25c1dd5c0276fdb3c46868f91b604049f5182723","summary":"The results indicate that neural-network-based instruction generation agents, while capable of effectively narrowing the search space, poorly predict the listener’s interpretations of their instructions and thus often fail to select the best instructions even from a small candidate set.","score":6},{"url":"https://www.semanticscholar.org/paper/0645c86da0f6329f13489654210eaaca87be2e22","title":"You Truly Understand What I Need: Intellectual and Friendly Dialogue Agents grounding Knowledge and Persona","venue":"ArXiv","year":2023,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/01/2023","authors":"J. Lim,Myunghoon Kang,Yuna Hur,Seung-Ju Jung,Jinsung Kim,Yoonna Jang,Dongyub Lee,Hyesung Ji,Donghoon Shin,Seung Wook Kim,Heu-Jeoung Lim","id":"0645c86da0f6329f13489654210eaaca87be2e22","summary":"This work proposes an effective dialogue agent that grounds external knowledge and persona simultaneously, and shows the retriever’s effectiveness in extracting relevant documents compared to the other previous retrievers, along with the comparison of multiple candidate scoring methods.","score":6},{"url":"https://www.semanticscholar.org/paper/c7ce496c49da196dc632533afae8da587bd3c338","title":"HeroNet: A Hybrid Retrieval-Generation Network for Conversational Bots","venue":"ArXiv","year":2023,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/01/2023","authors":"Bolin Zhang,Yunzhe Xu,Zhiying Tu,Dianhui Chu","id":"c7ce496c49da196dc632533afae8da587bd3c338","summary":"A hybrid retrieval-generation network (HeroNet) with the three-fold ideas to produce high-quality sentence representations, which is able to solve both retrieval and generation tasks simultaneously while maximizing performance of each other.","score":6},{"url":"https://www.semanticscholar.org/paper/83a5b5c4cf762fd9b8b4e6d8e607de52b7ecaa77","title":"Automatic question generation: a review of methodologies, datasets, evaluation metrics, and applications","venue":"Progress in Artificial Intelligence","year":2023,"referenceCount":150,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Nikahat Mulla,P. Gharpure","id":"83a5b5c4cf762fd9b8b4e6d8e607de52b7ecaa77","summary":"This review provides an overview of the research progress in automatic question generation and presents a comprehensive literature review covering the classification of Question Generation systems by categorizing them into three broad use-cases, namely standalone question generation, visual questiongeneration, and conversational question generation.","score":6},{"url":"https://www.semanticscholar.org/paper/79588eb402572fb37fe20d4cfc516ace8a661603","title":"Coherence and Diversity through Noise: Self-Supervised Paraphrase Generation via Structure-Aware Denoising","venue":"ArXiv","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/02/2023","authors":"Rishabh Gupta,V. Venktesh,M. Mohania,Vikram Goyal","id":"79588eb402572fb37fe20d4cfc516ace8a661603","summary":"SCANING considerably improves performance in terms of both semantic preservation and producing diverse paraphrases through extensive automated and manual evaluation across 4 datasets.","score":6},{"url":"https://www.semanticscholar.org/paper/13a66fc8689724e295548ceac9e5425fc46cc093","title":"SkCoder: A Sketch-based Approach for Automatic Code Generation","venue":"","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Jia Li,Yongming Li,Ge Li,Zhi Jin,Yiyang Hao,Xing Hu","id":"13a66fc8689724e295548ceac9e5425fc46cc093","summary":"This paper proposes a sketch-based code generation approach named SkCoder to mimic developers' code reuse behavior, which retrieves a similar code snippet, extracts relevant parts as a code sketch, and edits the sketch into the desired code.","score":6},{"url":"https://www.semanticscholar.org/paper/5ef821267fa68d3231ed8135ff8ec09f25bb1398","title":"ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models","venue":"","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/02/2023","authors":"Sheng Wang,Zihao Zhao,Xi Ouyang,Qian Wang,Dinggang Shen","id":"5ef821267fa68d3231ed8135ff8ec09f25bb1398","summary":"The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems.","score":6},{"url":"https://www.semanticscholar.org/paper/ca086f4c09cf8de705830ac2b70951737fab93ca","title":"A Review of Sparse Expert Models in Deep Learning","venue":"ArXiv","year":2022,"referenceCount":87,"citationCount":14,"influentialCitationCount":1,"publicationDate":"04/09/2022","authors":"W. Fedus,J. Dean,Barret Zoph","id":"ca086f4c09cf8de705830ac2b70951737fab93ca","summary":"The concept of sparse expert models is reviewed, a basic description of the common algorithms is provided, the advances in the deep learning era are contextualized, and areas for future work are highlighted.","score":6},{"url":"https://www.semanticscholar.org/paper/dcff38de0e5fb47bdb31d472c21b0c2d88cbc4fc","title":"AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":64,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/10/2022","authors":"Se Jung Kwon,Jeonghoon Kim,Jeongin Bae,Kang Min Yoo,Jin-Hwa Kim,Baeseong Park,Byeongwook Kim,Jung-Woo Ha,Nako Sung,Dongsoo Lee","id":"dcff38de0e5fb47bdb31d472c21b0c2d88cbc4fc","summary":"It is demonstrated that AlphaTuning, when applied to GPT-2 and OPT, performs competitively with full fine-tuning on a variety of downstream tasks while achieving>10x compression ratio under 4-bit quantization and>1,000x reduction in the number of trainable parameters.","score":6},{"url":"https://www.semanticscholar.org/paper/9eccd7b1d184ceb9268da754f35e399bc161ed3c","title":"Understanding BLOOM: An empirical study on diverse NLP tasks","venue":"ArXiv","year":2022,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/11/2022","authors":"Parag Dakle,Sai Krishna Rallabandi,Preethi Raghavan","id":"9eccd7b1d184ceb9268da754f35e399bc161ed3c","summary":"An evaluation of smaller BLOOM model variants on various natural language processing tasks, including GLUE language understanding, prompt-based zero-shot and few-shot text classification and extraction, question answering, Prompt-based text generation, and multi-lingual text classification to understand model strengths/weaknesses and behavior shows that BLOoms under-perform on all GLUE tasks, question-answering, and text generation.","score":6},{"url":"https://www.semanticscholar.org/paper/93fdf5cf598aefb0335f001039e83494dc721c3a","title":"General-Purpose In-Context Learning by Meta-Learning Transformers","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":3,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"Louis Kirsch,James Harrison,Jascha Narain Sohl-Dickstein,Luke Metz","id":"93fdf5cf598aefb0335f001039e83494dc721c3a","summary":"This paper shows that Transformers and other black-box models can be meta-trained to act as general-purpose in-context learners, and describes phase transitions between algorithms that generalize, algorithms that memorize, and algorithms that fail to meta-train at all, induced by changes in model size, number of tasks, and meta-optimization.","score":6},{"url":"https://www.semanticscholar.org/paper/bfe6fd05f09647b001c7eb6e333a95c881c88344","title":"Human-Timescale Adaptation in an Open-Ended Task Space","venue":"ArXiv","year":2023,"referenceCount":129,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/01/2023","authors":"Adaptive Agent Team,Jakob Bauer,Kate Baumli,Satinder Baveja,Feryal M. P. Behbahani,Avishkar Bhoopchand,N. Bradley-Schmieg,Michael Chang,Natalie Clay,Adrian Collister,Vibhavari Dasagi,Lucy Gonzalez,Karol Gregor,Edward Hughes,Sheleem Kashem,Maria Loks-Thompson,Hannah Openshaw,Jack Parker-Holder,Shreyaan Pathak,Nicolas Perez Nieves,Nemanja Rakicevic,Tim Rocktäschel,Yannick Schroecker,Jakub Sygnowski,K. Tuyls,Sarah York,Alexander Zacherl,Lei M. Zhang","id":"bfe6fd05f09647b001c7eb6e333a95c881c88344","summary":"It is demonstrated that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans.","score":6},{"url":"https://www.semanticscholar.org/paper/0545ec3a4e3eeaec924847d5bd9b3436bcc136d8","title":"Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning","venue":"ArXiv","year":2023,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Hyunsoo Cho,Choonghyun Park,Junyeop Kim,Hyuhng Joon Kim,Kang Min Yoo,Sang-goo Lee","id":"0545ec3a4e3eeaec924847d5bd9b3436bcc136d8","summary":"This study systematically explore how the ability to detect out-of-distribution (OOD) changes as the size of the PLM grows or the transfer methods are altered.","score":6},{"url":"https://www.semanticscholar.org/paper/94d84d1403a23a8a8486a151f52a126beb16875c","title":"Partitioning Distributed Compute Jobs with Reinforcement Learning and Graph Neural Networks","venue":"ArXiv","year":2023,"referenceCount":92,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Christopher W. F. Parsonson,Z. Shabka,A. Ottino,G. Zervas","id":"94d84d1403a23a8a8486a151f52a126beb16875c","summary":"It is shown that maximum parallelisation is sub-optimal in relation to user-critical metrics such as throughput and blocking rate, and a proposed PAC-ML (partitioning for asynchronous computing with machine learning) is proposed.","score":6},{"url":"https://www.semanticscholar.org/paper/da2fe6cd385194b0274d04d04ee72e8caf3854d4","title":"Learning Universal Policies via Text-Guided Video Generation","venue":"ArXiv","year":2023,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Yilun Du,Mengjiao Yang,Bo Dai,H. Dai,Ofir Nachum,J. Tenenbaum,D. Schuurmans,P. Abbeel","id":"da2fe6cd385194b0274d04d04ee72e8caf3854d4","summary":"This work casts the sequential decision making problem as a text-conditioned video generation problem, where a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video.","score":6},{"url":"https://www.semanticscholar.org/paper/95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/09/2022","authors":"Darren Key,Wen-Ding Li,Kevin Ellis","id":"95fa2b27ab7eb84738441ee16da97323538938f9","summary":"An approach for improving the trustworthiness and overall accuracy of program synthesizers based on large language models for source code by analyzing the agreement between programs and predicates to judge both which program is most likely to be correct and whether the language model is able to solve the programming problem in the first place.","score":6},{"url":"https://www.semanticscholar.org/paper/47a541269d4ef70f37f0d3a57483312c4c6c2ad5","title":"Crawling the Internal Knowledge-Base of Language Models","venue":"ArXiv","year":2023,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Roi Cohen,Mor Geva,Jonathan Berant,A. Globerson","id":"47a541269d4ef70f37f0d3a57483312c4c6c2ad5","summary":"The crawling procedure is decomposed into sub-tasks, realized through specially designed prompts that control for both precision and recall, and yields high precision graphs, while emit-ting a reasonable number of facts per entity.","score":6},{"url":"https://www.semanticscholar.org/paper/5581bf85386737bd3378eec68189759a05280bea","title":"FOLIO: Natural Language Reasoning with First-Order Logic","venue":"ArXiv","year":2022,"referenceCount":80,"citationCount":4,"influentialCitationCount":1,"publicationDate":"02/09/2022","authors":"Simeng Han,Hailey Schoelkopf,Yilun Zhao,Zhenting Qi,Martin Riddell,Luke Benson,Lucy Sun,E. Zubova,Yujie Qiao,Matthew Burtell,David Peng,Jonathan Fan,Yixin Liu,Brian Wong,Malcolm Sailor,Ansong Ni,Linyong Nan,Jungo Kasai,Tao Yu,Rui Zhang,Shafiq R. Joty,Alexander R. Fabbri,Wojciech Kryscinski,Xi Victoria Lin,Caiming Xiong,Dragomir R. Radev","id":"5581bf85386737bd3378eec68189759a05280bea","summary":"The results show that one of the most capable Large Language Model (LLM) publicly available, GPT-3 davinci, achieves only slightly better than random results with few-shot prompting on a subset of FOLIO, and the model is especially bad at predicting the correct truth values for False and Unknown conclusions.","score":6},{"url":"https://www.semanticscholar.org/paper/c43a6f12b062a50617244611af180a8146e792de","title":"Learning from Natural Language Feedback","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":1,"influentialCitationCount":1,"publicationDate":2022,"authors":"J. Scheurer,Jon Ander Campos,Jun Shern Chan,Angelica Chen,Kyunghyun Cho,Ethan Perez","id":"c43a6f12b062a50617244611af180a8146e792de","summary":"This work proposes to learn from natural language feedback, which conveys more information per human evaluation, using a three-step learning algorithm that fine-tunes a GPT-3 model to roughly human-level summarization ability.","score":6},{"url":"https://www.semanticscholar.org/paper/2aec574791fd33e9be32fd5191a66734f805a6a1","title":"Training Language Models with Natural Language Feedback","venue":"","year":2022,"referenceCount":35,"citationCount":5,"influentialCitationCount":1,"publicationDate":2022,"authors":"J. Scheurer,Jon Ander Campos,Jun Shern Chan,Angelica Chen,Kyunghyun Cho,Ethan Perez","id":"2aec574791fd33e9be32fd5191a66734f805a6a1","summary":"This work proposes to learn from natural language feedback, which conveys more information per human evaluation, from a GPT-3 model to roughly human-level summarization ability using a three-step learning algorithm.","score":6},{"url":"https://www.semanticscholar.org/paper/c8d4c5907ddce4f42b20662324a2ccff7bf9d6c9","title":"Nearest Neighbor Zero-Shot Inference","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":37,"citationCount":5,"influentialCitationCount":0,"publicationDate":2022,"authors":"Weijia Shi,Julian Michael,Suchin Gururangan,Luke Zettlemoyer","id":"c8d4c5907ddce4f42b20662324a2ccff7bf9d6c9","summary":"KNN-Prompt is introduced, a simple and effective kNN-LM with automatically expanded fuzzy verbalizers that is effective for domain adaptation with no further training, and gains increase with the size of the retrieval model.","score":6},{"url":"https://www.semanticscholar.org/paper/3ac5aa6ac59253611ef3cb72a95cbe21ef5dda1b","title":"Reframing Instructional Prompts to GPTk’s Language","venue":"Findings","year":2021,"referenceCount":44,"citationCount":55,"influentialCitationCount":6,"publicationDate":"16/09/2021","authors":"Swaroop Mishra,Daniel Khashabi,Chitta Baral,Yejin Choi,Hannaneh Hajishirzi","id":"3ac5aa6ac59253611ef3cb72a95cbe21ef5dda1b","summary":"This work studies several classes of reframing techniques for manual reformulation of prompts into more effective ones, and hopes these empirically-driven techniques will pave the way towards more effective future prompting algorithms.","score":6},{"url":"https://www.semanticscholar.org/paper/53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":118,"citationCount":92,"influentialCitationCount":13,"publicationDate":"16/01/2022","authors":"Tianbao Xie,Chen Henry Wu,Peng Shi,Ruiqi Zhong,Torsten Scholak,Michihiro Yasunaga,Chien-Sheng Wu,Ming Zhong,Pengcheng Yin,Sida I. Wang,Victor Zhong,Bailin Wang,Chengzu Li,Connor Boyle,Ansong Ni,Ziyu Yao,Dragomir R. Radev,Caiming Xiong,Lingpeng Kong,Rui Zhang,Noah A. Smith,Luke Zettlemoyer,Tao Yu","id":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","summary":"The UnifiedSKG framework is proposed, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset.","score":6},{"url":"https://www.semanticscholar.org/paper/41f44979cf1cd3f4cbd615dc130bc33721f5281b","title":"MemPrompt: Memory-assisted Prompt Editing with User Feedback","venue":"","year":2022,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/01/2022","authors":"Aman Madaan,Niket Tandon,Peter Clark,Yiming Yang","id":"41f44979cf1cd3f4cbd615dc130bc33721f5281b","summary":"It is shown how a (simulated) user can interac-tively teach a deployed GPT -3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT 3, a step towards the low-cost utility enhancement for very large pre-trained LMs.","score":6},{"url":"https://www.semanticscholar.org/paper/31e396eab8edb44f79e3158eeefc3280afb404f4","title":"How Many Data Samples is an Additional Instruction Worth?","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":8,"influentialCitationCount":0,"publicationDate":"17/03/2022","authors":"Ravsehaj Singh Puri,Swaroop Mishra,Mihir Parmar,Chitta Baral","id":"31e396eab8edb44f79e3158eeefc3280afb404f4","summary":"A subset of tasks in the expanded version of NATURAL INSTRUCTIONS is augmented with additional instructions and it significantly improves model performance (up to 35%), especially in the low-data regime.","score":6},{"url":"https://www.semanticscholar.org/paper/85b50df702604d738650d100a2dd6da40a8a6e4c","title":"Match-Prompt: Improving Multi-task Generalization Ability for Neural Text Matching via Prompt Learning","venue":"International Conference on Information and Knowledge Management","year":2022,"referenceCount":71,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/04/2022","authors":"Shicheng Xu,Liang Pang,Huawei Shen,Xueqi Cheng","id":"85b50df702604d738650d100a2dd6da40a8a6e4c","summary":"Experimental results on eighteen public datasets show that Match-Prompt can improve multi- Task generalization capability of PLMs in text matching and yield better in-domain multi-task, out-of-domainmulti-task and new task adaptation performance than multi- task and task-specific models trained by previous fine-tuning paradigm.","score":6},{"url":"https://www.semanticscholar.org/paper/a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":59,"influentialCitationCount":18,"publicationDate":"12/04/2022","authors":"Daniel Fried,Armen Aghajanyan,Jessy Lin,Sida I. Wang,Eric Wallace,Freda Shi,Ruiqi Zhong,Wen-tau Yih,Luke Zettlemoyer,M. Lewis","id":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","summary":"INCODER is introduced, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling) and the ability to condition on bidirectional context substantially improves performance on challenging tasks such as type inference, comment generation, and variable re-naming.","score":6},{"url":"https://www.semanticscholar.org/paper/15190e8b459bd85d546286f7d7da61b4f4f3f58a","title":"What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?","venue":"International Conference on Machine Learning","year":2022,"referenceCount":93,"citationCount":34,"influentialCitationCount":6,"publicationDate":"12/04/2022","authors":"Thomas Wang,Adam Roberts,Daniel Hesslow,Teven Le Scao,Hyung Won Chung,Iz Beltagy,Julien Launay,Colin Raffel","id":"15190e8b459bd85d546286f7d7da61b4f4f3f58a","summary":"A large-scale evaluation of modeling choices and their impact on zero-shot generalization of large pretrained Transformer language models focuses on text-to-text models and shows that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero- shot generalization after purely self-supervised pretraining.","score":6},{"url":"https://www.semanticscholar.org/paper/fd7c3c8fbe8cf88bd967ead02738b43081e306a7","title":"Training Language Models with Language Feedback","venue":"","year":2022,"referenceCount":43,"citationCount":7,"influentialCitationCount":2,"publicationDate":"29/04/2022","authors":"J. Scheurer,Jon Ander Campos,Jun Shern Chan,Angelica Chen,Kyunghyun Cho,Ethan Perez","id":"fd7c3c8fbe8cf88bd967ead02738b43081e306a7","summary":"This work proposes to learn from natural language feedback, which conveys more information per human evaluation, from a GPT-3 model to roughly human-level summarization ability using a three-step learning algorithm.","score":6},{"url":"https://www.semanticscholar.org/paper/55a250868627de2d202d06e7cb3f6cbcd3a66f88","title":"ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":84,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/05/2022","authors":"Akari Asai,Mohammadreza Salehi,Matthew E. Peters,Hannaneh Hajishirzi","id":"55a250868627de2d202d06e7cb3f6cbcd3a66f88","summary":"A new multi-task, parameter-efficient language model (LM) tuning method that learns to transfer knowledge across different tasks via a mixture of soft prompts—small prefix embedding vectors pre-trained for different tasks that significantly outperforms prompt tuning and outperforms or matches fully fine-tuned or other parameter- efficient tuning approaches that use 10 times more parameters.","score":6},{"url":"https://www.semanticscholar.org/paper/8f926c0c3f1557a9241b7e75609082a1f207a75e","title":"InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":124,"citationCount":10,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"Prakhar Gupta,Cathy Jiao,Yi-Ting Yeh,Shikib Mehri,M. Eskénazi,Jeffrey P. Bigham","id":"8f926c0c3f1557a9241b7e75609082a1f207a75e","summary":"This work introduces InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets, and introduces novel meta-tasks to ensure that models adhere to instructions.","score":6},{"url":"https://www.semanticscholar.org/paper/563a851106623b9f112d0e2a290d3950a871079c","title":"kNN-Prompt: Nearest Neighbor Zero-Shot Inference","venue":"","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/05/2022","authors":"Weijia Shi,Julian Michael,Suchin Gururangan,Luke Zettlemoyer","id":"563a851106623b9f112d0e2a290d3950a871079c","summary":"KNN-Prompt is introduced, a simple and effective kNN-LM with automatically expanded fuzzy verbalizers that is effective for domain adaptation with no further training, and gains increase with the size of the retrieval model.","score":6},{"url":"https://www.semanticscholar.org/paper/86d0d3855f94105e25d81cab9f3d269c6062a9c4","title":"Selective Annotation Makes Language Models Better Few-Shot Learners","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":16,"influentialCitationCount":1,"publicationDate":"05/09/2022","authors":"Hongjin Su,Jungo Kasai,Chen Henry Wu,Weijia Shi,Tianlu Wang,Jiayi Xin,Rui Zhang,Mari Ostendorf,Luke Zettlemoyer,Noah A. Smith,Tao Yu","id":"86d0d3855f94105e25d81cab9f3d269c6062a9c4","summary":"It is shown that the effectiveness of vote- k is consistent with different language model sizes and domain shifts between training and test data, and will help researchers and practitioners design new natural language tasks and beyond.","score":6},{"url":"https://www.semanticscholar.org/paper/b65b7f480a61d3dd31d8117b349cabc87c8ccf6c","title":"Bidirectional Language Models Are Also Few-shot Learners","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":3,"influentialCitationCount":1,"publicationDate":"29/09/2022","authors":"Ajay Patel,Bryan Li,Mohammad Sadegh Rasooli,Noah Constant,Colin Raffel,Chris Callison-Burch","id":"b65b7f480a61d3dd31d8117b349cabc87c8ccf6c","summary":"For the first time, prompt-based learning is an emergent property of a broader class of language models, rather than only unidirectional models, and is shown to be effective on question answering and summarization.","score":6},{"url":"https://www.semanticscholar.org/paper/0979695b5d74016e97ab8f306f632114e98bd6d9","title":"Task Compass: Scaling Multi-task Pre-training with Task Prefix","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":90,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Zhuosheng Zhang,Shuo Wang,Yichong Xu,Yuwei Fang,W. Yu,Yang Liu,H. Zhao,Chenguang Zhu,Michael Zeng","id":"0979695b5d74016e97ab8f306f632114e98bd6d9","summary":"This work proposes a task prefix guided multi-task pre-training framework to explore the relationships among tasks and shows that the model can not only serve as the strong foundation backbone for a wide range of tasks but also be feasible as a probing tool for analyzing task relationships.","score":6},{"url":"https://www.semanticscholar.org/paper/82cd40e926300b6b18c34ced2edeb07e84d9d6c7","title":"Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":97,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/10/2022","authors":"Yuxian Gu,Pei Ke,Xiaoyan Zhu,Minlie Huang","id":"82cd40e926300b6b18c34ced2edeb07e84d9d6c7","summary":"This work proposes Unlabeled Data Augmented Instruction Tuning (UDIT) to take better advantage of the instructions during IT by constructing pseudo-labeled data from unlabeled plain texts and comprehensively analyzes the key factors of UDIT to investigate how to better improve IT with unlabeling data.","score":6},{"url":"https://www.semanticscholar.org/paper/77a94f6c91ee1590dd2c6fd80b4a6d8bffdb91ac","title":"Help me write a Poem - Instruction Tuning as a Vehicle for Collaborative Poetry Writing","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":45,"citationCount":4,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Tuhin Chakrabarty,Vishakh Padmakumar,Hengxing He","id":"77a94f6c91ee1590dd2c6fd80b4a6d8bffdb91ac","summary":"","score":6},{"url":"https://www.semanticscholar.org/paper/b3d04ab5362b3fb171b5231dcf4c675c4c64ec02","title":"HyperTuning: Toward Adapting Large Language Models without Back-propagation","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":2,"influentialCitationCount":1,"publicationDate":"22/11/2022","authors":"Jason Phang,Yi Mao,Pengcheng He,Weizhu Chen","id":"b3d04ab5362b3fb171b5231dcf4c675c4c64ec02","summary":"This work proposes HyperTuning, a novel approach to model adaptation that uses a hypermodel to generate task-speciﬁc parameters for a ﬁxed downstream model and shows that using hypermodel-generated parameters as initializations for further parameter-efﬂcient ﬀne-tuning improves performance.","score":6},{"url":"https://www.semanticscholar.org/paper/05da1a63f448d1e4c41306d37b7a3d21a9974cef","title":"NIR-Prompt: A Multi-task Generalized Neural Information Retrieval Training Framework","venue":"ArXiv","year":2022,"referenceCount":80,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Shicheng Xu,Liang Pang,Huawei Shen,Xueqi Cheng","id":"05da1a63f448d1e4c41306d37b7a3d21a9974cef","summary":"Experiments show that NIR-Prompt can improve the generalization of PLMs in NIR for both retrieval and reranking stages compared with baselines and under in- domain multi-task, out-of-domain multi- task, and new task adaptation settings.","score":6},{"url":"https://www.semanticscholar.org/paper/364fda684ba3063715743dfbed099c5d1b43dbfd","title":"Self-Play and Self-Describe: Policy Adaptation with Vision-Language Foundation Models","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/12/2022","authors":"Yuying Ge,Annabella Macaluso,Erran L. Li,Ping Luo,Xiaolong Wang","id":"364fda684ba3063715743dfbed099c5d1b43dbfd","summary":"This work explores a new paradigm on leveraging the pre-trained foundation models with Self-PLAY and SelfDescribe with the focus on generalization on unseen objects, unseen tasks, unseen environments, and sim-to-real transfer and shows SPLAYD improves baseline in all cases.","score":6},{"url":"https://www.semanticscholar.org/paper/6db13f58ff662eefa823a660fa86faf8ddf75533","title":"Controllable Text Generation with Language Constraints","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Howard Chen,Huihan Li,Danqi Chen,Karthik Narasimhan","id":"6db13f58ff662eefa823a660fa86faf8ddf75533","summary":"A solution to leverage a language model’s own internal knowledge to guide generation and propose three forms of guidance (binary veriﬁer, top-k token, textual example), and employ pre-tuning approaches to distill the guidance to tackle diverse natural language constraints.","score":6},{"url":"https://www.semanticscholar.org/paper/89184ab496b2a1ae31e068e628479b4cd8f4b9d2","title":"Do We Still Need Clinical Language Models?","venue":"","year":2023,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Eric P. Lehman,Evan Hernandez,Diwakar Mahajan,Jonas Wulff,Micah J. Smith,Zachary M. Ziegler,Daniel Nadler,P. Szolovits,A. Johnson,Emily Alsentzer","id":"89184ab496b2a1ae31e068e628479b4cd8f4b9d2","summary":"It is shown that relatively small specialized clinical models substantially outperform all in-context learning approaches, even when finetuned on limited annotated data.","score":6},{"url":"https://www.semanticscholar.org/paper/76b547bc6c14940b262e9df2802d370cc5ef140e","title":"InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis","venue":"","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Kevin Scaria,Himanshu Gupta,S. Sawant,Swaroop Mishra,Chitta Baral","id":"76b547bc6c14940b262e9df2802d370cc5ef140e","summary":"InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models.","score":6},{"url":"https://www.semanticscholar.org/paper/10be7057efd37643a6aaf277ba4bfd8ab2a35775","title":"Controllable Factuality in Document-Grounded Dialog Systems Using a Noisy Channel Model","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Nico Daheim,David Thulke,C. Dugast,H. Ney","id":"10be7057efd37643a6aaf277ba4bfd8ab2a35775","summary":"This work presents a model for document-grounded response generation in dialog that is decomposed into two components according to Bayes theorem, and outlines how introducing scaling factors between the components allows for controlling the tradeoff between factuality and fluency in the model output.","score":6},{"url":"https://www.semanticscholar.org/paper/25c402db512d327f1da143de3b8e797ad6fbfe5b","title":"P ROG P ROMPT : Generating Situated Robot Task Plans using Large Language Models","venue":"","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Llm Gpt","id":"25c402db512d327f1da143de3b8e797ad6fbfe5b","summary":"This work presents a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks, and makes concrete recommendations about prompt structure and generation constraints through ablation experiments.","score":6},{"url":"https://www.semanticscholar.org/paper/36af02a2fbce04f536dd339ccccb0434b85cfde8","title":"Language and culture internalization for human-like autotelic AI","venue":"Nature Machine Intelligence","year":2022,"referenceCount":122,"citationCount":1,"influentialCitationCount":0,"publicationDate":"02/06/2022","authors":"Cédric Colas,Tristan Karch,Clément Moulin-Frier,P. Oudeyer","id":"36af02a2fbce04f536dd339ccccb0434b85cfde8","summary":"This work proposes Vygotskian autotelic agents — agents able to internalise their interactions with others and turn them into cognitive tools and focuses on language and shows how its structure and informational content may support the development of new cognitive functions in artiﬁcial agents as it does in humans.","score":6},{"url":"https://www.semanticscholar.org/paper/c03fa01fbb9c77fe3d10609ba5f1dee33a723867","title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":13,"influentialCitationCount":3,"publicationDate":"22/09/2022","authors":"Ishika Singh,Valts Blukis,Arsalan Mousavian,Ankit Goyal,Danfei Xu,Jonathan Tremblay,D. Fox,Jesse Thomason,Animesh Garg","id":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","summary":"This work presents a programmatic LLM prompt structure that enables plan generation functional across situated envi- ronments, robot capabilities, and tasks, and makes concrete recommendations about prompt structure and generationaints through ablation experiments.","score":6},{"url":"https://www.semanticscholar.org/paper/7619b0aad6f16a0328021c5fdd3d97239e362e97","title":"Visual Language Maps for Robot Navigation","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":4,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Chen Huang,Oier Mees,Andy Zeng,W. Burgard","id":"7619b0aad6f16a0328021c5fdd3d97239e362e97","summary":"VLMaps is proposed, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world and enable navigation according to more complex language instructions than existing methods.","score":6},{"url":"https://www.semanticscholar.org/paper/4bfc5b87891e95e1591e47f3ddd6eab19b616ce5","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":54,"citationCount":4,"influentialCitationCount":1,"publicationDate":"17/10/2022","authors":"A. M. H. Tiong,Junnan Li,Boyang Li,S. Savarese,S. Hoi","id":"4bfc5b87891e95e1591e47f3ddd6eab19b616ce5","summary":"This work first generate question-guided informative image captions, and pass the captions to a PLM as context for question answering, and achieves state-of-the-art results on zero-shot VQAv2 and GQA.","score":6},{"url":"https://www.semanticscholar.org/paper/f390e9d688f55d2518f0c09351a31c09ba3add88","title":"Manifestations of Xenophobia in AI Systems","venue":"ArXiv","year":2022,"referenceCount":339,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"Nenad Tomašev,J. L. Maynard,Iason Gabriel","id":"f390e9d688f55d2518f0c09351a31c09ba3add88","summary":"This work ground the impact of xenophobia by first identifying distinct types of xenophobic harms, and then applying this framework across a number of prominent AI application domains, reviewing the potential interplay between AI and xenophobia on social media and recommendation systems.","score":6},{"url":"https://www.semanticscholar.org/paper/a7b060413027cbd25b6144f4a6214c3bd4fb12e3","title":"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters","venue":"ArXiv","year":2022,"referenceCount":20,"citationCount":2,"influentialCitationCount":1,"publicationDate":"20/12/2022","authors":"Boshi Wang,Sewon Min,Xiang Deng,Jiaming Shen,You Wu,Luke Zettlemoyer,Huan Sun","id":"a7b060413027cbd25b6144f4a6214c3bd4fb12e3","summary":"It is shown that CoT reasoning is possible even with invalid demonstrations—prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference.","score":6},{"url":"https://www.semanticscholar.org/paper/a07a94168608322600fd3cab54df1410b96852b6","title":"Case-based Reasoning for Natural Language Queries over Knowledge Bases","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":94,"citationCount":48,"influentialCitationCount":10,"publicationDate":"18/04/2021","authors":"R. Das,M. Zaheer,Dung Ngoc Thai,Ameya Godbole,Ethan Perez,Jay Yoon Lee,Lizhen Tan,L. Polymenakos,A. McCallum","id":"a07a94168608322600fd3cab54df1410b96852b6","summary":"A neuro-symbolic CBR approach (CBR-KBQA) for question answering over large knowledge bases that is capable of using new cases without any further training and is able to successfully generate logical forms containing unseen KB entities as well as relations.","score":5},{"url":"https://www.semanticscholar.org/paper/de549c1592a62c129b8d49c8c0137aa6859b103f","title":"Internet-Augmented Dialogue Generation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":44,"citationCount":95,"influentialCitationCount":20,"publicationDate":"15/07/2021","authors":"M. Komeili,Kurt Shuster,J. Weston","id":"de549c1592a62c129b8d49c8c0137aa6859b103f","summary":"An approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information.","score":5},{"url":"https://www.semanticscholar.org/paper/2eb710b446570f48377b25eb279295648d05f65d","title":"On sample efficiency and systematic generalization of grounded language understanding with deep learning","venue":"","year":2020,"referenceCount":173,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/06/2020","authors":"Dzmitry Bahdanau","id":"2eb710b446570f48377b25eb279295648d05f65d","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/abba9a6f99d877fdd1b8412ddfcc26fdac6163dc","title":"SMART: Self-supervised Multi-task pretrAining with contRol Transformers","venue":"ArXiv","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"Yanchao Sun,Shuang Ma,Ratnesh Madaan,Rogerio Bonatti,Furong Huang,Ashish Kapoor","id":"abba9a6f99d877fdd1b8412ddfcc26fdac6163dc","summary":"This work formulates a general pretraining-finetuning pipeline for sequential decision making, and proposes a generic pretraining framework Self-supervised Multi-task pretrAining with contRol Transformer (SMART), which significantly improves the learning efficiency among seen and unseen downstream tasks and domains under different learning scenarios.","score":5},{"url":"https://www.semanticscholar.org/paper/7cd79ad685a23e7a9acc1b7a25e15f739a6dd2a5","title":"Teachable Reinforcement Learning via Advice Distillation","venue":"Neural Information Processing Systems","year":2022,"referenceCount":64,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/03/2022","authors":"Olivia Watkins,Trevor Darrell,P. Abbeel,Jacob Andreas,Abhishek Gupta","id":"7cd79ad685a23e7a9acc1b7a25e15f739a6dd2a5","summary":"In puzzle-solving, navigation, and locomotion domains, it is shown that agents that learn from advice can acquire new skills with signiﬁcantly less human supervision than standard reinforcement learning algorithms and often less than imitation learning.","score":5},{"url":"https://www.semanticscholar.org/paper/e03d6414dd5a3e7fcac7fe273089ca6e5ad848dd","title":"Efficient Exploration using Model-Based Quality-Diversity with Gradients","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Bryan Lim,Manon Flageat,Antoine Cully","id":"e03d6414dd5a3e7fcac7fe273089ca6e5ad848dd","summary":"This approach optimizes all members of a population simultaneously to maintain both performance and diversity efﬁciently by leveraging the effectiveness of QD algorithms as good data generators to train deep models and maintains the divergent search capabilities of population-based approaches on tasks with deceptive rewards.","score":5},{"url":"https://www.semanticscholar.org/paper/e83478d3752ca2a3dbd5a4ff1bae2ca3c69750df","title":"Actively Learning Costly Reward Functions for Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/11/2022","authors":"Andr'e Eberhard,Houssam Metni,G. Fahland,Alexander Stroh,Pascal Friederich","id":"e83478d3752ca2a3dbd5a4ff1bae2ca3c69750df","summary":"This work proposes to alleviate the problem of costly ground-truth rewards with rewards modeled by neural networks, counteracting non-stationarity of state and reward distributions during training with an active learning component, and shows that using the proposed ACRL method, it is possible to train agents in complex real-world environments orders of magnitudes faster.","score":5},{"url":"https://www.semanticscholar.org/paper/466317182e02c1cfdf5a227639778a296f48ca11","title":"Melting Pot 2.0","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/11/2022","authors":"J. Agapiou,A. Vezhnevets,Edgar A. Duéñez-Guzmán,Jayd Matyas,Yiran Mao,Peter Sunehag,R. Koster,Udari Madhushani,Kavya Kopparapu,R. Comanescu,D. Strouse,Michael Bradley Johanson,Sukhdeep Singh,Julia Haas,Igor Mordatch,D. Mobbs,Joel Z. Leibo","id":"466317182e02c1cfdf5a227639778a296f48ca11","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/aba015094c0f34b14a6744f63c6b5ea3cd9e0cc9","title":"Towards Deadlock Handling with Machine Learning in a Simulation-Based Learning Environment","venue":"Online World Conference on Soft Computing in Industrial Applications","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/12/2022","authors":"Marcel Müller,T. Reggelin,Iegor Kutsenko,Hartmut Zadek,L. Reyes-Rubiano","id":"aba015094c0f34b14a6744f63c6b5ea3cd9e0cc9","summary":"The first results show that artificial neural networks can learn to handle deadlock capable logistic systems with low complexity.","score":5},{"url":"https://www.semanticscholar.org/paper/63b5d00b658b5b7314676e0dd75eb61349298e8a","title":"Breaking the Curse of Multiagency: Provably Efficient Decentralized Multi-Agent RL with Function Approximation","venue":"","year":2023,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Yuanhao Wang,Qinghua Liu,Yunru Bai,Chi Jin","id":"63b5d00b658b5b7314676e0dd75eb61349298e8a","summary":"This paper presents the first line of MARL algorithms that provably resolve the curse of multiagency under function approximation, and designs a new decentralized algorithm -- V-Learning with Policy Replay, which gives the first polynomial sample complexity results for learning approximate Coarse Correlated Equilibria of Markov Games under decentralized linear function approximation.","score":5},{"url":"https://www.semanticscholar.org/paper/31ed72a18ed8a1008786130af9f1d61761cff4f3","title":"DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback","venue":"ArXiv","year":2018,"referenceCount":31,"citationCount":42,"influentialCitationCount":11,"publicationDate":"28/10/2018","authors":"Riku Arakawa,Sosuke Kobayashi,Y. Unno,Yuta Tsuboi,S. Maeda","id":"31ed72a18ed8a1008786130af9f1d61761cff4f3","summary":"This work demonstrates a real-world human-in-the-loop RL application where a camera automatically recognizes a user's facial expressions as feedback to the agent while the agent explores a maze and proposes an RL method called DQN-TAMER, which efficiently uses both human feedback and distant rewards.","score":5},{"url":"https://www.semanticscholar.org/paper/3e425dd366e0696d5b835d8fff92add9873077da","title":"On Efficient Reinforcement Learning for Full-length Game of StarCraft II","venue":"Journal of Artificial Intelligence Research","year":2022,"referenceCount":36,"citationCount":2,"influentialCitationCount":0,"publicationDate":"23/09/2022","authors":"Ruo-Ze Liu,Zhen-Jia Pang,Zhou-Yu Meng,Wenhai Wang,Yang Yu,Tong Lu","id":"3e425dd366e0696d5b835d8fff92add9873077da","summary":"This work investigates a set of RL techniques for the full-length game of StarCraft II and investigates a hierarchical RL approach, where the hierarchy involves two: the extracted macro-actions from experts’ demonstration trajectories and a hierarchical architecture of neural networks.","score":5},{"url":"https://www.semanticscholar.org/paper/2f63b84461a7fe159451996d9901856f019e9277","title":"Teacher-student curriculum learning for reinforcement learning","venue":"ArXiv","year":2022,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Yanick Schraner","id":"2f63b84461a7fe159451996d9901856f019e9277","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/a6c6e464b2ab15bb04176c26da3816d4038d5176","title":"Dungeons and Data: A Large-Scale NetHack Dataset","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Eric Hambro,Roberta Raileanu,Dan Rothermel,Vegard Mella,Tim Rocktäschel,Heinrich Küttler,Naila Murray","id":"a6c6e464b2ab15bb04176c26da3816d4038d5176","summary":"It is shown that, while current state-of-the-art methods in ofﬂine RL and learning from demonstrations can effectively make use of the dataset, playing NetHack at human-level performance remains an open research challenge.","score":5},{"url":"https://www.semanticscholar.org/paper/6fdbe62e8bedbbdb6292a37cd239208d93f2949b","title":"Progress and summary of reinforcement learning on energy management of MPS-EV","venue":"ArXiv","year":2022,"referenceCount":139,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/11/2022","authors":"Jincheng Hu,Yang Lin,Liang Chu,Zhuoran Hou,Jihan Li,Jingjing Jiang,Yuanjian Zhang","id":"6fdbe62e8bedbbdb6292a37cd239208d93f2949b","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/408b5fb36616f9a60b1911d148b88d6a5d6def7c","title":"Curiosity-Driven and Victim-Aware Adversarial Policies","venue":"Asia-Pacific Computer Systems Architecture Conference","year":2022,"referenceCount":67,"citationCount":5,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Chen Gong,Zhou Yang,Yunru Bai,Jieke Shi,Arunesh Sinha,Bowen Xu,D. Lo,Xinwen Hou,Guoliang Fan","id":"408b5fb36616f9a60b1911d148b88d6a5d6def7c","summary":"This paper develops curiosity-driven and victim-aware adversarial policy training, a novel method that can more effectively exploit the defects of victim agents and suggests that the method is harder to defend against a commonly used defensive strategy.","score":5},{"url":"https://www.semanticscholar.org/paper/ab4816876ef3ba7080d02b6d2cc487693ad3e5d8","title":"Tuning Synaptic Connections instead of Weights by Genetic Algorithm in Spiking Policy Network","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/12/2022","authors":"Duzhen Zhang,Tielin Zhang,Shuncheng Jia,Qing Wang,Bo Xu","id":"ab4816876ef3ba7080d02b6d2cc487693ad3e5d8","summary":"Inspired by biological research that the brain forms memories by forming new synaptic connections and rewires these connections based on new experiences, the SPN mimics the sensorimotor neuron pathway of insects and communicates through event-based spikes to solve given tasks.","score":5},{"url":"https://www.semanticscholar.org/paper/3cbdcec97acef3cb32f9759eb3b49ee6dafd9892","title":"Transformer in Transformer as Backbone for Deep Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/12/2022","authors":"Hangyu Mao,Rui Zhao,Hao Chen,Jianye Hao,Yiqun Chen,Dong Li,Junge Zhang,Zhen Xiao","id":"3cbdcec97acef3cb32f9759eb3b49ee6dafd9892","summary":"The Transformer in Transformer (TIT) backbone is proposed, which cascades two Transformers in a very natural way: the inner one is used to process a single observation, while the outer one is responsible for processing the observation history; combining both is expected to extract spatial-temporal representations for good decision-making.","score":5},{"url":"https://www.semanticscholar.org/paper/e72f3a2e0e3b3c6939164b68dccc073e0e947436","title":"Multi-Agent Reinforcement Learning Based Actuator Control for EV HVAC Systems","venue":"IEEE Access","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Sungho Joo,Dongmin Lee,Minseop Kim,Taehoon Lee,Sanghyeok Choi,Seung-Chun Kim,Jeyeol Lee,Joo-Whan Kim,Yongsub Lim,Jeonghoon Lee","id":"e72f3a2e0e3b3c6939164b68dccc073e0e947436","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/222baa4e9e7ce691fdfddbc826a70e027daed70d","title":"Reinforcement Learning in Healthcare: A Survey","venue":"ACM Computing Surveys","year":2019,"referenceCount":373,"citationCount":215,"influentialCitationCount":7,"publicationDate":"22/08/2019","authors":"Chao Yu,Jiming Liu,S. Nemati","id":"222baa4e9e7ce691fdfddbc826a70e027daed70d","summary":"This survey provides an extensive overview of RL applications in a variety of healthcare domains, ranging from dynamic treatment regimes in chronic diseases and critical care, automated medical diagnosis, and many other control or scheduling problems that have infiltrated every aspect of the healthcare system.","score":5},{"url":"https://www.semanticscholar.org/paper/9bf040845e5d27e4de5832c032b9c8f32fcf7b82","title":"Model-Based Transfer Reinforcement Learning Based on Graphical Model Representations","venue":"IEEE Transactions on Neural Networks and Learning Systems","year":2021,"referenceCount":77,"citationCount":3,"influentialCitationCount":0,"publicationDate":"20/09/2021","authors":"Yuewen Sun,Kun Zhang,Changyin Sun","id":"9bf040845e5d27e4de5832c032b9c8f32fcf7b82","summary":"Relation Transfer is defined as explainable and transferable learning based on graphical model representations, inferring the skeleton and relations among variables in a causal view and generalizing to the target domain.","score":5},{"url":"https://www.semanticscholar.org/paper/3c84a3c87ee58e9dc7c0073cea5c311d466d07bb","title":"Challenging Machine Learning-based Clone Detectors via Semantic-preserving Code Transformations","venue":"IEEE Transactions on Software Engineering","year":2021,"referenceCount":79,"citationCount":5,"influentialCitationCount":0,"publicationDate":"21/11/2021","authors":"Weiwei Zhang,Shengjian Guo,Hongyu Zhang,Yulei Sui,Yinxing Xue,Yun Xu","id":"3c84a3c87ee58e9dc7c0073cea5c311d466d07bb","summary":"Surprisingly, the experiments show that, despite the notable successes achieved by existing clone detectors, the ML models inside these detectors still cannot distinguish numerous clones produced by the code transformations in CloneGen.","score":5},{"url":"https://www.semanticscholar.org/paper/883fa47931757e8721b3357387f7164a4da6e1fd","title":"Proximal Policy Optimization with Adaptive Threshold for Symmetric Relative Density Ratio","venue":"Results in Control and Optimization","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/03/2022","authors":"Taisuke Kobayashi","id":"883fa47931757e8721b3357387f7164a4da6e1fd","summary":"A new PPO derived using relative Pearson (RPE) divergence, therefore so-called PPO-RPE, to design the threshold adaptively, to maximize the values of regularization of policy is proposed.","score":5},{"url":"https://www.semanticscholar.org/paper/db86eddbd96424a486c3a7301767b65349dfd604","title":"Modelling fine-sliced three dimensional electron diffraction data with dynamical Bloch-wave simulations","venue":"IUCrJ","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/08/2022","authors":"Anton Cleverley,R. Beanland","id":"db86eddbd96424a486c3a7301767b65349dfd604","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/2d0fb59b1e6a8cb80ae2a0060bd35637cf4e8bbb","title":"Systems Theoretic Process Analysis of a Run Time Assured Neural Network Control System","venue":"AIAA SCITECH 2023 Forum","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/09/2022","authors":"Kerianne L. Hobbs,Benjamin Heiner,Lillian Busse,Kyle Dunlap,Jonathan C. Rowanhill,A. Hocking,Aditya Zutshi","id":"2d0fb59b1e6a8cb80ae2a0060bd35637cf4e8bbb","summary":"This research considers the problem of identifying safety constraints and developing Run Time Assurance (RTA) for Deep Reinforcement Learning (RL) Tactical Autopilots that use neural network control systems (NNCS) and applies STAMP and STPA to an NNCS bounded by RTA.","score":5},{"url":"https://www.semanticscholar.org/paper/29250e776d98d1e2b0d0222eaf167ebe890e688d","title":"Distributed Multiagent Deep Reinforcement Learning for Multiline Dynamic Bus Timetable Optimization","venue":"IEEE Transactions on Industrial Informatics","year":2023,"referenceCount":30,"citationCount":4,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Haoyang Yan,Zhiyong Cui,Xinqiang Chen,Xiaolei Ma","id":"29250e776d98d1e2b0d0222eaf167ebe890e688d","summary":"This article considers the multiline dynamic bus timetable optimization problem as a Markov decision process model and proposes a multiagent deep reinforcement learning framework to ensure effective learning from the imperfect-information game, where the passenger demand and traffic condition are not always known in advance.","score":5},{"url":"https://www.semanticscholar.org/paper/53cf80e5eadc8e4b3df358ce856cf14cb71efc18","title":"Accelerating Policy Gradient by Estimating Value Function from Prior Computation in Deep Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Md Masudur Rahman,Yexiang Xue","id":"53cf80e5eadc8e4b3df358ce856cf14cb71efc18","summary":"This paper investigates the use of prior computation to estimate the value function to improve sample efficiency in on-policy policy gradient methods in reinforcement learning and learns a new value function for the target task while combining it with a value estimate from the prior computation.","score":5},{"url":"https://www.semanticscholar.org/paper/a19c67d9d1d416731930dff76d8c79ff5b8247c9","title":"Population-size-Aware Policy Optimization for Mean-Field Games","venue":"ArXiv","year":2023,"referenceCount":93,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Pengdeng Li,Xinrun Wang,Shuxin Li,Hau Chan,Bo An","id":"a19c67d9d1d416731930dff76d8c79ff5b8247c9","summary":"This work attempts to bridge the two fields of finite-agent and infinite-agent games, by studying how the optimal policies of agents evolve with the number of agents in mean-field games, an agent-centric perspective in contrast to the existing works focusing typically on the convergence of the empirical distribution of the population.","score":5},{"url":"https://www.semanticscholar.org/paper/8031bfd2408b9fa41e85b346b9452ebd63073076","title":"Distributional GFlowNets with Quantile Flows","venue":"ArXiv","year":2023,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/02/2023","authors":"Dinghuai Zhang,L. Pan,Ricky T. Q. Chen,Aaron C. Courville,Y. Bengio","id":"8031bfd2408b9fa41e85b346b9452ebd63073076","summary":"This work adopts a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training, and finds that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods.","score":5},{"url":"https://www.semanticscholar.org/paper/24c7d869f741b61dae384d97d4ca2abd4d2244f6","title":"Accelerating deep reinforcement learning via knowledge-guided policy network","venue":"Autonomous Agents and Multi-Agent Systems","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/02/2023","authors":"Yuanqiang Yu,Peng Zhang,Kai Zhao,Yan Zheng,Jianye Hao","id":"24c7d869f741b61dae384d97d4ca2abd4d2244f6","summary":"A knowledge-guided policy network, a novel framework that combines suboptimal human knowledge with reinforcement learning and RL that significantly improves the learning efficiency of basic RL algorithms, even with very low-performance human prior knowledge.","score":5},{"url":"https://www.semanticscholar.org/paper/e8787895af512169814655ba32ff0f46d0d665e4","title":"Research and Challenges of Reinforcement Learning in Cyber Defense Decision-Making for Intranet Security","venue":"Algorithms","year":2022,"referenceCount":131,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/04/2022","authors":"Wenhao Wang,Dingyuanhao Sun,Feng Jiang,Xingguo Chen,Cheng Zhu","id":"e8787895af512169814655ba32ff0f46d0d665e4","summary":"This work proposes a framework that defines four modules based on the life cycle of threats: pentest, design, response, recovery, and provides a systematic view for understanding and solving decision-making problems in the application of reinforcement learning to cyber defense.","score":5},{"url":"https://www.semanticscholar.org/paper/e258ac931efd168db6249f284aecda3b46c1d32d","title":"Variance Reduction for Policy-Gradient Methods via Empirical Variance Minimization","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/06/2022","authors":"M. Kaledin,Alexander Golubev,D. Belomestny","id":"e258ac931efd168db6249f284aecda3b46c1d32d","summary":"The experiments indicate that in terms of variance reduction EV-based methods are much better than A2C and allow stronger variance reduction, and some theoretical guarantees of the actual variance reduction under very general assumptions are proved.","score":5},{"url":"https://www.semanticscholar.org/paper/9dcc13ea4d9c988d044239497b5597f09c086e32","title":"Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/07/2022","authors":"Shunyu Liu,Xinchao Wang,Na Yu,Jie Song,Kaixuan Chen,Zunlei Feng,Mingli Song","id":"9dcc13ea4d9c988d044239497b5597f09c086e32","summary":"Experimental results demonstrate that the proposed advisor-in-the-loop actor-critic framework, termed as Ask-AC, improves the learning efﬁciency of the agent, and achieves the performances on par with those obtained by continuous advisor monitoring.","score":5},{"url":"https://www.semanticscholar.org/paper/28cbf0c09dccea15025252e99b26eac0522eec46","title":"Adaptive Stochastic ADMM for Decentralized Reinforcement Learning in Edge IoT","venue":"IEEE Internet of Things Journal","year":2022,"referenceCount":56,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Wanlu Lei,Yu Ye,M. Xiao,M. Skoglund,Zhu Han","id":"28cbf0c09dccea15025252e99b26eac0522eec46","summary":"An adaptive stochastic incremental ADMM (asI-ADMM) algorithm is proposed and applied to decentralized RL with edge-computing-empowered IoT networks and shows that the proposed algorithms outperform the state of the art in terms of communication costs and scalability and can well adapt to complex IoT environments.","score":5},{"url":"https://www.semanticscholar.org/paper/d7508c9354448860185a4537e4e3a052b4eb1019","title":"Defending Smart Electrical Power Grids against Cyberattacks with Deep \nQ\n-Learning","venue":"PRX Energy","year":2022,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"M. Moradi,Yang Weng,Y. Lai","id":"d7508c9354448860185a4537e4e3a052b4eb1019","summary":"Comparison with alternative reinforcement learning methods provides further support for the general applicability of the deep-Q learning framework in ensuring secure operation of modern power grid systems.","score":5},{"url":"https://www.semanticscholar.org/paper/b1daf688d32c85c85dc62891d5ec8c366f477fea","title":"Discrete space reinforcement learning algorithm based on twin support vector machine classification","venue":"Pattern Recognition Letters","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Wenguo Wu,Zhengchun Zhou,A. R. Adhikary,Bapi Dutta","id":"b1daf688d32c85c85dc62891d5ec8c366f477fea","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/5685abf9e7bb2c16449ae1eb181051e503602a55","title":"Reinforcement Learning based Recommender Systems: A Survey","venue":"ACM Computing Surveys","year":2021,"referenceCount":274,"citationCount":95,"influentialCitationCount":7,"publicationDate":"15/01/2021","authors":"M. Afsar,T. Crump,B. Far","id":"5685abf9e7bb2c16449ae1eb181051e503602a55","summary":"A survey on reinforcement learning based recommender systems (RLRSs) is presented and it is recognized and illustrated that RLRSs can be generally classified into RL- and DRL-based methods and proposed an RLRS framework with four components, i.e., state representation, policy optimization, reward formulation, and environment building.","score":5},{"url":"https://www.semanticscholar.org/paper/12075ea34f5fbe32ec5582786761ab34d401209b","title":"Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain","venue":"IEEE Transactions on Neural Networks and Learning Systems","year":2021,"referenceCount":235,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/09/2021","authors":"Tianpei Yang,Hongyao Tang,Chenjia Bai,Jinyi Liu,Jianye Hao,Zhaopeng Meng,Peng Liu,Zhen Wang","id":"12075ea34f5fbe32ec5582786761ab34d401209b","summary":"A comprehensive survey on existing exploration methods for both single-agent and multi-agent RL, identifying several key challenges to efﬁcient exploration and point out a few future directions.","score":5},{"url":"https://www.semanticscholar.org/paper/24d46159172993c67ccc83ae713d1b8dd21405b5","title":"Text-Driven Video Acceleration: A Weakly-Supervised Reinforcement Learning Method","venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/03/2022","authors":"W. Ramos,M. Silva,Edson R. Araujo,Victor Moura,Keller Clayderman Martins de Oliveira,Leandro Soriano Marcolino,Erickson R. Nascimento","id":"24d46159172993c67ccc83ae713d1b8dd21405b5","summary":"A novel weakly-supervised methodology based on a reinforcement learning formulation to accelerate instructional videos using text and the Extended Visually-guided Document Attention Network (VDAN+), which can generate a highly discriminative embedding space to represent both textual and visual data.","score":5},{"url":"https://www.semanticscholar.org/paper/4360de3e3303daa369515ac602c20b8f18be28bb","title":"Comparative Study of Cooperative Platoon Merging Control Based on Reinforcement Learning","venue":"Italian National Conference on Sensors","year":2023,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Ali Irshayyid,Jun Chen","id":"4360de3e3303daa369515ac602c20b8f18be28bb","summary":"A model-free deep reinforcement learning approach is used to find the optimal driving behavior in the scenario in which two platoons are merging into one, and results show that the proposed framework can reduce the energy consumed, and the average jerk can be decreased by up to 50%, all by only changing the cooperative merge behavior.","score":5},{"url":"https://www.semanticscholar.org/paper/5f90d43e6ece5c6ee6e8186e4b57d46c85377713","title":"DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization","venue":"","year":2023,"referenceCount":124,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Zhiqing Sun,Yiming Yang","id":"5f90d43e6ece5c6ee6e8186e4b57d46c85377713","summary":"DIFUSCO is introduced, a new graph-based diffusion framework for NPC combinatorial optimization that strongly outperforms the previous state-of-the-art neural solvers on the challenging SATLIB benchmark and investigates two types of diffusion models with Gaussian and Bernoulli noise, respectively.","score":5},{"url":"https://www.semanticscholar.org/paper/d547e1781771deefcab1adb621e77f1d36d70061","title":"A Memory Efficient Baseline for Open Domain Question Answering","venue":"ArXiv","year":2020,"referenceCount":25,"citationCount":27,"influentialCitationCount":3,"publicationDate":"30/12/2020","authors":"Gautier Izacard,Fabio Petroni,Lucas Hosseini,Nicola De Cao,Sebastian Riedel,Edouard Grave","id":"d547e1781771deefcab1adb621e77f1d36d70061","summary":"This paper considers three strategies to reduce the index size of dense retriever-reader systems: dimension reduction, vector quantization and passage filtering, and shows that it is possible to get competitive systems using less than 6Gb of memory.","score":5},{"url":"https://www.semanticscholar.org/paper/469d92f195aebfa09e9b411ad92b3c879bcd1eba","title":"Progressively Pretrained Dense Corpus Index for Open-Domain Question Answering","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":53,"citationCount":13,"influentialCitationCount":2,"publicationDate":"01/04/2020","authors":"Wenhan Xiong,Hong Wang,W. Wang","id":"469d92f195aebfa09e9b411ad92b3c879bcd1eba","summary":"This work proposes a sample-efficient method to pretrain the paragraph encoder using an existing pretrained sequence-to-sequence model to build a strong question generator that creates high-quality pretraining data and proposes a simple progressive pretraining algorithm to ensure the existence of effective negative samples in each batch.","score":5},{"url":"https://www.semanticscholar.org/paper/7d429ad73fc311a0a29ab9d02482b4e6b059d81f","title":"Generation-Augmented Retrieval for Open-Domain Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":44,"citationCount":80,"influentialCitationCount":15,"publicationDate":"17/09/2020","authors":"Yuning Mao,Pengcheng He,Xiaodong Liu,Yelong Shen,Jianfeng Gao,Jiawei Han,Weizhu Chen","id":"7d429ad73fc311a0a29ab9d02482b4e6b059d81f","summary":"It is shown that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy, and as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance.","score":5},{"url":"https://www.semanticscholar.org/paper/3416e5e5694855f7175125b5fe2e0b659c3cdbfa","title":"RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering","venue":"North American Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":53,"citationCount":246,"influentialCitationCount":67,"publicationDate":"16/10/2020","authors":"Yingqi Qu,Yuchen Ding,Jing Liu,Kai Liu,Ruiyang Ren,Xin Zhao,Daxiang Dong,Hua Wu,Haifeng Wang","id":"3416e5e5694855f7175125b5fe2e0b659c3cdbfa","summary":"This work proposes an optimized training approach, called RocketQA, to improving dense passage retrieval, which significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions and demonstrates that the performance of end-to-end QA can be improved based on theRocketQA retriever.","score":5},{"url":"https://www.semanticscholar.org/paper/93d3e45395117e21214d404c8753b578c29266d1","title":"Open Question Answering over Tables and Text","venue":"International Conference on Learning Representations","year":2020,"referenceCount":49,"citationCount":72,"influentialCitationCount":23,"publicationDate":"20/10/2020","authors":"Wenhu Chen,Ming-Wei Chang,Eva Schlinger,W. Wang,William W. Cohen","id":"93d3e45395117e21214d404c8753b578c29266d1","summary":"This work considers for the first time open QA over both tabular and textual data and presents a new large-scale dataset Open Table-and-Text Question Answering (OTT-QA) to evaluate performance on this task.","score":5},{"url":"https://www.semanticscholar.org/paper/c2482d0c49c2cfb1a50c24fb2177182893a9d1a0","title":"Answering Open-Domain Questions of Varying Reasoning Steps from Text","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":51,"citationCount":26,"influentialCitationCount":5,"publicationDate":"23/10/2020","authors":"Peng Qi,Haejun Lee,OghenetegiriTGSido,Christopher D. Manning","id":"c2482d0c49c2cfb1a50c24fb2177182893a9d1a0","summary":"A unified system to answer directly from text open-domain questions that may require a varying number of retrieval steps, using a single multi-task transformer model to perform all the necessary subtasks in an iterative fashion.","score":5},{"url":"https://www.semanticscholar.org/paper/a2a7033a5a859e3a6e6f0a83018326400b4c5faa","title":"Retrieval Augmentation Reduces Hallucination in Conversation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":68,"citationCount":113,"influentialCitationCount":19,"publicationDate":"15/04/2021","authors":"Kurt Shuster,Spencer Poff,Moya Chen,Douwe Kiela,J. Weston","id":"a2a7033a5a859e3a6e6f0a83018326400b4c5faa","summary":"This work explores the use of neural-retrieval-in-the-loop architectures recently shown to be effective in open-domain QA for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses.","score":5},{"url":"https://www.semanticscholar.org/paper/9bbdcc03d872987eef9165f4a63c3878a5b05189","title":"Condenser: a Pre-training Architecture for Dense Retrieval","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":52,"citationCount":87,"influentialCitationCount":19,"publicationDate":"16/04/2021","authors":"Luyu Gao,Jamie Callan","id":"9bbdcc03d872987eef9165f4a63c3878a5b05189","summary":"This paper proposes to pre-train towards dense encoder with a novel Transformer architecture, Condenser, where LM prediction CONditions on DENSE Representation improves over standard LM by large margins on various text retrieval and similarity tasks.","score":5},{"url":"https://www.semanticscholar.org/paper/baf34ac4080a365a7cec30b6877fa1a018eb31cf","title":"Joint Passage Ranking for Diverse Multi-Answer Retrieval","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":39,"citationCount":14,"influentialCitationCount":3,"publicationDate":"17/04/2021","authors":"Sewon Min,Kenton Lee,Ming-Wei Chang,Kristina Toutanova,Hannaneh Hajishirzi","id":"baf34ac4080a365a7cec30b6877fa1a018eb31cf","summary":"JPR is introduced, a joint passage retrieval model focusing on reranking that achieves significantly better answer coverage on three multi-answer datasets, and enables larger answer generation models since they need to consider fewer passages.","score":5},{"url":"https://www.semanticscholar.org/paper/d82c779e316aebf0a6b08904dff9cd26ba57219b","title":"R2-D2: A Modular Baseline for Open-Domain Question Answering","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":48,"citationCount":23,"influentialCitationCount":4,"publicationDate":"08/09/2021","authors":"Martin Fajcik,Martin Docekal,Karel Ondrej,P. Smrz","id":"d82c779e316aebf0a6b08904dff9cd26ba57219b","summary":"This work presents a novel four-stage open-domain QA pipeline R2-D2, composed of a retriever, passage reranker, extractive reader, generative reader and a mechanism that aggregates the prediction from all system’s components.","score":5},{"url":"https://www.semanticscholar.org/paper/7c5064305b8f8add57f211e185cdfeb9d68e6e31","title":"Recent Advances in Automated Question Answering In Biomedical Domain","venue":"ArXiv","year":2021,"referenceCount":211,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/11/2021","authors":"K. D. Baksi","id":"7c5064305b8f8add57f211e185cdfeb9d68e6e31","summary":"The basic methodologies used for developing general domain QA systems are introduced, followed by a thorough investigation of different aspects of biomedicalQA systems, including benchmark datasets and several proposed approaches, both using structured databases and collection of texts.","score":5},{"url":"https://www.semanticscholar.org/paper/5a37124345d0fb44ac1b4809dda85bf61ab79564","title":"Towards Unsupervised Dense Information Retrieval with Contrastive Learning","venue":"ArXiv","year":2021,"referenceCount":58,"citationCount":50,"influentialCitationCount":17,"publicationDate":2021,"authors":"Gautier Izacard,Mathilde Caron,Lucas Hosseini,Sebastian Riedel,Piotr Bojanowski,Armand Joulin,Edouard Grave","id":"5a37124345d0fb44ac1b4809dda85bf61ab79564","summary":"This work explores the limits of contrastive learning as a way to train unsupervised dense retrievers, and shows that it leads to strong retrieval performance on the BEIR benchmark.","score":5},{"url":"https://www.semanticscholar.org/paper/ca798e19f82266800f835d3b41672b385cdec0f6","title":"Empowering Dual-Encoder with Query Generator for Cross-Lingual Dense Retrieval","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Houxing Ren,Linjun Shou,Ning Wu,Ming Gong,Daxin Jiang","id":"ca798e19f82266800f835d3b41672b385cdec0f6","summary":"This paper proposes to use a query generator as the teacher in the cross-lingual setting, which is less dependent on enough training samples and high-quality negative samples, and proposes a novel enhancement method, which uses the query generator to help the dual-encoder align queries from different languages, but does not need any additional parallel sentences.","score":5},{"url":"https://www.semanticscholar.org/paper/2af67c1063172c924a977d97d4b848651cc1617e","title":"A Survey on Machine Reading Comprehension Systems","venue":"Natural Language Engineering","year":2020,"referenceCount":241,"citationCount":39,"influentialCitationCount":4,"publicationDate":"06/01/2020","authors":"Razieh Baradaran,Razieh Ghiasi,Hossein Amirkhani","id":"2af67c1063172c924a977d97d4b848651cc1617e","summary":"It is demonstrated that the focus of research has changed in recent years from answer extraction to answer generation, from single- to multi-document reading comprehension, and from learning from scratch to using pre-trained word vectors.","score":5},{"url":"https://www.semanticscholar.org/paper/82d60ef4c9439ff24a4ebdd1b6eab59396a6a2ce","title":"TopiOCQA: Open-domain Conversational Question Answering with Topic Switching","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":72,"citationCount":11,"influentialCitationCount":0,"publicationDate":"02/10/2021","authors":"Vaibhav Adlakha,S. Dhuliawala,Kaheer Suleman,Harm de Vries,Siva Reddy","id":"82d60ef4c9439ff24a4ebdd1b6eab59396a6a2ce","summary":"TopiOCQA is introduced, an open-domain conversational dataset with topic switches based on Wikipedia that poses a challenging test-bed for models, where efficient retrieval is required on multiple turns of the same conversation, in conjunction with constructing valid responses using conversational history.","score":5},{"url":"https://www.semanticscholar.org/paper/8bcb1e12aa1d12221808d4e3643559077cfe7db2","title":"Open Domain Question Answering with A Unified Knowledge Interface","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":53,"citationCount":8,"influentialCitationCount":1,"publicationDate":"16/10/2021","authors":"Kaixin Ma,Hao Cheng,Xiaodong Liu,Eric Nyberg,Jianfeng Gao","id":"8bcb1e12aa1d12221808d4e3643559077cfe7db2","summary":"This work proposes a verbalizer-retriever-reader framework for ODQA over data and text where verbalized tables from Wikipedia and graphs from Wikidata are used as augmented knowledge sources and shows that the Unified Data and Text QA, UDT-QA, can effectively benefit from the expanded knowledge index, leading to large gains over text-only baselines.","score":5},{"url":"https://www.semanticscholar.org/paper/c2b86e6dee44dd1dc711425e13eadcf04444dea9","title":"You Only Need One Model for Open-domain Question Answering","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":62,"citationCount":6,"influentialCitationCount":3,"publicationDate":"14/12/2021","authors":"Haejun Lee,Akhil Kedia,Jongwon Lee,Ashwin Paranjape,Christopher D. Manning,Kyoung-Gu Woo","id":"c2b86e6dee44dd1dc711425e13eadcf04444dea9","summary":"This work proposes casting the retriever and the reranker as internal passage-wise attention mechanisms applied sequentially within the transformer architecture and feeding computed representations to the reader, with the hidden representations progressively refined at each stage.","score":5},{"url":"https://www.semanticscholar.org/paper/196c925eb2d0be235d85b3944e013330d101901c","title":"A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering","venue":"","year":2022,"referenceCount":66,"citationCount":5,"influentialCitationCount":1,"publicationDate":"14/01/2022","authors":"Feng Gao,Q. Ping,G. Thattai,Aishwarya N. Reganti,Yingting Wu,Premkumar Natarajan","id":"196c925eb2d0be235d85b3944e013330d101901c","summary":"A Transform-Retrieve-Generate framework (TRiG) framework is proposed, which can be plug-and-played with alternative image-to-text models and textual knowledge bases, and outperforms all state-of-the-art supervised methods by at least 11.1% absolute margin.","score":5},{"url":"https://www.semanticscholar.org/paper/c2f9a27ab32bff87573e31594c97742af90f11b2","title":"Saving Dense Retriever from Shortcut Dependency in Conversational Search","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":69,"citationCount":4,"influentialCitationCount":0,"publicationDate":"15/02/2022","authors":"Sungdong Kim,Gangwoo Kim","id":"c2f9a27ab32bff87573e31594c97742af90f11b2","summary":"This paper demonstrates the existence of a retrieval shortcut in CS, which causes models to retrieve passages solely relying on partial history while disregarding the latest question, and explores various hard negative mining strategies to build more robust models against shortcut dependency.","score":5},{"url":"https://www.semanticscholar.org/paper/c1b68e138ef3352e5010eacf1211a4d0e43b1ef0","title":"Generative Retrieval for Long Sequences","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Hyunji Lee,Sohee Yang,Hanseok Oh,Minjoon Seo","id":"c1b68e138ef3352e5010eacf1211a4d0e43b1ef0","summary":"This paper uses an encoder-decoder model to memorize the target corpus in a generative manner and then uses it on query-to-passage generation, conjecture that generative retrieval is complementary to traditional retrieval, as it is conjecture that an ensemble of both outperforms homogeneous ensembles.","score":5},{"url":"https://www.semanticscholar.org/paper/65ff0735438cf12bbad87d92ce466f55dafd4eda","title":"QAMPARI: : An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":2,"influentialCitationCount":1,"publicationDate":"25/05/2022","authors":"Samuel Joseph Amouyal,Ori Yoran,Tomer Wolfson,Jonathan Herzig,Jonathan Berant","id":"65ff0735438cf12bbad87d92ce466f55dafd4eda","summary":"QQA models from the retrieve-and-read family are trained, showing that QAMP AR I is challenging in terms of both passage retrieval and answer generation, reaching an F 1 score of 26.6 at best.","score":5},{"url":"https://www.semanticscholar.org/paper/26217a04c8a2ce36e1d027cca7b35bf83cab957b","title":"ZusammenQA: Data Augmentation with Specialized Models for Cross-lingual Open-retrieval Question Answering System","venue":"MIA","year":2022,"referenceCount":55,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/05/2022","authors":"Chia-Chien Hung,Tommaso Green,Robert Litschko,Tornike Tsereteli,Sotaro Takeshita,Marco Bombieri,Goran Glavavs,Simone Paolo Ponzetto","id":"26217a04c8a2ce36e1d027cca7b35bf83cab957b","summary":"The proposed system for the MIA Shared Task on Cross-lingual Openretrieval Question Answering (COQA) is introduced, showing that language- and domain-specialization as well as data augmentation help, especially for low-resource languages.","score":5},{"url":"https://www.semanticscholar.org/paper/02720ba7a4c0c70506ef63e039387c10b227d8e3","title":"Transform-Retrieve-Generate: Natural Language-Centric Outside-Knowledge Visual Question Answering","venue":"Computer Vision and Pattern Recognition","year":2022,"referenceCount":67,"citationCount":2,"influentialCitationCount":2,"publicationDate":"01/06/2022","authors":"Feng Gao,Q. Ping,G. Thattai,Aishwarya N. Reganti,Yingting Wu,Premkumar Natarajan","id":"02720ba7a4c0c70506ef63e039387c10b227d8e3","summary":"This paper calls for an alternative paradigm for the OK-VQA task, which transforms the image into plain text, so that it can enable knowledge passage retrieval, and generative question-answering in the natural language space.","score":5},{"url":"https://www.semanticscholar.org/paper/5aaaf17b9bc115e6a6ff7a2b0d11b3997647b969","title":"Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/11/2022","authors":"Shujian Zhang,Chengyue Gong,Xingchao Liu","id":"5aaaf17b9bc115e6a6ff7a2b0d11b3997647b969","summary":"A learnable passage mask mechanism is introduced which desensitizes the impact from the top-rank retrieval passages and prevents the model from overfitting and enforces the answer generation to focus on the entire retrieval passages.","score":5},{"url":"https://www.semanticscholar.org/paper/56ac515d27cc7e1e471fe883d12cf5fd841114ee","title":"Data Discovery using Natural Language Questions via a Self-Supervised Approach","venue":"ArXiv","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/01/2023","authors":"Qiming Wang,R. Fernandez","id":"56ac515d27cc7e1e471fe883d12cf5fd841114ee","summary":"A self-supervised approach to assemble training datasets and train learned discovery systems without human intervention is introduced, and the new techniques outperform state-of-the-art approaches on well-known benchmarks.","score":5},{"url":"https://www.semanticscholar.org/paper/1bb82660573bbaf01572041da16842ee2398ae39","title":"Learning Robust Real-Time Cultural Transmission without Human Data","venue":"ArXiv","year":2022,"referenceCount":161,"citationCount":4,"influentialCitationCount":1,"publicationDate":"01/03/2022","authors":"Avishkar Bhoopchand,Bethanie Brownfield,Adrian Collister,Agustin Dal Lago,Ashley D. Edwards,Richard Everett,Alexandre Frechette,Y. Oliveira,Edward Hughes,K. Mathewson,Piermaria Mendolicchio,Julia Pawar,Miruna Pislar,A. Platonov,Evan Senter,Sukhdeep Singh,Alexander Zacherl,Lei M. Zhang","id":"1bb82660573bbaf01572041da16842ee2398ae39","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/05c522667f9dee976764f805a2e509f7c05ca80e","title":"RPM: Generalizable Behaviors for Multi-Agent Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/10/2022","authors":"Wei Qiu,Xiao Ma,Bo An,S. Obraztsova,Shuicheng Yan,Zhongwen Xu","id":"05c522667f9dee976764f805a2e509f7c05ca80e","summary":"This work proposes a simple yet effective method, ranked policy memory (RPM), to collect diverse multi-agent trajectories for training MARL policies with good generalizability, and implements RPM on top of MARL algorithms and conducts extensive experiments on Melting Pot.","score":5},{"url":"https://www.semanticscholar.org/paper/cc3cb6b0ea04eb35c1907e3917a4db4b435c95b1","title":"FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning","venue":"SSRN Electronic Journal","year":2022,"referenceCount":78,"citationCount":2,"influentialCitationCount":0,"publicationDate":"06/11/2022","authors":"Xiao-Yang Liu,Ziyi Xia,Jingyang Rui,Jiechao Gao,Hongyang Yang,Ming Zhu,Chris Wang,Zhaoran Wang,Jian Guo","id":"cc3cb6b0ea04eb35c1907e3917a4db4b435c95b1","summary":"This paper presents an openly accessible FinRL-Meta library that has been actively maintained by the FinRL community and provides hundreds of market environments through an automatic pipeline that collects dynamic datasets from real-world markets and processes them into standard gym-style market environments.","score":5},{"url":"https://www.semanticscholar.org/paper/82866d7a806dd20ae397dd5e544d2e2240fe0949","title":"Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction","venue":"ArXiv","year":2023,"referenceCount":49,"citationCount":1,"influentialCitationCount":1,"publicationDate":"21/01/2023","authors":"Shaofei Cai,Zihao Wang,Xiaojian Ma,Anji Liu,Yitao Liang","id":"82866d7a806dd20ae397dd5e544d2e2240fe0949","summary":"This work proposes Goal-Sensitive Backbone (GSB) for the policy to encourage the emergence of goal-relevant visual state representations in Minecraft and proposes an adaptive horizon prediction module that helps alleviate the learning uncertainty brought by the non-stationary dynamics.","score":5},{"url":"https://www.semanticscholar.org/paper/0e1d82d24d58433ce9e211551605a0bfd296624f","title":"Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":66,"citationCount":42,"influentialCitationCount":3,"publicationDate":2021,"authors":"Tushar Khot,Daniel Khashabi,Kyle Richardson,Peter Clark,Ashish Sabharwal","id":"0e1d82d24d58433ce9e211551605a0bfd296624f","summary":"ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.","score":5},{"url":"https://www.semanticscholar.org/paper/32ccd0f725fc5f7c81b57cad7787dc15b99151d0","title":"Neural methods for effective, efficient, and exposure-aware information retrieval","venue":"SIGIR Forum","year":2020,"referenceCount":470,"citationCount":2,"influentialCitationCount":0,"publicationDate":"21/12/2020","authors":"Bhaskar Mitra","id":"32ccd0f725fc5f7c81b57cad7787dc15b99151d0","summary":"This thesis presents novel neural architectures and methods motivated by the specific needs and challenges of IR tasks, and develops a framework to incorporate query term independence into any arbitrary deep model that enables large-scale precomputation and the use of inverted index for fast retrieval.","score":5},{"url":"https://www.semanticscholar.org/paper/2e1214b4a59b5931131b6c58e19b0eb16f1b365c","title":"Can Question Rewriting Help Conversational Question Answering?","venue":"First Workshop on Insights from Negative Results in NLP","year":2022,"referenceCount":39,"citationCount":3,"influentialCitationCount":0,"publicationDate":"13/04/2022","authors":"Etsuko Ishii,Yan Xu,Samuel Cahyawijaya,Bryan Wilie","id":"2e1214b4a59b5931131b6c58e19b0eb16f1b365c","summary":"A reinforcement learning approach is investigated that integrates QR and CQA tasks and does not require corresponding QR datasets for targeted CZA and finds that the RL method is on par with the end-to-end baseline.","score":5},{"url":"https://www.semanticscholar.org/paper/3d5a107d0f9803e450bee409491b5a54f25b0d7a","title":"Brick Tic-Tac-Toe: Exploring the Generalizability of AlphaZero to Novel Test Environments","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/07/2022","authors":"John Tan Chong Min,M. Motani","id":"3d5a107d0f9803e450bee409491b5a54f25b0d7a","summary":"The Brick Tic-Tac-Toe (BTTT) test bed is introduced, where the brick position in the test environment is different from that in the training environment, and it is shown that traditional RL state-search approaches such as Monte Carlo Tree Search and Minimax are more generalizable to novel test environments than AlphaZero is.","score":5},{"url":"https://www.semanticscholar.org/paper/08b2b38cd8f03def99059fd32d5563580933e81b","title":"Robust Searching-based Gradient Collaborative Management in Intelligent Transportation System","venue":"ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/07/2022","authors":"Hong Shi,Hao Wang,Ruhui Ma,Yang Hua,Tao Song,Honghao Gao,Haibing Guan","id":"08b2b38cd8f03def99059fd32d5563580933e81b","summary":"Robust Searching-based Gradient Collaborative Management in Intelligent Transportation System (RSGCM) is proposed, a practical ring-based gradient managing algorithm for communication schedules across devices to deal with ITS malfunction and increases the robustness of ITS.","score":5},{"url":"https://www.semanticscholar.org/paper/cc0f7a7742f05b18af1405dfa7e15cce4584e44e","title":"A Survey on Reinforcement Learning in Aviation Applications","venue":"ArXiv","year":2022,"referenceCount":137,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/11/2022","authors":"Pouria Razzaghi,Amin Tabrizian,Wei Guo,Shulu Chen,Abenezer G. Taye,Ellis E. Thompson,Alexis Bregeon,A. Baheri,Peng Wei","id":"cc0f7a7742f05b18af1405dfa7e15cce4584e44e","summary":"This survey paper first describes standard RL formulations and solutions, then surveys the landscape of existing RL-based applications in aviation, and suggests future directions of RL research in aviation.","score":5},{"url":"https://www.semanticscholar.org/paper/1c6435cb353271f3cb87b27ccc6df5b727d55f26","title":"Model-based Reinforcement Learning: A Survey","venue":"Found. Trends Mach. Learn.","year":2020,"referenceCount":327,"citationCount":19,"influentialCitationCount":0,"publicationDate":"30/06/2020","authors":"T. Moerland,J. Broekens,C. Jonker","id":"1c6435cb353271f3cb87b27ccc6df5b727d55f26","summary":"A survey of the integration of model-based reinforcement learning and planning, better known as model- based reinforcement learning, and a broad conceptual overview of planning-learning combinations for MDP optimization are presented.","score":5},{"url":"https://www.semanticscholar.org/paper/75135f01f1ba144b87902a0603c754924a1b394f","title":"A Succinct Summary of Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/01/2023","authors":"S. Ahilan","id":"75135f01f1ba144b87902a0603c754924a1b394f","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/0f99ec01472af668be23cf14b0f6953f170082ad","title":"Knowledge-integrated machine learning for materials: lessons from gameplaying and robotics","venue":"Nature Reviews Materials","year":2023,"referenceCount":152,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"K. Hippalgaonkar,Qianxiao Li,Xiaonan Wang,John Fisher,J. Kirkpatrick,T. Buonassisi","id":"0f99ec01472af668be23cf14b0f6953f170082ad","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/7758711c910bafcd5976d35eeabd69e0a1264156","title":"Policy Expansion for Bridging Offline-to-Online Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Haichao Zhang,We Xu,Haonan Yu","id":"7758711c910bafcd5976d35eeabd69e0a1264156","summary":"A policy expansion scheme that ensures that the policy previously learned offline is fully retained during online learning, thus mitigating the potential issues such as destroying the useful behaviors of the offline policy in the initial stage of online learning while allowing the offline Policy to participate in the exploration naturally in an adaptive manner.","score":5},{"url":"https://www.semanticscholar.org/paper/754fad16ccde2328b302162571650254acd38203","title":"Query Expansion Using Contextual Clue Sampling with Language Models","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Linqing Liu,Minghan Li,Jimmy Lin,Sebastian Riedel,Pontus Stenetorp","id":"754fad16ccde2328b302162571650254acd38203","summary":"This work argues that expansion terms from language models to generate query-related contexts should balance two key aspects: diversity and relevance, and proposes a combination of an effective filtering strategy and fusion of the retrieved documents based on the generation probability of each context.","score":5},{"url":"https://www.semanticscholar.org/paper/bdeef928a5ed19315a4eb8e776794cfa9d3119e7","title":"Disentangled Retrieval and Reasoning for Implicit Question Answering.","venue":"IEEE Transactions on Neural Networks and Learning Systems","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Qiang Liu,Xiubo Geng,Yu Wang,E. Cambria,Daxin Jiang","id":"bdeef928a5ed19315a4eb8e776794cfa9d3119e7","summary":"This article proposes a systematic solution denoted as DisentangledQA, which disentangles topic, attribute, and reasoning strategy from the implicit question to guide the retrieval and reasoning of evidence retrieval and answer reasoning.","score":5},{"url":"https://www.semanticscholar.org/paper/00675f1591392622b0db2d9cd37a8a1f32e37aa8","title":"Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Kaiser Sun,Peng Qi,Yuhao Zhang,Lan Liu,William Yang Wang,Zhiheng Huang","id":"00675f1591392622b0db2d9cd37a8a1f32e37aa8","summary":"It is shown that, with consistent tokenization, the model performs better in both in-domain and out-of-domain datasets, with a notable average of +1.7 F 1 gain when a BART model is trained on SQuAD and evaluated on 8 QA datasets.","score":5},{"url":"https://www.semanticscholar.org/paper/7138b55809694d052a9cc9bab77c880d87872616","title":"EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval","venue":"ArXiv","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Seungyeon Kim,A. Rawat,M. Zaheer,Sadeep Jayasumana,Veeranjaneyulu Sadhanala,Wittawat Jitkrittum,A. Menon,R. Fergus,Surinder Kumar","id":"7138b55809694d052a9cc9bab77c880d87872616","summary":"The proposed distillation approach supports both retrieval and re-ranking stages and crucially leverages the relative geometry among queries and documents learned by the large teacher model to provide stronger signals about local geometry via embedding matching and attaining better coverage of data manifold globally via query generation.","score":5},{"url":"https://www.semanticscholar.org/paper/a23b8f072625d6481ead4c8b6193f01cdddd7fe0","title":"Symbolic Discovery of Optimization Algorithms","venue":"","year":2023,"referenceCount":104,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Xiangning Chen,Chen Liang,Da Huang,Esteban Real,Kaiyuan Wang,Yao Liu,Hieu Pham,Xuanyi Dong,Thang Luong,Cho-Jui Hsieh,Yifeng Lu,Quoc V. Le","id":"a23b8f072625d6481ead4c8b6193f01cdddd7fe0","summary":"The method discovers a simple and effective optimization algorithm, Lion, which is more memory-efficient than Adam and requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function.","score":5},{"url":"https://www.semanticscholar.org/paper/b97369cf35c58d7c518e7364dff54535d344c94c","title":"Learning a model is paramount for sample efficiency in reinforcement learning control of PDEs","venue":"","year":2023,"referenceCount":75,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/02/2023","authors":"Stefan Werner,Sebastian Peitz","id":"b97369cf35c58d7c518e7364dff54535d344c94c","summary":"It is demonstrated that learning an actuated model in parallel to training the RL agent significantly reduces the total amount of required data sampled from the real system, and that iteratively updating the model is of major importance to avoid biases in the RL training.","score":5},{"url":"https://www.semanticscholar.org/paper/12bc45e2268a5742d21a8a37109f8793417cefcc","title":"More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering","venue":"ArXiv","year":2021,"referenceCount":74,"citationCount":4,"influentialCitationCount":2,"publicationDate":"25/09/2021","authors":"Yang Bai,D. Wang","id":"12bc45e2268a5742d21a8a37109f8793417cefcc","summary":"A survey of 47 recent textual QA benchmark datasets and a new taxonomy from an application point of view is proposed, which summarizes 8 evaluation metrics oftextual QA tasks and suggests directions for future work.","score":5},{"url":"https://www.semanticscholar.org/paper/e02a757617c2c42eb62889cc4d4aee3765928303","title":"The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus","venue":"ArXiv","year":2021,"referenceCount":68,"citationCount":21,"influentialCitationCount":0,"publicationDate":"18/12/2021","authors":"Aleksandra Piktus,Fabio Petroni,Vladimir Karpukhin,Dmytro Okhonko,Samuel Broscheit,Gautier Izacard,Patrick Lewis,Barlas Ouguz,Edouard Grave,Wen-tau Yih,Sebastian Riedel","id":"e02a757617c2c42eb62889cc4d4aee3765928303","summary":"It is observed that while a dense index can outperform a sparse BM25 baseline on Wikipedia, on S PHERE this is not yet possible, and to facilitate further research and minimise the community’s reliance on propri-etary, black-box search engines, the indices, evaluation metrics and infrastructure are shared.","score":5},{"url":"https://www.semanticscholar.org/paper/60dfb269f6ec44ccf993c52ba688481bc32f2b31","title":"M3: A Multi-View Fusion and Multi-Decoding Network for Multi-Document Reading Comprehension","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Liang Wen,Houfeng Wang,Yingwei Luo,Xiaolin Wang","id":"60dfb269f6ec44ccf993c52ba688481bc32f2b31","summary":"This work proposes a novel method that tries to employ a multi-view fusion and multi-decoding mechanism to achieve fine-grained fusion of evidence clues from different documents in the encoder and decoder concurrently.","score":5},{"url":"https://www.semanticscholar.org/paper/5aec43e6d48c1b187778ee3beb6e8e3d41267077","title":"CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training","venue":"NAACL-HLT","year":2021,"referenceCount":31,"citationCount":6,"influentialCitationCount":1,"publicationDate":"14/10/2021","authors":"Patrick Huber,A. Aghajanyan,Barlas Oğuz,Dmytro Okhonko,Wen-tau Yih,Sonal Gupta,Xilun Chen","id":"5aec43e6d48c1b187778ee3beb6e8e3d41267077","summary":"A novel open-domain question-answering dataset based on the Common Crawl project that achieves promising results in zero-shot, low resource, and tuned settings across multiple tasks, models and benchmarks is proposed.","score":5},{"url":"https://www.semanticscholar.org/paper/b4c793fc05644979b405e79d9a6112dcbf85b11c","title":"TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval","venue":"Findings","year":2022,"referenceCount":52,"citationCount":5,"influentialCitationCount":0,"publicationDate":"18/04/2022","authors":"Megan Leszczynski,Daniel Y. Fu,Mayee F. Chen,Christopher R'e","id":"b4c793fc05644979b405e79d9a6112dcbf85b11c","summary":"TABi is introduced, a method to jointly train bi-encoders on knowledge graph types and unstructured text for entity retrieval for open-domain tasks and leverages a type-enforced contrastive loss to encourage entities and queries of similar types to be close in the embedding space.","score":5},{"url":"https://www.semanticscholar.org/paper/39a26d50587e2e3e17e53b10ad3a7c0dce88f608","title":"Modeling Multi-hop Question Answering as Single Sequence Prediction","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":52,"citationCount":5,"influentialCitationCount":0,"publicationDate":"18/05/2022","authors":"Semih Yavuz,Kazuma Hashimoto,Yingbo Zhou,N. Keskar,Caiming Xiong","id":"39a26d50587e2e3e17e53b10ad3a7c0dce88f608","summary":"This work proposes a simple generative approach (PathFid) that extends the task beyond just answer generation by explicitly modeling the reasoning process to resolve the answer for multi-hop questions by linearizing the hierarchical reasoning path of supporting passages, their key sentences, and finally the factoid answer.","score":5},{"url":"https://www.semanticscholar.org/paper/2f3efe44083af91cef562c1a3451eee2f8601d22","title":"WebGPT: Browser-assisted question-answering with human feedback","venue":"ArXiv","year":2021,"referenceCount":42,"citationCount":113,"influentialCitationCount":22,"publicationDate":"17/12/2021","authors":"Reiichiro Nakano,Jacob Hilton,S. Balaji,Jeff Wu,Long Ouyang,Christina Kim,Christopher Hesse,Shantanu Jain,V. Kosaraju,W. Saunders,Xu Jiang,Karl Cobbe,Tyna Eloundou,Gretchen Krueger,Kevin Button,Matthew Knight,Benjamin Chess,J. Schulman","id":"2f3efe44083af91cef562c1a3451eee2f8601d22","summary":"GPT-3 is tuned to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web, and the best model is obtained by using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences.","score":5},{"url":"https://www.semanticscholar.org/paper/bb775023683e0909a47f680850bb04a8cabf962f","title":"Integrating Question Rewrites in Conversational Question Answering: A Reinforcement Learning Approach","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Etsuko Ishii,Bryan Wilie,Yan Xu,Samuel Cahyawijaya,Pascale Fung","id":"bb775023683e0909a47f680850bb04a8cabf962f","summary":"A reinforcement learning approach that integrates QR and CQA tasks without corresponding labeled QR datasets is proposed and the experimental results show that this approach can bring improvement over the pipeline approaches.","score":5},{"url":"https://www.semanticscholar.org/paper/203636315f7c9526189d88c541bedf623d63ea7c","title":"ASQA: Factoid Questions Meet Long-Form Answers","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/04/2022","authors":"Ivan Stelmakh,Yi Luan,Bhuwan Dhingra,Ming-Wei Chang","id":"203636315f7c9526189d88c541bedf623d63ea7c","summary":"A novel dataset and a task that is called ASQA (Answer Summaries for Questions which are Ambiguous) are released and an agreement between this metric and human judgments are demonstrated, and a considerable gap between human performance and strong baselines is revealed.","score":5},{"url":"https://www.semanticscholar.org/paper/72c47862e2eee0e65bc25b6cd6baeb2a50ef4bc7","title":"tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation","venue":"ArXiv","year":2023,"referenceCount":145,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/01/2023","authors":"Damien Sileo","id":"72c47862e2eee0e65bc25b6cd6baeb2a50ef4bc7","summary":"This work identifies patterns across previous preprocessings, e.g. mapping of column names, and extraction of a specific sub-field from structured data in a column, and proposes a structured annotation framework that makes the authors' annotations fully exposed and not buried in unstructured code.","score":5},{"url":"https://www.semanticscholar.org/paper/f057ffb46bf799ae83b2d20e784103b9252b0596","title":"Do BERTs Learn to Use Browser User Interface? Exploring Multi-Step Tasks with Unified Vision-and-Language BERTs","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/03/2022","authors":"Taichi Iki,Akiko Aizawa","id":"f057ffb46bf799ae83b2d20e784103b9252b0596","summary":"It is suggested that although room for improvement exists, BERTs to multi-step tasks, such as 025 using graphical user interfaces, can be transferred using graphicaluser interfaces.","score":5},{"url":"https://www.semanticscholar.org/paper/15df0e2c602ae8ccedcf50accea080c4ba76f8ba","title":"End-to-End Training of Neural Retrievers for Open-Domain Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":37,"citationCount":47,"influentialCitationCount":10,"publicationDate":"02/01/2021","authors":"Devendra Singh Sachan,M. Patwary,M. Shoeybi,Neel Kant,Wei Ping,William L. Hamilton,Bryan Catanzaro","id":"15df0e2c602ae8ccedcf50accea080c4ba76f8ba","summary":"An approach of unsupervised pre-training with the Inverse Cloze Task and masked salient spans, followed by supervised finetuning using question-context pairs leads to absolute gains over the previous best result in the top-20 retrieval accuracy on Natural Questions and TriviaQA datasets.","score":5},{"url":"https://www.semanticscholar.org/paper/9b23fb9c472c202fd53aba9d0fcaf6ae469260b4","title":"Calibration of Machine Reading Systems at Scale","venue":"Findings","year":2022,"referenceCount":42,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/03/2022","authors":"S. Dhuliawala,Leonard Adolphs,R. Das,Mrinmaya Sachan","id":"9b23fb9c472c202fd53aba9d0fcaf6ae469260b4","summary":"It is shown that calibrating such complex systems which contain discrete retrieval and deep reading components is challenging and current calibration techniques fail to scale to these settings, and proposes simple extensions to existing calibration approaches that allows them to adapt them toThese settings.","score":5},{"url":"https://www.semanticscholar.org/paper/515cf674fcdced5a7d5bb156dd5fcc1f5290e79b","title":"In-context Examples Selection for Machine Translation","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":8,"influentialCitationCount":2,"publicationDate":"05/12/2022","authors":"Sweta Agrawal,Chunting Zhou,M. Lewis,Luke Zettlemoyer,Marjan Ghazvininejad","id":"515cf674fcdced5a7d5bb156dd5fcc1f5290e79b","summary":"It is shown that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated example can have a catastrophic impact on output quality.","score":5},{"url":"https://www.semanticscholar.org/paper/5aa4e5b90827f1c16bed100982e2a1871925d445","title":"Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition","venue":"","year":2023,"referenceCount":101,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Minsu Kim,Hyungil Kim,Y. Ro","id":"5aa4e5b90827f1c16bed100982e2a1871925d445","summary":"The proposed prompt tuning methods of Deep Neural Networks for speaker-adaptive VSR are proposed and it is shown that the performance of the pre-trained VSR model on unseen speakers can be largely improved by using a small amount of adaptation data, even if thePre-trained model is already developed with large speaker variations.","score":5},{"url":"https://www.semanticscholar.org/paper/884c0b6db564208d99cadf2548f0aa96dee5f859","title":"Commonsense Reasoning with Implicit Knowledge in Natural Language","venue":"Conference on Automated Knowledge Base Construction","year":2021,"referenceCount":73,"citationCount":2,"influentialCitationCount":0,"publicationDate":2021,"authors":"Pratyay Banerjee,Swaroop Mishra","id":"884c0b6db564208d99cadf2548f0aa96dee5f859","summary":"This work takes a middle ground where it uses smaller language models together with a relatively smaller but targeted natural language text corpora and proposes four methods competitive to state-of-the-art methods to reason with implicit commonsense.","score":5},{"url":"https://www.semanticscholar.org/paper/9ba50f992ccd92f428503ea6246157260a26cd77","title":"Do Prompt-Based Models Really Understand the Meaning of Their Prompts?","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":67,"citationCount":64,"influentialCitationCount":9,"publicationDate":"02/09/2021","authors":"Albert Webson,Ellie Pavlick","id":"9ba50f992ccd92f428503ea6246157260a26cd77","summary":"It is found that models can learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively “good” prompts, and instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots.","score":5},{"url":"https://www.semanticscholar.org/paper/8c8868d75f5fc7a055fdbc8610ab20b0a4304829","title":"Deep Reinforcement Learning: Opportunities and Challenges","venue":"ArXiv","year":2022,"referenceCount":324,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yuxi Li","id":"8c8868d75f5fc7a055fdbc8610ab20b0a4304829","summary":"In this article, a brief introduction to reinforcement learning (RL), and its relationship with deep learning, machine learning and AI is given, and a discussion is attempted, attempting to answer: “Why has RL not been widely adopted in practice yet?” and “When is RL helpful?’.","score":5},{"url":"https://www.semanticscholar.org/paper/21dfd2731fd26612dce5f6a4025a51185bed9520","title":"Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/10/2022","authors":"L. Guan,Karthik Valmeekam,Subbarao Kambhampati","id":"21dfd2731fd26612dce5f6a4025a51185bed9520","summary":"This work proposes two practical methods that can learn to model any kind of behavioral attributes from ordered behavior clips, and demonstrates the effectiveness of the methods on four tasks with nine different behavioral attributes, showing that once the attributes are learned, end users can produce desirable agent behaviors relatively effortlessly, by providing feedback just around ten times.","score":5},{"url":"https://www.semanticscholar.org/paper/f43138cf1e9fb0d9c073a3ba6db2506d4e9533a3","title":"Conversation Regression Testing: A Design Technique for Prototyping Generalizable Prompt Strategies for Pre-trained Language Models","venue":"ArXiv","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/02/2023","authors":"J. D. Zamfirescu-Pereira,Bjoern Hartmann,Qiang Yang","id":"f43138cf1e9fb0d9c073a3ba6db2506d4e9533a3","summary":"This work embodies the concept of regression testing in an interactive design tool, BotDesigner, that lets designers identify archetypal errors across multiple conversations; shows common threads of conversation using a graph visualization; and highlights the effects of prompt changes across bot design iterations.","score":5},{"url":"https://www.semanticscholar.org/paper/95c3cdcf0fae3a3427a375244088b74879b4d972","title":"Vygotskian Autotelic Artificial Intelligence: Language and Culture Internalization for Human-Like AI","venue":"ArXiv","year":2022,"referenceCount":128,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Cédric Colas,Tristan Karch,Clément Moulin-Frier,P. Oudeyer","id":"95c3cdcf0fae3a3427a375244088b74879b4d972","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/e8770b45ddfd9529b4f5a70658affc001bb0b287","title":"MAQA: A Multimodal QA Benchmark for Negation","venue":"ArXiv","year":2023,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/01/2023","authors":"Judith Yue Li","id":"e8770b45ddfd9529b4f5a70658affc001bb0b287","summary":"This study presents a new multimodal question answering (QA) benchmark adapted from labeled music videos in AudioSet and demonstrates that augmenting the original training task distributions with negated QA examples allow the model to reliably reason with negation.","score":5},{"url":"https://www.semanticscholar.org/paper/ef12cbed0377de452351552bc1d41a5fd6638cab","title":"Policy-Induced Self-Supervision Improves Representation Finetuning in Visual RL","venue":"ArXiv","year":2023,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/02/2023","authors":"Sébastien M. R. Arnold,Fei Sha","id":"ef12cbed0377de452351552bc1d41a5fd6638cab","summary":"A self-supervised objective is proposed that clusters representations according to the policy they induce, as opposed to traditional representation similarity measures which are policy-agnostic (e.g. Euclidean norm, cosine similarity).","score":5},{"url":"https://www.semanticscholar.org/paper/6059e073b11b96af7566efddd1d4ee0e25046c54","title":"Forecasting Future World Events with Neural Networks","venue":"ArXiv","year":2022,"referenceCount":58,"citationCount":2,"influentialCitationCount":0,"publicationDate":"30/06/2022","authors":"Andy Zou,Tristan Xiao,Ryan Jia,Joe Kwon,Mantas Mazeika,Richard Li,Dawn Song,J. Steinhardt,Owain Evans,Dan Hendrycks","id":"6059e073b11b96af7566efddd1d4ee0e25046c54","summary":"Autocast is introduced, a dataset containing thousands of forecasting questions and an accompanying news corpus that poses a novel challenge for large language models and improved performance could bring large practical beneﬁts.","score":5},{"url":"https://www.semanticscholar.org/paper/f7d2c630cf62a88ffbff2c62a0ad94536d46224d","title":"Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling","venue":"ArXiv","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/01/2023","authors":"Kolby Nottingham,Prithviraj Ammanabrolu,Alane Suhr,Yejin Choi,Hanna Hajishirzi,Sameer Singh,Roy Fox","id":"f7d2c630cf62a88ffbff2c62a0ad94536d46224d","summary":"The method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efﬁciency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM—successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.","score":5},{"url":"https://www.semanticscholar.org/paper/50b0c6ee2b3d53ba5af69d6c00b5d60888a9026f","title":"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":53,"citationCount":22,"influentialCitationCount":3,"publicationDate":"24/05/2022","authors":"Jaehun Jung,Lianhui Qin,S. Welleck,Faeze Brahman,Chandra Bhagavatula,Ronan Le Bras,Yejin Choi","id":"50b0c6ee2b3d53ba5af69d6c00b5d60888a9026f","summary":"Maieutic Prompting is developed, which aims to infer a correct answer to a question even from the unreliable generations of LM, and achieves up to 20% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models.","score":5},{"url":"https://www.semanticscholar.org/paper/4217467e747182b9ad8035e8a2d657d2ce80af07","title":"On Reality and the Limits of Language Data","venue":"ArXiv","year":2022,"referenceCount":82,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/08/2022","authors":"N. Collier,Fangyu Liu,Ehsan Shareghi","id":"4217467e747182b9ad8035e8a2d657d2ce80af07","summary":"The objective of this work is to explore how far can language data alone enable computers to understand the necessary truth about the physical world using a novel and tightly controlled reasoning test and to highlight what models might learn directly from pure linguistic data.","score":5},{"url":"https://www.semanticscholar.org/paper/fadc0a6bcf968ed2ac71f567a48cd302dd62adde","title":"RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Tengxiao Liu,Qipeng Guo,Xiangkun Hu,Yuechen Zhang,Xipeng Qiu,Zheng Zhang","id":"fadc0a6bcf968ed2ac71f567a48cd302dd62adde","summary":"RLET is proposed, a Reinforcement Learning based Entailment Tree generation framework, which is trained utilising the cumulative signals across the whole tree, and is the first to introduce RL into the entailment tree generation task.","score":5},{"url":"https://www.semanticscholar.org/paper/ca2ea26b851fea6914a65b233b7daf8f32e38073","title":"CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":84,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Abhilasha Ravichander,Matt Gardner,Ana Marasović","id":"ca2ea26b851fea6914a65b233b7daf8f32e38073","summary":"CONDAQA is presented, the first English reading comprehension dataset which requires reasoning about the implications of negated statements in paragraphs, and is challenging for current state-of-the-art models.","score":5},{"url":"https://www.semanticscholar.org/paper/624ea7bdaf7e8e3f7bd76f72aa665b562f0dd70a","title":"When Do Decompositions Help for Machine Reading?","venue":"ArXiv","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Kangda Wei,Dawn J Lawrie,Benjamin Van Durme,Yunmo Chen,Orion Weller","id":"624ea7bdaf7e8e3f7bd76f72aa665b562f0dd70a","summary":"It is shown that decompositions can be helpful in the few-shot case, giving several points of improvement in exact match, but it is also shown that when models are given access to around a few hundred or more examples, decomposition are not helpful (and can actually be detrimental).","score":5},{"url":"https://www.semanticscholar.org/paper/1e0ae8f7c12c64824040624634dbae49428ce10f","title":"Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining","venue":"ArXiv","year":2023,"referenceCount":99,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/02/2023","authors":"Zekun Qi,Runpei Dong,Guo Fan,Zheng Ge,Xiangyu Zhang,Kaisheng Ma,Li Yi","id":"1e0ae8f7c12c64824040624634dbae49428ce10f","summary":"This paper proposes contrast with reconstruct (ReCon) that unifies these two paradigms, and achieves a new state-of-the-art in 3D representation learning, e.g., 91.26% accuracy on ScanObjectNN.","score":5},{"url":"https://www.semanticscholar.org/paper/f2a2401a35b6b892d43642b31700e83e88b2ebb8","title":"SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning","venue":"International Conference on Learning Representations","year":2022,"referenceCount":56,"citationCount":13,"influentialCitationCount":5,"publicationDate":"18/03/2022","authors":"Jongjin Park,Younggyo Seo,Jinwoo Shin,Honglak Lee,P. Abbeel,Kimin Lee","id":"f2a2401a35b6b892d43642b31700e83e88b2ebb8","summary":"SURF is presented, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation and improves the feedback-efficiency of the state-ofthe-art preference-based method on a variety of locomotion and robotic manipulation tasks.","score":5},{"url":"https://www.semanticscholar.org/paper/0bec67b22a85cc4344bbbeb837f369afde091288","title":"The Expertise Problem: Learning from Specialized Feedback","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/11/2022","authors":"Oliver Daniels-Koch,Rachel Freedman","id":"0bec67b22a85cc4344bbbeb837f369afde091288","summary":"This work formalizes the expertise problem of RLHF algorithms that learn from multiple teachers, implements it as an extension of an existing RLHF benchmark, evaluates the performance of a state-of-the-art RLHF algorithm, and explores techniques to improve query and teacher selection.","score":5},{"url":"https://www.semanticscholar.org/paper/c3349d0493a56f76c27d74c5056ab039f665b7b4","title":"Synthesizing Human Gaze Feedback for Improved NLP Performance","venue":"ArXiv","year":2023,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/02/2023","authors":"Varun Khurana,Yaman Kumar Singla,Nora Hollenstein,Rajesh Kumar,Balaji Krishnamurthy","id":"c3349d0493a56f76c27d74c5056ab039f665b7b4","summary":"ScanTextGAN, a novel model for generating human scanpaths over text, is proposed and it is shown that ScanTextGAN-generated scan Paths can approximate meaningful cognitive signals in human gaze patterns and improve the performance of all downstream NLP tasks.","score":5},{"url":"https://www.semanticscholar.org/paper/95f78c60698a1e7deb6a4ff8fcbdc3485c829dc6","title":"UGIF: UI Grounded Instruction Following","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"S. Venkatesh,Partha P. Talukdar,S. Narayanan","id":"95f78c60698a1e7deb6a4ff8fcbdc3485c829dc6","summary":"This work proposes a natural language based instruction following agent that operates over the UI and shows the user how to perform various tasks on the smartphone and analyzes the common failure modes of existing models on this task and point out areas for improvement.","score":5},{"url":"https://www.semanticscholar.org/paper/44772fe1c3fa422a3da7e25092db2544893d6bfb","title":"Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming","venue":"","year":2022,"referenceCount":43,"citationCount":3,"influentialCitationCount":1,"publicationDate":2022,"authors":"Hanlin Zhang,Jian-Hui Huang,Ziyang Li,M. Naik,Eric Xing","id":"44772fe1c3fa422a3da7e25092db2544893d6bfb","summary":"DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion and leads to improved logical reasoning of pre-trained LMs.","score":5},{"url":"https://www.semanticscholar.org/paper/76aa6eb43db7f0684a5fc4619bd41c384a699c5d","title":"Dynamic Generation of Interpretable Inference Rules in a Neuro-Symbolic Expert System","venue":"ArXiv","year":2022,"referenceCount":70,"citationCount":2,"influentialCitationCount":0,"publicationDate":"16/09/2022","authors":"Nathaniel Weir,Benjamin Van Durme","id":"76aa6eb43db7f0684a5fc4619bd41c384a699c5d","summary":"This novel reasoning engine, N ELLIE, dynamically instantiates interpretable inference rules that capture and score entailment (de)compositions over natural language statements that provide competitive performance on scientiﬁc QA datasets requiring structured explanations over multiple facts.","score":5},{"url":"https://www.semanticscholar.org/paper/b1054e448186822bfe9445bb6f4533d157e3da5e","title":"True Detective: A Challenging Benchmark for Deep Abductive Reasoning in Foundation Models","venue":"ArXiv","year":2022,"referenceCount":10,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Maksym Del,Mark Fishel","id":"b1054e448186822bfe9445bb6f4533d157e3da5e","summary":"The results show that state-of-the-art GPT models perform significantly worse than human solvers on this benchmark, indicating that there is still a signiﬁcant gap in the abductive reasoning abilities of LLMs and highlights the need for further research in this area.","score":5},{"url":"https://www.semanticscholar.org/paper/934733a641c6f46cec4ba74213c82fc76127eece","title":"A Categorical Archive of ChatGPT Failures","venue":"ArXiv","year":2023,"referenceCount":45,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/02/2023","authors":"A. Borji","id":"934733a641c6f46cec4ba74213c82fc76127eece","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/efa06fe7c6a4abbe465dbea4f7130f45720ac6f0","title":"Tuning computer vision models with task rewards","venue":"","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"André Susano Pinto,Alexander Kolesnikov,Yuge Shi,L. Beyer,Xiaohua Zhai","id":"efa06fe7c6a4abbe465dbea4f7130f45720ac6f0","summary":"This work adopts a reinforcement learning approach and shows its surprising effectiveness across multiple computer vision tasks, such as object detection, panoptic segmentation, colorization and image captioning.","score":5},{"url":"https://www.semanticscholar.org/paper/83f5f43ec419470508ce14355b0ecf0e9036dc0f","title":"Huge Frozen Language Models as Readers for Open-Domain Question Answering","venue":"","year":2022,"referenceCount":14,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yoav Levine,Ori Ram,Daniel Jannai,Barak Lenz,S. Shalev-Shwartz,A. Shashua,K. Leyton-Brown,Y. Shoham","id":"83f5f43ec419470508ce14355b0ecf0e9036dc0f","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/361b3c2d78c4ed44d0e11b8e16600b29fc27479d","title":"Reasoning over Public and Private Data in Retrieval-Based Systems","venue":"ArXiv","year":2022,"referenceCount":80,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/03/2022","authors":"Simran Arora,Patrick Lewis,Angela Fan,Jacob Kahn,Christopher R'e","id":"361b3c2d78c4ed44d0e11b8e16600b29fc27479d","summary":"This work defines the PUBLIC-PRIVATE AUTOREGRESSIVE INFORMATION RETRIEVAL (PAIR) privacy framework for the novel retrieval setting over multiple privacy scopes and argues that an adequate benchmark is missing to study PAIR since existing textual benchmarks require retrieving from a single data distribution.","score":5},{"url":"https://www.semanticscholar.org/paper/5a3b1fe3073d4ab6043542537c24b17f602eaf92","title":"Asking for Knowledge: Training RL Agents to Query External Knowledge Using Language","venue":"International Conference on Machine Learning","year":2022,"referenceCount":66,"citationCount":4,"influentialCitationCount":0,"publicationDate":"12/05/2022","authors":"Iou-Jen Liu,Xingdi Yuan,Marc-Alexandre Côté,P. Oudeyer,A. Schwing","id":"5a3b1fe3073d4ab6043542537c24b17f602eaf92","summary":"The AFK agent is proposed, which learns to generate language commands to query for meaningful knowledge that helps solve the tasks and outperforms recent baselines on the challenging Q-BabyAI and Q-TextWorld environments.","score":5},{"url":"https://www.semanticscholar.org/paper/64b0c2f0e89f56f3fb64a21fe699560316e56ef5","title":"Contextualized Generative Retrieval","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/10/2022","authors":"Hyunji Lee,Jaeyoung Kim,Hoyeon Chang,Hanseok Oh,Sohee Yang,Vladimir Karpukhin,Yi Lu,Minjoon Seo","id":"64b0c2f0e89f56f3fb64a21fe699560316e56ef5","summary":"The embeddings parametric generative retrieval generative vocab embeddins decoding generative and the embeddments parametric Generative embedding space bottleneck parametric space are presented.","score":5},{"url":"https://www.semanticscholar.org/paper/eebe62c685bffd1c8880fb3ce70e3b6d42b4b9e9","title":"DSI++: Updating Transformer Memory with New Documents","venue":"ArXiv","year":2022,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Sanket Vaibhav Mehta,Jai Gupta,Yi Tay,M. Dehghani,V. Tran,J. Rao,Marc-Alexander Najork,Emma Strubell,Donald Metzler","id":"eebe62c685bffd1c8880fb3ce70e3b6d42b4b9e9","summary":"This work introduces DSI++, a continual learning challenge for DSI to incrementally index new documents while being able to answer queries related to both previously and newly indexed documents, and introduces a generative memory to sample pseudo-queries for documents and supplement them during continual indexing to prevent forgetting for the retrieval task.","score":5},{"url":"https://www.semanticscholar.org/paper/714beceee04a4ab07a971ff69961972b2e740eb5","title":"Is GPT-3 a Psychopath? Evaluating Large Language Models from a Psychological Perspective","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Xingxuan Li,Yutong Li,Linlin Liu,Lidong Bing,Shafiq R. Joty","id":"714beceee04a4ab07a971ff69961972b2e740eb5","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/36589346063ff26506330451976280011273b935","title":"Towards Teachable Reasoning Systems","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":10,"influentialCitationCount":3,"publicationDate":2022,"authors":"Bhavana Dalvi,Oyvind Tafjord,Peter Clark","id":"36589346063ff26506330451976280011273b935","summary":"Generated chains of reasoning show how answers are implied by the system’s own internal beliefs, and are both faithful and truthful, which suggests new opportunities for using language models in an interactive setting where users can inspect, debug, correct, and improve a system‘s performance over time.","score":5},{"url":"https://www.semanticscholar.org/paper/7865ae1c4d7ba0e3ba48aef54270c3fdf61ec11d","title":"Symbolic Data Augmentation for Assisted Neural Reasoning","venue":"","year":2022,"referenceCount":82,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Muhan Li","id":"7865ae1c4d7ba0e3ba48aef54270c3fdf61ec11d","summary":"It is demonstrated that SDAR can boost the performance of smaller models to a comparable degree of or even surpass larger models with a magnitude more parameters, and establish the state-of-the-art single model performance on OPENBOOKQA.","score":5},{"url":"https://www.semanticscholar.org/paper/7b9c43ee43b7e21af079ff0c098b2b17cefe461f","title":"DREAM: Improving Situational QA by First Elaborating the Situation","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":49,"citationCount":7,"influentialCitationCount":2,"publicationDate":"16/12/2021","authors":"Yuling Gu,Bhavana Dalvi,Peter Clark","id":"7b9c43ee43b7e21af079ff0c098b2b17cefe461f","summary":"Adding focused elaborations about a situation can improve a system’s reasoning about it, and may serve as an effective way of injecting new scenario-based knowledge into QA models.","score":5},{"url":"https://www.semanticscholar.org/paper/9ffefdf1fcd780cb71450b0a7a29247c66aa87be","title":"The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning","venue":"","year":2022,"referenceCount":69,"citationCount":8,"influentialCitationCount":0,"publicationDate":"06/05/2022","authors":"Xi Ye,Greg Durrett","id":"9ffefdf1fcd780cb71450b0a7a29247c66aa87be","summary":"Analysis in three settings shows that explanations judged by humans to be good—logically consistent with the input and the prediction—more likely cooccur with accurate predictions, and trains calibrators using automatically extracted scores that assess the reliability of explanations to improve performance post-hoc.","score":5},{"url":"https://www.semanticscholar.org/paper/d400a649f0f0a3de22b89a268f48aff2dcb06a09","title":"Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":38,"citationCount":3,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Oyvind Tafjord,Bhavana Dalvi,Peter Clark","id":"d400a649f0f0a3de22b89a268f48aff2dcb06a09","summary":"This work recursively combines a trained backward-chaining model, capable of generating a set of premises entailing an answer hypothesis, with a verifier that checks that the model itself believes those premises (and the entailment itself) through self-querying.","score":5},{"url":"https://www.semanticscholar.org/paper/04db9b694280134f09af5fa787a306907edba29d","title":"How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN","venue":"ArXiv","year":2021,"referenceCount":81,"citationCount":18,"influentialCitationCount":2,"publicationDate":"18/11/2021","authors":"R. Thomas McCoy,P. Smolensky,Tal Linzen,Jianfeng Gao,Asli Celikyilmaz","id":"04db9b694280134f09af5fa787a306907edba29d","summary":"AVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure, is introduced, showing that GPT-2's novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues.","score":5},{"url":"https://www.semanticscholar.org/paper/9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":3,"influentialCitationCount":0,"publicationDate":"26/06/2022","authors":"Disha Shrivastava,H. Larochelle,Daniel Tarlow","id":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","summary":"This work proposes a framework called Repo-Level Prompt Generator that learns to generate example-speciﬁc prompts using prompt proposals, and demonstrates that an oracle constructed from these prompt proposals gives a remarkably high relative improvement over Codex, showing the quality of these proposals.","score":5},{"url":"https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":19,"influentialCitationCount":5,"publicationDate":"11/07/2022","authors":"Cem Anil,Yuhuai Wu,Anders Andreassen,Aitor Lewkowycz,Vedant Misra,V. Ramasesh,Ambrose Slone,Guy Gur-Ari,Ethan Dyer,Behnam Neyshabur","id":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","summary":"It is shown that combining pretrained large language models’ in-context learning abilities with scratchpad prompting results in a dramatic improvement in length generalization, and is run to identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.","score":5},{"url":"https://www.semanticscholar.org/paper/4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Daoguang Zan,Bei Chen,Zeqi Lin,Bei Guan,Yongji Wang,Jian-Guang Lou","id":"4fbe0cb0777b228e39243692bf29e2829060b8de","summary":"This paper investigates how to equip pre-trained language models with the ability of code generation for private libraries, and proposes a novel framework with two modules: the APIRetriever finds useful APIs, and the APICoder generates code using these APIs.","score":5},{"url":"https://www.semanticscholar.org/paper/95915aa592fdfc73f039c13472a21d3e4220f129","title":"On the Compositional Generalization Gap of In-Context Learning","venue":"BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Arian Hosseini,A. Vani,Dzmitry Bahdanau,Alessandro Sordoni,Aaron C. Courville","id":"95915aa592fdfc73f039c13472a21d3e4220f129","summary":"This work evaluates four model families, OPT, BLOOM, CodeGen and Codex on three semantic parsing datasets, CFQ, SCAN and GeoQuery with different number of exemplars, and observes a trend of decreasing relative generalization gap as models are scaled up.","score":5},{"url":"https://www.semanticscholar.org/paper/8bfd39e6e8f15531ffb071f2c6470e1e6e0a4aff","title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":3,"influentialCitationCount":1,"publicationDate":"08/12/2022","authors":"Chan Hee Song,Jiaman Wu,Clay Washington,Brian M. Sadler,Wei-Lun Chao,Yu Su","id":"8bfd39e6e8f15531ffb071f2c6470e1e6e0a4aff","summary":"A novel method is proposed, LLM-Planner, that harnesses the power of large language models (LLMs) such as GPT-3 to do few-shot planning for embodied agents and further proposes a simple but effective way to enhance LLMs with physical grounding to generate plans that are grounded in the current environment.","score":5},{"url":"https://www.semanticscholar.org/paper/d98fd1dd218bf522722a42b7c56f53dd6b1d20b0","title":"Diverse Demonstrations Improve In-context Compositional Generalization","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":2,"influentialCitationCount":1,"publicationDate":"13/12/2022","authors":"Itay Levy,Ben Bogin,Jonathan Berant","id":"d98fd1dd218bf522722a42b7c56f53dd6b1d20b0","summary":"This work proposes a method to select diverse demonstrations that aims to collectively cover all of the structures required in the output program, in order to encourage the model to generalize to new structures from these demonstrations.","score":5},{"url":"https://www.semanticscholar.org/paper/6a53eeada90d83b9508e7e451d62fdc9d2476350","title":"Using cognitive psychology to understand GPT-3","venue":"Proceedings of the National Academy of Sciences of the United States of America","year":2022,"referenceCount":58,"citationCount":14,"influentialCitationCount":0,"publicationDate":"21/06/2022","authors":"Marcel Binz,Eric Schulz","id":"6a53eeada90d83b9508e7e451d62fdc9d2476350","summary":"Much of GPT-3's behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning.","score":5},{"url":"https://www.semanticscholar.org/paper/a20875e70a38cb053cd34e170038c4746f85dac9","title":"A Case Study in Engineering a Conversational Programming Assistant's Persona","venue":"ArXiv","year":2023,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/01/2023","authors":"Steven I. Ross,Michael J. Muller,Fernando Martinez,Stephanie Houde,Justin D. Weisz","id":"a20875e70a38cb053cd34e170038c4746f85dac9","summary":"The Programmer’s Assistant is an experimental prototype software development environment that integrates a chatbot with a code editor that establishes a conversational interaction pattern, a set of conventions.","score":5},{"url":"https://www.semanticscholar.org/paper/658fbf9874c35b442ca67e3d0077682064808cbf","title":"Norm-based Generalization Bounds for Compositionally Sparse Neural Networks","venue":"ArXiv","year":2023,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/01/2023","authors":"Tomer Galanti,Mengjia Xu,Liane Galanti,T. Poggio","id":"658fbf9874c35b442ca67e3d0077682064808cbf","summary":"The Rademacher complexity of deep sparse neural networks is investigated and generalization bounds for multilayered sparse ReLU neural networks, including convolutional neural networks are proved, suggesting that compositional sparsity of the underlying target function is critical to the success of deep neural networks.","score":5},{"url":"https://www.semanticscholar.org/paper/327333ba2362c48b11b76c9673475d25c076a363","title":"Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming","venue":"","year":2023,"referenceCount":85,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Majeed Kazemitabaar,J. Chow,Carl Ka To Ma,Barbara J. Ericson,David Weintrop,Tovi Grossman","id":"327333ba2362c48b11b76c9673475d25c076a363","summary":"Using OpenAI Codex significantly increased code-authoring performance while not decreasing performance on manual code-modification tasks, and learners with access to Codex during the training phase performed slightly better on the evaluation post-tests conducted one week later, although this difference did not reach statistical significance.","score":5},{"url":"https://www.semanticscholar.org/paper/10e8fd0de2c06056393dd1e5e16376bd83c88b42","title":"To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Dheeru Dua,Emma Strubell,Sameer Singh,Pat Verga","id":"10e8fd0de2c06056393dd1e5e16376bd83c88b42","summary":"It is found that not only do models fail to generalize, but high retrieval scores often still yield poor answer prediction accuracy, and several intervention methods are proposed which improve end-to-end answer F1 score by up to ∼ 24 points.","score":5},{"url":"https://www.semanticscholar.org/paper/4b256efcc9627d5f83a91de577623ee5479bc8f9","title":"Generating Full Length Wikipedia Biographies: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies","venue":"ArXiv","year":2022,"referenceCount":96,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/04/2022","authors":"Angela Fan,Claire Gardent","id":"4b256efcc9627d5f83a91de577623ee5479bc8f9","summary":"A model for English text is developed that uses a retrieval mechanism to identify relevant sup-porting information on the web and a cache-based pre-trained encoder-decoder to generate long-form biographies section by section, including citation information.","score":5},{"url":"https://www.semanticscholar.org/paper/816593b1abbc9b09763fcb2894ca3778db341769","title":"Generative Knowledge Graph Construction: A Review","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":109,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"Hongbin Ye,Ningyu Zhang,Hui Chen,Huajun Chen","id":"816593b1abbc9b09763fcb2894ca3778db341769","summary":"This study summarizes the recent compelling progress in generative knowledge graph construction and presents a detailed, complete taxonomy for the generative KGC methods to provide theoretical insight and empirical analysis.","score":5},{"url":"https://www.semanticscholar.org/paper/1338b3771c27090dee722cc5b351ace179ebae76","title":"Mathematics, word problems, common sense, and artificial intelligence","venue":"ArXiv","year":2023,"referenceCount":18,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/01/2023","authors":"E. Davis","id":"1338b3771c27090dee722cc5b351ace179ebae76","summary":"It is argued that it is not clear whether these kinds of limitations will be important in developing AI technology for pure mathematical research, but that they will beImportant in applications of mathematics, and may well beimportant in developing programs capable of reading and understanding mathematical content written by humans.","score":5},{"url":"https://www.semanticscholar.org/paper/eaa88d697f92739f3569564329e9d037aabbe2d7","title":"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics","venue":"","year":2021,"referenceCount":105,"citationCount":1,"influentialCitationCount":1,"publicationDate":"02/03/2021","authors":"Qing Li,Siyuan Huang,Yining Hong,Yixin Zhu,Y. Wu,Song-Chun Zhu","id":"eaa88d697f92739f3569564329e9d037aabbe2d7","summary":"Models show a gap toward human-level generalization when tested with new concepts in a few-shot setting, and the results suggest that current models still struggle in extrapolation to long-range syntactic dependency and semantics.","score":5},{"url":"https://www.semanticscholar.org/paper/70f8e3c72e9178b408667e3619a87a153fd853e6","title":"Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned","venue":"BIGSCIENCE","year":2022,"referenceCount":77,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sameera Horawalavithana,Ellyn Ayton,Shivam Sharma,Scott Howland,Megha Subramanian,Scott Vasquez,Robin Cosbey,M. Glenski,Svitlana Volkova","id":"70f8e3c72e9178b408667e3619a87a153fd853e6","summary":"This work develops large-scale general-purpose models for chemistry that can be effectively used to perform a wide range of in-domain and out-of-domain tasks and demonstrates that model size significantly contributes to the task performance when evaluated in a zero-shot setting.","score":5},{"url":"https://www.semanticscholar.org/paper/03488f1a193066b5ea8b9b800e119f07df5c1d9e","title":"Reasoning Like Program Executors","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":78,"citationCount":18,"influentialCitationCount":4,"publicationDate":"27/01/2022","authors":"Xinyu Pi,Qian Liu,Bei Chen,Morteza Ziyadi,Zeqi Lin,Yan Gao,Qiang Fu,Jian-Guang Lou,Weizhu Chen","id":"03488f1a193066b5ea8b9b800e119f07df5c1d9e","summary":"Experimental results on six benchmarks demonstrate that POET can significantly boost model performance in natural language reasoning, such as numerical reasoning, logical reasoning, and multi-hop reasoning.","score":5},{"url":"https://www.semanticscholar.org/paper/5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode","venue":"Science","year":2022,"referenceCount":81,"citationCount":152,"influentialCitationCount":33,"publicationDate":"08/02/2022","authors":"Yujia Li,David H. Choi,Junyoung Chung,Nate Kushman,Julian Schrittwieser,Rémi Leblond,Tom,Eccles,James Keeling,Felix Gimeno,Agustin Dal Lago,T. Hubert,Peter Choy,Cyprien de,Masson d’Autume,I. Babuschkin,Xinyun Chen,Po-Sen Huang,Johannes Welbl,Sven Gowal,Alexey,Cherepanov,James Molloy,D. Mankowitz,Esme Sutherland Robson,Pushmeet Kohli,Nando de,Freitas,K. Kavukcuoglu,Oriol Vinyals","id":"5cbe278b65a81602a864184bbca37de91448a5f5","summary":"AlphaCode is introduced, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform, marking the first time an artificial intelligence system has performed competitively in programming competitions.","score":5},{"url":"https://www.semanticscholar.org/paper/155d712236147d90516045a71b66d5d32c03846f","title":"Autoregressive Search Engines: Generating Substrings as Document Identifiers","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":18,"influentialCitationCount":7,"publicationDate":"22/04/2022","authors":"Michele Bevilacqua,G. Ottaviano,Patrick Lewis,Wen-tau Yih,Sebastian Riedel,Fabio Petroni","id":"155d712236147d90516045a71b66d5d32c03846f","summary":"This work proposes an alternative that doesn’t force any structure in the search space: using all ngrams in a passage as its possible identiﬁer, which not only outperforms prior autoregressive approaches but also leads to an average improvement over more established retrieval solutions for passage-level retrieval on the KILT benchmark.","score":5},{"url":"https://www.semanticscholar.org/paper/26c9075f42ffa3e9b563c024dbf3e08f7d8df8f5","title":"Factuality Enhanced Language Models for Open-Ended Text Generation","venue":"ArXiv","year":2022,"referenceCount":78,"citationCount":7,"influentialCitationCount":0,"publicationDate":"09/06/2022","authors":"Nayeon Lee,Wei Ping,Peng Xu,M. Patwary,M. Shoeybi,Bryan Catanzaro","id":"26c9075f42ffa3e9b563c024dbf3e08f7d8df8f5","summary":"This work measures and improves the factual accuracy of large-scale LMs for open-ended text generation, and proposes a factuality-enhanced training method that uses T OPIC P REFIX for better awareness of facts and sentence completion as the training objective, which can vastly reduce the factual errors.","score":5},{"url":"https://www.semanticscholar.org/paper/07dc375b95aaeb748d7b0560bfa7d81f1bddc8b2","title":"Predicting the Future of AI with AI: High-quality link prediction in an exponentially growing knowledge network","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/09/2022","authors":"M. Krenn,L. Buffoni,B. Coutinho,S. Eppel,Jacob G. Foster,Andrew Gritsevskiy,Harlin Lee,Yichao Lu,João P. Moutinho,Nima Sanjabi,Rishi Sonthalia,Ngoc M. Tran,Francisco Valente,Yangxinyu Xie,Rose Yu,Michael Kopp","id":"07dc375b95aaeb748d7b0560bfa7d81f1bddc8b2","summary":"A new graph-based benchmark based on real-world data – the Science4Cast benchmark is developed, which aims to predict the future state of an evolving semantic network of AI.","score":5},{"url":"https://www.semanticscholar.org/paper/9f93f293fd2a188c6eb2c86aa55fd135548bb7a4","title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":7,"influentialCitationCount":2,"publicationDate":"18/11/2022","authors":"Guangxuan Xiao,Ji Lin,Mickael Seznec,Julien Demouth,Song Han","id":"9f93f293fd2a188c6eb2c86aa55fd135548bb7a4","summary":"This work proposes SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight,8-bit activation (W8A8) quantization for LLMs, and integrates it into FasterTransformer, a state-of-the-art LLM serving framework, to achieve faster inference speed with half the number of GPUs compared to FP16.","score":5},{"url":"https://www.semanticscholar.org/paper/775b2dc88cf04993f8596332444a906bec2db807","title":"Foundation models in brief: A historical, socio-technical focus","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/12/2022","authors":"Johannes Schneider","id":"775b2dc88cf04993f8596332444a906bec2db807","summary":"This paper contributes by crafting a crisp distinction between foundation models and prior deep learning models, providing a history of machine learning leading to foundation models, elaborating more on socio-technical aspects, i.e., organizational issues and end-user interaction, and a discussion of future research.","score":5},{"url":"https://www.semanticscholar.org/paper/3c9ba25baca64151af4e9d50c7947de28eb2a599","title":"Survey of Hallucination in Natural Language Generation","venue":"ACM Computing Surveys","year":2022,"referenceCount":250,"citationCount":61,"influentialCitationCount":4,"publicationDate":"08/02/2022","authors":"Ziwei Ji,Nayeon Lee,Rita Frieske,Tiezheng Yu,D. Su,Yan Xu,Etsuko Ishii,Yejin Bang,Wenliang Dai,Andrea Madotto,Pascale Fung","id":"3c9ba25baca64151af4e9d50c7947de28eb2a599","summary":"A broad overview of the research progress and challenges in the hallucination problem in NLG is provided, including task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation.","score":5},{"url":"https://www.semanticscholar.org/paper/0f29d13896f1851422ada71eb15d31f1ab61cddf","title":"Effidit: Your AI Writing Assistant","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/08/2022","authors":"Shuming Shi,Enbo Zhao,Duyu Tang,Yan Wang,Piji Li,Wei Bi,Haiyun Jiang,Guoping Huang,Leyang Cui,Xinting Huang,Cong Zhou,Yong Dai,Dongyang Ma","id":"0f29d13896f1851422ada71eb15d31f1ab61cddf","summary":"The main contents of this report include major modules of Efﬁdit, methods for implementing these modules, and evaluation results of some key methods.","score":5},{"url":"https://www.semanticscholar.org/paper/366bae3f3e269ed9e1c3b6b8ef5b0984e8d5476f","title":"Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective","venue":"ArXiv","year":2022,"referenceCount":118,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/10/2022","authors":"Adaku Uchendu,Thai Le,Dongwon Lee","id":"366bae3f3e269ed9e1c3b6b8ef5b0984e8d5476f","summary":"A comprehensive review of recent literature on the attribution and obfuscation of neural text authorship from a Data Mining perspective, and the view on their limitations and promising research directions is shared.","score":5},{"url":"https://www.semanticscholar.org/paper/9a523de464d0096a4f2f722ecda5ef11a42bc6eb","title":"Robustness of Demonstration-based Learning Under Limited Data Scenario","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/10/2022","authors":"Hongxin Zhang,Yanzhe Zhang,Ruiyi Zhang,Diyi Yang","id":"9a523de464d0096a4f2f722ecda5ef11a42bc6eb","summary":"This paper designs pathological demonstrations by gradually removing intuitively useful information from the standard ones to take a deep dive of the robustness of demonstration-based sequence labeling and shows that demonstrations composed of random tokens still make the model a better few-shot learner.","score":5},{"url":"https://www.semanticscholar.org/paper/95d54e3ce577f7d91ab4b2c52c73b501245e484d","title":"ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":53,"citationCount":1,"influentialCitationCount":0,"publicationDate":"22/10/2022","authors":"Jiacheng Ye,Jiahui Gao,Jiangtao Feng,Zhiyong Wu,Tao Yu,Lingpeng Kong","id":"95d54e3ce577f7d91ab4b2c52c73b501245e484d","summary":"A progressive zero-shot dataset generation framework, ProGen, which leverages the feedback from the task-specific model to guide the generation of new training data via in- context examples, which achieves on-par or superior performance with only 1\\% synthetic dataset size compared to baseline methods without in-context feedback.","score":5},{"url":"https://www.semanticscholar.org/paper/fe123b27a3ecc63c7087cb0d3a04e43790661c2d","title":"LAD: Language Augmented Diffusion for Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Edwin Zhang,Yujie Lu,W. Wang,Amy Zhang","id":"fe123b27a3ecc63c7087cb0d3a04e43790661c2d","summary":"This paper demonstrates the comparable performance of LAD with the state-of-the-art on the CALVIN language robotics benchmark with a much simpler architecture that contains no inductive biases special-ized to robotics, achieving an average success rate of 72% compared to the best performance of 76%.","score":5},{"url":"https://www.semanticscholar.org/paper/20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":2,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Spyridon Mouselinos,Mateusz Malinowski,H. Michalewski","id":"20fae749e3d469c331731ffa2f811079db792fdc","summary":"This work shows that current code generation systems exhibit biases inherited from large language model backbones, which might leak into generated code under specific circumstances, and proposes a framework that automatically removes hints and exposes various biases that these code generation models use.","score":5},{"url":"https://www.semanticscholar.org/paper/066316c88c708bbc58b967a24f5dfdd4be371295","title":"Prompting PaLM for Translation: Assessing Strategies and Performance","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":4,"influentialCitationCount":0,"publicationDate":"16/11/2022","authors":"David Vilar,Markus Freitag,Colin Cherry,Jiaming Luo,Viresh Ratnakar,George F. Foster","id":"066316c88c708bbc58b967a24f5dfdd4be371295","summary":"An in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date, and an analysis of PaLM’s MT output which reveals some interesting properties and prospects for future work.","score":5},{"url":"https://www.semanticscholar.org/paper/f318ab67ac22cb758e38a16dafdc8e486b7b9756","title":"Planning with Large Language Models via Corrective Re-prompting","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/11/2022","authors":"S. S. Raman,Vanya Cohen,Eric Rosen,Ifrah Idrees,D. Paulius,Stefanie Tellex","id":"f318ab67ac22cb758e38a16dafdc8e486b7b9756","summary":"This work proposes a prompting-based strategy for extracting executable plans from an LLM, which leverages a novel and readily-accessible source of information: precondition errors, and improves executability and semantic correctness of plans, while also reducing the number of re-prompts required when querying actions.","score":5},{"url":"https://www.semanticscholar.org/paper/471a49220cea2069e8b8a76821b1d2434204a732","title":"FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/11/2022","authors":"Akhil Kedia,Mohd Abbas Zaidi,Haejun Lee","id":"471a49220cea2069e8b8a76821b1d2434204a732","summary":"This work proposes to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples, and proposes an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples.","score":5},{"url":"https://www.semanticscholar.org/paper/6d951d939d3f27054215f2606a0cf89ed21550e9","title":"Improving Few-Shot Performance of Language Models via Nearest Neighbor Calibration","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Feng Nie,Meixi Chen,Zhirui Zhang,Xuan Cheng","id":"6d951d939d3f27054215f2606a0cf89ed21550e9","summary":"Experiments on various few-shot text classiﬁcation tasks demonstrate that the proposed nearest-neighbor calibration framework for in- context learning proves in-context learning, while even achieving comparable performance with state-of-the-art tuning-based approaches in some sentiment analysis tasks.","score":5},{"url":"https://www.semanticscholar.org/paper/efa92d27065501981f2ade15c1cd884fdf644f44","title":"APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/12/2022","authors":"Jia Sun,Hang Zhang,Chen Lin,Yeyun Gong,Jian Guo,Nan Duan","id":"efa92d27065501981f2ade15c1cd884fdf644f44","summary":"This work proposes APOLLO to improve the long-form numerical reasoning framework, and adopts a number-aware negative sampling strategy to enable the retriever to be more discriminative on key numerical facts.","score":5},{"url":"https://www.semanticscholar.org/paper/afdf2daa43ff4415d54a4c4501a339d619a2d13b","title":"MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"Kun Zhou,Xiao Liu,Yeyun Gong,Wayne Xin Zhao,Daxin Jiang,Nan Duan,Ji-rong Wen","id":"afdf2daa43ff4415d54a4c4501a339d619a2d13b","summary":"Experimental results show that the proposed approach can achieve new state-of-the-art performances in dense retrieval indatasets, e.g., MS-MARCO Passage Ranking, TREC Deep Learning Track, Natural Questions and BEIR zero-shot retrieval benchmark.","score":5},{"url":"https://www.semanticscholar.org/paper/fa50f2bc03d6d53fe50f37a1978107b13af24ea7","title":"SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization","venue":"ArXiv","year":2022,"referenceCount":59,"citationCount":3,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Hyunwoo Kim,Jack Hessel,Liwei Jiang,Ximing Lu,Youngjae Yu,Pei Zhou,Ronan Le Bras,Malihe Alikhani,Gunhee Kim,Maarten Sap,Yejin Choi","id":"fa50f2bc03d6d53fe50f37a1978107b13af24ea7","summary":"C OSMO is a generalizable conversation agent outperforming previous best-performing agents on both in- and out-of-domain datasets, and human evaluation shows that dialogues in S ODA are more consistent, speciﬁc, and (surprisingly) natural than prior human-authored datasets.","score":5},{"url":"https://www.semanticscholar.org/paper/8a5c9f0606b985dee71b6d8417023805f0938a9f","title":"Are Deep Neural Networks SMARTer than Second Graders?","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"A. Cherian,Kuan-Chuan Peng,Suhas Lohit,Kevin A. Smith,J. Tenenbaum","id":"8a5c9f0606b985dee71b6d8417023805f0938a9f","summary":"The experiments reveal that while powerful deep models offer reasonable performances on puzzles that they are trained on, they are not better than random accuracy when analyzed for generalization, and the recent ChatGPT large language model is evaluated.","score":5},{"url":"https://www.semanticscholar.org/paper/12a4e62c43b829dabdb8afc60eee76aa80fa3f6e","title":"Fuzzing Deep-Learning Libraries via Large Language Models","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/12/2022","authors":"Yinlin Deng,Chun Xia,Haoran Peng,Chenyuan Yang,Lingming Zhang","id":"12a4e62c43b829dabdb8afc60eee76aa80fa3f6e","summary":"LLMFuzz is the first automated approach to directly leveraging Large Pre-trained Language Models (LLMs) to generate input programs for fuzzing DL libraries, and is able to detect 65 bugs, with 41 already confirmed as previously unknown bugs.","score":5},{"url":"https://www.semanticscholar.org/paper/5f72c4f7a0991e7035ccc87b69410f4b3e6244ce","title":"Masked Contrastive Representation Learning for Reinforcement Learning","venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence","year":2020,"referenceCount":63,"citationCount":13,"influentialCitationCount":2,"publicationDate":"15/10/2020","authors":"Jinhua Zhu,Yingce Xia,Lijun Wu,Jiajun Deng,Wen-gang Zhou,Tao Qin,Houqiang Li","id":"5f72c4f7a0991e7035ccc87b69410f4b3e6244ce","summary":"This work proposes a new algorithm, i.e., masked contrastive representation learning for RL (M-CURL), which takes the correlation among consecutive inputs into consideration and achieves consistent improvements over CURL on 14 out of 16 environments from DMControl suite and 23 out of 26 environments from Atari 2600 Games.","score":5},{"url":"https://www.semanticscholar.org/paper/de91ff679800fbdb224b96d68f84a78c8fe629c7","title":"Mind the gap: Challenges of deep learning approaches to Theory of Mind","venue":"Artificial Intelligence Review","year":2022,"referenceCount":110,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/03/2022","authors":"J. Aru,Aqeel Labash,Oriol Corcoll,Raul Vicente","id":"de91ff679800fbdb224b96d68f84a78c8fe629c7","summary":"It is argued that when studying Theory of Mind with deep learning, the research's main focus and contribution ought to be opening up the network's representations, and researchers use tools from the field of interpretability of AI to study the relationship between different network components and aspects of Theory ofMind.","score":5},{"url":"https://www.semanticscholar.org/paper/c432aff446d55e72a28394a1508e760cc9a25c08","title":"Why do Nearest Neighbor Language Models Work?","venue":"ArXiv","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/01/2023","authors":"Frank F. Xu,Uri Alon,Graham Neubig","id":"c432aff446d55e72a28394a1508e760cc9a25c08","summary":"This paper identifies three main reasons why k N-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate k NN search, and the importance of softmax temperature for the k Nn distribution.","score":5},{"url":"https://www.semanticscholar.org/paper/7ec58d26c4dddb4bc3b6829fa0654a22cc26fdfe","title":"Memory Augmented Large Language Models are Computationally Universal","venue":"ArXiv","year":2023,"referenceCount":23,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Dale Schuurmans","id":"7ec58d26c4dddb4bc3b6829fa0654a22cc26fdfe","summary":"It is established that an existing large language model, Flan-U-PaLM 540B, can be combined with an associative read-write memory to exactly simulate the execution of a universal Turing machine, U 15 , 2.","score":5},{"url":"https://www.semanticscholar.org/paper/db5423d1d5737aa90c48bc121239160d24dccb36","title":"Blind Judgement: Agent-Based Supreme Court Modelling With GPT","venue":"ArXiv","year":2023,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2023","authors":"S. Hamilton","id":"db5423d1d5737aa90c48bc121239160d24dccb36","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/92999f7a304e866a2be7176e59c745481ed01042","title":"Co-Writing with Opinionated Language Models Affects Users' Views","venue":"ArXiv","year":2023,"referenceCount":112,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Maurice Jakesch,Advait Bhat,D. Buschek,Lior Zalmanson,Mor Naaman","id":"92999f7a304e866a2be7176e59c745481ed01042","summary":"Whether a language-model-powered writing assistant that generates some opinions more often than others impacts what users write - and what they think is investigated.","score":5},{"url":"https://www.semanticscholar.org/paper/05deb6c1862b2f129d6652a09eaedbc1f655cc8f","title":"STEP: Learning N: M Structured Sparsity Masks from Scratch with Precondition","venue":"ArXiv","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Yucheng Lu,Shivani Agrawal,Suvinay Subramanian,Oleg Rybakov,Chris De Sa,A. Yazdanbakhsh","id":"05deb6c1862b2f129d6652a09eaedbc1f655cc8f","summary":"STEP is proposed, an Adam-aware recipe that learns N:M masks with two phases: first, STEP calculates a reliable variance estimate (precondition phase) and subsequently, the variance remains fixed and is used as a precondition to learn N: M masks (mask-learning phase).","score":5},{"url":"https://www.semanticscholar.org/paper/d55b69a533dea69c8b2673cde8de90c6626ee789","title":"A Text-guided Protein Design Framework","venue":"ArXiv","year":2023,"referenceCount":102,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/02/2023","authors":"Shengchao Liu,Yutao Zhu,Jiarui Lu,Zhao Xu,Weili Nie,A. Gitter,Chaowei Xiao,Jian Tang,Hongyu Guo,Anima Anandkumar","id":"d55b69a533dea69c8b2673cde8de90c6626ee789","summary":"The proposed ProteinDT is a multi-modal framework that leverages textual descriptions for protein design that consistently superior performance on four out of six protein property prediction benchmarks and promising results for zero-shot text-guided protein editing.","score":5},{"url":"https://www.semanticscholar.org/paper/897241ffbcb7b32f15cc45599ea3f297613f0c90","title":"MarioGPT: Open-Ended Text2Level Generation through Large Language Models","venue":"ArXiv","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/02/2023","authors":"Shyam Sudhakaran,Miguel Gonz'alez-Duque,Claire Glanois,M. Freiberger,Elias Najarro,S. Risi","id":"897241ffbcb7b32f15cc45599ea3f297613f0c90","summary":"MarioGPT is the first text-to-level model trained to generate tile-based game levels, and it is shown that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques.","score":5},{"url":"https://www.semanticscholar.org/paper/4805470c7e5abf36781bf89f6fe8743c7344ab90","title":"Gradient-Based Automated Iterative Recovery for Parameter-Efficient Tuning","venue":"","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Maximilian Mozes,Tolga Bolukbasi,Ann Yuan,Frederick Liu,Nithum Thain,Lucas Dixon","id":"4805470c7e5abf36781bf89f6fe8743c7344ab90","summary":"It is shown that G-BAIR can recover LLM performance on benchmarks after manually corrupting training labels, suggesting that influence methods like TracIn can be used to automatically perform data cleaning, and introduces the potential for interactive debugging and relabeling for PET-based transfer learning methods.","score":5},{"url":"https://www.semanticscholar.org/paper/4290a70025f29d7054c550c75ae6b24c38a79d12","title":"SE Factual Knowledge in Frozen Giant Code Model: A Study on FQN and its Retrieval","venue":"ArXiv","year":2022,"referenceCount":74,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Qing Huang,Dianshu Liao,Zhenchang Xing,Zhiqiang Yuan,Qinghua Lu,Xiwei Xu,Jiaxing Lu","id":"4290a70025f29d7054c550c75ae6b24c38a79d12","summary":"This work conducts the first systematic study on the SE factual knowledge in the state-of-the-art PCM CoPilot, focusing on APIs’ Fully Qualiﬁed Names (FQNs), the fundamental knowledge for effective code analysis, search and reuse.","score":5},{"url":"https://www.semanticscholar.org/paper/df239785e6d26a45e9c8e06551cfecba92d1ecad","title":"Exploring AI Ethics of ChatGPT: A Diagnostic Analysis","venue":"ArXiv","year":2023,"referenceCount":93,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Terry Yue Zhuo,Yujin Huang,Chunyang Chen,Zhenchang Xing","id":"df239785e6d26a45e9c8e06551cfecba92d1ecad","summary":"A qualitative research method on OpenAI’s ChatGPT is performed to better understand the practical features of ethical dangers in recent LLMs, and it is found that a signiﬁcant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies.","score":5},{"url":"https://www.semanticscholar.org/paper/3ed1c94ec4fdd2a9235afeb2d929fde965b1d723","title":"Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models","venue":"ArXiv","year":2023,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/02/2023","authors":"Renat Aksitov,Chung-Ching Chang,D. Reitter,Siamak Shakeri,Yun-Hsuan Sung","id":"3ed1c94ec4fdd2a9235afeb2d929fde965b1d723","summary":"The relationship between fluency and attribution in LLMs prompted with retrieved evidence in knowledge-heavy dialog settings is examined and a recipe is proposed that could allow smaller models to both close the gap with larger models and preserve the benefits of top-k retrieval while avoiding its drawbacks.","score":5},{"url":"https://www.semanticscholar.org/paper/8a13d58ce02734f92dcc787272afbea4bbc7f09f","title":"Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models","venue":"ArXiv","year":2022,"referenceCount":107,"citationCount":6,"influentialCitationCount":0,"publicationDate":"16/06/2022","authors":"M. Rauh,John F. J. Mellor,J. Uesato,Po-Sen Huang,Johannes Welbl,Laura Weidinger,Sumanth Dathathri,A. Glaese,Geoffrey Irving,Iason Gabriel,William S. Isaac,Lisa Anne Hendricks","id":"8a13d58ce02734f92dcc787272afbea4bbc7f09f","summary":"Six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks are outlined and applied in a case study of the Perspective API, a toxicity classiﬁer that is widely used in harm benchmarks.","score":5},{"url":"https://www.semanticscholar.org/paper/67b4e3822405cb3d7f6bc75c3dfa49832dfac9c0","title":"Re2G: Retrieve, Rerank, Generate","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":52,"citationCount":7,"influentialCitationCount":1,"publicationDate":"13/07/2022","authors":"Michael R. Glass,Gaetano Rossiello,Md. Faisal Mahbub Chowdhury,Ankita Rajaram Naik,Pengshan Cai,A. Gliozzo","id":"67b4e3822405cb3d7f6bc75c3dfa49832dfac9c0","summary":"This work proposes Re2G, which combines both neural initial retrieval and reranking into a BART-based sequence-to-sequence generation, and introduces a novel variation of knowledge distillation to train the initial retrieval, reranker and generation using only ground truth on the target sequence output.","score":5},{"url":"https://www.semanticscholar.org/paper/0d975a8bbc1e0495cb95df8666d42111b546ab34","title":"Retrieval-Augmented Transformer for Image Captioning","venue":"International Conference on Content-Based Multimedia Indexing","year":2022,"referenceCount":66,"citationCount":4,"influentialCitationCount":0,"publicationDate":"26/07/2022","authors":"Sara Sarto,Marcella Cornia,L. Baraldi,R. Cucchiara","id":"0d975a8bbc1e0495cb95df8666d42111b546ab34","summary":"This paper investigates the development of an image captioning approach with a kNN memory, with which knowledge can be retrieved from an external corpus to aid the generation process and increase caption quality.","score":5},{"url":"https://www.semanticscholar.org/paper/565712c543e2443ff05edd87f3c1d20c07617a3a","title":"Low-Resource Dense Retrieval for Open-Domain Question Answering: A Comprehensive Survey","venue":"ArXiv","year":2022,"referenceCount":204,"citationCount":3,"influentialCitationCount":0,"publicationDate":"05/08/2022","authors":"Xiaoyu Shen,Svitlana Vakulenko,Marco Del Tredici,Gianni Barlacchi,B. Byrne,A. Gispert","id":"565712c543e2443ff05edd87f3c1d20c07617a3a","summary":"A thorough structured overview of mainstream techniques for low-resource DR, dividing the techniques into three main categories based on their required resources, and highlighting the open issues and pros and cons.","score":5},{"url":"https://www.semanticscholar.org/paper/6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"Fengji Zhang,Jin Liu,Yao Wan,Xiao Yu,Xiao Liu,J. Keung","id":"6032212d5790b6a580d68d469a9895aad6238c89","summary":"A novel approach to automatically generate multiple post titles from the given code snippets, using the maximal marginal multiple nucleus sampling strategy to generate multiple high-quality and diverse title candidates at a time for the developers to choose from.","score":5},{"url":"https://www.semanticscholar.org/paper/befde3e07ce97f02f12fe92ab27e99f23ccd17aa","title":"Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation","venue":"medRxiv","year":2022,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/08/2022","authors":"F. Yu,M. Endo,R. Krishnan,I. Pan,A. Tsai,E. P. Reis,E. Fonseca,H. M. H. Lee,Z. H. Abad,A. Y. Ng,C. Langlotz,V. Venugopal,P. Rajpurkar","id":"befde3e07ce97f02f12fe92ab27e99f23ccd17aa","summary":"This study quantitatively examines the correlation between automated metrics and the scoring of reports by radiologists, and proposes a composite metric, called RadCliQ, that is able to rank the quality of reports similarly to radiologists and better than existing metrics.","score":5},{"url":"https://www.semanticscholar.org/paper/a2b8faa634d6768bd5d043963071fa5583805914","title":"Evaluation of Question Answering Systems: Complexity of judging a natural language","venue":"ArXiv","year":2022,"referenceCount":205,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/09/2022","authors":"A. Farea,Z. Yang,Kien Duong,Nadeesha Perera,F. Emmert-Streib","id":"a2b8faa634d6768bd5d043963071fa5583805914","summary":"This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of Q a system, and hypothesize that the quantitative formalization of human judgment is an open problem.","score":5},{"url":"https://www.semanticscholar.org/paper/d8f23086687fc4b4984921026a55d6e06a415e15","title":"Chain of Explanation: New Prompting Method to Generate Higher Quality Natural Language Explanation for Implicit Hate Speech","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":2,"influentialCitationCount":0,"publicationDate":"11/09/2022","authors":"Fan Huang,Haewoon Kwak,Jisun An","id":"d8f23086687fc4b4984921026a55d6e06a415e15","summary":"The Chain of Explanation Prompting method, inspired by the chain of thoughts study, is proposed, to generate high-quality NLE for implicit hate speech and a benchmark is built based on the selected mainstream Pre-trained Language Models, including GPT-2,GPT-Neo, OPT, T5, and BART.","score":5},{"url":"https://www.semanticscholar.org/paper/6da5686a64a3daaf677e8d320a1253a3890fb3e7","title":"A Unified Encoder-Decoder Framework with Entity Memory","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":53,"citationCount":3,"influentialCitationCount":1,"publicationDate":"07/10/2022","authors":"Zhihan Zhang,W. Yu,Chenguang Zhu,Meng Jiang","id":"6da5686a64a3daaf677e8d320a1253a3890fb3e7","summary":"EDMem is a unified framework that can be used on various entity-intensive question answering and generation tasks and outperforms both memory-based auto-encoder models and non-memory encoder-decoder models.","score":5},{"url":"https://www.semanticscholar.org/paper/ab5b8f0becb66054b55347829602f730d1a99446","title":"Multimodal Event Transformer for Image-guided Story Ending Generation","venue":"ArXiv","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/01/2023","authors":"Yucheng Zhou,Guodong Long","id":"ab5b8f0becb66054b55347829602f730d1a99446","summary":"A multimodal event transformer is proposed, an event-based reasoning framework for IgSEG to generate a story ending based on given story plots and ending image and utilizes cross-modal fusion to integrate different-modality features.","score":5},{"url":"https://www.semanticscholar.org/paper/1c6015ffff034b9c304477bb31e55ca5a55f3a99","title":"Adversarial Transformer Language Models for Contextual Commonsense Inference","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"Pedro Colon-Hernandez,H. Lieberman,Yida Xin,Claire Yin,C. Breazeal,Peter Chin","id":"1c6015ffff034b9c304477bb31e55ca5a55f3a99","summary":"The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases.","score":5},{"url":"https://www.semanticscholar.org/paper/ec0ef1cd3d177e3f13598a74753e62a64dfb7527","title":"Generative Language Models for Paragraph-Level Question Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/10/2022","authors":"Asahi Ushio,Fernando Alva-Manchego,José Camacho-Collados","id":"ec0ef1cd3d177e3f13598a74753e62a64dfb7527","summary":"QG-Bench is introduced, a multilingual and multidomain benchmark for QG that unifies existing question answering datasets by converting them to a standard QG setting and proposes robust QG baselines based on fine-tuning generative language models.","score":5},{"url":"https://www.semanticscholar.org/paper/041f5dbfcd07a3369ac44a6b902ee4b145eccf2b","title":"Towards a Unified Multi-Dimensional Evaluator for Text Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":53,"citationCount":5,"influentialCitationCount":1,"publicationDate":"13/10/2022","authors":"Ming Zhong,Yang Liu,Da Yin,Yuning Mao,Yizhu Jiao,Peng Liu,Chenguang Zhu,Heng Ji,Jiawei Han","id":"041f5dbfcd07a3369ac44a6b902ee4b145eccf2b","summary":"This paper re-frame NLG evaluation as a Boolean Question Answering (QA) task, and by guiding the model with different questions, they can use one evaluator to evaluate from multiple dimensions, and introduces an intermediate learning phase that enables UniEval to incorporate external knowledge from multiple related tasks and gain further improvement.","score":5},{"url":"https://www.semanticscholar.org/paper/d891de52e7c58e2443199a20bba7468bdc0736ae","title":"Q-TOD: A Query-driven Task-oriented Dialogue System","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Xin Tian,Yingzhan Lin,Mengfei Song,Siqi Bao,Fan Wang,H. He,Shuqi Sun,Hua Wu","id":"d891de52e7c58e2443199a20bba7468bdc0736ae","summary":"This paper introduces a novel query-driven task-oriented dialogue system, namely Q-TOD, which outperforms strong baselines and establishes a new state-of-the-art performance on these datasets.","score":5},{"url":"https://www.semanticscholar.org/paper/1948447570f236338dd771a692a478b0fd091931","title":"Controllable Fake Document Infilling for Cyber Deception","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":90,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/10/2022","authors":"Yibo Hu,Yu Lin,Eric Parolin,Latif Khan,Kevin W. Hamlen","id":"1948447570f236338dd771a692a478b0fd091931","summary":"This work proposes a novel context-aware model, Fake Document Infilling (FDI), by converting the problem to a controllable mask-then-infill procedure, and shows that FDI outperforms the baselines in generating highly believable fakes with moderate modification to protect critical information and deceive adversaries.","score":5},{"url":"https://www.semanticscholar.org/paper/7de0d0800fa2046ce74fff0bf60ff63a966e166d","title":"Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":50,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Xingwei He,Yeyun Gong,Alex Jin,Weizhen Qi,Hang Zhang,Jian Jiao,Bartuer Zhou,Biao Cheng,Sm Yiu,Nan Duan","id":"7de0d0800fa2046ce74fff0bf60ff63a966e166d","summary":"The metric distillation rule is proposed to distill knowledge from the metric (e.g., BLEU) to the ranker and transfer the critical knowledge summarized by the distilled ranker to the retriever, so that the relevance scores of candidate sentences predicted by theRanker and retriever will be more consistent with their quality measured by the metric.","score":5},{"url":"https://www.semanticscholar.org/paper/764a616937a5923aaf22288b35f6b991ae41521d","title":"ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":47,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/10/2022","authors":"Junyi Li,Tianyi Tang,Wayne Xin Zhao,J. Nie,Ji-rong Wen","id":"764a616937a5923aaf22288b35f6b991ae41521d","summary":"ELMER is proposed: an efficient and effective PLM for NAR text generation to explicitly model the token dependency during NAR generation by leveraging the early exit technique and enabling the token generations at different layers, according to their prediction confidence.","score":5},{"url":"https://www.semanticscholar.org/paper/75a843259ab9a1dd5c8fd0f25c7a74d5327e1bdd","title":"Universal Evasion Attacks on Summarization Scoring","venue":"BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP","year":2022,"referenceCount":87,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Wenchuan Mu,Kwan Hui Lim","id":"75a843259ab9a1dd5c8fd0f25c7a74d5327e1bdd","summary":"The evasion attacks in this work indicate the low robustness of current scoring systems at the system level, and it is hoped that the highlighting of these proposed attack will facilitate the development of summary scores.","score":5},{"url":"https://www.semanticscholar.org/paper/b9f229a7491924404db36e91fcbd76449f3cec19","title":"Fine-Grained Emotional Paraphrasing along Emotion Gradients","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/10/2022","authors":"Justin J Xie","id":"b9f229a7491924404db36e91fcbd76449f3cec19","summary":"A new task of ﬁne-grained emotional paraphrasing along emotion gradients, that is, altering the emotional intensities of the paraphrases in grain following smooth variations in aﬀective dimensions while preserving the meanings of the orig-inals is introduced.","score":5},{"url":"https://www.semanticscholar.org/paper/587373fe118f4d6ae8a184d7ee622fb9f7c25dd8","title":"Leveraging Pre-trained Models for Failure Analysis Triplets Generation","venue":"ArXiv","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Kenneth Ezukwoke,Anis Hoayek,M. Batton-Hubert,Xavier Boucher,Pascal Gounet,Jerome Adrian","id":"587373fe118f4d6ae8a184d7ee622fb9f7c25dd8","summary":"It is observed that Generative Pre-trained Transformer 2 (GPT2) outperformed other transformer model for the failure analysis triplet generation (FATG) task and Levenshstein Sequential Evaluation metric (LESE) compares exactly with human judgement than existing metrics.","score":5},{"url":"https://www.semanticscholar.org/paper/99e16c2e93c69d240ea976acbab287a98c63d95f","title":"RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question","venue":"ArXiv","year":2022,"referenceCount":93,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/11/2022","authors":"Ali Mohammadshahi,Thomas Scialom,Majid Yazdani,Pouya Yanki,Angela Fan,J. Henderson,Marzieh Saeidi","id":"99e16c2e93c69d240ea976acbab287a98c63d95f","summary":"It is shown that RQUGE has a higher correlation with human judgment without re-lying on the reference question, and can improve the performance of QA models on out-of-domain datasets by tuning on the synthetic data generated by a question generation model and re-ranked by RQuge.","score":5},{"url":"https://www.semanticscholar.org/paper/c36ab572b5be7a470b725b87e369038295d5e7ab","title":"Time-aware Prompting for Text Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/11/2022","authors":"Shuyang Cao,Lu Wang","id":"c36ab572b5be7a470b725b87e369038295d5e7ab","summary":"It is shown that linear prompts on encoder and textual prompts improve the generation quality on all datasets, and despite having less performance drop when testing on data drawn from a later time, linear prompts focus more on non-temporal information and are less sensitive to the given timestamps.","score":5},{"url":"https://www.semanticscholar.org/paper/de4bce89f5297a00d41d10d4752fe28fbb0db9a4","title":"Generative Transformers for Design Concept Generation","venue":"Journal of Computing and Information Science in Engineering","year":2022,"referenceCount":118,"citationCount":2,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Qihao Zhu,Jianxi Luo","id":"de4bce89f5297a00d41d10d4752fe28fbb0db9a4","summary":"This study explores the recent advance of the natural language generation (NLG) technique in the artificial intelligence (AI) field to automate the early-stage design concept generation with a novel approach utilizing the generative pre-trained transformer (GPT).","score":5},{"url":"https://www.semanticscholar.org/paper/80bc1022ffd4cde85d9c24a50dcc37dbd0ea8f60","title":"Reranking Overgenerated Responses for End-to-End Task-Oriented Dialogue Systems","venue":"ArXiv","year":2022,"referenceCount":117,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Songbo Hu,Ivan Vulic,Fangyu Liu,A. Korhonen","id":"80bc1022ffd4cde85d9c24a50dcc37dbd0ea8f60","summary":"A simple yet effective reranking method which aims to select high-quality items from the lists of responses initially over-generated by the system, using any sequence-level (similarity) scoring function to divide the semantic space of responses into high-scoring versus low-scoring partitions.","score":5},{"url":"https://www.semanticscholar.org/paper/db3b99c407ff8a06bdc96151dfae1328fadfb858","title":"Grafting Pre-trained Models for Multimodal Headline Generation","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Lingfeng Qiao,Chen Wu,Ye Liu,Haoyuan Peng,Di Yin,Bo Ren","id":"db3b99c407ff8a06bdc96151dfae1328fadfb858","summary":"A novel approach to graft the video encoder from the pre- trained video-language model on the generative pre-trained language model, and presents a consensus fusion mechanism for the integration of different components, via in-ter/intra modality relation.","score":5},{"url":"https://www.semanticscholar.org/paper/ad6322a120cfe96d1b4dddbc383174f8c84967c9","title":"Contextual and selective attention networks for image captioning","venue":"Science China Information Sciences","year":2022,"referenceCount":68,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/11/2022","authors":"Jing Wang,Yehao Li,Yingwei Pan,Ting Yao,Jinhui Tang,Tao Mei","id":"ad6322a120cfe96d1b4dddbc383174f8c84967c9","summary":"A contextual and selective attention network that novelly memorizes contextual attention and brings out the principal components from each attention is presented, and extensive experiments on the COCO image captioning dataset demonstrate the superiority of CoSA-Net.","score":5},{"url":"https://www.semanticscholar.org/paper/9485adb961ba54e14f195c57f58f3f4e653bcb92","title":"VER: Learning Natural Language Representations for Verbalizing Entities and Relations","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/11/2022","authors":"Jie Huang,K. Chang","id":"9485adb961ba54e14f195c57f58f3f4e653bcb92","summary":"This paper attempts to build a system that takes any entity or entity set as input and generates a sentence to represent entities and relations, named “natural language representation”, and demonstrates that the model can generate high-quality sentences describing entities and entity relationships and facilitate various tasks on entities and Relations, including definition modeling, relation modeling, and generative commonsense reasoning.","score":5},{"url":"https://www.semanticscholar.org/paper/cb7d65ac2324afc92c48acb8efbd3d2f201f5840","title":"In-sample Curriculum Learning by Sequence Completion for Natural Language Generation","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Qi Jia,Yizhu Liu,Haifeng Tang,Kenny Q. Zhu","id":"cb7d65ac2324afc92c48acb8efbd3d2f201f5840","summary":"This work proposes to do in-sample curriculum learning for natural language generation tasks, Inspired by the “easy-to-hard” intuition, and starts training the model to generate the last few words, and gradually extends to generated the whole output sequence.","score":5},{"url":"https://www.semanticscholar.org/paper/15d6ef576fb07bb5fc07fef6f63708e440396dd9","title":"Aesthetically Relevant Image Captioning","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/11/2022","authors":"Zhipeng Zhong,Fei Zhou,G. Qiu","id":"15d6ef576fb07bb5fc07fef6f63708e440396dd9","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/1f84b6075ffa2d4ed685f31668ea1324bfba4fe5","title":"A Survey on Medical Document Summarization","venue":"ArXiv","year":2022,"referenceCount":132,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/12/2022","authors":"Raghav Jain,Anubhav Jangra,S. Saha,A. Jatowt","id":"1f84b6075ffa2d4ed685f31668ea1324bfba4fe5","summary":"This paper gives a comprehensive survey of the current techniques and trends in medical summarization.","score":5},{"url":"https://www.semanticscholar.org/paper/bd0c720e885dfd912f93d6b812497d2becbe4d82","title":"Collaborating Heterogeneous Natural Language Processing Tasks via Federated Learning","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Chenhe Dong,Yuexiang Xie,Bolin Ding,Ying Shen,Yaliang Li","id":"bd0c720e885dfd912f93d6b812497d2becbe4d82","summary":"This study further broaden the application scope of FL in NLP by proposing an A SSIGN -T HEN -C ONTRAST (denoted as ATC) framework, which enables clients with heterogeneous NLP tasks to construct an FL course and learn useful knowledge from each other.","score":5},{"url":"https://www.semanticscholar.org/paper/19e97c83e5637a751f6945344dc19ba658b67468","title":"Are Multimodal Models Robust to Image and Text Perturbations?","venue":"ArXiv","year":2022,"referenceCount":117,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"Jielin Qiu,Yi Zhu,Xingjian Shi,F. Wenzel,Zhiqiang Tang,D. Zhao,Bo Li,Mu Li","id":"19e97c83e5637a751f6945344dc19ba658b67468","summary":"This paper investigates the robustness of 9 popular open-sourced image-text models under common perturbations on five tasks (image-text retrieval, visual reasoning, visual entailment, image captioning, and text-to-image generation), and proposes several new multimodal robustness benchmarks.","score":5},{"url":"https://www.semanticscholar.org/paper/e8059434aa997cf486e6ae83cfbf355d4829a95c","title":"PoE: a Panel of Experts for Generalized Automatic Dialogue Assessment","venue":"ArXiv","year":2022,"referenceCount":81,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Chen Zhang,L. F. D’Haro,Qiquan Zhang,Thomas Friedrichs,Haizhou Li","id":"e8059434aa997cf486e6ae83cfbf355d4829a95c","summary":"A Panel of Experts (PoE) network is proposed, a multitask network that consists of a shared transformer encoder and a collection of lightweight adapters that exhibits better zero-shot generalization than existing state-of-the-art ADEMs and the ability to easily adapt to new domains with few-shot transfer learning.","score":5},{"url":"https://www.semanticscholar.org/paper/9b93ebd7ff4d995ad92902096f6c55d9451c2239","title":"An AI Dungeon Master's Guide: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Pei Zhou,Andrew Zhu,Jennifer Hu,J. Pujara,Xiang Ren,Chris Callison-Burch,Yejin Choi,Prithviraj Ammanabrolu","id":"9b93ebd7ff4d995ad92902096f6c55d9451c2239","summary":"Human and automated evaluations show that a DM trained with RL to generate guidance by incorporating a theory-of-mind of the players signif-icantly improves the players’ ability to achieve goals grounded in their shared world.","score":5},{"url":"https://www.semanticscholar.org/paper/447ddcb2b16c2cacabeec2933273918bd6fe1d79","title":"MAUVE Scores for Generative Models: Theory and Practice","venue":"ArXiv","year":2022,"referenceCount":115,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/12/2022","authors":"Krishna Pillutla,Lang Liu,John Thickstun,S. Welleck,Swabha Swayamdipta,Rowan Zellers,Sewoong Oh,Yejin Choi,Z. Harchaoui","id":"447ddcb2b16c2cacabeec2933273918bd6fe1d79","summary":"MAUVE, a family of comparison measures between pairs of distributions such as those encountered in the generative modeling of text or images, is presented, finding that the proposed scores paired with a range of f -divergences and statistical estimation methods can quantify the gaps between the distributions of human-written text and those of modern neural language models.","score":5},{"url":"https://www.semanticscholar.org/paper/1c83f3f9789df43bf937ae2618721e2da83dcc06","title":"From Show to Tell: A Survey on Deep Learning-Based Image Captioning","venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence","year":2021,"referenceCount":254,"citationCount":28,"influentialCitationCount":2,"publicationDate":"14/07/2021","authors":"Matteo Stefanini,Marcella Cornia,L. Baraldi,S. Cascianelli,G. Fiameni,R. Cucchiara","id":"1c83f3f9789df43bf937ae2618721e2da83dcc06","summary":"This work aims at providing a comprehensive overview of image captioning approaches, from visual encoding and text generation to training strategies, datasets, and evaluation metrics, and quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies.","score":5},{"url":"https://www.semanticscholar.org/paper/93d829fd65d77d2ba5fa141fac85564f451707b2","title":"A dataset for plain language adaptation of biomedical abstracts","venue":"Scientific Data","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Kush Attal,Brian D. Ondov,Dina Demner-Fushman","id":"93d829fd65d77d2ba5fa141fac85564f451707b2","summary":"The Plain Language Adaptation of Biomedical Abstracts dataset is the first manually adapted dataset that is both document- and sentence-aligned, and contains 750 adapted abstracts, totaling 7643 sentence pairs.","score":5},{"url":"https://www.semanticscholar.org/paper/13579f2d5d522779caeb4aa916fb9ab283f13670","title":"SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph","venue":"ArXiv","year":2023,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/01/2023","authors":"Yuxing Long,Binyuan Hui,Fulong Ye,Yanyang Li,Zhuoxin Han,C. Yuan,Yongbin Li,Xiaojie Wang","id":"13579f2d5d522779caeb4aa916fb9ab283f13670","summary":"A Situated Conversation Agent Petrained with Multimodal Questions from INcremental Layout Graph (SPRING) with abilities of reasoning multi-hops spatial relations and connecting them with visual attributes in crowded situated scenarios and significantly outperforms state-of-the-art approaches on both SIMMC 1.0 and SIMMC 2.0 datasets.","score":5},{"url":"https://www.semanticscholar.org/paper/2d4cc999cf1a4202849ae09d0972660800c14eb4","title":"Embodied Agents for Efficient Exploration and Smart Scene Description","venue":"ArXiv","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/01/2023","authors":"Roberto Bigazzi,Marcella Cornia,S. Cascianelli,L. Baraldi,R. Cucchiara","id":"2d4cc999cf1a4202849ae09d0972660800c14eb4","summary":"This work proposes and evaluates an approach that combines recent advances in visual robotic exploration and image captioning on images generated through agent-environment interaction that can effectively describe the robot’s point of view during exploration, improving the human- friendly interpretability of its observations.","score":5},{"url":"https://www.semanticscholar.org/paper/cebbdc53a5fddbce44dd23d4ed1a45953a4c94a9","title":"Visual Semantic Relatedness Dataset for Image Captioning","venue":"ArXiv","year":2023,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/01/2023","authors":"Ahmed Sabir,F. Moreno-Noguer,Llu'is Padr'o","id":"cebbdc53a5fddbce44dd23d4ed1a45953a4c94a9","summary":"This paper proposes a textual visual context dataset for captioning, in which the publicly available dataset COCO Captions has been extended with information about the scene (such as objects in the image), which can be used to leverage any NLP task into captioning systems.","score":5},{"url":"https://www.semanticscholar.org/paper/637384f1f6074ee6e8d2e2d058b5e0cf20edb5ed","title":"Fashion-Oriented Image Captioning with External Knowledge Retrieval and Fully Attentive Gates","venue":"Italian National Conference on Sensors","year":2023,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2023","authors":"Nicholas Moratelli,Manuele Barraco,Davide Morelli,Marcella Cornia,L. Baraldi,R. Cucchiara","id":"637384f1f6074ee6e8d2e2d058b5e0cf20edb5ed","summary":"Experimental analyses were carried out on the fashion captioning dataset (FACAD) for fashion image captioning, validating the effectiveness of the proposed approach and the proposed architectural strategies in comparison with carefully designed baselines and state-of-the-art approaches.","score":5},{"url":"https://www.semanticscholar.org/paper/ead6e2873c776448d38c7596220a5f74b718b60c","title":"Counterfactual Editing for Search Result Explanation","venue":"ArXiv","year":2023,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/01/2023","authors":"Zhichao Xu,Hemank Lamba,Qingyao Ai,Joel Tetreault,A. Jaimes","id":"ead6e2873c776448d38c7596220a5f74b718b60c","summary":"A user study is conducted to investigate if counterfactual explanations indeed improve search sessions’ effectiveness and a method is proposed to provide pairwise explanations to search engine result page that outperforms baselines on both metrics and human evaluation.","score":5},{"url":"https://www.semanticscholar.org/paper/ce032d151caeffeafeab355e69454d89c3c0b6a9","title":"Style-Aware Contrastive Learning for Multi-Style Image Captioning","venue":"ArXiv","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/01/2023","authors":"Yucheng Zhou,Guodong Long","id":"ce032d151caeffeafeab355e69454d89c3c0b6a9","summary":"This work presents a style-aware visual encoder with contrastive learning to mine potential visual content relevant to style, and proposes astyle-aware triplet contrast objective to distinguish whether the image, style and caption matched.","score":5},{"url":"https://www.semanticscholar.org/paper/e4efdfe15d27c8a3240cafa4b6654e748b409ef1","title":"On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex","venue":"ArXiv","year":2023,"referenceCount":82,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Terry Yue Zhuo,Zhuang Li,Yujin Huang,Yuan-Fang Li,Weiqing Wang,Gholamreza Haffari,Fatemeh Shiri","id":"e4efdfe15d27c8a3240cafa4b6654e748b409ef1","summary":"This paper presents the first empirical study on the adversarial robustness of a large prompt-based language model of code, \\codex, and proposes methods for improving robustness without the need for significant amounts of labeled data or heavy computational resources.","score":5},{"url":"https://www.semanticscholar.org/paper/a764d4f28612a7b5c4767ea6a2540b169fdb052c","title":"CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models","venue":"ArXiv","year":2023,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Hao Yu,Bo Shen,Dezhi Ran,Jiaxin Zhang,Qi Zhang,Yu Ma,Guangtai Liang,Ying Li,Tao Xie,Qianxiang Wang","id":"a764d4f28612a7b5c4767ea6a2540b169fdb052c","summary":"A benchmark named CoderEval of pragmatic code generation with generative pre-trained models is proposed, compared with the widely-used HumanEval benchmark from OpenAI, which can be used to assess the performance of models against pragmatic codegeneration beyond just generating standalone functions.","score":5},{"url":"https://www.semanticscholar.org/paper/0445dec0464fcbd85c6cefa5da87e82d49460fb3","title":"Towards Local Visual Modeling for Image Captioning","venue":"Pattern Recognition","year":2023,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Yiwei Ma,Jiayi Ji,Xiaoshuai Sun,Yiyi Zhou,R. Ji","id":"0445dec0464fcbd85c6cefa5da87e82d49460fb3","summary":"Experimental results show that the proposed Locality-Sensitive Transformer Network (LSTNet) is not only capable of local visual modeling, but also outperforms a bunch of state-of-the-art captioning models on offline and online testings, i.e., 134.8 CIDEr and 136.3 C IDEr, respectively.","score":5},{"url":"https://www.semanticscholar.org/paper/c4b957f7e5f8cdeb60575447396d3fed0091bfee","title":"Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models in Engineering Design","venue":"ArXiv","year":2023,"referenceCount":107,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/02/2023","authors":"Lyle Regenwetter,Akash Srivastava,Dan Gutfreund,Faez Ahmed","id":"c4b957f7e5f8cdeb60575447396d3fed0091bfee","summary":"A set of design-specific metrics which have been proposed across different research communities and can be used for evaluating deep generative models focus on unique requirements in design and engineering, such as constraint satisfaction, functional performance, novelty, and conditioning.","score":5},{"url":"https://www.semanticscholar.org/paper/dfe52273e90dcc2a4907079b34c4a2cbfc8593f8","title":"Protecting Language Generation Models via Invisible Watermarking","venue":"ArXiv","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/02/2023","authors":"Xuandong Zhao,Yu-xiang Wang,Lei Li","id":"dfe52273e90dcc2a4907079b34c4a2cbfc8593f8","summary":"GINSEW, a novel method to protect text generation models from being stolen through distillation by injecting secret signals into the probability vector of the decoding steps for each target token, is proposed.","score":5},{"url":"https://www.semanticscholar.org/paper/39de27b3ba14f358ff4c9adc0a79fd1206fc7292","title":"CCRep: Learning Code Change Representations via Pre-Trained Code Model and Query Back","venue":"ArXiv","year":2023,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Zhongxin Liu,Zhijie Tang,Xin Xia,Xiaohu Yang","id":"39de27b3ba14f358ff4c9adc0a79fd1206fc7292","summary":"This work proposes a novel Code Change Representation learning approach named CCRep, which can learn to encode code changes as feature vectors for diverse downstream tasks and applies it to three tasks: commit message generation, patch correctness assessment, and just-in-time defect prediction.","score":5},{"url":"https://www.semanticscholar.org/paper/c39fc0b30d956088d6e4d0b82b38dc3a754cd862","title":"NapSS: Paragraph-level Medical Text Simplification via Narrative Prompting and Sentence-matching Summarization","venue":"ArXiv","year":2023,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/02/2023","authors":"Junru Lu,Jiazheng Li,Byron C. Wallace,Yulan He,Gabriele Pergola","id":"c39fc0b30d956088d6e4d0b82b38dc3a754cd862","summary":"This work proposes a summarize-then-simplify two-stage strategy, which is called NapSS, identifying the relevant content to simplify while ensuring that the original narrative flow is preserved, and introduces new metrics that take into account both lexical and high-level semantic similarity.","score":5},{"url":"https://www.semanticscholar.org/paper/5c982f5dbb653d748e1ada858004e0847e09edff","title":"Position Matters! Empirical Study of Order Effect in Knowledge-grounded Dialogue","venue":"ArXiv","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/02/2023","authors":"Hsuan Su,Shachi H. Kumar,Sahisnu Mazumder,Wenda Chen,R. Manuvinakurike,Eda Okur,Saurav Sahay,L. Nachman,Shang-Tse Chen,Hung-yi Lee","id":"5c982f5dbb653d748e1ada858004e0847e09edff","summary":"This paper proposes a simple and novel technique to alleviate the order effect by modifying the position embeddings of knowledge input in transformer-based models, and shows that each knowledge statement is uniformly considered to generate responses.","score":5},{"url":"https://www.semanticscholar.org/paper/7a771d84301d12049cf72a017f9e62c3d1b6222d","title":"Developer-Intent Driven Code Comment Generation","venue":"","year":2023,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/02/2023","authors":"Fangwen Mu,Xiao Chen,Lin Shi,Song Wang,Qing Wang","id":"7a771d84301d12049cf72a017f9e62c3d1b6222d","summary":"This work proposes DOME, an approach that utilizes Intent-guided Selective Attention to explicitly select intent-relevant information from the source code, and produces various comments reflecting different intents, which outperforms the state-of-the-art baselines.","score":5},{"url":"https://www.semanticscholar.org/paper/927a5203363fc9c8ba48599dc749cf0cc647444b","title":"Compute Trends Across Three Eras of Machine Learning","venue":"IEEE International Joint Conference on Neural Network","year":2022,"referenceCount":176,"citationCount":30,"influentialCitationCount":3,"publicationDate":"11/02/2022","authors":"J. Sevilla,Lennart Heim,A. Ho,T. Besiroglu,Marius Hobbhahn,Pablo Villalobos","id":"927a5203363fc9c8ba48599dc749cf0cc647444b","summary":"This paper curates a dataset with the training compute of 123 milestone ML systems, 3× larger than previous such datasets, and frames the trends in compute in in three eras - the Pre Deep Learning Era, the Deep learning Era, and the Large-Scale Era.","score":5},{"url":"https://www.semanticscholar.org/paper/9f96d8b71551e735171283976ce2160e0b88e824","title":"Can deep learning match the efficiency of human visual long-term memory in storing object details?","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/04/2022","authors":"A. Orhan","id":"9f96d8b71551e735171283976ce2160e0b88e824","summary":"This paper asks whether deep learning via gradient descent can match the eﬃciency of human visual long-term memory to incorporate new information in a rigorous, head-to-head, quantitative comparison and answers in the negative.","score":5},{"url":"https://www.semanticscholar.org/paper/999217305f51c69677ce4879495c60f499c08e99","title":"Cluster-based Evaluation of Automatically Generated Text","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Tiago Pimentel,Clara Meister,Ryan Cotterell","id":"999217305f51c69677ce4879495c60f499c08e99","summary":"This work analyzes the general paradigm of language generator evaluation and proposes the use of distributions over clusters, where strings are cluster based on their text embeddings (obtained from a pretrained language model), finding these clusters may simply be better equipped to evaluate state-of-the-art language models.","score":5},{"url":"https://www.semanticscholar.org/paper/8cf974fd3973900c0598730ee5d3617900ac8c3d","title":"Transformers with Learnable Activation Functions","venue":"ArXiv","year":2022,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/08/2022","authors":"Haishuo Fang,Ji-Ung Lee,N. Moosavi,Iryna Gurevych","id":"8cf974fd3973900c0598730ee5d3617900ac8c3d","summary":"Analysis of the shapes of learned RAFs unveils that they substantially vary between different layers of the pre-trained model and mostly look very different from conventional activation functions, which opens a new research direction for analyzing and interpreting pre- trained models according to the learned activation functions.","score":5},{"url":"https://www.semanticscholar.org/paper/f1de30035ab2778aae09426ee7d068337fa661af","title":"Large-Scale Bidirectional Training for Zero-Shot Image Captioning","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/11/2022","authors":"Taehoon Kim,Mark A Marsden,Pyunghwan Ahn,Sangyun Kim,Sihaeng Lee,Alessandra Sala,Seung Hwan Kim","id":"f1de30035ab2778aae09426ee7d068337fa661af","summary":"It is found that large-scale bidirectional training between image and text enables zero-shot image captioning and a new evaluation benchmark is proposed which comprises of high quality datasets and an extensive set of metrics to properly evaluate zero- shot captioning accuracy and societal bias.","score":5},{"url":"https://www.semanticscholar.org/paper/7932b714e2ae1def5828df52b97f1decb9bebd32","title":"Considerations for Differentially Private Learning with Large-Scale Public Pretraining","venue":"ArXiv","year":2022,"referenceCount":70,"citationCount":3,"influentialCitationCount":1,"publicationDate":"13/12/2022","authors":"Florian Tramèr,Gautam Kamath,Nicholas Carlini","id":"7932b714e2ae1def5828df52b97f1decb9bebd32","summary":"Whether the use of large Web-scraped datasets should be viewed as diﬀerential-privacy-preserving and whether existing machine learning benchmarks are appropriate for measuring the ability of pretrained models to generalize to sensitive domains, which may be poorly represented in public Web data.","score":5},{"url":"https://www.semanticscholar.org/paper/f10d25c3a19d5bf3544dc56076d90a02ed99eae9","title":"ClimaX: A foundation model for weather and climate","venue":"ArXiv","year":2023,"referenceCount":113,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"Tung Nguyen,Johannes Brandstetter,Ashish Kapoor,Jayesh K. Gupta,Aditya Grover","id":"f10d25c3a19d5bf3544dc56076d90a02ed99eae9","summary":"ClimaX is developed and demonstrated, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning different variables, spatio-temporal coverage, and physical groundings and results in superior performance on benchmarks for weather forecasting and climate projections, even when pretrained at lower resolutions and compute budgets.","score":5},{"url":"https://www.semanticscholar.org/paper/a9e4d604d7a2bb4e0a3ddd5222d8f8bc7edda532","title":"Analyzing Leakage of Personally Identifiable Information in Language Models","venue":"ArXiv","year":2023,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Nils Lukas,A. Salem,Robert Sim,Shruti Tople,L. Wutschitz,Santiago Zanella-B'eguelin","id":"a9e4d604d7a2bb4e0a3ddd5222d8f8bc7edda532","summary":"A taxonomy of PII leakage in LMs is proposed, novel attacks that can extract up to 10 times more PII sequences as existing attacks are proposed, and a subtle connection between record-level membership inference and PII reconstruction is made.","score":5},{"url":"https://www.semanticscholar.org/paper/101958920c32d67b6509cd849e6644b28d7e473f","title":"Binarized Neural Machine Translation","venue":"ArXiv","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/02/2023","authors":"Yichi Zhang,Ankush Garg,Yuan Cao,Lukasz Lew,B. Ghorbani,Zhiru Zhang,Orhan Firat","id":"101958920c32d67b6509cd849e6644b28d7e473f","summary":"This work proposes a novel binarization technique for Transformers applied to machine translation (BMT), the first of its kind, and identifies and addresses the problem of inflated dot-product variance when using one-bit weights and activations.","score":5},{"url":"https://www.semanticscholar.org/paper/5ecc8ca550baa8e4a08a942af06fbc2af2c74543","title":"RL with KL penalties is better viewed as Bayesian inference","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":46,"citationCount":6,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Tomasz Korbak,Ethan Perez,C. Buckley","id":"5ecc8ca550baa8e4a08a942af06fbc2af2c74543","summary":"This paper analyzes challenges associated with treating a language model as an RL policy and shows how avoiding those challenges requires moving beyond the RL paradigm, and shows that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by the reward function.","score":5},{"url":"https://www.semanticscholar.org/paper/e32185936ab3b23f39b1dd93e1507e6d80a71776","title":"The Debate Over Understanding in AI's Large Language Models","venue":"ArXiv","year":2022,"referenceCount":86,"citationCount":3,"influentialCitationCount":1,"publicationDate":"14/10/2022","authors":"M. Mitchell,D. Krakauer","id":"e32185936ab3b23f39b1dd93e1507e6d80a71776","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/cbf98ebe967e0f3f3236e7932f37013b98244e94","title":"ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning","venue":"International Conference on Learning Representations","year":2021,"referenceCount":158,"citationCount":73,"influentialCitationCount":10,"publicationDate":"22/11/2021","authors":"V. Aribandi,Yi Tay,Tal Schuster,J. Rao,Huaixiu Zheng,Sanket Vaibhav Mehta,Honglei Zhuang,V. Tran,Dara Bahri,Jianmo Ni,Jai Gupta,Kai Hui,Sebastian Ruder,Donald Metzler","id":"cbf98ebe967e0f3f3236e7932f37013b98244e94","summary":"ExMIX (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families is introduced, and a model pre-trained using a multi-task objective of self-supervised span denoising and supervised EXMIX is proposed.","score":5},{"url":"https://www.semanticscholar.org/paper/82b3a2559b5cf1528348b9d35285868e1c154fce","title":"Overleaf Example","venue":"","year":2022,"referenceCount":109,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Pierre Colombo","id":"82b3a2559b5cf1528348b9d35285868e1c154fce","summary":"This paper proposes a new procedure to rank systems based on their performance across different tasks, motivated by the social choice theory, and shows that this method yields different conclusions on state-of-the-art systems than the mean-aggregation procedure while being both more reliable and robust.","score":5},{"url":"https://www.semanticscholar.org/paper/7db7fb25a8753c9efc6b3722d178f94fcc1f82d3","title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Few-Shot NLP","venue":"ArXiv","year":2022,"referenceCount":118,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yufei Wang,Jiayi Zheng,Can Xu,Xiubo Geng,Tao Shen,Chongyang Tao,Daxin Jiang","id":"7db7fb25a8753c9efc6b3722d178f94fcc1f82d3","summary":"It is demonstrated that the proposed Knowledge Mixture enables pre-trained language models the capability of generating proper synthetic instances from scratch for complicated tasks (i.e., the data sample has long sequences or multiple sentences).","score":5},{"url":"https://www.semanticscholar.org/paper/5ef91beb3037a35f296d8df8c556ac5353d1160a","title":"Describing Differences between Text Distributions with Natural Language","venue":"International Conference on Machine Learning","year":2022,"referenceCount":76,"citationCount":4,"influentialCitationCount":2,"publicationDate":"28/01/2022","authors":"Ruiqi Zhong,Charles Burton Snell,D. Klein,J. Steinhardt","id":"5ef91beb3037a35f296d8df8c556ac5353d1160a","summary":"This work fine-tune GPT-3 to automatically describe the differences between two distributions of text by “learning a natural language hypothesis”, and applies the system to describe distribution shifts, debug dataset shortcuts, summarize unknown tasks, and label text clusters, and present analyses based on automatically generated descriptions.","score":5},{"url":"https://www.semanticscholar.org/paper/c0a1811021d0f4e6864446c62658bd4375893b66","title":"Summarizing Differences between Text Distributions with Natural Language","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Ruiqi Zhong,Charles Burton Snell,D. Klein,J. Steinhardt","id":"c0a1811021d0f4e6864446c62658bd4375893b66","summary":"This work fine-tune GPT-3 to propose descriptions with the prompt “how do two distributions of text differ”, and applies this system to describe distribution shifts, debug dataset shortcuts, summarize unknown tasks, and label text clusters, and present analyses based on automatically generated descriptions.","score":5},{"url":"https://www.semanticscholar.org/paper/3e9b550e4a432886a627dde89e402a236975b844","title":"What are the best systems? New perspectives on NLP Benchmarking","venue":"ArXiv","year":2022,"referenceCount":109,"citationCount":8,"influentialCitationCount":2,"publicationDate":"08/02/2022","authors":"Pierre Colombo,Nathan Noiry,Ekhine Irurozki,S. Clémençon","id":"3e9b550e4a432886a627dde89e402a236975b844","summary":"This paper proposes a new procedure to rank systems based on their performance across different tasks, motivated by the social choice theory, and shows that this method yields different conclusions on stateof-the-art systems than the mean-aggregation procedure while being both more reliable and robust.","score":5},{"url":"https://www.semanticscholar.org/paper/bf81b2a50009fc7370d25f2ae6f8acc09c7da5d9","title":"GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":21,"influentialCitationCount":3,"publicationDate":"14/03/2022","authors":"Archiki Prasad,Peter Hase,Xiang Zhou,Mohit Bansal","id":"bf81b2a50009fc7370d25f2ae6f8acc09c7da5d9","summary":"Gradientfree Instructional Prompt Search (GRIPS), a gradient-free, edit-based search approach for improving task instructions for large language models, which outperforms manual rewriting following the guidelines in Mishra et al. (2022b) and also outperforms purely examplebased prompts while controlling for the available compute and data budget.","score":5},{"url":"https://www.semanticscholar.org/paper/9060d9408fda662f717de99b004045ec1168e581","title":"Hyperdecoders: Instance-specific decoders for multi-task NLP","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":75,"citationCount":5,"influentialCitationCount":0,"publicationDate":"15/03/2022","authors":"Hamish Ivison,Matthew E. Peters","id":"9060d9408fda662f717de99b004045ec1168e581","summary":"This work investigates input-conditioned hypernetworks for multi-tasking in NLP, generating parameter-efficient adaptations for a decoder using a hypernetwork conditioned on the output of an encoder, finding that it surpasses previous parameter efficient fine-tuning methods and often outperforms fully finetuning the underlying model.","score":5},{"url":"https://www.semanticscholar.org/paper/590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":28,"citationCount":6,"influentialCitationCount":0,"publicationDate":"16/03/2022","authors":"Kirby Kuznia,Swaroop Mishra,Mihir Parmar,Chitta Baral","id":"590f6817b42407f96b079e82c935fae298196359","summary":"This work creates a meta-dataset from the frequently used APPS dataset and the newly created CodeContests dataset for the program synthesis task and shows that summaries significantly improve performance for introductory and interview related programming questions and shows improvement for competitive programming questions.","score":5},{"url":"https://www.semanticscholar.org/paper/237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","title":"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/03/2022","authors":"Gabriel Orlanski","id":"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","summary":"Collect and standardize prompts from a diverse range of tasks for use with tasks they were not designed for and evaluate these prompts across multiple choice datasets for a quantitative analysis of how certain attributes of a prompt affect performance.","score":5},{"url":"https://www.semanticscholar.org/paper/ec64e324ce1210fe5245dfd0fb5a92058732e5b9","title":"Benchmarking Generalization via In-Context Instructions on 1, 600+ Language Tasks","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":36,"influentialCitationCount":10,"publicationDate":2022,"authors":"Yizhong Wang,Swaroop Mishra,Pegah Alipoormolabashi,Yeganeh Kordi,Amirreza Mirzaei,Anjana Arunkumar,Arjun Ashok,Arut Selvan Dhanasekaran,Atharva Naik,David Stap,Eshaan Pathak,Giannis Karamanolakis,Haizhi Gary Lai,I. Purohit,Ishani Mondal,Jacob Anderson,Kirby Kuznia,Krima Doshi,Maitreya Patel,Kuntal Kumar Pal,M. Moradshahi,Mihir Parmar,Mirali Purohit,Neeraj Varshney,Phani Rohitha Kaza,Pulkit Verma,Ravsehaj Singh Puri,Rushang Karia,Shailaja Keyur Sampat,Savan Doshi,S. Mishra,Sujan Reddy,Sumanta Patro,Tanay Dixit,Xudong Shen,Chitta Baral,Yejin Choi,Hannaneh Hajishirzi,Noah A. Smith,Daniel Khashabi","id":"ec64e324ce1210fe5245dfd0fb5a92058732e5b9","summary":"This work introduces N ATURAL -I NSTRUCTIONS v 2, a collection of 1,600+ diverse language tasks and their expert written instructions that covers 70+ distinct task types, such as tagging, in-ﬁlling, and rewriting.","score":5},{"url":"https://www.semanticscholar.org/paper/eac7022fe02f867140514018806a3cae1da6864f","title":"Unsupervised Cross-Task Generalization via Retrieval Augmentation","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":13,"influentialCitationCount":5,"publicationDate":"17/04/2022","authors":"Bill Yuchen Lin,Kangmin Tan,Chris Miller,Beiwen Tian,Xiang Ren","id":"eac7022fe02f867140514018806a3cae1da6864f","summary":"This paper proposes a retrieval-augmentation method named ReCross that takes a few unlabelled examples as queries to retrieve a small subset of upstream data and uses them to update the multi-task model for better generalization.","score":5},{"url":"https://www.semanticscholar.org/paper/81986b8a3d3fe6c5be06fc4527953fb514ad12e8","title":"Improving In-Context Few-Shot Learning via Self-Supervised Training","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":89,"citationCount":5,"influentialCitationCount":1,"publicationDate":"03/05/2022","authors":"Mingda Chen,Jingfei Du,Ramakanth Pasunuru,Todor Mihaylov,Srini Iyer,V. Stoyanov,Zornitsa Kozareva","id":"81986b8a3d3fe6c5be06fc4527953fb514ad12e8","summary":"This paper proposes to use self-supervision in an intermediate training stage between pretraining and downstream few-shot usage with the goal to teach the model to perform in-context few shot learning.","score":5},{"url":"https://www.semanticscholar.org/paper/7cdaa08890895e1ad92afb5fad429690ad7b1dac","title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning","venue":"ArXiv","year":2022,"referenceCount":81,"citationCount":52,"influentialCitationCount":13,"publicationDate":"11/05/2022","authors":"Haokun Liu,Derek Tam,Mohammed Muqeeth,Jay Mohta,Tenghao Huang,Mohit Bansal,Colin Raffel","id":"7cdaa08890895e1ad92afb5fad429690ad7b1dac","summary":"A new parameter-efﬁcient ﬁne-tuning method called (IA) 3 that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters.","score":5},{"url":"https://www.semanticscholar.org/paper/d304d0bdfa81fd10b187aa0e4f41d410eb19d6e3","title":"Fine-tuned Language Models are Continual Learners","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":46,"citationCount":2,"influentialCitationCount":0,"publicationDate":"24/05/2022","authors":"Thomas Scialom,Tuhin Chakrabarty,S. Muresan","id":"d304d0bdfa81fd10b187aa0e4f41d410eb19d6e3","summary":"The resulting model Continual-T0 (CT0) is able to learn 8 new diverse language generation tasks, while still maintaining good performance on previous tasks, spanning in total of 70 datasets, demonstrating some level of instruction compositionality.","score":5},{"url":"https://www.semanticscholar.org/paper/07759a84f27e43cfa5bc8d579f8227c96e6ae1dc","title":"RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":138,"citationCount":23,"influentialCitationCount":3,"publicationDate":"25/05/2022","authors":"Mingkai Deng,Jianyu Wang,Cheng-Ping Hsieh,Yihan Wang,Han Guo,Tianmin Shu,Meng Song,E. Xing,Zhiting Hu","id":"07759a84f27e43cfa5bc8d579f8227c96e6ae1dc","summary":"RLPrompt is proposed, an efficient discrete prompt optimization approach with reinforcement learning (RL) that formulates a parameter-efficient policy network that generates the optimized discrete prompt after training with reward.","score":5},{"url":"https://www.semanticscholar.org/paper/686d9ee744fa013cc21cdd86acd864c936e9e456","title":"Large language models are few-shot clinical information extractors","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":98,"citationCount":10,"influentialCitationCount":2,"publicationDate":"25/05/2022","authors":"Monica Agrawal,S. Hegselmann,Hunter Lang,Yoon Kim,D. Sontag","id":"686d9ee744fa013cc21cdd86acd864c936e9e456","summary":"This work shows that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain, and demonstrates how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs.","score":5},{"url":"https://www.semanticscholar.org/paper/6650e44f00f74c596b3981d01493b5f75cfa6f63","title":"LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Muhammad Khalifa,L. Logeswaran,Moontae Lee,Honglak Lee,Lu Wang","id":"6650e44f00f74c596b3981d01493b5f75cfa6f63","summary":"Though unsupervised, LEPUS yields competitive reranking performance against state-of-the-art methods that are trained on thousands of examples, and when integrated with a reader module, it can obtain competitive multi-hop QA performance, e.g., out-performing fully-supervised QA systems.","score":5},{"url":"https://www.semanticscholar.org/paper/e852c5ed7f4f1a4375f0f3bce1b4e445c7684e7e","title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP","venue":"","year":2022,"referenceCount":117,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/06/2022","authors":"Yufei Wang,Jiayi Zheng,Can Xu,Xiubo Geng,Tao Shen,Chongyang Tao,Daxin Jiang","id":"e852c5ed7f4f1a4375f0f3bce1b4e445c7684e7e","summary":"The goal of KoMT is to condense diverse NLP task-speciﬁc knowledge into the single KnowDA model (i.e., all-in-one) such that KnowDA could utilize these knowledge to quickly grasp the inherent synthesis law of the target task through limited training instances.","score":5},{"url":"https://www.semanticscholar.org/paper/6cf8a4d05e66266233380f989edaf647eba7e1a5","title":"BioTABQA: Instruction Learning for Biomedical Table Question Answering","venue":"Conference and Labs of the Evaluation Forum","year":2022,"referenceCount":40,"citationCount":7,"influentialCitationCount":0,"publicationDate":"06/07/2022","authors":"Man Luo,S. Saxena,Swaroop Mishra,Mihir Parmar,Chitta Baral","id":"6cf8a4d05e66266233380f989edaf647eba7e1a5","summary":"Experimental results show that the instruction-tuned model outperforms single and multi task baselines on an average by ∼ 23% and ∼ 6% across various evaluation settings, and more importantly, instruction- Tuned models outperforms baselines by ∼ 5% on cross-tasks.","score":5},{"url":"https://www.semanticscholar.org/paper/27fd24b7b9a7fd5777e9bda6e3b0d33c08472001","title":"Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization","venue":"ArXiv","year":2022,"referenceCount":52,"citationCount":6,"influentialCitationCount":1,"publicationDate":"21/08/2022","authors":"Pengcheng He,Baolin Peng,Liyang Lu,Songhe Wang,Jie Mei,Yang Liu,Ruochen Xu,Hany Hassan Awadalla,Yu Shi,Chenguang Zhu,Wayne Xiong,Michael Zeng,Jianfeng Gao,Xuedong Huang","id":"27fd24b7b9a7fd5777e9bda6e3b0d33c08472001","summary":"Z-Code++ is a new pre-trained language model optimized for abstractive text summarization that outperforms the 600x larger PaLM 540B on XSum, and the 200x larger GPT3 175B on SAMSum in zero-shot and few-shot settings.","score":5},{"url":"https://www.semanticscholar.org/paper/06272f6ab3c450e196b1f9879edb33b707d79da7","title":"Efficient Methods for Natural Language Processing: A Survey","venue":"ArXiv","year":2022,"referenceCount":209,"citationCount":6,"influentialCitationCount":0,"publicationDate":"31/08/2022","authors":"Marcos Vinícius Treviso,Tianchu Ji,Ji-Ung Lee,Betty van Aken,Qingqing Cao,Manuel R. Ciosici,Michael Hassid,Kenneth Heafield,Sara Hooker,Pedro Henrique Martins,André F. T. Martins,Peter Milder,Colin Raffel,Edwin Simpson,N. Slonim,Niranjan Balasubramanian,Leon Derczynski,Roy Schwartz","id":"06272f6ab3c450e196b1f9879edb33b707d79da7","summary":"This survey relates and synthesises methods andings in those efﬁciencies in NLP, aiming to guide new researchers in the field and inspire the development of new methods.","score":5},{"url":"https://www.semanticscholar.org/paper/4989c08930e42d322b3bfed167d7ea434a698f2c","title":"CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Tanay Dixit,Bhargavi Paranjape,Hannaneh Hajishirzi,Luke Zettlemoyer","id":"4989c08930e42d322b3bfed167d7ea434a698f2c","summary":"Experiments on natural language inference and sentiment analysis benchmarks show that CORE counterfactuals are more effective at improving generalization to OOD data compared to other DA approaches, and the CORE retrieval framework can be used to encourage diversity in manually authored perturbations.","score":5},{"url":"https://www.semanticscholar.org/paper/2922ebbfbf71885f1a7cce6f41c79e950f69ceac","title":"Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey","venue":"ArXiv","year":2022,"referenceCount":100,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Sachin Kumar,Vidhisha Balachandran,Lucille Njoo,Antonios Anastasopoulos,Yulia Tsvetkov","id":"2922ebbfbf71885f1a7cce6f41c79e950f69ceac","summary":"This work provides a survey of practical methods for addressing potential threats and societal harms from language generation models and draws on several prior works’ taxonomies of language model risks to present a structured overview of strategies for detecting andiorating different kinds of risks/harms of language generators.","score":5},{"url":"https://www.semanticscholar.org/paper/9c36c8f398a074801d6098287c4353bcf87a1d6c","title":"Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Yejin Bang,Tiezheng Yu,Andrea Madotto,Zhaojiang Lin,Mona Diab,Pascale Fung","id":"9c36c8f398a074801d6098287c4353bcf87a1d6c","summary":"A framework for value-aligned classiﬁcation that performs prediction based on explicitly written human values in the command that proves both inclusivity & explainability in AI is introduced.","score":5},{"url":"https://www.semanticscholar.org/paper/8862ed012fe06a794fda3ceae3f471a0c2a40fbe","title":"Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/10/2022","authors":"Ping Yang,Junjie Wang,Ruyi Gan,Xinyu Zhu,Lin Zhang,Ziwei Wu,Xinyu Gao,Jiaxing Zhang,Tetsuya Sakai","id":"8862ed012fe06a794fda3ceae3f471a0c2a40fbe","summary":"This work proposes a new paradigm for zero-shot learners that is format agnostic, i.e., it is compatible with any format and applicable to a list of language tasks, such as text classification, commonsense reasoning, coreference resolution, and sentiment analysis.","score":5},{"url":"https://www.semanticscholar.org/paper/3c414e3125c12dbd23f62e6c1b85c1a4dc9a522e","title":"Boosting Natural Language Generation from Instructions with Meta-Learning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":30,"citationCount":3,"influentialCitationCount":0,"publicationDate":"20/10/2022","authors":"Budhaditya Deb,Guoqing Zheng,A. Awadallah","id":"3c414e3125c12dbd23f62e6c1b85c1a4dc9a522e","summary":"This paper proposes to adapt meta-learning to MTIL in three directions: Model Agnostic Meta Learning (MAML), Hyper-Network (HNet) based adaptation to generate task specific parameters conditioned on instructions, and an approach combining HNet and MAML.","score":5},{"url":"https://www.semanticscholar.org/paper/55adc6c9ad132d814e8c6e81b4e229fc9e6bcb82","title":"Preserving In-Context Learning ability in Large Language Model Fine-tuning","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":2,"influentialCitationCount":1,"publicationDate":"01/11/2022","authors":"Yihan Wang,Si Si,Daliang Li,M. Lukasik,Felix Yu,Cho-Jui Hsieh,I. Dhillon,Surinder Kumar","id":"55adc6c9ad132d814e8c6e81b4e229fc9e6bcb82","summary":"ProMoT is proposed, a simple yet effective two-stage ﬁne-tuning framework that preserves in-context abilities of the pretrained model and shows remarkable generalization ability on tasks that have different formats, e.g. natural language inference and English-French translation.","score":5},{"url":"https://www.semanticscholar.org/paper/78281482c1fdad8e167bab39cc9955c73d58ae8f","title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale","venue":"ArXiv","year":2022,"referenceCount":125,"citationCount":7,"influentialCitationCount":1,"publicationDate":"14/11/2022","authors":"Yuxin Fang,Wen Wang,Binhui Xie,Quan-Sen Sun,Ledell Yu Wu,Xinggang Wang,Tiejun Huang,Xinlong Wang,Yue Cao","id":"78281482c1fdad8e167bab39cc9955c73d58ae8f","summary":"Evaluating the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models.","score":5},{"url":"https://www.semanticscholar.org/paper/f26d0419a3e8e0d3128279c0a1c962a67f30b729","title":"UnifiedABSA: A Unified ABSA Framework Based on Multi-task Instruction Tuning","venue":"ArXiv","year":2022,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/11/2022","authors":"Zengzhi Wang,Rui Xia,Jianfei Yu","id":"f26d0419a3e8e0d3128279c0a1c962a67f30b729","summary":"U NIFIED ABSA is presented, a general-purpose ABSA framework based on multi- task instruction tuning, which can uniformly model various tasks and capture the inter-task dependency with multi-task learning.","score":5},{"url":"https://www.semanticscholar.org/paper/0ba0091c60c0346493b9ffb46ac682eee5453a53","title":"Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping","venue":"ArXiv","year":2022,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/12/2022","authors":"Jiyan He,Xuechen Li,Da Yu,Huishuai Zhang,Janardhan Kulkarni,Y. Lee,A. Backurs,Nenghai Yu,J. Bian","id":"0ba0091c60c0346493b9ffb46ac682eee5453a53","summary":"Better performance than previously reported on the CIFAR-10 task with WRN16-4 and the SST-2 task with RoBERTa-base model is found.","score":5},{"url":"https://www.semanticscholar.org/paper/b4c80344081d8c548fc4d68868b8262182a146fa","title":"Structured information extraction from complex scientific text with fine-tuned large language models","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":3,"influentialCitationCount":1,"publicationDate":"10/12/2022","authors":"Alex Dunn,John Dagdelen,N. Walker,Sanghoon Lee,Andrew S. Rosen,G. Ceder,Kristin Persson,Anubhav Jain","id":"b4c80344081d8c548fc4d68868b8262182a146fa","summary":"This work presents a simple sequence-to-sequence approach to joint named entity recognition and relation extraction for complex hierarchical information in scientiﬁc text that leverages a pre-trained large language model (LLM), GPT-3, that is capable of accurately extracting useful records of complex knowledge for three representative tasks in materials chemistry.","score":5},{"url":"https://www.semanticscholar.org/paper/5d96a7b57f4959ebcf65eaa21b9f6004b7cdeae9","title":"A fine-grained comparison of pragmatic language understanding in humans and language models","venue":"ArXiv","year":2022,"referenceCount":74,"citationCount":3,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"Jennifer Hu,Sammy Floyd,O. Jouravlev,Evelina Fedorenko,E. Gibson","id":"5d96a7b57f4959ebcf65eaa21b9f6004b7cdeae9","summary":"","score":5},{"url":"https://www.semanticscholar.org/paper/3585e08f2491859679b761eae8444afe7ec62f74","title":"FewFedWeight: Few-shot Federated Learning Framework across Multiple NLP Tasks","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Weilong Dong,Xinwei Wu,Junzhuo Li,Shuangzhi Wu,Chao Bian,Deyi Xiong","id":"3585e08f2491859679b761eae8444afe7ec62f74","summary":"Experiments show that F EW F ED W EIGHT can signiﬁcantly improve the performance of client models on 61% tasks with an average performance improvement rate of 30.5% over the baseline and substantially outperform FedAvg and other decentralized learning methods.","score":5},{"url":"https://www.semanticscholar.org/paper/96bdc84fba47a71f2a4dbdeb58439fa16693873f","title":"Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Jingjing Xu,Qingxiu Dong,Hongyi Liu,Lei Li","id":"96bdc84fba47a71f2a4dbdeb58439fa16693873f","summary":"This work revisits masked language modeling and presents a geometry-guided self-supervised learning method (Go-tuning for short) by taking a small number of task-aware self- supervised data to update language models further, which can enable T5-small (80M) competitive zero-shot results compared with large language models.","score":5},{"url":"https://www.semanticscholar.org/paper/df15f1efd70e7fa4d088c24931afef4d7539da4f","title":"Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Artidoro Pagnoni,Alexander R. Fabbri,Wojciech Kryscinski,Chien-Sheng Wu","id":"df15f1efd70e7fa4d088c24931afef4d7539da4f","summary":"The results show that S OCRATIC pretraining cuts task-speciﬁc labeled data requirements in half, is more faithful to user-provided queries, and achieves state-of-the-art performance on QMSum and SQuALITY.","score":5},{"url":"https://www.semanticscholar.org/paper/ed5ebed7ff668fd7362d531a40b49b3aea33b3a9","title":"Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Justus Mattern,Zhijing Jin,Mrinmaya Sachan,Rada Mihalcea,B. Schölkopf","id":"ed5ebed7ff668fd7362d531a40b49b3aea33b3a9","summary":"This paper proposes a new framework for robustly measuring and quantifying biases exhibited by generative language models and uses this framework to investigate GPT-3’s occupational gender bias and propose prompting techniques for mitigating these biases without the need for tuning.","score":5},{"url":"https://www.semanticscholar.org/paper/70d89d380ca5d20564e1dd8ed2f4c59f5c7b3656","title":"HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot Generalisation","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Hamish Ivison,Akshita Bhagia,Yizhong Wang,Hannaneh Hajishirzi,Matthew E. Peters","id":"70d89d380ca5d20564e1dd8ed2f4c59f5c7b3656","summary":"Hypernetworks for INstruction Tuning (HINT) is introduced, which convert task instructions and examples using a pretrained text encoder into parameter-efﬁcient modules inserted into an underlying model, eliminating the need to include instructions in the model input.","score":5},{"url":"https://www.semanticscholar.org/paper/d2aa89bbfa5eb972626f189cd7454f7d6d0af7c3","title":"MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Zhiyang Xu,Ying Shen,Lifu Huang","id":"d2aa89bbfa5eb972626f189cd7454f7d6d0af7c3","summary":"This work introduces M ULTI I N - STRUCT, the first multimodal instruction tuning benchmark dataset, and designs a new evaluation metric – Sensitivity – to evaluate how sensitive the model is to the variety of instructions.","score":5},{"url":"https://www.semanticscholar.org/paper/658a8195ca7dc839bf254d4b2eb67c50384c5c6e","title":"Language models generalize beyond natural proteins","venue":"bioRxiv","year":2022,"referenceCount":75,"citationCount":5,"influentialCitationCount":0,"publicationDate":"22/12/2022","authors":"Robert Verkuil,Ori Kabeli,Yilun Du,B. Wicky,L. Milles,J. Dauparas,D. Baker,S. Ovchinnikov,Tom Sercu,Alexander Rives","id":"658a8195ca7dc839bf254d4b2eb67c50384c5c6e","summary":"The results show that language models, though only trained on sequences, learn a deep grammar that enables the design of protein structure, extending beyond natural proteins.","score":5},{"url":"https://www.semanticscholar.org/paper/b13668fe9c944b8ad441edb473c218d4cf303de8","title":"Towards Few-Shot Identification of Morality Frames using In-Context Learning","venue":"NLPCSS","year":2023,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"Shamik Roy,Nishanth Nakshatri,Dan Goldwasser","id":"b13668fe9c944b8ad441edb473c218d4cf303de8","summary":"This paper proposes prompting based approaches using pretrained Large Language Models for identification of morality frames, relying only on few-shot exemplars, and compares the models’ performance with few- shot RoBERTa and found promising results.","score":5},{"url":"https://www.semanticscholar.org/paper/0392d58335ce674a70f5e58ac8c438de296a0e6a","title":"Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models","venue":"IEEE Transactions on Visualization and Computer Graphics","year":2022,"referenceCount":46,"citationCount":8,"influentialCitationCount":0,"publicationDate":"16/08/2022","authors":"Hendrik Strobelt,Albert Webson,Victor Sanh,Benjamin Hoover,J. Beyer,H. Pfister,Alexander M. Rush","id":"0392d58335ce674a70f5e58ac8c438de296a0e6a","summary":"A workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task, and then allows easy deployment of the newly created ad-hoc models.","score":5},{"url":"https://www.semanticscholar.org/paper/80c3bc88913815bfe9f2c96c6008231433264dfd","title":"Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing","venue":"ArXiv","year":2023,"referenceCount":77,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/01/2023","authors":"Shruthi Bannur,Stephanie L. Hyland,Qianchu Liu,Fernando Pérez-García,Maximilian Ilse,Daniel Coelho de Castro,Benedikt Boecking,H. Sharma,Kenza Bouzid,Anja Thieme,Anton Schwaighofer,M. Wetscherek,M. Lungren,Aditya Nori,J. Alvarez-Valle,O. Oktay","id":"80c3bc88913815bfe9f2c96c6008231433264dfd","summary":"The results show the advantages of incorporating prior images and reports to make most use of the data, and release a novel multi-modal temporal benchmark dataset, MS-CXR-T, to quantify the quality of vision–language representations in terms of temporal semantics.","score":5},{"url":"https://www.semanticscholar.org/paper/feee27c7a717fcb329b4e863b2c720a9defad504","title":"Prompt-Based Editing for Text Style Transfer","venue":"ArXiv","year":2023,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Guoqing Luo,Yu Tong Han,Lili Mou,Mauajama Firdaus","id":"feee27c7a717fcb329b4e863b2c720a9defad504","summary":"A prompt-based editing approach for text style transfer that is a training-free process and more controllable than the autoregressive generation of sentences, and largely outperforms the state-of-the-art systems that have 20 times more parameters.","score":5},{"url":"https://www.semanticscholar.org/paper/f9338e1b32cb450d95094ce71957375754823e4b","title":"Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models","venue":"","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/02/2023","authors":"Shrimai Prabhumoye,M. Patwary,M. Shoeybi,Bryan Catanzaro","id":"f9338e1b32cb450d95094ce71957375754823e4b","summary":"The results indicate that the best performing strategy (INST) substantially reduces the toxicity probability up to 61% while preserving the accuracy on five benchmark NLP tasks as well as improving AUC scores on four bias detection tasks by 1.3%.","score":5},{"url":"https://www.semanticscholar.org/paper/0ef848249ff85ec6083865d56c7fe3ad517d0820","title":"Natural Response Generation for Chinese Reading Comprehension","venue":"","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/02/2023","authors":"Nuo Chen,Hongguang Li,Yinan Bao,Baoyuan Wang,Jia Li","id":"0ef848249ff85ec6083865d56c7fe3ad517d0820","summary":"Penguin is the first benchmark towards natural response generation in Chinese MRC on a relatively large scale and develops two strong baselines: end-to-end and two-stage frameworks.","score":5},{"url":"https://www.semanticscholar.org/paper/cdf54c147434c83a4a380916b6c1279b0ca19fc2","title":"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action","venue":"ArXiv","year":2022,"referenceCount":75,"citationCount":17,"influentialCitationCount":1,"publicationDate":"10/07/2022","authors":"Dhruv Shah,B. Osinski,Brian Ichter,S. Levine","id":"cdf54c147434c83a4a380916b6c1279b0ca19fc2","summary":"Each model is pre-trained on its own dataset, and it is shown that the complete system can execute a variety of user-speciﬁed instructions in real-world outdoor environments — choosing the correct sequence of landmarks through a combination of language and spatial context — and handle mistakes.","score":5},{"url":"https://www.semanticscholar.org/paper/30d0a5a5f8e776c844a37afe3b1ace0b21b24859","title":"Retrospectives on the Embodied AI Workshop","venue":"ArXiv","year":2022,"referenceCount":220,"citationCount":4,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Matt Deitke,Dhruv Batra,Yonatan Bisk,Tommaso Campari,Angel X. Chang,Devendra Singh Chaplot,Changan Chen,Claudia P'erez D'Arpino,Kiana Ehsani,Ali Farhadi,Li Fei-Fei,Anthony Francis,Chuang Gan,K. Grauman,David Hall,Winson Han,Unnat Jain,Aniruddha Kembhavi,Jacob Krantz,Stefan Lee,Chengshu Li,Sagnik Majumder,Oleksandr Maksymets,Roberto Mart'in-Mart'in,Roozbeh Mottaghi,Sonia Raychaudhuri,Mike Roberts,S. Savarese,M. Savva,Mohit Shridhar,N. Sunderhauf,Andrew Szot,Ben Talbot,J. Tenenbaum,Jesse Thomason,Alexander Toshev,Joanne Truong,Luca Weihs,Jiajun Wu","id":"30d0a5a5f8e776c844a37afe3b1ace0b21b24859","summary":"A retrospective on the state of Embodied AI research is presented and 13 challenges presented at the EmbodiedAI Workshop at CVPR are grouped into three themes: visual navigation, rearrangement and integration.","score":5},{"url":"https://www.semanticscholar.org/paper/30477855d76058a9b542cabea3058aad1a837d51","title":"A Case for Business Process-Specific Foundation Models","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Yara Rizk,P. Venkateswaran,Vatche Isahagian,Vinod Muthusamy","id":"30477855d76058a9b542cabea3058aad1a837d51","summary":"It is argued that business process data representations have unique characteristics that warrant the development of a new class of foundation models to handle tasks like process mining, optimization, and decision making.","score":5},{"url":"https://www.semanticscholar.org/paper/a2d2bbe4c542173662a444b33b76c66992697830","title":"InstructPix2Pix: Learning to Follow Image Editing Instructions","venue":"ArXiv","year":2022,"referenceCount":72,"citationCount":10,"influentialCitationCount":1,"publicationDate":"17/11/2022","authors":"Tim Brooks,Aleksander Holynski,Alexei A. Efros","id":"a2d2bbe4c542173662a444b33b76c66992697830","summary":"The conditional diffusion model, InstructPix2Pix, is trained on generated data, and generalizes to real images and user-written instructions at inference time, and shows compelling editing results for a diverse collection of input images and written instructions.","score":5},{"url":"https://www.semanticscholar.org/paper/fe3ead702e8e8948d00caef9bc9dd075dc560236","title":"I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Muhammad Ferjad Naeem,Muhammad Gul Zain Ali Khan,Yongqin Xian,Muhammad Zeshan Afzal,D. Stricker,L. Gool,F. Tombari","id":"fe3ead702e8e8948d00caef9bc9dd075dc560236","summary":"This work establishes a new state-of-the-art on three public benchmark datasets for zero-shot image classiﬁcation with unsupervised semantic embeddings and shows that each text view of a class provides complementary information allowing a model to learn a highly discriminative class embedding.","score":5},{"url":"https://www.semanticscholar.org/paper/933b37b21e9d61139660088adb032ff3fdf56d86","title":"Learning Video Representations from Large Language Models","venue":"ArXiv","year":2022,"referenceCount":86,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"Yue Zhao,Ishan Misra,Philipp Krahenbuhl,Rohit Girdhar","id":"933b37b21e9d61139660088adb032ff3fdf56d86","summary":"This work repurposes pre-trained LLMs to be conditioned on visual input, and finetune them to create automatic video narrators, which offer a number of advantages, including dense cover-age of long videos, better temporal synchronization of the visual information and text, and much higher diversity of text.","score":5},{"url":"https://www.semanticscholar.org/paper/35bf7a8d6b74c0d8dc3d6d876c560532e230cb97","title":"Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":1,"influentialCitationCount":1,"publicationDate":"09/12/2022","authors":"Shen Yan,Tao Zhu,Zirui Wang,Yuan Cao,Mi Zhang,Soham Ghosh,Yonghui Wu,Jiahui Yu","id":"35bf7a8d6b74c0d8dc3d6d876c560532e230cb97","summary":"This work presents VideoCoCa, an efficient approach to establish a foundational video-text model for tasks including open-vocabulary video classification, text-to-video retrieval, video captioning and video question-answering, and explores lightweight finetuning on top of this model.","score":5},{"url":"https://www.semanticscholar.org/paper/c39ced3609bc8cf854786b29d1c5b85b17c061a3","title":"LaMPP: Language Models as Probabilistic Priors for Perception and Action","venue":"ArXiv","year":2023,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"Belinda Z. Li,William Chen,Pratyusha Sharma,Jacob Andreas","id":"c39ced3609bc8cf854786b29d1c5b85b17c061a3","summary":"This work describes how to leverage language models for *non-linguistic* perception and control tasks by casting labeling and decision-making as inference in probabilistic graphical models in which language models parameterize prior distributions over labels, decisions and parameters.","score":5},{"url":"https://www.semanticscholar.org/paper/cd6652fe413d57d05b44e0f3aa036c54f0eef464","title":"Towards Boosting the Open-Domain Chatbot with Human Feedback","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/08/2022","authors":"Hua Lu,Siqi Bao,H. He,Fan Wang,Hua Wu,Haifeng Wang","id":"cd6652fe413d57d05b44e0f3aa036c54f0eef464","summary":"A novel andcient approach Diamante to boost the open-domain chatbot, where two kinds of human feedback are collected and leveraged and the implicit preference in the data collection process and the generation-evaluation joint training is introduced.","score":5},{"url":"https://www.semanticscholar.org/paper/2fa11f97ae084591d79013c0cbc65d9931d976df","title":"When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":1,"influentialCitationCount":0,"publicationDate":"28/10/2022","authors":"Weiyan Shi,Emily Dinan,Kurt Shuster,J. Weston,Jing Xu","id":"2fa11f97ae084591d79013c0cbc65d9931d976df","summary":"J UICER, a framework to make use of both binary and free-form textual human feedback, works by extending sparse binary feedback by training a satisfaction class to label the unlabeled data and training a reply corrector to map the bad replies to good ones.","score":5},{"url":"https://www.semanticscholar.org/paper/5ff9cd8fcb959ca6b458c11e780d61c3f2bf7691","title":"PLACES: Prompting Language Models for Social Conversation Synthesis","venue":"ArXiv","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Maximillian Chen,A. Papangelis,Chenyang Tao,Seokhwan Kim,Andrew Rosenbaum,Yang Liu,Zhou Yu,Dilek Z. Hakkani-Tür","id":"5ff9cd8fcb959ca6b458c11e780d61c3f2bf7691","summary":"This work uses a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting, and demonstrates that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi- party tasks.","score":5},{"url":"https://www.semanticscholar.org/paper/7b82fb74a28783aff4ff77340ef6f06dd9ed6f5e","title":"Causal Inference Multi-Agent Reinforcement Learning for Traffic Signal Control","venue":"Information Fusion","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Shantian Yang,Bo Yang,Zheng Zeng,Zhongfeng Kang","id":"7b82fb74a28783aff4ff77340ef6f06dd9ed6f5e","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/9651c3f83b9310829622305f5316443253861fba","title":"Weakly Supervised Pre-Training for Multi-Hop Retriever","venue":"Findings","year":2021,"referenceCount":24,"citationCount":6,"influentialCitationCount":1,"publicationDate":"18/06/2021","authors":"Yeon Seonwoo,Sang-Woo Lee,Ji-Hoon Kim,Jung-Woo Ha,Alice H. Oh","id":"9651c3f83b9310829622305f5316443253861fba","summary":"A new method for weakly supervised multi-hop retriever pretraining without human efforts is proposed, which includes a pre-training task for generating vector representations of complex questions, and a scalable data generation method that produces the nested structure of question and subquestion as weak supervision for pre- training.","score":4},{"url":"https://www.semanticscholar.org/paper/a19920428f9ad1e0e760d46695e6bb09adde5fcd","title":"Talk2Data: High-Level Question Decomposition for Data-Oriented Question and Answering","venue":"ArXiv","year":2021,"referenceCount":65,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/07/2021","authors":"Danqing Shi,Yi Guo,Mingjuan Guo,Yanqiu Wu,Qing Chen,Nan Cao","id":"a19920428f9ad1e0e760d46695e6bb09adde5fcd","summary":"Talk2Data is introduced, a data-oriented online question and answering system that supports answering both low-level and high- level questions and leverages a novel deep-learning model to resolve high-level questions into a series of low-levels questions that can be answered by data facts.","score":4},{"url":"https://www.semanticscholar.org/paper/3bdb464777dad270ad1c80426614af16c08dd361","title":"Battlesnake Challenge: A Multi-agent Reinforcement Learning Playground with Human-in-the-loop","venue":"ArXiv","year":2020,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/07/2020","authors":"Jonathan Chung,Anna Luo,Xavier Raffin,Scott Perry","id":"3bdb464777dad270ad1c80426614af16c08dd361","summary":"The results show that agents with the proposed HILL methods consistently outperform agents without HILL, and heuristics of reward manipulation had the best performance in the online competition.","score":4},{"url":"https://www.semanticscholar.org/paper/243da4a2378386a7238398eace6f4da76e7fa397","title":"Modifying Simulated Perfect Oracles for Interactive Reinforcement Learning: a Participatory Design Methodology","venue":"","year":2021,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"","id":"243da4a2378386a7238398eace6f4da76e7fa397","summary":"This paper presents a participatory design approach that can modify perfect oracles to human-like oracles for I-RL algorithm testing and runs a user study and experiments with modified oracles generated from participants.","score":4},{"url":"https://www.semanticscholar.org/paper/a9fef6c25124db63818ce49367eb8f06b0ba5f98","title":"Correct Me If I am Wrong: Interactive Learning for Robotic Manipulation","venue":"IEEE Robotics and Automation Letters","year":2021,"referenceCount":40,"citationCount":7,"influentialCitationCount":0,"publicationDate":"07/10/2021","authors":"Eugenio Chisari,T. Welschehold,J. Boedecker,W. Burgard,A. Valada","id":"a9fef6c25124db63818ce49367eb8f06b0ba5f98","summary":"The proposed CEILing (Corrective and Evaluative Interactive Learning) framework combines both corrective and evaluative feedback from the teacher to train a stochastic policy in an asynchronous manner, and employs a dedicated mechanism to trade off human corrections with the robot’s own experience.","score":4},{"url":"https://www.semanticscholar.org/paper/dff829abffea125d72b9a2f2e0c1a0119b72438f","title":"Reinforcement learning in medical image analysis: Concepts, applications, challenges, and future directions","venue":"Journal of Applied Clinical Medical Physics","year":2022,"referenceCount":89,"citationCount":1,"influentialCitationCount":0,"publicationDate":"28/06/2022","authors":"MingZhe Hu,Jiahan Zhang,L. Matkovic,Tian Liu,Xiaofeng Yang","id":"dff829abffea125d72b9a2f2e0c1a0119b72438f","summary":"The aim of this review is to help the readers formulate and solve their medical image analysis research through the lens of reinforcement learning, and to discuss the reviewed reinforcement learning approaches’ limitations and possible future improvements.","score":4},{"url":"https://www.semanticscholar.org/paper/25d1df96dc08071719bd891c227fb2cf873c26cf","title":"BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA","venue":"Findings","year":2020,"referenceCount":25,"citationCount":33,"influentialCitationCount":1,"publicationDate":"01/05/2020","authors":"Nora Kassner,Hinrich Schütze","id":"25d1df96dc08071719bd891c227fb2cf873c26cf","summary":"BERT-kNN outperforms BERT on cloze-style QA by large margins without any further training and excels for rare facts.","score":4},{"url":"https://www.semanticscholar.org/paper/5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization","venue":"International Conference on Learning Representations","year":2020,"referenceCount":68,"citationCount":49,"influentialCitationCount":7,"publicationDate":"08/10/2020","authors":"Ekin Akyürek,Afra Feyza Akyurek,Jacob Andreas","id":"5cc8ea815bd05be3b28519b489afe6de278a4209","summary":"This work presents a family of learned data augmentation schemes that support a large category of compositional generalizations without appeal to latent symbolic structure in settings requiring Compositional generalization beyond the training data.","score":4},{"url":"https://www.semanticscholar.org/paper/183350b0c092c448198bb994d4ed1010f57e6b0d","title":"\\infty-former: Infinite Memory Transformer","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":50,"citationCount":3,"influentialCitationCount":0,"publicationDate":"01/09/2021","authors":"Pedro Henrique Martins,Zita Marinho,André F. T. Martins","id":"183350b0c092c448198bb994d4ed1010f57e6b0d","summary":"The \\infty-former is proposed, which extends the vanilla transformer with an unbounded long-term memory, and maintains “sticky memories,” being able to model arbitrarily long contexts while keeping the computation budget fixed.","score":4},{"url":"https://www.semanticscholar.org/paper/2a672342035defd8d75b54e08597ef124c6a0172","title":"Fusing Sentence Embeddings Into LSTM-based Autoregressive Language Models","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":1,"influentialCitationCount":0,"publicationDate":"04/08/2022","authors":"Vilém Zouhar,Marius Mosbach,D. Klakow","id":"2a672342035defd8d75b54e08597ef124c6a0172","summary":"An LSTM-based autoregressive language model which uses pre-trained on text embeddings from a pretrained masked language model via fusion (e.g. concatenation) to obtain a richer context representation for language modelling to improve the perplexity.","score":4},{"url":"https://www.semanticscholar.org/paper/f7b755474da275c021f6950590c89f7461b5c274","title":"Joint Retrieval and Generation Training for Grounded Text Generation","venue":"ArXiv","year":2021,"referenceCount":48,"citationCount":14,"influentialCitationCount":0,"publicationDate":2021,"authors":"Yizhe Zhang,Siqi Sun,Xiang Gao,Yuwei Fang,Chris Brockett,Michel Galley,Jianfeng Gao,Bill Dolan","id":"f7b755474da275c021f6950590c89f7461b5c274","summary":"This work proposes a framework that alleviates data constraint by jointly training a grounded generator and document retriever on the language model signal, and demonstrates that both generator and retriever can take advantage of this joint training and work synergistically to produce more informative and relevant text in both prose and dialogue generation.","score":4},{"url":"https://www.semanticscholar.org/paper/9491da2a7097ac44d7509cbd26d1d09dbcf3a17e","title":"Information Retrieval & Question Answering in Aviation Safety Domain","venue":"","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Raju Gite,P. Bhattacharyya","id":"9491da2a7097ac44d7509cbd26d1d09dbcf3a17e","summary":"This paper presents a Question Answering System using Deep Learning for the aviation domain, and presents two approaches: Deep Learning based QA (DLQA) and KG guided DL basedQA, which discusses techniques falling into two categories: open-book and closed-book approaches.","score":4},{"url":"https://www.semanticscholar.org/paper/39a9f750c9b79fba4a0404179fdac6a7cb922838","title":"RetGen: A Joint Framework for Retrieval and Grounded Text Generation Modeling","venue":"AAAI Conference on Artificial Intelligence","year":2021,"referenceCount":54,"citationCount":5,"influentialCitationCount":1,"publicationDate":"14/05/2021","authors":"Yizhe Zhang,Siqi Sun,Xiang Gao,Yuwei Fang,Chris Brockett,Michel Galley,Jianfeng Gao,Bill Dolan","id":"39a9f750c9b79fba4a0404179fdac6a7cb922838","summary":"This work proposes a framework that alleviates this data constraint by jointly training a grounded generator and document retriever on the language model signal, and demonstrates that both generator and retriever can take advantage of this joint training and work synergistically to produce more informative and relevant text in both prose and dialogue generation.","score":4},{"url":"https://www.semanticscholar.org/paper/2555cfe0f77d55cc0d887da00fd58857d0c6edd5","title":"Conformer-Kernel with Query Term Independence for Document Retrieval","venue":"ArXiv","year":2020,"referenceCount":65,"citationCount":19,"influentialCitationCount":2,"publicationDate":"20/07/2020","authors":"Bhaskar Mitra,Sebastian Hofstätter,Hamed Zamani,Nick Craswell","id":"2555cfe0f77d55cc0d887da00fd58857d0c6edd5","summary":"It is demonstrated that incorporating explicit term matching signal into the model can be particularly useful in the full retrieval setting, and to reduce the memory complexity of the Transformer layers with respect to the input sequence length, a new Conformer layer is proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/590bcbe623f212c1e27ab2edb0e400888f3f2601","title":"Improving Transformer-Kernel Ranking Model Using Conformer and Query Term Independence","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2021,"referenceCount":108,"citationCount":4,"influentialCitationCount":0,"publicationDate":"19/04/2021","authors":"Bhaskar Mitra,Sebastian Hofstätter,Hamed Zamani,Nick Craswell","id":"590bcbe623f212c1e27ab2edb0e400888f3f2601","summary":"This work proposes a novel Conformer layer as an alternative approach to scale TK to longer input sequences and incorporates query term independence and explicit term matching to extend the model to the full retrieval setting.","score":4},{"url":"https://www.semanticscholar.org/paper/ba9865c11e6b5b18d8323e4fa2eed89309e66e6f","title":"SEINE: SEgment-based Indexing for NEural information retrieval","venue":"","year":2022,"referenceCount":62,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sibo Dong,Justin Goldstein,G. Yang","id":"ba9865c11e6b5b18d8323e4fa2eed89309e66e6f","summary":"A novel SEgment-based Neural Indexing method, SEINE, is proposed, which provides a general indexing framework that can flexibly support a variety of interaction-based neural retrieval methods and proposes to use segment-level inverted index to store the atomic query-document interaction values.","score":4},{"url":"https://www.semanticscholar.org/paper/bbf0390847159f3d56089807a0ad8348986db5fd","title":"CREPE: Can Vision-Language Foundation Models Reason Compositionally?","venue":"ArXiv","year":2022,"referenceCount":92,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"Zixian Ma,Jerry Hong,Mustafa Omer Gul,Mona Gandhi,Irena Gao,Ranjay Krishna","id":"bbf0390847159f3d56089807a0ad8348986db5fd","summary":"A new compositionality evaluation benchmark, CREPE, is introduced, which measures two important aspects of compositionality identified by cognitive science literature: systematicity and productivity.","score":4},{"url":"https://www.semanticscholar.org/paper/db20bd3bb82d1011ce704d440d8c2578f665e6e1","title":"Aligning Robot and Human Representations","venue":"ArXiv","year":2023,"referenceCount":177,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"Andreea Bobu,Andi Peng,Pulkit Agrawal,J. Shah,A. Dragan","id":"db20bd3bb82d1011ce704d440d8c2578f665e6e1","summary":"It is suggested that because humans will be the ultimate evaluator of robot performance in the world, it is critical that the authors explicitly focus their efforts on aligning learned task representations with humans, in addition to learning the downstream task.","score":4},{"url":"https://www.semanticscholar.org/paper/0e34addae55a571d7efd3a5e2543e86dd7d41a83","title":"Interactive Language: Talking to Robots in Real Time","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":6,"influentialCitationCount":1,"publicationDate":"12/10/2022","authors":"Corey Lynch,Ayzaan Wahid,Jonathan Tompson,Tianli Ding,James Betker,Robert K. Baruch,Travis Armstrong,Peter R. Florence","id":"0e34addae55a571d7efd3a5e2543e86dd7d41a83","summary":"A framework for building interactive, real- time, natural language-instructable robots in the real world is presented, and the same policy is capable of being guided by a human via real-time language to address a wide range of precise long-horizon rearrange- ment goals.","score":4},{"url":"https://www.semanticscholar.org/paper/91cac43160ca45e5a1a41e0c5b7e6ec5a74033b3","title":"Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":2,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Ted Xiao,Harris Chan,P. Sermanet,Ayzaan Wahid,Anthony Brohan,Karol Hausman,S. Levine,Jonathan Tompson","id":"91cac43160ca45e5a1a41e0c5b7e6ec5a74033b3","summary":"DIAL is introduced, which utilizes semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration data and then train language-conditioned policies on the augmented datasets, enabling cheaper acquisition of useful language descriptions compared to expensive human labels.","score":4},{"url":"https://www.semanticscholar.org/paper/832fff14d2ed50eb7969c4c4b976c35776548f56","title":"REALM: Retrieval-Augmented Language Model Pre-Training","venue":"ArXiv","year":2020,"referenceCount":42,"citationCount":533,"influentialCitationCount":91,"publicationDate":"10/02/2020","authors":"Kelvin Guu,Kenton Lee,Z. Tung,Panupong Pasupat,Ming-Wei Chang","id":"832fff14d2ed50eb7969c4c4b976c35776548f56","summary":"The effectiveness of Retrieval-Augmented Language Model pre-training (REALM) is demonstrated by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA) and is found to outperform all previous methods by a significant margin, while also providing qualitative benefits such as interpretability and modularity.","score":4},{"url":"https://www.semanticscholar.org/paper/a20712b1b9779ee43ce143a19b3f67f0cacbbf57","title":"Neural Databases","venue":"ArXiv","year":2020,"referenceCount":58,"citationCount":4,"influentialCitationCount":0,"publicationDate":"14/10/2020","authors":"James Thorne,Majid Yazdani,Marzieh Saeidi,F. Silvestri,Sebastian Riedel,A. Halevy","id":"a20712b1b9779ee43ce143a19b3f67f0cacbbf57","summary":"This paper describes NeuralDB, a database system with no pre-defined schema, in which updates and queries are given in natural language, and describes an algorithm that learns how to create the appropriate sets of facts to be fed into each of the Neural SPJ operators.","score":4},{"url":"https://www.semanticscholar.org/paper/64435711f6542aa6b53e95c6e084a0ccd2ec1c16","title":"HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions","venue":"AAAI Conference on Artificial Intelligence","year":2020,"referenceCount":28,"citationCount":18,"influentialCitationCount":2,"publicationDate":"31/12/2020","authors":"Shaobo Li,Xiaoguang Li,Lifeng Shang,Xin Jiang,Qun Liu,Chengjie Sun,Zhenzhou Ji,Bingquan Liu","id":"64435711f6542aa6b53e95c6e084a0ccd2ec1c16","summary":"This paper proposes a new retrieval target, hop, to collect the hidden reasoning evidence from Wikipedia for complex question answering and builds HopRetriever which retrieves hops over Wikipedia to answer complex questions.","score":4},{"url":"https://www.semanticscholar.org/paper/17b4a75b432b9f58de143918608de9234a4da988","title":"Towards Continual Entity Learning in Language Models for Conversational Agents","venue":"ArXiv","year":2021,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/07/2021","authors":"R. Gadde,I. Bulyko","id":"17b4a75b432b9f58de143918608de9234a4da988","summary":"EALM is introduced, where entity models trained on catalogues of entities into the pre-trained LMs are integrated and a combined language model adaptively adds information from 7 the entity models into thePre-trained LM depending on the sentence context.","score":4},{"url":"https://www.semanticscholar.org/paper/c95d1a92163ebacac5b3a58b31c8d37447beafc0","title":"Vision-based navigation and obstacle avoidance via deep reinforcement learning","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"P. Blum,Peter Crowley,G. Lykotrafitis","id":"c95d1a92163ebacac5b3a58b31c8d37447beafc0","summary":"A deep Dyna-Q learning algorithm for room evacuation and obstacle avoidance in partially observable environments based on low-resolution raw image data from an onboard camera is employed and it is evident that the agent can navigate to a goal location while avoiding multiple static and dynamic obstacles, and can escape from a concave obstacle while searching for and navigating to the exit.","score":4},{"url":"https://www.semanticscholar.org/paper/2b93e59f2ab24bdfd2f9160eb47b6d1936cbe1ae","title":"Receding-Horizon Control of Constrained Switched Systems with Neural Networks as Parametric Function Approximators","venue":"SN Computer Science","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Lukas Markolf,O. Stursberg","id":"2b93e59f2ab24bdfd2f9160eb47b6d1936cbe1ae","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/aa76febe2e6f6a96f1daa83828f2340bf871042e","title":"Global Convergence of Localized Policy Iteration in Networked Multi-Agent Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Yizhou Zhang,Guannan Qu,Pan Xu,Yiheng Lin,Zaiwei Chen,A. Wierman","id":"aa76febe2e6f6a96f1daa83828f2340bf871042e","summary":"A Localized Policy Iteration (LPI) algorithm that provably learns a near-globally-optimal policy using only local information is proposed, which explicitly captures the trade-off between optimality and computational complexity in choosing κ.","score":4},{"url":"https://www.semanticscholar.org/paper/17e99278239bae41a2d2546a0af4aea88a6247ba","title":"Recent Advances in Artificial Intelligence and Tactical Autonomy: Current Status, Challenges, and Perspectives","venue":"Italian National Conference on Sensors","year":2022,"referenceCount":163,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"D. Hagos,D. Rawat","id":"17e99278239bae41a2d2546a0af4aea88a6247ba","summary":"This paper provides the state-of-the-art advanced AI methods available for tactical autonomy and is the first work that addresses the important current trends, strategies, critical challenges, tactical complexities, and future research directions of tactical autonomy.","score":4},{"url":"https://www.semanticscholar.org/paper/f5bc137361ab303aa0c53c71418430c68ce52dc4","title":"NARS vs. Reinforcement learning: ONA vs. Q-Learning","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/12/2022","authors":"Ali Beikmohammadi","id":"f5bc137361ab303aa0c53c71418430c68ce52dc4","summary":"This project is looking to investigate the capability of NARS and answer the question of whether NARS has the potential to be a substitute for RL or not, and making a comparison between Q -Learning and ONA on some environments developed by an Open AI gym.","score":4},{"url":"https://www.semanticscholar.org/paper/1abed54dc77454fcb33f4da04b5bede4ccff8235","title":"CoBeL-RL: A neuroscience-oriented simulation framework for complex behavior and learning","venue":"bioRxiv","year":2022,"referenceCount":60,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/12/2022","authors":"Nicolas Diekmann,Sandhiya Vijayabaskaran,Xiangshuai Zeng,D. Kappel,Matheus Chaves Menezes,Sen Cheng","id":"1abed54dc77454fcb33f4da04b5bede4ccff8235","summary":"CoBeL-RL is a closed-loop simulator of complex behavior and learning based on RL and deep neural networks that provides a neuroscience-oriented framework for efficiently setting up and running simulations and fills an important gap in the software toolbox of computational neuroscience.","score":4},{"url":"https://www.semanticscholar.org/paper/8cc1912faa44981ff2dacfd749da6471344b9125","title":"A deep reinforcement learning method for structural dominant failure modes searching based on self-play strategy","venue":"Reliability Engineering &amp; System Safety","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Xiaoshu Guan,Huabin Sun,Rongrong Hou,Yang Xu,Y. Bao,Hui Li","id":"8cc1912faa44981ff2dacfd749da6471344b9125","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/41125af933ca21d5318b9fa2eb44e045a28bf216","title":"The Wisdom of the Crowd: Reliable Deep Reinforcement Learning Through Ensembles of Q-Functions","venue":"IEEE Transactions on Neural Networks and Learning Systems","year":2018,"referenceCount":34,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/09/2018","authors":"D. Elliott,C. Anderson","id":"41125af933ca21d5318b9fa2eb44e045a28bf216","summary":"This work investigates a novel technique which harnesses the wisdom of crowds by combining Q-function approximator estimates utilizing a simple combination scheme similar to the supervised learning approach known as bagging, and demonstrates that the stability in learning allows an actor-critic method to find more efficient solutions.","score":4},{"url":"https://www.semanticscholar.org/paper/009f39969ec181ac8b8548193a37d29cbafb1079","title":"Variational Information Bottleneck Regularized Deep Reinforcement Learning for Efficient Robotic Skill Adaptation","venue":"Italian National Conference on Sensors","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Guofei Xiang,S. Dian,Shaofeng Du,Zhonghui Lv","id":"009f39969ec181ac8b8548193a37d29cbafb1079","summary":"Empirical results show that the proposed variational information bottleneck regularized deep reinforcement learning algorithm can improve sample efficiency by 200–5000 times on new tasks and achieves substantial asymptotic performance improvement.","score":4},{"url":"https://www.semanticscholar.org/paper/fa13325dcc7af7f9bd9486acb8a974e9a8cf2d9c","title":"On The Convergence Of Policy Iteration-Based Reinforcement Learning With Monte Carlo Policy Evaluation","venue":"ArXiv","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2023","authors":"Anna Winnicki,R. Srikant","id":"fa13325dcc7af7f9bd9486acb8a974e9a8cf2d9c","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/bdb971079c7cbbffd752197ae246146939bfc18c","title":"Obstacle avoidance for environmentally-driven USVs based on deep reinforcement learning in large-scale uncertain environments","venue":"Ocean Engineering","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Peng Wang,Ranran Liu,Xinliang Tian,Xiantao Zhang,Lei Qiao,Yuntao Wang","id":"bdb971079c7cbbffd752197ae246146939bfc18c","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/45f573f302dc7e77cbc5d1a74ccbac3564bbebc8","title":"PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training","venue":"International Conference on Machine Learning","year":2021,"referenceCount":79,"citationCount":47,"influentialCitationCount":14,"publicationDate":"09/06/2021","authors":"Kimin Lee,Laura Smith,P. Abbeel","id":"45f573f302dc7e77cbc5d1a74ccbac3564bbebc8","summary":"This work presents an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off- policy learning, and is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.","score":4},{"url":"https://www.semanticscholar.org/paper/f70e82b8b7c792a3cdbf2c9bf2e7af06fd6a7269","title":"Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback","venue":"Conference on Robot Learning","year":2021,"referenceCount":39,"citationCount":13,"influentialCitationCount":0,"publicationDate":"11/08/2021","authors":"Xiaofei Wang,Kimin Lee,Kourosh Hakhamaneshi,P. Abbeel,M. Laskin","id":"f70e82b8b7c792a3cdbf2c9bf2e7af06fd6a7269","summary":"Skill Preferences is presented, an algorithm that learns a model over human preferences and uses it to extract human-aligned skills from of-ine data and substantially outperforms prior leading RL algorithms with human preferences as well as leading skill extraction algorithms without human preferences.","score":4},{"url":"https://www.semanticscholar.org/paper/239534e1d046c93bc4e98ace47f4657de67d96ec","title":"On-policy learning-based deep reinforcement learning assessment for building control efficiency and stability","venue":"Science and Technology for the Built Environment","year":2022,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/08/2022","authors":"Joon-Yong Lee,Aowabin Rahman,Sen Huang,Amanda D. Smith,S. Katipamula","id":"239534e1d046c93bc4e98ace47f4657de67d96ec","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/a0202138a0e3291a853e1b9beaeafcfb9369340a","title":"Researches advanced in credit assignment in reinforcement learning","venue":"Other Conferences","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Juerui Li","id":"a0202138a0e3291a853e1b9beaeafcfb9369340a","summary":"The existing solutions to the credit assignment issue are reviewed, the impact of different credit assignment strategies on algorithms’ final performance is compared, and some suggestions for the future research directions are given.","score":4},{"url":"https://www.semanticscholar.org/paper/c5f96f151f23e75e5e99dd97d4a4ab4ca52a8f8e","title":"Multi-Timeframe Algorithmic Trading Bots Using Thick Data Heuristics with Deep Reinforcement Learning","venue":"Artificial Intelligence Evolution","year":2022,"referenceCount":81,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/11/2022","authors":"Gregory Roy,J. Fiaidhi,Sabah Mohammed","id":"c5f96f151f23e75e5e99dd97d4a4ab4ca52a8f8e","summary":"An augmented Artificial Intelligence algorithmic trading approach that combines Thick Data Heuristic (TDH), with Deep Reinforcement Learning (DRL), to successfully learn trading execution timing policies is presented.","score":4},{"url":"https://www.semanticscholar.org/paper/0fd6c747b48526ba4abc05b4ae9260f93718ce8f","title":"Efficient Meta Reinforcement Learning for Preference-based Fast Adaptation","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/11/2022","authors":"Zhizhou Ren,Anji Liu,Yitao Liang,Jian Peng,Jianzhu Ma","id":"0fd6c747b48526ba4abc05b4ae9260f93718ce8f","summary":"A meta-RL algorithm that enables fast policy adaptation with preference-based feedback and can adapt to new tasks by querying human’s preference between behavior trajectories instead of using per-step numeric rewards is developed.","score":4},{"url":"https://www.semanticscholar.org/paper/34861a134bc2f515bf937e488b5b0b46f94fe64c","title":"State-Aware Proximal Pessimistic Algorithms for Offline Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"Chen Chen,Hongyao Tang,Yi Ma,Chao Wang,Qianli Shen,Dong Li,Jianye Hao","id":"34861a134bc2f515bf937e488b5b0b46f94fe64c","summary":"The key idea of SA-PP is leveraging discounted stationary state distribution ratios between the learning policy and the ofﬂine dataset to modulate the degree of behavior regularization in a state-wise manner, so that pessimism can be implemented in a more appropriate way.","score":4},{"url":"https://www.semanticscholar.org/paper/b6a71f4ab3a6fa53bde2c606abd195c033e9eb31","title":"CrowdHMT: Crowd Intelligence With the Deep Fusion of Human, Machine, and IoT","venue":"IEEE Internet of Things Journal","year":2022,"referenceCount":190,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"Bin Guo,Yan Liu,Sicong Liu,Zhiwen Yu,Xingshe Zhou","id":"b6a71f4ab3a6fa53bde2c606abd195c033e9eb31","summary":"The vision of the next generation of MCSC, crowd intelligence with the deep fusion of human, machine, and Internet of Things (IoT) is presented, namely, CrowdHMT, which aims to build a self-organizing, self-learning,self-adaptive, and continuous-evolving smart space with theDeep fusion of Crowdsourced human,Machine, and IoT intelligence.","score":4},{"url":"https://www.semanticscholar.org/paper/203ce4bd674b127df54824047e94051fccb239be","title":"Offline Robot Reinforcement Learning with Uncertainty-Guided Human Expert Sampling","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Ashish Kumar,Ilya Kuzovkin","id":"203ce4bd674b127df54824047e94051fccb239be","summary":"This work proposes a novel approach that uses uncertainty estimation to trigger the injection of human demonstration data and guide policy training towards optimal behavior while reducing overall sample complexity of existing ofﬂine reinforcement learning algorithms.","score":4},{"url":"https://www.semanticscholar.org/paper/bb7ae5aa82d3241743436a1699752814f46a2f22","title":"Joint Optimization of Jamming Link and Power Control in Communication Countermeasures: A Multiagent Deep Reinforcement Learning Approach","venue":"Wireless Communications and Mobile Computing","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/12/2022","authors":"Ning Rao,Hua Xu,Yue Zhang,Dan Wang,Lei Jiang,Xiang Peng","id":"bb7ae5aa82d3241743436a1699752814f46a2f22","summary":"A novel decentralized jamming resource allocation algorithm based on multiagent deep reinforcement learning (MADRL) to improve the efficiency of jamming resources allocation in battlefield communication countermeasures and develops the multiagent soft actor-critic (MASAC) algorithm to enhance the exploration capability of agents and accelerate the learning of cooperative policies among agents by leveraging the maximum policy entropy criterion.","score":4},{"url":"https://www.semanticscholar.org/paper/7edae4a0acd6ff65a561a5c040a97107a8cd24a9","title":"Review of Machine Learning and Artificial Intelligence (ML/AI) for the Pediatric Neurologist.","venue":"Pediatric Neurology","year":2023,"referenceCount":85,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Grace Y. Gombolay,N. Gopalan,A. Bernasconi,R. Nabbout,J. T. Megerian,Benjamin I. Siegel,Jamika Hallman-cooper,S. Bhalla,M. Gombolay","id":"7edae4a0acd6ff65a561a5c040a97107a8cd24a9","summary":"An overview of AI and ML (AI/ML), including definitions of common terms, is provided and instances of how AI/ML can be applied to pediatric neurology are provided.","score":4},{"url":"https://www.semanticscholar.org/paper/235ec101999a9c26ef24a59367dcd7738a40c2ec","title":"Reinforcement Learning informs optimal treatment strategies to limit antibiotic resistance","venue":"bioRxiv","year":2023,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2023","authors":"Davis T. Weaver,Jeff Maltas,Jacob G. Scott","id":"235ec101999a9c26ef24a59367dcd7738a40c2ec","summary":"This work shows that it is possible for RL agents to learn effective drug cycling protocols using current population fitness as the only training input, and represents a proof-of-concept for using AI to control complex evolutionary processes.","score":4},{"url":"https://www.semanticscholar.org/paper/fcf0cbae0d4986fb33b201f99426723a437d16e7","title":"SaFormer: A Conditional Sequence Modeling Approach to Offline Safe Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/01/2023","authors":"Q. Zhang,Linrui Zhang,Haoran Xu,Li Shen,Bowen Wang,Yongzhe Chang,Xueqian Wang,Bo Yuan,Dacheng Tao","id":"fcf0cbae0d4986fb33b201f99426723a437d16e7","summary":"A novel ofﬂine safe RL approach referred to as SaFormer, which tackles the above issues via conditional sequence modeling, and proposes cost-related tokens to restrict the action space and a posterior safety veriﬁcation to enforce the constraint explicitly.","score":4},{"url":"https://www.semanticscholar.org/paper/215e07a9d3796b10294a840c9e369e4c70743d17","title":"Continuous Control for High-Dimensional State Spaces: An Interactive Learning Approach","venue":"IEEE International Conference on Robotics and Automation","year":2019,"referenceCount":24,"citationCount":9,"influentialCitationCount":0,"publicationDate":"01/05/2019","authors":"Rodrigo Pérez-Dattari,C. Celemin,Javier Ruiz-del-Solar,J. Kober","id":"215e07a9d3796b10294a840c9e369e4c70743d17","summary":"Experimental results validate the efficiency of the D-COACH framework in three different problems, and show that its enhanced version reduces the human training effort considerably, and makes it feasible to learn policies within periods of time in which a DRL agent do not reach any improvement.","score":4},{"url":"https://www.semanticscholar.org/paper/2ae3b5fc450bd22bec0b86f7b2f12b35617c65e3","title":"Reinforcement Learning for Machine Translation: from Simulations to Real-World Applications","venue":"","year":2020,"referenceCount":259,"citationCount":0,"influentialCitationCount":0,"publicationDate":2020,"authors":"Julia Kreutzer","id":"2ae3b5fc450bd22bec0b86f7b2f12b35617c65e3","summary":"This thesis investigates solutions for machine learning updates, the suitability of feedback interfaces, and the dependency on reliability and expertise for different types of feedback, and proposes a self-regulation approach, where the learner decides which type of feedback to choose for each input.","score":4},{"url":"https://www.semanticscholar.org/paper/357b2db482e3f59f22493de3d8a2f6cf9ca9d634","title":"Real-Time Holding Control for Transfer Synchronization via Robust Multiagent Reinforcement Learning","venue":"IEEE transactions on intelligent transportation systems (Print)","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Xinlian Yu,A. Khani,Jingxu Chen,Hongli Xu,Haijun Mao","id":"357b2db482e3f59f22493de3d8a2f6cf9ca9d634","summary":"The promising results in terms of both online computational efficiency and solution effectiveness suggest that the proposed RL method is a valid candidate for real-time transit control when the dynamics cannot be modeled perfectly with system uncertainties, as is the case for the network-wide transfer synchronization problem.","score":4},{"url":"https://www.semanticscholar.org/paper/c8a2d383a42ffe4ee819844c846c6cc35c72b8e4","title":"CRC-RL: A Novel Visual Feature Representation Architecture for Unsupervised Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Darshita Jain,A. Majumder,S. Dutta,Swagat Kumar","id":"c8a2d383a42ffe4ee819844c846c6cc35c72b8e4","summary":"The proposed architecture, called CRC-RL, is shown to outperform the existing state-of-the-art methods on the challenging Deep mind control suite environments by a signiﬁcant margin thereby creating a new benchmark in thisRL field.","score":4},{"url":"https://www.semanticscholar.org/paper/9c6ee29930206b508d69799cea3bf7091912fc11","title":"Non-blocking Asynchronous Training for Reinforcement Learning in Real-World Environments","venue":"IEEE/RJS International Conference on Intelligent RObots and Systems","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"P. Bohm,P. Pounds,Archie C. Chapman","id":"9c6ee29930206b508d69799cea3bf7091912fc11","summary":"This paper proposes a non-blocking and asynchronous DRL training architecture for non-linear, real-time dynamical systems tailored to handling variable delays and demonstrates the efficacy of this architecture with a physical implementations of a commodity-grade swing-up pendulum and a quadrupedal robot.","score":4},{"url":"https://www.semanticscholar.org/paper/e0eb9870a6c105cadd92cae8f5218b2a84955849","title":"Active Task Randomization: Learning Visuomotor Skills for Sequential Manipulation by Proposing Feasible and Novel Tasks","venue":"ArXiv","year":2022,"referenceCount":98,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/11/2022","authors":"Kuan Fang,Toki Migimatsu,Ajay Mandlekar,Li Fei-Fei,J. Bohg","id":"e0eb9870a6c105cadd92cae8f5218b2a84955849","summary":"This work introduces Active Task Randomization (ATR), an approach that learns visuomotor skills for sequential manipulation by automatically creating feasible and novel tasks in simulation by developing a relational neural network that maps each task parameter into a compact embedding.","score":4},{"url":"https://www.semanticscholar.org/paper/382504c7013f4578d0f1829fdcd413fed529cde1","title":"Learning to Solve Complex Tasks by Talking to Agents","venue":"ArXiv","year":2021,"referenceCount":61,"citationCount":5,"influentialCitationCount":1,"publicationDate":2021,"authors":"Tushar Khot,Kyle Richardson,Daniel Khashabi,Ashish Sabharwal","id":"382504c7013f4578d0f1829fdcd413fed529cde1","summary":"This work proposes a new benchmark called COMMAQA that contains three kinds of complex reasoning tasks that are designed to be solved by “talking” to four agents with different capabilities and hopes it serves as a novel benchmark to enable the development of “green” AI systems that build upon existing agents.","score":4},{"url":"https://www.semanticscholar.org/paper/5564fcc4a0da606029867678317c42fc0f20fb13","title":"Retrieval Data Augmentation Informed by Downstream Question Answering Performance","venue":"FEVER","year":2022,"referenceCount":16,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"James Ferguson,Hannaneh Hajishirzi,Pradeep Dasigi,Tushar Khot","id":"5564fcc4a0da606029867678317c42fc0f20fb13","summary":"This work identifies relevant passages based on whether they are useful for a trained QA model to arrive at the correct answers, and develops a search process guided by the QAmodel’s loss that generalizes better to the end QA task.","score":4},{"url":"https://www.semanticscholar.org/paper/dbeff5429ff0caa85f9e02621928e787e789ca2b","title":"Hey AI, Can You Solve Complex Tasks by Talking to Agents?","venue":"Findings","year":2021,"referenceCount":61,"citationCount":6,"influentialCitationCount":0,"publicationDate":"16/10/2021","authors":"Tushar Khot,Kyle Richardson,Daniel Khashabi,Ashish Sabharwal","id":"dbeff5429ff0caa85f9e02621928e787e789ca2b","summary":"A synthetic benchmark, CommaQA, with three complex reasoning tasks designed to be solved by communicating with existing QA agents, showing that black-box models struggle to learn this task from scratch even with access to each agent’s knowledge and gold facts supervision.","score":4},{"url":"https://www.semanticscholar.org/paper/5c80c7c898bd880d65ee1565f627acd454bd7413","title":"Adaptive Discount Factor for Deep Reinforcement Learning in Continuing Tasks with Uncertainty","venue":"Italian National Conference on Sensors","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/09/2022","authors":"Myeongseop Kim,Jung-Su Kim,Myoung-Su Choi,Jae-Han Park","id":"5c80c7c898bd880d65ee1565f627acd454bd7413","summary":"The proposed adaptive discount factor automatically finds a discount factor that leads to comparable training performance, and that can be applied to representative deep reinforcement learning problems.","score":4},{"url":"https://www.semanticscholar.org/paper/522ac9eb08bb0c5a422700bb254ea1c44e9157de","title":"Discovered Policy Optimisation","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":3,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Chris Xiaoxuan Lu,J. Kuba,Alistair Letcher,Luke Metz,C. S. D. Witt,J. Foerster","id":"522ac9eb08bb0c5a422700bb254ea1c44e9157de","summary":"By analysing LPO, this paper gains original insights into policy optimisation which are used to formulate a novel, closed-form RL algorithm, Discovered Policy Optimisation (DPO), and explores the Mirror Learning space by meta-learning a “drift” function.","score":4},{"url":"https://www.semanticscholar.org/paper/3e5b3258df48dd41f53ffbd76ea391775b1d9dd3","title":"Policy Gradient With Serial Markov Chain Reasoning","venue":"ArXiv","year":2022,"referenceCount":96,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Edoardo Cetin,O. Çeliktutan","id":"3e5b3258df48dd41f53ffbd76ea391775b1d9dd3","summary":"A new framework that performs decision-making in reinforcement learning (RL) as an iterative reasoning process, optimized with a new tractable estimate of the policy gradient that allows agent behavior to approximate any continuous distribution over actions by parameterizing the RMC with a simple Gaussian transition function.","score":4},{"url":"https://www.semanticscholar.org/paper/8a79ac36b26348c1293433b68bc5e538706f321b","title":"Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion","venue":"ArXiv","year":2022,"referenceCount":63,"citationCount":3,"influentialCitationCount":0,"publicationDate":"18/10/2022","authors":"Zipeng Fu,Xuxin Cheng,Deepak Pathak","id":"8a79ac36b26348c1293433b68bc5e538706f321b","summary":"This work proposes to learn a uniﬁed policy for whole-body control of a legged manipulator using reinforcement learning, Regularized Online Adaptation to bridge the Sim2Real gap for high-DoF control, and Advantage Mixing exploiting the causal dependency in the action space to overcome local minima during training the whole- body system.","score":4},{"url":"https://www.semanticscholar.org/paper/d32855c1af70db05db88615508fb063055e1b9ad","title":"DIAMBRA Arena: a New Reinforcement Learning Platform for Research and Experimentation","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/10/2022","authors":"A. Palmas","id":"d32855c1af70db05db88615508fb063055e1b9ad","summary":"DIAMBRA Arena is presented, a new platform for reinforcement learning research and experimentation, featuring a collection of high-quality environments exposing a Python API fully compliant with OpenAI Gym standard, and software capabilities are demonstrated by successfully training multiple deep reinforcement learning agents with proximal policy optimization obtaining human-like behavior.","score":4},{"url":"https://www.semanticscholar.org/paper/ddbc085f17efb92a90b1e1038ff9fe15e464262d","title":"Keeping Humans in the Loop: Teaching via Feedback in Continuous Action Space Environments","venue":"IEEE/RJS International Conference on Intelligent RObots and Systems","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"Isaac S. Sheidlower,Allison Moore,Elaine Schaertl Short","id":"ddbc085f17efb92a90b1e1038ff9fe15e464262d","summary":"Continuous Action-space Interactive Reinforcement learning (CAIR) is presented: the first continuous action-space IntRL algorithm that is capable of using teacher feedback to out-perform state-of-the-art RL algorithms in those tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/ca365e7a6bb383287bfcbc39720bd572e25e6eca","title":"Global and Local Analysis of Interestingness for Competency-Aware Deep Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/11/2022","authors":"Pedro Sequeira,Jesse Hostetler,M. Gervasio","id":"ca365e7a6bb383287bfcbc39720bd572e25e6eca","summary":"A recently-proposed framework for explainable RL that is based on anal- yses of “interestingness” is extended, providing various measures of RL agent competence stemming from inter- estingness analysis and is applicable to a wide range of RL algorithms.","score":4},{"url":"https://www.semanticscholar.org/paper/9a9779825a6706d985232d5ae2946bab3214ebde","title":"From deterministic to stochastic: an interpretable stochastic model-free reinforcement learning framework for portfolio optimization","venue":"Applied intelligence (Boston)","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/11/2022","authors":"Zitao Song,Yining Wang,Pin Qian,Sifan Song,Frans Coenen,Zhengyong Jiang,Jionglong Su","id":"9a9779825a6706d985232d5ae2946bab3214ebde","summary":"This work proposes a novel interpretable stochastic reinforcement learning framework which tailors a Stochastic policy parameterized by Gaussian Mixtures and a distributional critic realized by quantiles for the problem of portfolio optimization.","score":4},{"url":"https://www.semanticscholar.org/paper/158b8bdfbb8542ba949a513ab02efa229fe71e43","title":"Shortest-Path-Based Deep Reinforcement Learning for EV Charging Routing Under Stochastic Traffic Condition and Electricity Prices","venue":"IEEE Internet of Things Journal","year":2022,"referenceCount":50,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Jiangliang Jin,Yunjian Xu","id":"158b8bdfbb8542ba949a513ab02efa229fe71e43","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/1711bda8fbf86301c9297b2925419fd8680cd43f","title":"Simultaneously Updating All Persistence Values in Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Luca Sabbioni,Luca Al Daire,L. Bisi,A. Metelli,Marcello Restelli","id":"1711bda8fbf86301c9297b2925419fd8680cd43f","summary":"This work derives a novel All-Persistence Bellman Operator, which allows an effective use of both the low-persistence experience, by decomposition into sub-transition, and the high-persistent experience, thanks to the introduction of a suitable bootstrap procedure.","score":4},{"url":"https://www.semanticscholar.org/paper/b8ba8aebb5f222d7ec93cc3c737a4e88dcf3fd8e","title":"Learning to Box: Reinforcement Learning using Heuristic Three-step Curriculum Learning","venue":"International Conference on Control, Automation and Systems","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/11/2022","authors":"Heeseon Rho,Yeonguk Yu,Kyoobin Lee","id":"b8ba8aebb5f222d7ec93cc3c737a4e88dcf3fd8e","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/dc40ab903403d46fea6d0bef0dcee92498facbb0","title":"Domain Knowledge-Assisted Deep Reinforcement Learning Power Allocation for MIMO Radar Detection","venue":"IEEE Sensors Journal","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Yuedong Wang,Yan Liang,Huixia Zhang,Yijing Gu","id":"dc40ab903403d46fea6d0bef0dcee92498facbb0","summary":"A domain-knowledge-assisted DRL (DKADRL) framework in which a domain- knowledge-based timely reward generator is utilized to generate timely rewards that assist the agent’s policy learning is proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/e449f48a676af35e0a22c6251fd3bbfd3960454a","title":"Adaptive Cooperative Exploration for Reinforcement Learning from Imperfect Demonstrations","venue":"Pattern Recognition Letters","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Fuxian Huang,Naye Ji,Huajian Ni,Shijian Li,Xi Li","id":"e449f48a676af35e0a22c6251fd3bbfd3960454a","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/e5f567508a6b1a135f4258411f9b4ea48a49b379","title":"A Deep Reinforcement Learning Approach for Airport Departure Metering Under Spatial–Temporal Airside Interactions","venue":"IEEE transactions on intelligent transportation systems (Print)","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Hasnain Ali,D. Pham,S. Alam,M. Schultz","id":"e5f567508a6b1a135f4258411f9b4ea48a49b379","summary":"Results, on a typical day of simulated operations at Singapore Changi Airport, demonstrate that DRL can learn an effective DM policy to contain congestion on the taxiways, reduce total fuel consumption and better manage the airside traffic.","score":4},{"url":"https://www.semanticscholar.org/paper/a49d00573720f58e775aefa8b9056900cd9ba77e","title":"Multi-Robot Real-time Game Strategy Learning based on Deep Reinforcement Learning","venue":"IEEE International Conference on Robotics and Biomimetics","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Ki Deng,Yanjie Li,Songshuo Lu,Yongjin Mu,Xizheng Pang,Qi Liu","id":"a49d00573720f58e775aefa8b9056900cd9ba77e","summary":"A reinforcement-learning-based multi-robot control method that enhanced the reinforcement learning algorithm PPO with the self-play method by easy-to-implement tricks for multi-agent training and improve training efficiency is proposed to improve the strategy performance.","score":4},{"url":"https://www.semanticscholar.org/paper/5cef819106a386fe876a868b52d933e98e5c32e4","title":"Leveraging Efficiency through Hybrid Prioritized Experience Replay in Door Environment","venue":"IEEE International Conference on Robotics and Biomimetics","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Haofu Qian,Shiqiang Zhu,Hongjiang Ge,Haolei Shi,Jianfeng Liao,Wei Song,Jianjun Gu","id":"5cef819106a386fe876a868b52d933e98e5c32e4","summary":"A method called Proximal Policy Optimization with Hybrid Prioritized Experience Replay (HPER-PPO) to adjust the sample priority and guide the selection, through which the policy can be better optimized and the cumulative reward can be maximized.","score":4},{"url":"https://www.semanticscholar.org/paper/13b120fad7ebe58f4d2257a1ee7901c30219feaf","title":"Policy Transfer via Enhanced Action Space","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/12/2022","authors":"Zheng Zhang,Qingrui Zhang,Bo Zhu,Xiaohan Wang,Tianjiang Hu","id":"13b120fad7ebe58f4d2257a1ee7901c30219feaf","summary":"A novel algorithm named EASpace (Enhanced Action Space) is proposed in this paper to transfer the knowledge of multiple sub-optimal expert policies to improve the data efﬁciency and alleviate the non-stationarity issue in multi-agent settings.","score":4},{"url":"https://www.semanticscholar.org/paper/8b211a7e6ccff5cc8cd482da51761ecc9d21662c","title":"An immediate-return reinforcement learning for the atypical Markov decision processes","venue":"Frontiers in Neurorobotics","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"Zebang Pan,G. Wen,Zhao Tan,Shan Yin,Xiaoyan Hu","id":"8b211a7e6ccff5cc8cd482da51761ecc9d21662c","summary":"This paper proposes an immediate-return algorithm for the atypical MDPs with continuous action space by designing an unbiased and low variance target Q-value and a simplified network framework and shows significant advantages in learning efficiency, the effective rate of control, and computing resource usage.","score":4},{"url":"https://www.semanticscholar.org/paper/3d3b0704c61d47c7bafb70ae2670b2786b8e4d81","title":"Efficient Exploration in Resource-Restricted Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/12/2022","authors":"Zhihai Wang,Taoxing Pan,Qi Zhou,Jie Wang","id":"3d3b0704c61d47c7bafb70ae2670b2786b8e4d81","summary":"Experiments demonstrate that the proposed RAEB outperforms state-of-the-art exploration strategies in resource-restricted reinforce- ment learning environments, improving the sample efﬁciency by up to an order of magnitude.","score":4},{"url":"https://www.semanticscholar.org/paper/991d16740d6aabcaa11de3241037ad6c3fced3f2","title":"Design and control of soft biomimetic pangasius fish robot using fin ray effect and reinforcement learning","venue":"Scientific Reports","year":2022,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Samuel M Youssef,Mennaallah Soliman,M. A. Saleh,Ahmed H. Elsayed,A. Radwan","id":"991d16740d6aabcaa11de3241037ad6c3fced3f2","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/bd6be9e58ed58aad21b2f334c3a8c18fd3072786","title":"On Deep Recurrent Reinforcement Learning for Active Visual Tracking of Space Noncooperative Objects","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/12/2022","authors":"D. Zhou,Guanghui Sun,Zhao-jie Zhang,Ligang Wu","id":"bd6be9e58ed58aad21b2f334c3a8c18fd3072786","summary":"A novel tracker based on deep recur- rent reinforcement learning, named as RAMAVT which drives the chasing spacecraft to follow arbitrary space noncooperative object with high-frequency and near-optimal velocity control commands is proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/b3758ef7307842ea5df27c9cdca58cff03d471a1","title":"Sim-to-real via latent prediction: Transferring visual non-prehensile manipulation policies","venue":"Frontiers in Robotics and AI","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2023","authors":"Carlo Rizzardo,Fei Chen,Darwin Caldwell","id":"b3758ef7307842ea5df27c9cdca58cff03d471a1","summary":"This work proposes a sim-to-real technique that trains a Soft-Actor Critic agent together with a decoupled feature extractor and a latent-space dynamics model, and shows how this architecture can allow the transfer of a trained agent from simulation to reality without retraining or finetuning the control policy, but using real-world data only for adapting the feature Extractor.","score":4},{"url":"https://www.semanticscholar.org/paper/e408af9c5f68beb9838532405cd0abba86681cb9","title":"Offloading Mechanisms Based on Reinforcement Learning and Deep Learning Algorithms in the Fog Computing Environment","venue":"IEEE Access","year":2023,"referenceCount":141,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Dezheen Abdulazeez,Shavan K. Askar","id":"e408af9c5f68beb9838532405cd0abba86681cb9","summary":"A systematic analysis of using RL or DRL algorithms to address offloading-related issues in fog computing is presented and the future research directions and open issues are discussed thoroughly.","score":4},{"url":"https://www.semanticscholar.org/paper/dd5c088f13c44a45a386a072b3e2ebb65d43e304","title":"SMIX(λ): Enhancing Centralized Value Functions for Cooperative Multiagent Reinforcement Learning","venue":"IEEE Transactions on Neural Networks and Learning Systems","year":2020,"referenceCount":47,"citationCount":18,"influentialCitationCount":2,"publicationDate":"03/04/2020","authors":"Xinghu Yao,Chao Wen,Yuhui Wang,Xiaoyang Tan","id":"dd5c088f13c44a45a386a072b3e2ebb65d43e304","summary":"This article proposes an approach, named SMIX, that uses an OFF-policy training to achieve this by avoiding the greedy assumption commonly made in CVF learning and can be used as a general tool to improve the overall performance of other centralized training with decentralized execution (CTDE)-type algorithms by enhancing their CVFs.","score":4},{"url":"https://www.semanticscholar.org/paper/24a4f8bf9725477097453d895ca3390fa011d3b3","title":"Phy-Q as a measure for physical reasoning intelligence","venue":"Nature Machine Intelligence","year":2021,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/08/2021","authors":"Cheng Xue,Vimukthini Pinto,C. Gamage,Ekaterina Nikonova,Peng Zhang,Jochen Renz","id":"24a4f8bf9725477097453d895ca3390fa011d3b3","summary":"A new testbed that requires an agent to reason about physical scenarios and take an action appropriately is proposed that is based on a two-dimensional physics environment and encourages the development of intelligent agents that can reach the human-level Phy-Q score.","score":4},{"url":"https://www.semanticscholar.org/paper/1395a90c8affba1b29e5dc12d12f1950b4c112fa","title":"Reinforcement learning for automatic quadrilateral mesh generation: a soft actor-critic approach","venue":"Neural Networks","year":2022,"referenceCount":79,"citationCount":4,"influentialCitationCount":1,"publicationDate":"19/03/2022","authors":"J. Pan,Jingwei Huang,G. Cheng,Yong Zeng","id":"1395a90c8affba1b29e5dc12d12f1950b4c112fa","summary":"A reinforcement learning (RL) algorithm called \"soft actor-critic\" to automatically learn from trials the policy of actions for mesh generation allows to build a fully automatic mesh generation system without human intervention and any extra clean-up operations, which fills the gap in the existing mesh generation tools.","score":4},{"url":"https://www.semanticscholar.org/paper/472babf52b449c510e5b3777c3ee45abb5135e43","title":"A Comparison of Deep Reinforcement Learning Models for Isolated Traffic Signal Control","venue":"IEEE Intelligent Transportation Systems Magazine","year":2023,"referenceCount":55,"citationCount":3,"influentialCitationCount":1,"publicationDate":"01/01/2023","authors":"Feng Mao,Zhiheng Li,Li Li","id":"472babf52b449c510e5b3777c3ee45abb5135e43","summary":"Testing results indicate that the soft actor–critic (SAC) outperforms other DRL algorithms and the maximum pressure method in most cases and has enlightening effects on other traffic decision management problems, such as ramp and multi-intersection control.","score":4},{"url":"https://www.semanticscholar.org/paper/5aebd1906852fc6c4ced4a336bf131b985c99287","title":"Fully Automated Design Method Based on Reinforcement Learning and Surrogate Modeling for Antenna Array Decoupling","venue":"IEEE Transactions on Antennas and Propagation","year":2023,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Zhaohui Wei,Zhao Zhou,Peng Wang,Jian-lin Ren,Ying-Zheng Yin,G. Pedersen,Ming Shen","id":"5aebd1906852fc6c4ced4a336bf131b985c99287","summary":"The decoupling metasurfaces designed by the proposed fully automated method based on RL showed satisfactory results comparable to the results achievable by human designers, indicating that the proposed method can be used to build powerful tools to boost the design efficiency of EM devices.","score":4},{"url":"https://www.semanticscholar.org/paper/397156af1b4fc1f1c3bbbe5c2dbc698ef0b9b6ec","title":"Policy Pre-training for End-to-end Autonomous Driving via Self-supervised Geometric Modeling","venue":"ArXiv","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/01/2023","authors":"Peng Wu,Li Chen,Hongyang Li,Xiaosong Jia,Junchi Yan,Y. Qiao","id":"397156af1b4fc1f1c3bbbe5c2dbc698ef0b9b6ec","summary":"PPGeo (Policy Pre-training via Geometric modeling), an intuitive and straightforward fully self-supervised framework curated for the policy pre-training in visuomotor driving, aimed at learning policy representations as a powerful abstraction by modeling 3D geometric scenes on large-scale unlabeled and uncalibrated YouTube driving videos.","score":4},{"url":"https://www.semanticscholar.org/paper/6eba2f014a17b26e15d251463b8e9dd1dbda2d3d","title":"Centralized Cooperative Exploration Policy for Continuous Control Tasks","venue":"ArXiv","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/01/2023","authors":"C. Li,Chen Gong,Qiang He,Xinwen Hou,Yu Liu","id":"6eba2f014a17b26e15d251463b8e9dd1dbda2d3d","summary":"This work proposes CCEP, which utilizes underestimation and overestimation of value functions to maintain the capacity of exploration and ensures it outperforms the current state-of-the-art methods across multiple continuous control tasks shown in experiments.","score":4},{"url":"https://www.semanticscholar.org/paper/6074a6b7262be53f3501b461ff2a6bfd8082d02d","title":"PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets","venue":"ArXiv","year":2023,"referenceCount":80,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/01/2023","authors":"Shuo Sun,Molei Qin,Xinrun Wang,Bo An","id":"6074a6b7262be53f3501b461ff2a6bfd8082d02d","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/1860740f211064e430dae614a771df256f1e3685","title":"Neuro-Symbolic World Models for Adapting to Open World Novelty","venue":"ArXiv","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/01/2023","authors":"Jonathan C. Balloch,Zhiyu Lin,R. Wright,Xiangyu Peng,Mustafa Hussain,Aarun Srinivas,Julia Kim,Mark O. Riedl","id":"1860740f211064e430dae614a771df256f1e3685","summary":"WorldCloner is intro-duce, an end-to-end trainable neuro-symbolic world model for rapid novelty adaptation that helps its policy adapt more efficiently than neural-only reinforcement learning methods.","score":4},{"url":"https://www.semanticscholar.org/paper/2ba32d52efa4e08567b5a44f9c9e42fa1a854dec","title":"Improving Monte Carlo Evaluation with Offline Data","venue":"ArXiv","year":2023,"referenceCount":76,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Shuze Liu,Shangtong Zhang","id":"2ba32d52efa4e08567b5a44f9c9e42fa1a854dec","summary":"This work designs a tailored behavior policy such that the variance of the oﬀ-policy MC estimator is provably smaller than the ordinaryMC estimator, and can be eﬃciently learned from existing data, i,e.","score":4},{"url":"https://www.semanticscholar.org/paper/54e3065e2cfdc9d30d1a190dafb1c80335e88662","title":"Learning Cut Selection for Mixed-Integer Linear Programming via Hierarchical Sequence Model","venue":"ArXiv","year":2023,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Zhihai Wang,Xijun Li,Jie Wang,Yufei Kuang,M. Yuan,Jianguo Zeng,Yongdong Zhang,Feng Wu","id":"54e3065e2cfdc9d30d1a190dafb1c80335e88662","summary":"A novel hierarchical sequence model (HEM) is proposed to learn cut selection policies via reinforcement learning that significantly improves the efficiency of solving MILPs compared to human-designed and learning-based baselines on both synthetic and large-scale real-world MILPs, including MIPLIB 2017.","score":4},{"url":"https://www.semanticscholar.org/paper/c199d061ac01d731b216a6dbb9bd51457b42fc5d","title":"Diversity Induced Environment Design via Self-Play","venue":"ArXiv","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/02/2023","authors":"Dexun Li,WenJu Sun,Pradeep Varakantham","id":"c199d061ac01d731b216a6dbb9bd51457b42fc5d","summary":"This paper proposes a task-agnostic method to identify observed/hidden states that are representative of a given level and incorporates the self-play technique that allows the environment generator to automatically generate environments that are of great benefit to the training agent.","score":4},{"url":"https://www.semanticscholar.org/paper/21d97df47384fb355cd23fedbb89c502a1e0a884","title":"Sample Dropout: A Simple yet Effective Variance Reduction Technique in Deep Policy Optimization","venue":"ArXiv","year":2023,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/02/2023","authors":"Zichuan Lin,Xiapeng Wu,Mingfei Sun,Deheng Ye,Qiang Fu,Wei Yang,Wei Liu","id":"21d97df47384fb355cd23fedbb89c502a1e0a884","summary":"This paper shows in a principled way that the variance of importance sampling estimate grows quadratically with importance ratios and the large ratios could consequently jeopardize the effectiveness of surrogate objective optimization, and proposes a technique called sample dropout to bound the estimation variance by dropping out samples when their ratio deviation is too high.","score":4},{"url":"https://www.semanticscholar.org/paper/fbf48e013c963aa90fa3bfd04604935d952d674a","title":"Unlabeled Imperfect Demonstrations in Adversarial Imitation Learning","venue":"","year":2023,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Yunke Wang,Bo Du,Changming Xu","id":"fbf48e013c963aa90fa3bfd04604935d952d674a","summary":"A positive-unlabeled adversarial imitation learning algorithm is developed to dynamically sample expert demonstrations that can well match the trajectories from the constantly optimized agent policy.","score":4},{"url":"https://www.semanticscholar.org/paper/32b3f9220a72fc7eecbd78dc1fabe7f8823091e1","title":"EdgeMatrix: A Resource-Redefined Scheduling Framework for SLA-Guaranteed Multi-Tier Edge-Cloud Computing Systems","venue":"IEEE Journal on Selected Areas in Communications","year":2023,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/03/2023","authors":"Shihao Shen,Yuanming Ren,Yanli Ju,Xiaofei Wang,Wenyu Wang,Victor C. M. Leung","id":"32b3f9220a72fc7eecbd78dc1fabe7f8823091e1","summary":"A multi-tier edge-cloud computing framework, EdgeMatrix, is proposed to maximize the throughput of the system while guaranteeing different SLA priorities, and a multi-task mechanism is designed in EdgeMatrix to solve the problem of Joint Service Orchestration and Request Dispatch.","score":4},{"url":"https://www.semanticscholar.org/paper/1ec9c5004e62f709abff4185c5c77747054a2ab4","title":"A Deep Reinforcement Learning Approach to Supply Chain Inventory Management","venue":"","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/04/2022","authors":"Francesco Stranieri,Fabio Stella","id":"1ec9c5004e62f709abff4185c5c77747054a2ab4","summary":"Numerical experiments show that DRL performs consistently better than standard reorder policies, such as the static ( s , Q )-policy, and can be considered a practical and effective option for solving real-world instances of the stochastic two-echelon SCIM problem.","score":4},{"url":"https://www.semanticscholar.org/paper/525934efb9ea7ac84de9d504f6a16ee54f936815","title":"Deep reinforcement learning applied to an assembly sequence planning problem with user preferences","venue":"The International Journal of Advanced Manufacturing Technology","year":2022,"referenceCount":32,"citationCount":2,"influentialCitationCount":0,"publicationDate":"05/08/2022","authors":"M. Neves,P. Neto","id":"525934efb9ea7ac84de9d504f6a16ee54f936815","summary":"An approach to the implementation of DRL methods in assembly sequence planning problem, which introduces in the RL environment parametric actions to improve training time and sample efficiency and uses two different reward signals: user’s preferences and total assembly time duration.","score":4},{"url":"https://www.semanticscholar.org/paper/052e9eae2c404ed85639265dcb9688c24ac13683","title":"Dependent Task Offloading for Edge Computing based on Deep Reinforcement Learning","venue":"IEEE transactions on computers","year":2022,"referenceCount":39,"citationCount":15,"influentialCitationCount":1,"publicationDate":"01/10/2022","authors":"Jin Wang,Jia Hu,G. Min,Wenhan Zhan,Albert Y. Zomaya,N. Georgalas","id":"052e9eae2c404ed85639265dcb9688c24ac13683","summary":"This work proposes an intelligent task offloading scheme leveraging off-policy reinforcement learning empowered by a Sequence-to-Sequence (S2S) neural network, where the dependent tasks are represented by a Directed Acyclic Graph (DAG) to improve the training efficiency.","score":4},{"url":"https://www.semanticscholar.org/paper/a826d00e7f1035d67b915941b409b5b26673b9fd","title":"Spatial-Temporal Aligned Multi-Agent Learning for Visual Dialog Systems","venue":"ACM Multimedia","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Yong Zhuang,Tong Yu,Junda Wu,Shiqu Wu,Shuai Li","id":"a826d00e7f1035d67b915941b409b5b26673b9fd","summary":"A novel spatial-temporal aligned sta multi-agent reinforcement learning framework is proposed to better align the multimodal data within and between agents over time and develops sample-efficient visual dialog systems.","score":4},{"url":"https://www.semanticscholar.org/paper/fe929e3210fec3d4fcaa35eb93792d6e6de69e9c","title":"Climate Change Policy Exploration using Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"Theodore Wolf","id":"fe929e3210fec3d4fcaa35eb93792d6e6de69e9c","summary":"It is found that in this simplistic model, the growth of the economy is a significant feature for the agents when deciding which policies to enact, and there is much more to be done for this framework to be applicable to climate related policy.","score":4},{"url":"https://www.semanticscholar.org/paper/2a3b65244350fdd9051d14f499a6773a5b0f2cb0","title":"Geometry and convergence of natural policy gradient methods","venue":"ArXiv","year":2022,"referenceCount":75,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/11/2022","authors":"Johannes Muller,Guido Montúfar","id":"2a3b65244350fdd9051d14f499a6773a5b0f2cb0","summary":"This work shows linear convergence for unregularized and regularized NPGs and reward functions with the metrics proposed by Kakade and Morimura and co-authors by observing that these arise from the Hessian geometries of conditional entropy and entropy respectively.","score":4},{"url":"https://www.semanticscholar.org/paper/cfd232ade1fdee8f00d90a0c1e6148b8ee530e29","title":"Choreographer: Learning and Adapting Skills in Imagination","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/11/2022","authors":"Pietro Mazzaglia,T. Verbelen,B. Dhoedt,Alexandre Lacoste,Sai Rajeswar","id":"cfd232ade1fdee8f00d90a0c1e6148b8ee530e29","summary":"This work presents Choreographer, a model-based agent that exploits its world model to learn and adapt skills in imagination, and decouples the exploration and skill learning processes, being able to discover skills in the latent state space of the model.","score":4},{"url":"https://www.semanticscholar.org/paper/a2fcc41996acbd70cd87cc7d27d8cb1f6ebfc6ee","title":"Analysis of Measure-Valued Derivatives in a Reinforcement Learning Actor-Critic Framework","venue":"Online World Conference on Soft Computing in Industrial Applications","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/12/2022","authors":"Kim van den Houten,Emile van Krieken,B. Heidergott","id":"a2fcc41996acbd70cd87cc7d27d8cb1f6ebfc6ee","summary":"The empirical results of this study suggest that measure-valued derivatives can serve as low-variance alternative to score functions in on-policy actor-critic and indeed reduce the need for variance reduction techniques.","score":4},{"url":"https://www.semanticscholar.org/paper/1558a92344ab38b5b650d703166c0c92bc565f55","title":"Understanding the Complexity Gains of Single-Task RL with a Curriculum","venue":"ArXiv","year":2022,"referenceCount":106,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/12/2022","authors":"Qiyang Li,Yuexiang Zhai,Yi Ma,S. Levine","id":"1558a92344ab38b5b650d703166c0c92bc565f55","summary":"Under mild regularity conditions on the curriculum, it is shown that sequentially solving each task in the multi-task RL problem is more computationally efficient than solving the original single-task problem, without any explicit exploration bonuses or other exploration strategies.","score":4},{"url":"https://www.semanticscholar.org/paper/9b448ed51816d6226fc616d6c76ae43bb5649db8","title":"Robustness Analysis and Enhancement of Deep Reinforcement Learning-Based Schedulers","venue":"IEEE Transactions on Parallel and Distributed Systems","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Shaojun Zhang,Chen Wang,Albert Y. Zomaya","id":"9b448ed51816d6226fc616d6c76ae43bb5649db8","summary":"The black-box perturbation system is devised, in which, a proxy model is trained to mimic the DRL-based scheduling policy and it is shown that the high-faith proxy model can help to craft effective perturbations.","score":4},{"url":"https://www.semanticscholar.org/paper/50cfa8f4f98be6435fee58df1f876c1b95db083c","title":"Quasi-optimal Learning with Continuous Treatments","venue":"ArXiv","year":2023,"referenceCount":99,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/01/2023","authors":"Yuhan Li,Wenzhuo Zhou,R. Zhu","id":"50cfa8f4f98be6435fee58df1f876c1b95db083c","summary":"A novel quasi-optimal learning algorithm is developed which can be easily optimized in off-policy settings with guaranteed convergence under general function approximations and is evaluated with comprehensive simulated experiments and a dose suggestion real application to Ohio Type 1 diabetes dataset.","score":4},{"url":"https://www.semanticscholar.org/paper/15474fb68ce1451e9099fcba6631d7b8de3a5452","title":"Beyond Exponentially Fast Mixing in Average-Reward Reinforcement Learning via Multi-Level Monte Carlo Actor-Critic","venue":"ArXiv","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/01/2023","authors":"Wesley A. Suttle,A. S. Bedi,Bhrij Patel,Brian M. Sadler,Alec Koppel,Dinesh Manocha","id":"15474fb68ce1451e9099fcba6631d7b8de3a5452","summary":"This work proposes an RL methodology attuned to the mixing time by employing a multi-level Monte Carlo estimator for the critic, the actor, and the average reward embedded within an actor-critic (AC) algorithm and achieves a convergence rate comparable to the state-of-the-art AC algorithms.","score":4},{"url":"https://www.semanticscholar.org/paper/da6fd7839d15e956f095441cb52e01c2446c0ace","title":"OSTTD: Offloading of Splittable Tasks With Topological Dependence in Multi-Tier Computing Networks","venue":"IEEE Journal on Selected Areas in Communications","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Rui Zhang,Xuesen Chu,Ruhui Ma,Meng Zhang,Liwei Lin,Honghao Gao,Haibing Guan","id":"da6fd7839d15e956f095441cb52e01c2446c0ace","summary":"OSTTD is the first method in which the topological dependence among sub-tasks of the splittable task is fully considered and can significantly reduce the task processing time, thus, improving the overall task processing efficiency in multi-tier computing networks.","score":4},{"url":"https://www.semanticscholar.org/paper/71b2cf7e5516ca8a3a245cd61be2032ca259380a","title":"On the Robustness of Question Rewriting Systems to Questions of Varying Hardness","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":39,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Hai Ye,H. Ng,Wenjuan Han","id":"71b2cf7e5516ca8a3a245cd61be2032ca259380a","summary":"To enhance the robustness of QR systems to questions of varying hardness, a novel learning framework for QR is proposed that first trains a QR model independently on each subset of questions of a certain level of hardness, then combines these QR models as one joint model for inference.","score":4},{"url":"https://www.semanticscholar.org/paper/4e76896393ef578807d3428f7a7d820cf30b58ca","title":"Adaptable Claim Rewriting with Offline Reinforcement Learning for Effective Misinformation Discovery","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Ashkan Kazemi,Artem Abzaliev,Naihao Deng,Rui Hou,Davis Liang,Scott A. Hale,Verónica Pérez-Rosas,Rada Mihalcea","id":"4e76896393ef578807d3428f7a7d820cf30b58ca","summary":"A novel system to help fact-checkers formulate search queries for known misinformation claims and effectively search across multiple social media platforms using an adaptable rewriting strategy that uses a decision transformer to learn a sequence of editing actions that maximize query retrieval metrics such as mean average precision.","score":4},{"url":"https://www.semanticscholar.org/paper/89e2154d608cc8eced17bd7b276e278c89f8f0c1","title":"Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding","venue":"ArXiv","year":2020,"referenceCount":44,"citationCount":19,"influentialCitationCount":6,"publicationDate":"13/09/2020","authors":"Shuohang Wang,Luowei Zhou,Zhe Gan,Yen-Chun Chen,Yuwei Fang,S. Sun,Yu Cheng,Jingjing Liu","id":"89e2154d608cc8eced17bd7b276e278c89f8f0c1","summary":"Cluster-Former is proposed, a novel clustering-based sparse Transformer to perform attention across chunked sequences that allows information integration beyond local windows, which is especially beneficial for question answering (QA) and language modeling tasks that rely on long-range dependencies.","score":4},{"url":"https://www.semanticscholar.org/paper/42aa8ae1eb726b0bcc6aca785eab6132a447c8f4","title":"Is Retriever Merely an Approximator of Reader?","venue":"ArXiv","year":2020,"referenceCount":34,"citationCount":31,"influentialCitationCount":4,"publicationDate":"28/09/2020","authors":"Sohee Yang,Minjoon Seo","id":"42aa8ae1eb726b0bcc6aca785eab6132a447c8f4","summary":"This work makes a careful conjecture that the architectural constraint of the retriever, which has been originally intended for enabling approximate search, seems to also make the model more robust in large-scale search, and proposes to distill the reader into the retriver so that the retrivers absorbs the strength of the reader while keeping its own benefit.","score":4},{"url":"https://www.semanticscholar.org/paper/4a9a2b64ced3cee261bece01c429083708d77ecb","title":"Retrieve, Rerank, Read, then Iterate: Answering Open-Domain Questions of Arbitrary Complexity from Text","venue":"ArXiv","year":2020,"referenceCount":45,"citationCount":14,"influentialCitationCount":1,"publicationDate":"23/10/2020","authors":"Peng Qi,Haejun Lee,OghenetegiriTGSido,Christopher D. Manning","id":"4a9a2b64ced3cee261bece01c429083708d77ecb","summary":"This work proposes a unified system to answer open-domain questions of arbitrary complexity directly from text that works with off-the-shelf retrieval systems on arbitrary text collections and achieves strong performance on a new unified benchmark.","score":4},{"url":"https://www.semanticscholar.org/paper/c6ba01a86927c3abb891ed268353c14f9faa8097","title":"End-to-End QA on COVID-19: Domain Adaptation with Synthetic Training","venue":"ArXiv","year":2020,"referenceCount":46,"citationCount":15,"influentialCitationCount":2,"publicationDate":"02/12/2020","authors":"R. Reddy,Bhavani Iyer,Md Arafat Sultan,Rong Zhang,Avirup Sil,Vittorio Castelli,Radu Florian,S. Roukos","id":"c6ba01a86927c3abb891ed268353c14f9faa8097","summary":"This work combines neural IR and MRC systems and shows significant improvements in end-to-end QA on the CORD-19 collection over a state-of-the-art open-domain QA baseline.","score":4},{"url":"https://www.semanticscholar.org/paper/b59d64ba042afbc198a806954517007074213b86","title":"Big Bird: Transformers for Longer Sequences – Appendix","venue":"","year":2021,"referenceCount":113,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"","id":"b59d64ba042afbc198a806954517007074213b86","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/e47d338ca879a184c1977ffa8623d2a225b0a319","title":"Multi-Task Dense Retrieval via Model Uncertainty Fusion for Open-Domain Question Answering","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":45,"citationCount":5,"influentialCitationCount":0,"publicationDate":2021,"authors":"Minghan Li,Ming Li,Kun Xiong,Jimmy J. Lin","id":"e47d338ca879a184c1977ffa8623d2a225b0a319","summary":"This work proposes to train individual dense passage retrievers (DPR) for different tasks and aggregate their predictions during test time, where they use uncertainty estimation as weights to in-dicate how probable a speciﬁc query belongs to each expert’s expertise.","score":4},{"url":"https://www.semanticscholar.org/paper/77db3d91786935f91656d4193a742220651f869f","title":"Multi-stage Training with Improved Negative Contrast for Neural Passage Retrieval","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":32,"citationCount":12,"influentialCitationCount":2,"publicationDate":2021,"authors":"Jing-Bin Lu,Gustavo Hernández Ábrego,Ji Ma,Jianmo Ni,Yinfei Yang","id":"77db3d91786935f91656d4193a742220651f869f","summary":"This work proposes a multi-stage framework comprising of pre-training with synthetic data, fine-tuning with labeled data, and negative sampling at both stages, and explores fusion methods that combine negatives from different strategies.","score":4},{"url":"https://www.semanticscholar.org/paper/a1890836dbd0fc7b4e2a50f267e7347491468ae5","title":"D ISTILLING K NOWLEDGE FROM R EADER TO R ETRIEVER FOR Q UESTION A NSWERING","venue":"","year":2021,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Gautier Izacard,Edouard Grave","id":"a1890836dbd0fc7b4e2a50f267e7347491468ae5","summary":"This paper describes a method to train the retriever to reproduce the ranking of documents corresponding to a metric using a database of documents.documents.","score":4},{"url":"https://www.semanticscholar.org/paper/43003de1bdf12e14e917c98807ad0ab244caa923","title":"Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":34,"citationCount":3,"influentialCitationCount":1,"publicationDate":2021,"authors":"Chen Zhao,Chenyan Xiong,Jordan L. Boyd-Graber,Hal Daumé","id":"43003de1bdf12e14e917c98807ad0ab244caa923","summary":"This paper introduces a novel approach that iteratively improves over a weak retriever by alternately finding evidence from the up-to-date model and encouraging the model to learn the most likely evidence, and confirms that DistDR finds more accurate evidence over iterations, which leads to model improvements.","score":4},{"url":"https://www.semanticscholar.org/paper/fe7ea2dea7d24b5c661ea901c14e82c92c5fbe20","title":"MultiReQA: A Cross-Domain Evaluation forRetrieval Question Answering Models","venue":"ADAPTNLP","year":2020,"referenceCount":39,"citationCount":28,"influentialCitationCount":3,"publicationDate":"05/05/2020","authors":"Mandy Guo,Yinfei Yang,Daniel Matthew Cer,Qinlan Shen,Noah Constant","id":"fe7ea2dea7d24b5c661ea901c14e82c92c5fbe20","summary":"This dataset paper presents MultiReQA, a new multi-domain ReQA evaluation suite composed of eight retrieval QA tasks drawn from publicly available QA datasets, which explores systematic retrieval based evaluation and transfer learning across domains over these datasets using a number of strong base-lines.","score":4},{"url":"https://www.semanticscholar.org/paper/c9b8593db099869fe7254aa1fa53f3c9073b0176","title":"Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval","venue":"International Conference on Learning Representations","year":2020,"referenceCount":64,"citationCount":475,"influentialCitationCount":141,"publicationDate":"01/07/2020","authors":"Lee Xiong,Chenyan Xiong,Ye Li,Kwok-Fung Tang,Jialin Liu,Paul Bennett,Junaid Ahmed,Arnold Overwijk","id":"c9b8593db099869fe7254aa1fa53f3c9073b0176","summary":"Approximate nearest neighbor Negative Contrastive Estimation (ANCE) is presented, a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances.","score":4},{"url":"https://www.semanticscholar.org/paper/aea909a04e4d848855afcf8ce8ae55a81a2623e4","title":"SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval","venue":"North American Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":59,"citationCount":35,"influentialCitationCount":7,"publicationDate":"28/09/2020","authors":"Tiancheng Zhao,XiaoPeng Lu,Kyusong Lee","id":"aea909a04e4d848855afcf8ce8ae55a81a2623e4","summary":"SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, CMRC and etc.","score":4},{"url":"https://www.semanticscholar.org/paper/2b4bc49a3b23229a060609380752666b24b435fb","title":"Distilling Knowledge from Reader to Retriever for Question Answering","venue":"International Conference on Learning Representations","year":2020,"referenceCount":47,"citationCount":99,"influentialCitationCount":20,"publicationDate":"08/12/2020","authors":"Gautier Izacard,Edouard Grave","id":"2b4bc49a3b23229a060609380752666b24b435fb","summary":"This paper proposes a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents.","score":4},{"url":"https://www.semanticscholar.org/paper/89181e26874c4ea418937d7a6980d1476d4c0b0b","title":"Multi-Task Retrieval for Knowledge-Intensive Tasks","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":41,"citationCount":38,"influentialCitationCount":4,"publicationDate":"01/01/2021","authors":"Jean Maillard,Vladimir Karpukhin,Fabio Petroni,Wen-tau Yih,Barlas Oğuz,Veselin Stoyanov,Gargi Ghosh","id":"89181e26874c4ea418937d7a6980d1476d4c0b0b","summary":"This work proposes a multi-task trained model for neural retrieval that not only outperforms previous methods in the few-shot setting, but also rivals specialised neural retrievers, even when in-domain training data is abundant.","score":4},{"url":"https://www.semanticscholar.org/paper/17d680f0dc9211492107eac5ff62b08b627f107a","title":"UnitedQA: A Hybrid Approach for Open Domain Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":35,"citationCount":23,"influentialCitationCount":5,"publicationDate":"01/01/2021","authors":"Hao Cheng,Yelong Shen,Xiaodong Liu,Pengcheng He,Weizhu Chen,Jianfeng Gao","id":"17d680f0dc9211492107eac5ff62b08b627f107a","summary":"It is demonstrated that a simple hybrid approach by combining answers from both readers can efficiently take advantages of extractive and generative answer inference strategies and outperforms single models as well as homogeneous ensembles.","score":4},{"url":"https://www.semanticscholar.org/paper/3661a8d4f2f40efc4d60daf16b0465e9cd55cb4b","title":"SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/01/2021","authors":"XiaoPeng Lu,Kyusong Lee,Tiancheng Zhao","id":"3661a8d4f2f40efc4d60daf16b0465e9cd55cb4b","summary":"SF-QA framework modularizes the pipeline open-domain QA system, which makes the task itself easily accessible and reproducible to research groups without enough computing resources.","score":4},{"url":"https://www.semanticscholar.org/paper/5505d608a1d482fdc083796db812379ec1cb8723","title":"Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches","venue":"ArXiv","year":2021,"referenceCount":107,"citationCount":18,"influentialCitationCount":1,"publicationDate":2021,"authors":"Nelson F. Liu,Tony Lee,Robin Jia,Percy Liang","id":"5505d608a1d482fdc083796db812379ec1cb8723","summary":"Small, targeted synthetic benchmarks are constructed that do not resemble natural language, yet have high concurrence with SQuAD, demonstrating that naturalness and size are not necessary for reflecting historical modeling improvements on SQuad.","score":4},{"url":"https://www.semanticscholar.org/paper/870711939dc3b666d0551102e175212ab31be202","title":"Pruning the Index Contents for Memory Efficient Open-Domain QA","venue":"ArXiv","year":2021,"referenceCount":43,"citationCount":5,"influentialCitationCount":0,"publicationDate":"21/02/2021","authors":"Martin Fajcik,Martin Docekal,Karel Ondrej,P. Smrz","id":"870711939dc3b666d0551102e175212ab31be202","summary":"This work presents a simple approach for pruning the contents of a massive index such that the open-domain QA system altogether with index, OS, and library components fits into 6GiB docker image while retaining only 8% of original index contents and losing only 3% EM accuracy1.","score":4},{"url":"https://www.semanticscholar.org/paper/0994da8e3d70ce64e4ebcade3a592caa513161af","title":"Towards Robust Neural Retrieval Models with Synthetic Pre-Training","venue":"ArXiv","year":2021,"referenceCount":24,"citationCount":8,"influentialCitationCount":2,"publicationDate":"15/04/2021","authors":"Revanth Reddy Gangi Reddy,Vikas Yadav,Md Arafat Sultan,M. Franz,Vittorio Castelli,Heng Ji,Avirup Sil","id":"0994da8e3d70ce64e4ebcade3a592caa513161af","summary":"In-domain and out-of-domain evaluations of neural IR are conducted, and synthetic training examples generated using a sequence-tosequence generator can be effective towards its robustness across different scenarios, including zeroshot settings.","score":4},{"url":"https://www.semanticscholar.org/paper/807600ef43073cd9c59d4208ee710e90cf14efa8","title":"BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":95,"citationCount":191,"influentialCitationCount":63,"publicationDate":"17/04/2021","authors":"Nandan Thakur,Nils Reimers,Andreas Ruckl'e,Abhishek Srivastava,Iryna Gurevych","id":"807600ef43073cd9c59d4208ee710e90cf14efa8","summary":"This work extensively analyzes different retrieval models and provides several suggestions that it believes may be useful for future work, finding that performing well consistently across all datasets is challenging.","score":4},{"url":"https://www.semanticscholar.org/paper/6c8fa47803b9958f66b144d5f0bcca78f6d99e50","title":"Weakly-Supervised Question Answering with Effective Rank and Weighted Loss over Candidates","venue":"The Web Conference","year":2021,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/04/2021","authors":"Haozhe Qin,Jiangang Zhu,Beijun Shen","id":"6c8fa47803b9958f66b144d5f0bcca78f6d99e50","summary":"This paper designs several simple yet effective scoring functions to rank the candidate solutions in order to reduce the spuriousness of candidate solutions used for training, and presents an effective method to learn a question answering model in a weak supervision way.","score":4},{"url":"https://www.semanticscholar.org/paper/fea3e850e3d18026e0f0d761f75016bf6ccdbf53","title":"Few-Shot Conversational Dense Retrieval","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2021,"referenceCount":49,"citationCount":33,"influentialCitationCount":8,"publicationDate":"10/05/2021","authors":"S. Yu,Zhenghao Liu,Chenyan Xiong,Tao Feng,Zhiyuan Liu","id":"fea3e850e3d18026e0f0d761f75016bf6ccdbf53","summary":"The analyses reveal that the advantages of ConvDR come from its ability to capture informative context while ignoring the unrelated context in previous conversation rounds, which makes ConvDR more effective as conversations evolve while previous systems may get confused by the increased noise from previous turns.","score":4},{"url":"https://www.semanticscholar.org/paper/16529f7194bf7faee8a4e43fd54aefeb8730f236","title":"Database reasoning over text","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":35,"citationCount":15,"influentialCitationCount":1,"publicationDate":"02/06/2021","authors":"James Thorne,Majid Yazdani,Marzieh Saeidi,F. Silvestri,Sebastian Riedel,A. Halevy","id":"16529f7194bf7faee8a4e43fd54aefeb8730f236","summary":"This work proposes a modular architecture to answer database-style queries over multiple spans from text and aggregating these at scale, which scales to databases containing thousands of facts whereas contemporary models are limited by how many facts can be encoded.","score":4},{"url":"https://www.semanticscholar.org/paper/4572ce690e5eff7d7d38c6f3f67561d0b263b34f","title":"Training Adaptive Computation for Open-Domain Question Answering with Computational Constraints","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":29,"citationCount":4,"influentialCitationCount":0,"publicationDate":"05/07/2021","authors":"Yuxiang Wu,Pasquale Minervini,Pontus Stenetorp,S. Riedel","id":"4572ce690e5eff7d7d38c6f3f67561d0b263b34f","summary":"The proposed Adaptive Passage Encoder keeps the parameters of the base ODQA model fixed, but it overrides the default layer-by-layer computation of the encoder with an AC policy that is trained to optimise the computational efficiency of the model.","score":4},{"url":"https://www.semanticscholar.org/paper/824666e5e6721bc83a1768579062077d719f7089","title":"Synthetic Target Domain Supervision for Open Retrieval QA","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2021,"referenceCount":40,"citationCount":4,"influentialCitationCount":1,"publicationDate":"11/07/2021","authors":"Revanth Reddy Gangi Reddy,Bhavani Iyer,Md Arafat Sultan,Rong Zhang,Avirup Sil,Vittorio Castelli,Radu Florian,S. Roukos","id":"824666e5e6721bc83a1768579062077d719f7089","summary":"This work stress-test the Dense Passage Retriever (DPR)---a state-of-the-art (SOTA) open domain neural retrieval model---on closed and specialized target domains such as COVID-19, and finds that it lags behind standard BM25 in this important real-world setting.","score":4},{"url":"https://www.semanticscholar.org/paper/285428daf56a1530b11b6deb515f10c8f1dc5739","title":"Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":45,"citationCount":12,"influentialCitationCount":1,"publicationDate":"05/08/2021","authors":"A. Li,Patrick Ng,Peng Xu,Henghui Zhu,Zhiguo Wang,Bing Xiang","id":"285428daf56a1530b11b6deb515f10c8f1dc5739","summary":"A hybrid framework that takes both textual and tabular evidences as input and generates either direct answers or SQL queries depending on which form could better answer the question is proposed, which achieves the state-of-the-art performance on OpenSQuAD dataset using a T5-base model.","score":4},{"url":"https://www.semanticscholar.org/paper/4097ae9aca6444fd7536bfbed1e62560521b70d3","title":"PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval","venue":"Findings","year":2021,"referenceCount":35,"citationCount":39,"influentialCitationCount":9,"publicationDate":"13/08/2021","authors":"Ruiyang Ren,Shangwen Lv,Yingqi Qu,Jing Liu,Wayne Xin Zhao,Qiaoqiao She,Hua Wu,Haifeng Wang,Ji-rong Wen","id":"4097ae9aca6444fd7536bfbed1e62560521b70d3","summary":"This work proposes a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval that significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions datasets.","score":4},{"url":"https://www.semanticscholar.org/paper/1c2edf38c684b1938304cf024f5993241642f3d4","title":"Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback","venue":"International Conference on Information and Knowledge Management","year":2021,"referenceCount":35,"citationCount":23,"influentialCitationCount":6,"publicationDate":"30/08/2021","authors":"HongChien Yu,Chenyan Xiong,Jamie Callan","id":"1c2edf38c684b1938304cf024f5993241642f3d4","summary":"ANCE-PRF, a new query encoder that uses pseudo relevance feedback (PRF) to improve query representations for dense retrieval, significantly outperforms ANCE and other recent dense retrieval systems on several datasets.","score":4},{"url":"https://www.semanticscholar.org/paper/248816ded4e927fff2c092721d85aeb8038b49a0","title":"Unsupervised Open-Domain Question Answering","venue":"ArXiv","year":2021,"referenceCount":19,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/08/2021","authors":"Pengfei Zhu,Xiaoguang Li,Jian Li,Hai Zhao","id":"248816ded4e927fff2c092721d85aeb8038b49a0","summary":"This paper pioneers the work of unsupervised ODQA by formally introducing the task and proposing a series of key data construction methods and inspiringly shows un supervised ODZA can reach up to 86% performance of supervised ones.","score":4},{"url":"https://www.semanticscholar.org/paper/f7a6b57adebb5f6a10d16e120f0b0ef55aab7b2b","title":"Simple Entity-Centric Questions Challenge Dense Retrievers","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":32,"citationCount":53,"influentialCitationCount":12,"publicationDate":"17/09/2021","authors":"Christopher Sciavolino,Zexuan Zhong,Jinhyuk Lee,Danqi Chen","id":"f7a6b57adebb5f6a10d16e120f0b0ef55aab7b2b","summary":"This paper investigates the issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training, and demonstrates that data augmentation is unable to fix the generalization problem.","score":4},{"url":"https://www.semanticscholar.org/paper/3cf7a6153518805a26787a514aa38594b83e68ef","title":"Towards Universal Dense Retrieval for Open-domain Question Answering","venue":"ArXiv","year":2021,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/09/2021","authors":"Christopher Sciavolino","id":"3cf7a6153518805a26787a514aa38594b83e68ef","summary":"This paper introduces an entity-rich question answering dataset constructed from Wikidata facts and demonstrates dense models are unable to generalize to unseen input question distributions, and encourages the field to further investigate the creation of a single, universal dense retrieval model that generalizes well across all input distributions.","score":4},{"url":"https://www.semanticscholar.org/paper/50ec3d960ac458573a1e4a1556420c5e96d58609","title":"Distantly-Supervised Evidence Retrieval Enables Question Answering without Evidence Annotation","venue":"ArXiv","year":2021,"referenceCount":36,"citationCount":2,"influentialCitationCount":0,"publicationDate":"10/10/2021","authors":"Chenyan Xiong,Hal Daumé","id":"50ec3d960ac458573a1e4a1556420c5e96d58609","summary":"This paper introduces a novel approach (DistDR) that iteratively improves over a weak retriever by alternately finding evidence from the up-to-date model and encouraging the model to learn the most likely evidence, which leads to model improvements.","score":4},{"url":"https://www.semanticscholar.org/paper/6e0cfc8a2e743e3a90ad089f0fd4e4985f2f6834","title":"RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":48,"citationCount":75,"influentialCitationCount":21,"publicationDate":"14/10/2021","authors":"Ruiyang Ren,Yingqi Qu,Jing Liu,Wayne Xin Zhao,Qiaoqiao She,Hua Wu,Haifeng Wang,Ji-rong Wen","id":"6e0cfc8a2e743e3a90ad089f0fd4e4985f2f6834","summary":"A novel joint training approach for dense passage retrieval and passage reranking is proposed, where the dynamic listwise distillation is introduced, where a unified listwise training approach is designed for both the retriever and the re-ranker.","score":4},{"url":"https://www.semanticscholar.org/paper/2d48cfd946d727cacdb54a49724db67a4d22351e","title":"Cross-Lingual GenQA: A Language-Agnostic Generative Question Answering Approach for Open-Domain Question Answering","venue":"ArXiv","year":2021,"referenceCount":46,"citationCount":4,"influentialCitationCount":0,"publicationDate":2021,"authors":"Benjamin Muller,Luca Soldaini,Rik Koncel-Kedziorski,Eric Lind,Alessandro Moschitti","id":"2d48cfd946d727cacdb54a49724db67a4d22351e","summary":"This paper presents the first generalization of the GENQA approach for the multilingual environment, and presents the GEN-TYDIQA dataset, which extends the TyDiQA evaluation data with natural-sounding, well-formed answers in Arabic, Bengali, English, Japanese, and Russian.","score":4},{"url":"https://www.semanticscholar.org/paper/5a166cfd781ac871cd4fc26fd8b7fb0e0a1fa795","title":"Open Domain Question Answering over Virtual Documents: A Unified Approach for Data and Text","venue":"ArXiv","year":2021,"referenceCount":48,"citationCount":7,"influentialCitationCount":0,"publicationDate":2021,"authors":"Kaixin Ma,Hao Cheng,Xiaodong Liu,Eric Nyberg,Jianfeng Gao","id":"5a166cfd781ac871cd4fc26fd8b7fb0e0a1fa795","summary":"This work uses the data-to-text method as a means for coding structured knowledge for knowledge- intensive applications, i.e. open-domain answering (ODQA), and poses a verbalizer-retriever-reader framework for ODQA over data and text where verbalized tables from Wikipedia and graphs from Wikidata are used as augmented knowledge sources.","score":4},{"url":"https://www.semanticscholar.org/paper/a729503f528d5f0be0f897aa1841e1ff8ffcb313","title":"Dense Hierarchical Retrieval for Open-Domain Question Answering","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":32,"citationCount":4,"influentialCitationCount":0,"publicationDate":"28/10/2021","authors":"Ye Liu,Kazuma Hashimoto,Yingbo Zhou,Semih Yavuz,Caiming Xiong,Philip S. Yu","id":"a729503f528d5f0be0f897aa1841e1ff8ffcb313","summary":"Dense Hierarchical Retrieval (DHR) is proposed, a hierarchical framework which can generate accurate dense representations of passages by utilizing both macroscopic semantics in the document and microscopic semantics specific to each passage.","score":4},{"url":"https://www.semanticscholar.org/paper/8cda255351168fab95c7b59d12d24590ad57a6ac","title":"Question Answering Survey: Directions, Challenges, Datasets, Evaluation Matrices","venue":"Journal of Xidian University","year":2021,"referenceCount":121,"citationCount":4,"influentialCitationCount":0,"publicationDate":"07/12/2021","authors":"Hariom A. Pandya,Brijesh S. Bhatt","id":"8cda255351168fab95c7b59d12d24590ad57a6ac","summary":"In this review work, the research directions of QA field are analyzed based on the type of question, answer type, source of evidence-answer, and modeling approach, followed by open challenges of the field like automatic question generation, similarity detection and, low resource availability for a language.","score":4},{"url":"https://www.semanticscholar.org/paper/05cb33c55c44ab26848a865eb6062b5b027caacc","title":"An Encoder Attribution Analysis for Dense Passage Retriever in Open-Domain Question Answering","venue":"TRUSTNLP","year":2022,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Minghan Li,Xueguang Ma,Jimmy Lin","id":"05cb33c55c44ab26848a865eb6062b5b027caacc","summary":"It is found that the passage encoder contributes more than the question encoder to in-domain retrieval accuracy, and a probabilistic framework called encoder marginalization is formulated, where the contribution of a single encoder is quantified by marginalizing other variables.","score":4},{"url":"https://www.semanticscholar.org/paper/d14db1ed48129b55f88f6be56b47180a9ea7b059","title":"Dynamically Generated Question Answering Evidence using Efficient Context-preserving Subdivision","venue":"International Conference on Agents and Artificial Intelligence","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"A. Bleiweiss","id":"d14db1ed48129b55f88f6be56b47180a9ea7b059","summary":"It is proposed to dynamically draw content corresponding to an article-section url from the most updated online Wikipedia rather than from an archived snapshot, approaching space complexity of O(1), downward from O(n) for a dataset that is fully populated with static context.","score":4},{"url":"https://www.semanticscholar.org/paper/7c85f8b5f78b0010328a9063f6460156b30dd4ed","title":"In defense of dual-encoders for neural ranking","venue":"International Conference on Machine Learning","year":2022,"referenceCount":67,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"A. Menon,Sadeep Jayasumana,A. Rawat,Seungyeon Kim,Sashank J. Reddi,Surinder Kumar","id":"7c85f8b5f78b0010328a9063f6460156b30dd4ed","summary":"This paper establishes theoretically that with a sufﬁciently large encoder size, dual-encoder models can capture a broad class of scores without cross-attention, and proposes a distillation strategy that focuses on preserving the ordering amongst documents, and its impact on neural re-ranking benchmarks.","score":4},{"url":"https://www.semanticscholar.org/paper/1540045f8967a0c70384d9f095c64cfc88a2fc4a","title":"UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering","venue":"NAACL-HLT","year":2020,"referenceCount":58,"citationCount":34,"influentialCitationCount":10,"publicationDate":"29/12/2020","authors":"Barlas Oğuz,Xilun Chen,Vladimir Karpukhin,Stanislav Peshterliev,Dmytro Okhonko,M. Schlichtkrull,Sonal Gupta,Yashar Mehdad,S. Yih","id":"1540045f8967a0c70384d9f095c64cfc88a2fc4a","summary":"It is demonstrated that the UniK-QA model is a simple and yet effective way to combine heterogeneous sources of knowledge, advancing the state-of-the-art results on two popular question answering benchmarks, NaturalQuestions and WebQuestions, by 3.5 and 2.6 points, respectively.","score":4},{"url":"https://www.semanticscholar.org/paper/9f808d689639f3eb577614d2236ad7feaa3b6ecb","title":"Semantic Models for the First-Stage Retrieval: A Comprehensive Review","venue":"ACM Trans. Inf. Syst.","year":2021,"referenceCount":254,"citationCount":29,"influentialCitationCount":3,"publicationDate":"08/03/2021","authors":"Yinqiong Cai,Yixing Fan,Jiafeng Guo,Fei Sun,Ruqing Zhang,Xueqi Cheng","id":"9f808d689639f3eb577614d2236ad7feaa3b6ecb","summary":"The current landscape of the first-stage retrieval models under a unified framework is described to clarify the connection between classical term-based retrieval methods, early semantic retrieved methods, and neural semantic retrieval methods.","score":4},{"url":"https://www.semanticscholar.org/paper/18eb32d43ed10cf57abd386287016f44182c7a55","title":"FaVIQ: FAct Verification from Information-seeking Questions","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":47,"citationCount":7,"influentialCitationCount":3,"publicationDate":"05/07/2021","authors":"Jungsoo Park,Sewon Min,Jaewoo Kang,Luke Zettlemoyer,Hannaneh Hajishirzi","id":"18eb32d43ed10cf57abd386287016f44182c7a55","summary":"This paper constructs a large-scale challenging fact verification dataset called FAVIQ, consisting of 188k claims derived from an existing corpus of ambiguous information-seeking questions, verified to be natural, contain little lexical bias, and require a complete understanding of the evidence for verification.","score":4},{"url":"https://www.semanticscholar.org/paper/ec307b17f193b14292206b65a1bcc95bfd8f02ed","title":"♫ MuSiQue: Multihop Questions via Single-hop Question Composition","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":56,"citationCount":15,"influentialCitationCount":4,"publicationDate":"02/08/2021","authors":"H. Trivedi,Niranjan Balasubramanian,Tushar Khot,Ashish Sabharwal","id":"ec307b17f193b14292206b65a1bcc95bfd8f02ed","summary":"A bottom–up approach is introduced that systematically selects composable pairs of single-hop questions that are connected, that is, where one reasoning step critically relies on information from another, to create a new multihop QA dataset with 25K 2–4 hop questions.","score":4},{"url":"https://www.semanticscholar.org/paper/a69d0b93d2ffcf25c964d93cc9b1adb73085232d","title":"Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations","venue":"Findings","year":2021,"referenceCount":54,"citationCount":20,"influentialCitationCount":6,"publicationDate":"14/10/2021","authors":"Ji Xin,Chenyan Xiong,A. Srinivasan,Ankita Sharma,Damien Jose,Paul Bennett","id":"a69d0b93d2ffcf25c964d93cc9b1adb73085232d","summary":"Momentum adversarial Domain Invariant Representation learning (MoDIR) is proposed, which introduces a momentum method to train a domain classifier that distinguishes source versus target domains, and then adversarially updates the DR encoder to learn domain invariant representations.","score":4},{"url":"https://www.semanticscholar.org/paper/753fd6952c9f06f3bbd46e37129acc3f7a984896","title":"Hindsight: Posterior-guided training of retrievers for improved open-ended generation","venue":"International Conference on Learning Representations","year":2021,"referenceCount":40,"citationCount":19,"influentialCitationCount":2,"publicationDate":"14/10/2021","authors":"Ashwin Paranjape,O. Khattab,Christopher Potts,M. Zaharia,Christopher D. Manning","id":"753fd6952c9f06f3bbd46e37129acc3f7a984896","summary":"This work model the guide retriever after the posterior distribution Q of passages given the input and the target output and train it jointly with the standard retriever and the generator by maximizing the evidence lower bound (ELBo) in expectation over Q.","score":4},{"url":"https://www.semanticscholar.org/paper/aaea853381050b4456d0d8e2e4b0c282391e41dc","title":"Learning to Retrieve Passages without Supervision","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":55,"citationCount":28,"influentialCitationCount":5,"publicationDate":"14/12/2021","authors":"Ori Ram,Gal Shachaf,Omer Levy,Jonathan Berant,A. Globerson","id":"aaea853381050b4456d0d8e2e4b0c282391e41dc","summary":"The resulting model, named Spider, performs surprisingly well without any labeled training examples on a wide range of ODQA datasets, and significantly outperforms all other pretrained baselines in a zero-shot setting, and is competitive with BM25, a strong sparse baseline.","score":4},{"url":"https://www.semanticscholar.org/paper/2e2f4e4ec35a527211adcf7d89661fce5ed373a7","title":"Boosted Dense Retriever","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":74,"citationCount":11,"influentialCitationCount":2,"publicationDate":"14/12/2021","authors":"Patrick Lewis,Barlas Oğuz,Wenhan Xiong,Fabio Petroni,Wen-tau Yih,Sebastian Riedel","id":"2e2f4e4ec35a527211adcf7d89661fce5ed373a7","summary":"DrBoost is a dense retrieval ensemble inspired by boosting, which produces representations which are 4x more compact, while delivering comparable retrieval results, reducing latency and bandwidth needs by another 4x.","score":4},{"url":"https://www.semanticscholar.org/paper/97f456643712e9618edd7465676c62af3c8ae690","title":"A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models","venue":"ArXiv","year":2022,"referenceCount":75,"citationCount":7,"influentialCitationCount":0,"publicationDate":"17/02/2022","authors":"Da Yin,Li Dong,Hao Cheng,Xiaodong Liu,Kai-Wei Chang,Furu Wei,Jianfeng Gao","id":"97f456643712e9618edd7465676c62af3c8ae690","summary":"This paper aims to summarize the current progress of pre-trained language modelbased knowledge-enhanced models (PLMKEs) by dissecting their three vital elements: knowledge sources, knowledge-intensive NLP tasks, and knowledge fusion methods.","score":4},{"url":"https://www.semanticscholar.org/paper/7a54248d35448e1c95743373ddbb9170ad7c039a","title":"DuReader-Retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":54,"citationCount":4,"influentialCitationCount":1,"publicationDate":"19/03/2022","authors":"Yifu Qiu,Hongyu Li,Yingqi Qu,Ying Chen,Qiaoqiao She,Jing Liu,Hua Wu,Haifeng Wang","id":"7a54248d35448e1c95743373ddbb9170ad7c039a","summary":"The experiments demonstrate that DuReader-retrieval is challenging and a number of problems remain unsolved, such as the salient phrase mismatch and the syntactic mismatch between queries and paragraphs.","score":4},{"url":"https://www.semanticscholar.org/paper/9e522f6a2d6ee97c7414ebd2fdc8910ca791d198","title":"A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine Reading Comprehension","venue":"ArXiv","year":2022,"referenceCount":87,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/09/2022","authors":"Xanh Ho,Johannes Mario Meissner,Saku Sugawara,Akiko Aizawa","id":"9e522f6a2d6ee97c7414ebd2fdc8910ca791d198","summary":"This survey paper focuses on the task of machine reading comprehension (MRC), an important task for showcasing high-level language understanding that also suffers from a range of shortcuts, and summarizes the available techniques for measuring and mitigating shortcuts.","score":4},{"url":"https://www.semanticscholar.org/paper/d72ae3c981a2ef6685049183452466a29b85f96c","title":"A Study on the Efficiency and Generalization of Light Hybrid Retrievers","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"04/10/2022","authors":"Man Luo,Shashank Jain,Anchit Gupta,Arash Einolghozati,Barlas Oğuz,Debojeet Chatterjee,Xilun Chen,Chitta Baral,P. Heidari","id":"d72ae3c981a2ef6685049183452466a29b85f96c","summary":"A lighter dense retriever (LITE) is introduced which is jointly trained on contrastive learning and knowledge distillation from DrBoost and achieves better robustness performance than both sparse and dense retrievers.","score":4},{"url":"https://www.semanticscholar.org/paper/0caf0fa48aced139c0d8214be4a795d9576b9990","title":"Natural Test Generation for Precise Testing of Question Answering Software","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Qingchao Shen,Junjie Chen,J Zhang,Haoyu Wang,Shuang Liu,Menghan Tian","id":"0caf0fa48aced139c0d8214be4a795d9576b9990","summary":"QA, a sentence-level mutation based metamorphic testing technique for QA software that leverages five Metamorphic Relations as well as semantics-guided search and enhanced test oracles, outperforms the state-of-the-art in both quantity and quality.","score":4},{"url":"https://www.semanticscholar.org/paper/09ff29678778e3f887b9a358e6ab493ab379e1cd","title":"Semantic matching in machine reading comprehension: An empirical study","venue":"Information Processing &amp; Management","year":2022,"referenceCount":77,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Qian Liu,Rui Mao,Xiubo Geng,E. Cambria","id":"09ff29678778e3f887b9a358e6ab493ab379e1cd","summary":"This paper forms a twostage framework which consists of a semantic matching model and a reading model, based on pre-trained language models, and finds that semantic matching is helpful for answering who/where/when/what/how/which questions, whereas it decreases the MRC performance on why questions.","score":4},{"url":"https://www.semanticscholar.org/paper/412494a830fb7ee13db268879db1539612d6c6e6","title":"Diverse Multi-Answer Retrieval with Determinantal Point Processes","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":24,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"Poojitha Nandigam,Nikhil Rayaprolu,Manish Shrivastava","id":"412494a830fb7ee13db268879db1539612d6c6e6","summary":"This paper addresses multi-answer retrieval which entails retrieving passages that can capture majority of the diverse answers to the question, and proposes a re-ranking based approach using Determinantal point processes utilizing BERT as kernels.","score":4},{"url":"https://www.semanticscholar.org/paper/7e44002c4f78458987a90dc7a0408d60dd5cdb7c","title":"Defending Against Poisoning Attacks in Open-Domain Question Answering","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Orion Weller,Aleem Khan,Nathaniel Weir,Dawn J Lawrie,Benjamin Van Durme","id":"7e44002c4f78458987a90dc7a0408d60dd5cdb7c","summary":"A new method is introduced that uses query augmentation to search for a diverse set of retrieved passages that could answer the original question, comparing the predicted answer to its appearance in the retrieved contexts and providing gains of 5-20% exact match across varying levels of data poisoning.","score":4},{"url":"https://www.semanticscholar.org/paper/c3d091c4ab12cc2d1503d40aeb25374e477f16ae","title":"Modeling Sequential Sentence Relation to Improve Cross-lingual Dense Retrieval","venue":"ArXiv","year":2023,"referenceCount":75,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"Shunyu Zhang,Yaobo Liang,Ming Gong,Daxin Jiang,Nan Duan","id":"c3d091c4ab12cc2d1503d40aeb25374e477f16ae","summary":"Comprehensive experiments on four cross-lingual retrieval tasks show MSM significantly outperforms existing advanced pre-training models, demonstrating the effectiveness and stronger cross-lingsual retrieval capabilities of this approach.","score":4},{"url":"https://www.semanticscholar.org/paper/65fc1f1c567801fee3788974e753cdbf934f07e9","title":"Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos","venue":"ArXiv","year":2022,"referenceCount":85,"citationCount":40,"influentialCitationCount":8,"publicationDate":"23/06/2022","authors":"Bowen Baker,Ilge Akkaya,P. Zhokhov,Joost Huizinga,Jie Tang,Adrien Ecoffet,Brandon Houghton,Raul Sampedro,J. Clune","id":"65fc1f1c567801fee3788974e753cdbf934f07e9","summary":"This work extends the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos, and is the first to report computer agents that can craft diamond tools.","score":4},{"url":"https://www.semanticscholar.org/paper/cb0396b055e03d68eaca98f53ce56f2de69aeab6","title":"How to Enable Uncertainty Estimation in Proximal Policy Optimization","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/10/2022","authors":"Eugene Bykovets,Yannick Metz,Mennatallah El-Assady,D. Keim,J. Buhmann","id":"cb0396b055e03d68eaca98f53ce56f2de69aeab6","summary":"This work proposesitions of uncertainty and OOD for Actor-Critic RL algorithms, namely, proximal policy optimization (PPO), and presents possible applicable measures, and discusses the concepts of value and policy uncertainty.","score":4},{"url":"https://www.semanticscholar.org/paper/4dcf08e1b4bf77fc27af4db2242efec929bcc2aa","title":"Answering Complex Open-domain Questions Through Iterative Query Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2019,"referenceCount":31,"citationCount":81,"influentialCitationCount":14,"publicationDate":"15/10/2019","authors":"Peng Qi,Xiaowen Lin,L. Mehr,Zijian Wang,Christopher D. Manning","id":"4dcf08e1b4bf77fc27af4db2242efec929bcc2aa","summary":"This work presents GoldEn (Gold Entity) Retriever, which iterates between reading context and retrieving more supporting documents to answer open-domain multi-hop questions, and demonstrates that it outperforms the best previously published model despite not using pretrained language models such as BERT.","score":4},{"url":"https://www.semanticscholar.org/paper/569b731b2cf30c63d1919a38c875e95e79e278ab","title":"Learning with imperfect supervision for language understanding","venue":"","year":2020,"referenceCount":340,"citationCount":7,"influentialCitationCount":0,"publicationDate":"20/03/2020","authors":"M. Dehghani","id":"569b731b2cf30c63d1919a38c875e95e79e278ab","summary":"It is argued that even noisy and limited signals can contain a great deal of valid information that can be incorporated along with prior knowledge and biases that are encoded into learning algorithms in order to solve complex problems.","score":4},{"url":"https://www.semanticscholar.org/paper/3ca320cb73c962cd29b8211cb4fd8074c5e8c9b8","title":"Contextualized Representations Using Textual Encyclopedic Knowledge","venue":"ArXiv","year":2020,"referenceCount":64,"citationCount":18,"influentialCitationCount":2,"publicationDate":"24/04/2020","authors":"Mandar Joshi,Kenton Lee,Yi Luan,Kristina Toutanova","id":"3ca320cb73c962cd29b8211cb4fd8074c5e8c9b8","summary":"It is shown that integrating background knowledge from text is effective for tasks focusing on factual reasoning and allows direct reuse of powerful pretrained BERT-style encoders and knowledge integration can be further improved with suitable pretraining via a self-supervised masked language model objective over words in background-augmented input text.","score":4},{"url":"https://www.semanticscholar.org/paper/653927114923a82bbe92e4872e5dd555f078c056","title":"Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":42,"citationCount":18,"influentialCitationCount":7,"publicationDate":"22/06/2021","authors":"Gangwoo Kim,Hyunjae Kim,Jungsoo Park,Jaewoo Kang","id":"653927114923a82bbe92e4872e5dd555f078c056","summary":"A novel framework, ExCorD (Explicit guidance on how to resolve Conversational Dependency) is proposed to enhance the abilities of QA models in comprehending conversational context, while addressing the limitations of the existing approaches.","score":4},{"url":"https://www.semanticscholar.org/paper/238946a8dc0b88707cbff3512065ff1f37828bbf","title":"Can Open Domain Question Answering Systems Answer Visual Knowledge Questions?","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/02/2022","authors":"Jiawen Zhang,Abhijit Mishra,Avinesh P.V.S.,Siddharth Patwardhan,Sachin Agarwal","id":"238946a8dc0b88707cbff3512065ff1f37828bbf","summary":"This work proposes a potentially data-efficient approach that reuses existing systems for image analysis, question rewriting, and text-based question answering to answer many visual questions, and explores two rewriting strategies that combines adaptive rewriting and reinforcement learning techniques to use the implicit feedback from the QA system.","score":4},{"url":"https://www.semanticscholar.org/paper/42edbc3c29af476c27f102b3de9f04e56b5c642d","title":"A Survey of Generalisation in Deep Reinforcement Learning","venue":"ArXiv","year":2021,"referenceCount":203,"citationCount":103,"influentialCitationCount":11,"publicationDate":2021,"authors":"Robert Kirk,Amy Zhang,Edward Grefenstette,Tim Rocktaschel","id":"42edbc3c29af476c27f102b3de9f04e56b5c642d","summary":"It is argued that taking a purely procedural content generation approach to benchmark design is not conducive to progress in generalisation, and fast online adaptation and tackling RL-specific problems as some areas for future work on methods for generalisation are suggested.","score":4},{"url":"https://www.semanticscholar.org/paper/5f4f196fc6277185d85816501bb814dfaeed69e4","title":"The Free Energy Principle for Perception and Action: A Deep Learning Perspective","venue":"Entropy","year":2022,"referenceCount":193,"citationCount":4,"influentialCitationCount":0,"publicationDate":"01/02/2022","authors":"Pietro Mazzaglia,T. Verbelen,Ozan Çatal,B. Dhoedt","id":"5f4f196fc6277185d85816501bb814dfaeed69e4","summary":"This work investigates the tool of deep learning to design and realize artificial agents based on active inference, presenting a deep-learning oriented presentation of the free energy principle, surveying works that are relevant in both machine learning and active inference areas, and discussing the design choices that are involved in the implementation process.","score":4},{"url":"https://www.semanticscholar.org/paper/6d0adac188152fbaa45a88ba4da788926ed8144a","title":"Reinforcement Learning in Practice: Opportunities and Challenges","venue":"","year":2022,"referenceCount":333,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/02/2022","authors":"Yuxi Li","id":"6d0adac188152fbaa45a88ba4da788926ed8144a","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/af4b9a5bbcda271f2a08a4c3437a3fbe6125ed6e","title":"Train timetabling with the general learning environment and multi-agent deep reinforcement learning","venue":"Transportation Research Part B: Methodological","year":2022,"referenceCount":31,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/03/2022","authors":"Wenqing Li,S. Ni","id":"af4b9a5bbcda271f2a08a4c3437a3fbe6125ed6e","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/a62c2b9cce3bd3df9ba1eea42aceeb070bd089b2","title":"A Novel Reinforcement Learning Collision Avoidance Algorithm for USVs Based on Maneuvering Characteristics and COLREGs","venue":"Italian National Conference on Sensors","year":2022,"referenceCount":48,"citationCount":5,"influentialCitationCount":0,"publicationDate":"01/03/2022","authors":"Yunsheng Fan,Zhe Sun,Guofeng Wang","id":"a62c2b9cce3bd3df9ba1eea42aceeb070bd089b2","summary":"A reinforcement learning collision avoidance (RLCA) algorithm is proposed that complies with USV maneuverability that bridged the divide between USV navigation status information and collision avoidance behavior, resulting in successfully planning a safe and economical path to the terminal.","score":4},{"url":"https://www.semanticscholar.org/paper/0f4fff63f5f637e0f807532e37462e0619c86568","title":"On the link between conscious function and general intelligence in humans and machines","venue":"ArXiv","year":2022,"referenceCount":253,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/03/2022","authors":"A. Juliani,Kai Arulkumaran,Shuntaro Sasai,R. Kanai","id":"0f4fff63f5f637e0f807532e37462e0619c86568","summary":"This work examines the cognitive abilities associated with three contemporary theories of conscious function: Global Workspace Theory (GWT), Information Generation Theory (IGT), and Attention Schema Theory (AST) to propose ways in which insights from each of the three theories may be combined into a uniﬁed model.","score":4},{"url":"https://www.semanticscholar.org/paper/9064845595d2fe7dd860c612050e4818a191ff62","title":"Reinforcement learning on graphs: A survey","venue":"","year":2022,"referenceCount":209,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/04/2022","authors":"Mingshuo Nie,Dongming Chen,Dongqi Wang","id":"9064845595d2fe7dd860c612050e4818a191ff62","summary":"This survey provides a comprehensive overview of RL and graph mining methods and generalize these methods to Graph Reinforcement Learning (GRL) as a uniﬁed formulation and creates an online open- source for both interested scholars who want to enter this rapidly developing domain and experts who would like to compare GRL methods.","score":4},{"url":"https://www.semanticscholar.org/paper/5d15c5a5bde2ae457038a0432dd6aa1a61227977","title":"Scalable Multi-Agent Model-Based Reinforcement Learning","venue":"Adaptive Agents and Multi-Agent Systems","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"Vladimir Egorov,A. Shpilman","id":"5d15c5a5bde2ae457038a0432dd6aa1a61227977","summary":"It is argued that communication between agents is enough to sustain a world model for each agent during execution phase while imaginary rollouts can be used for training, removing the necessity to interact with the environment.","score":4},{"url":"https://www.semanticscholar.org/paper/ad053a050d6095a2bdb2bd00bfec2a6253280fba","title":"Generalized Data Distribution Iteration","venue":"International Conference on Machine Learning","year":2022,"referenceCount":50,"citationCount":2,"influentialCitationCount":0,"publicationDate":"07/06/2022","authors":"Jiajun Fan,Changnan Xiao","id":"ad053a050d6095a2bdb2bd00bfec2a6253280fba","summary":"It is argued that there is still a long way to go before obtaining real superhuman agents in ALE, and theoretical guarantee of the superiority of GDI compared with GPI is concluded.","score":4},{"url":"https://www.semanticscholar.org/paper/955536024c5db4166e63d41406c290fcf7ade696","title":"A Relational Intervention Approach for Unsupervised Dynamics Generalization in Model-Based Reinforcement Learning","venue":"International Conference on Learning Representations","year":2022,"referenceCount":79,"citationCount":6,"influentialCitationCount":2,"publicationDate":"09/06/2022","authors":"Jixian Guo,Mingming Gong,Dacheng Tao","id":"955536024c5db4166e63d41406c290fcf7ade696","summary":"It is empirically show that ˆ Z estimated by this method can signiﬁcantly reduce dynamics prediction errors and improve the performance of model-based RL methods on zero-shot new environments with unseen dynamics.","score":4},{"url":"https://www.semanticscholar.org/paper/25bc06b508b2c63b9faf77881e528530b147b988","title":"DayDreamer: World Models for Physical Robot Learning","venue":"ArXiv","year":2022,"referenceCount":73,"citationCount":17,"influentialCitationCount":1,"publicationDate":"28/06/2022","authors":"Philipp Wu,Alejandro Escontrela,Danijar Hafner,Ken Goldberg,P. Abbeel","id":"25bc06b508b2c63b9faf77881e528530b147b988","summary":"This paper applies Dreamer to four robots and tasks to learn online and directly in the real world, without any simulators, suggesting that Dreamer is capable of online learning in thereal world, establishing a strong baseline.","score":4},{"url":"https://www.semanticscholar.org/paper/9d516df01688e830b4b3acc3bc995746f6fecfb5","title":"A Game-Theoretic Perspective of Generalization in Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/08/2022","authors":"Chang Yang,Rui Wang,Xinrun Wang,Zhen Wang","id":"9d516df01688e830b4b3acc3bc995746f6fecfb5","summary":"GiRL is propound, a game-theoretic framework for generalization in reinforcement learning, where a RL agent is trained against an adversary over a set of tasks, over which the adversary can manipulate the distributions within a given threshold.","score":4},{"url":"https://www.semanticscholar.org/paper/be33823886361b68d27a33f7dfb0986a8414ac33","title":"Techniques and Paradigms in Modern Game AI Systems","venue":"Algorithms","year":2022,"referenceCount":80,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/08/2022","authors":"Yunlong Lu,Wenxin Li","id":"be33823886361b68d27a33f7dfb0986a8414ac33","summary":"It is claimed that deep reinforcement learning is the most general methodology to become a mainstream method for games with higher complexity and will bring inspiration to the game AI community for future directions.","score":4},{"url":"https://www.semanticscholar.org/paper/ee6e701be37ab3f2257197a09a2724f8e9ac7681","title":"Speedup Training Artificial Intelligence for Mahjong via Reward Variance Reduction","venue":"2022 IEEE Conference on Games (CoG)","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/08/2022","authors":"Jinqiu Li,Shuang Wu,Haobo Fu,Qiang Fu,Enmin Zhao,Junliang Xing","id":"ee6e701be37ab3f2257197a09a2724f8e9ac7681","summary":"Results show that RVR significantly reduces the variance in Mahjong AI training and improves the model performance, as well as improving the training stability using an expected reward network to adapt to the complex, dynamic, and highly stochastic reward environment.","score":4},{"url":"https://www.semanticscholar.org/paper/d038814af60dd92d14325a986a9f06a37dbbd0a9","title":"Reinforcement Learning using Reward Expectations in Scenarios with Aleatoric Uncertainties","venue":"2022 IEEE Conference on Games (CoG)","year":2022,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/08/2022","authors":"Yubin Wang,Yifeng Sun,Jiang Wu,Hao Hu,Zhiqiang Wu,Weigui Huang","id":"d038814af60dd92d14325a986a9f06a37dbbd0a9","summary":"In scenarios with aleatoric uncertainties, the reward got by an agent when executing the same action in the same state is random, which can reduce the stability and convergence speed of the reinforcement algorithms.","score":4},{"url":"https://www.semanticscholar.org/paper/46d16ec6cc28081742454f7967b9170afd3650b9","title":"Transformers are Sample Efficient World Models","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":11,"influentialCitationCount":2,"publicationDate":"01/09/2022","authors":"Vincent Micheli,Eloi Alonso,Franccois Fleuret","id":"46d16ec6cc28081742454f7967b9170afd3650b9","summary":"IRIS is a data-efﬁcient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer that sets a new state of the art for methods without lookahead search, and even surpasses MuZero.","score":4},{"url":"https://www.semanticscholar.org/paper/fccebab2f14ed96365684137002c80b849045a45","title":"Off-Policy Actor-critic for Recommender Systems","venue":"ACM Conference on Recommender Systems","year":2022,"referenceCount":77,"citationCount":3,"influentialCitationCount":0,"publicationDate":"18/09/2022","authors":"Minmin Chen,Can Xu,Vince Gatto,Devanshu Jain,Aviral Kumar,Ed H. Chi","id":"fccebab2f14ed96365684137002c80b849045a45","summary":"The key designs in setting up an off-policy actor-critic agent for production recommender systems are shared and it is demonstrated in offline and live experiments that the new framework out-performs baseline and improves long term user experience.","score":4},{"url":"https://www.semanticscholar.org/paper/2a2cbe0bd0889ff7b1653a4de977ab662b8154dd","title":"Integrating artificial and biological neural networks to improve animal task performance using deep reinforcement learning","venue":"bioRxiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/09/2022","authors":"Chenguang Li,Gabriel Kreiman,S. Ramanathan","id":"2a2cbe0bd0889ff7b1653a4de977ab662b8154dd","summary":"This work interfaced the nervous system of the nematode Caenorhabditis elegans with a deep reinforcement learning agent, enabling the animal to navigate to targets and enhancing its natural ability to search for food.","score":4},{"url":"https://www.semanticscholar.org/paper/6942045726eb5ff6b51bfe79519987ebd9c5785a","title":"Unsupervised Model-based Pre-training for Data-efficient Control from Pixels","venue":"ArXiv","year":2022,"referenceCount":58,"citationCount":3,"influentialCitationCount":1,"publicationDate":"24/09/2022","authors":"Sai Rajeswar,Pietro Mazzaglia,T. Verbelen,Alexandre Pich'e,B. Dhoedt,Aaron C. Courville,Alexandre Lacoste","id":"6942045726eb5ff6b51bfe79519987ebd9c5785a","summary":"This work designs an effective unsupervised RL strategy for data-efﬁcient visual control and demonstrates robust performance on the Real-Word RL benchmark, hinting that the approach generalizes to noisy environments.","score":4},{"url":"https://www.semanticscholar.org/paper/e48e12d51d6805d42f60a148133af6dab29b3a60","title":"Hierarchical Reinforcement Learning for Furniture Layout in Virtual Indoor Scenes","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/10/2022","authors":"Xinhan Di,Pengqian Yu","id":"e48e12d51d6805d42f60a148133af6dab29b3a60","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/b9263e73468186b64af6991c54eeb8a612d205e4","title":"Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"Weirui Ye,P. Abbeel,Yang Gao","id":"b9263e73468186b64af6991c54eeb8a612d205e4","summary":"Experiments show that the Virtual MCTS (V-MCTS) can achieve comparable performances to the original search algorithm while requiring less than 50% search time on average, and it is believed that this approach is a viable alternative for tasks under limited time and resources.","score":4},{"url":"https://www.semanticscholar.org/paper/1cc6f304f019ce1ae0ba57877c17019da2238616","title":"On All-Action Policy Gradients","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/10/2022","authors":"Michal Nauman,Marek Cygan","id":"1cc6f304f019ce1ae0ba57877c17019da2238616","summary":"The decompose the variance of SPG and derive an optimality condition, which shows when all-action SPG should be preferred over single-action counterpart and allows to determine a variance-minimizing sampling scheme in SPG estimation.","score":4},{"url":"https://www.semanticscholar.org/paper/7f1c79aba76005d09156b684ba2e133280c47277","title":"Autonomous maneuver decision-making method based on reinforcement learning and Monte Carlo tree search","venue":"Frontiers in Neurorobotics","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Hongpeng Zhang,Huan Zhou,Yujie Wei,Changqiang Huang","id":"7f1c79aba76005d09156b684ba2e133280c47277","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/ac1c0034d1ec0e685c30736deb50023d5c9b2d6e","title":"Mastering construction heuristics with self-play deep reinforcement learning","venue":"Neural computing & applications (Print)","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/10/2022","authors":"Qi Wang,Yuqing He,Chunlei Tang","id":"ac1c0034d1ec0e685c30736deb50023d5c9b2d6e","summary":"Inspired by AlphaGo Zero, this paper proposes a novel self-play reinforcement learning algorithm (CH-Zero) based on the Monte Carlo tree search (MCTS) for routing optimization problems in this paper and applies self- play reinforcement learning without MCTS to train offline policy and value networks.","score":4},{"url":"https://www.semanticscholar.org/paper/bc4c90256a13379ee9f2856c22e2e626fbe57d55","title":"Reservoir Computing via Quantum Recurrent Neural Networks","venue":"ArXiv","year":2022,"referenceCount":119,"citationCount":2,"influentialCitationCount":0,"publicationDate":"04/11/2022","authors":"Samuel Yen-Chi Chen,D. Fry,Amol Deshmukh,V. Rastunkov,Charlee Stefanski","id":"bc4c90256a13379ee9f2856c22e2e626fbe57d55","summary":"This work approaches sequential modeling by applying a reservoir computing (RC) framework to quantum recurrent neural networks (QRNN-RC) that are based on classical RNN, LSTM and GRU, and shows that the quantum version learns faster by requiring fewer training epochs in most cases.","score":4},{"url":"https://www.semanticscholar.org/paper/7ddff9d44d8b04060f1955b83a01836510eb0dc1","title":"Reward-Predictive Clustering","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Lucas Lehnert,Michael J. Frank,M. Littman","id":"7ddff9d44d8b04060f1955b83a01836510eb0dc1","summary":"A clustering algorithm is provided that enables the application of such state abstractions to deep learning settings, providing compressed representations of an agent’s inputs that preserve the ability to predict sequences of reward.","score":4},{"url":"https://www.semanticscholar.org/paper/fd15dce0aa20f0b0691a334995f46e792371d8ad","title":"Coordinating CAV Swarms at Intersections with a Deep Learning Model","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Jiawei Zhang,Sheng Li,Li Li","id":"fd15dce0aa20f0b0691a334995f46e792371d8ad","summary":"A novel cooperative driving algorithm (AlphaOrder) that combines of deep learning and online tree searching to generate a near-optimal passing order in real-time and provides a general approach to managing preemptive resource sharing between swarm robotics.","score":4},{"url":"https://www.semanticscholar.org/paper/ce4b01c6eca06ae3b24e5bc96c1a4b7405240569","title":"Multi-agent reinforcement learning for autonomous vehicles: a survey","venue":"Autonomous Intelligent Systems","year":2022,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/11/2022","authors":"Joris Dinneweth,Abderrahmane Boubezoul,R. Mandiau,S. Espié","id":"ce4b01c6eca06ae3b24e5bc96c1a4b7405240569","summary":"This paper investigates recent advances of multi-agent reinforcement learning (MARL) algorithms in mixed traffic problems and analyzes how the authors formulated the MARL problem in terms of observation, action, and rewards to match the paradigm they apply.","score":4},{"url":"https://www.semanticscholar.org/paper/2e1ca97d8f7604e3b334ff4903bfa67267379317","title":"The Surprising Effectiveness of Latent World Models for Continual Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"Samuel Kessler,Piotr Milo's,Jack Parker-Holder,Stephen J. Roberts","id":"2e1ca97d8f7604e3b334ff4903bfa67267379317","summary":"This work studies the use of model-based reinforcement learning methods, in particular, world models for continual reinforcement learning, and shows that world models are a simple and effective continual reinforcementlearning baseline.","score":4},{"url":"https://www.semanticscholar.org/paper/9ec568cb93702a0efa75532e296af22740834843","title":"Deep Deterministic Policy Gradient-Based Autonomous Driving for Mobile Robots in Sparse Reward Environments","venue":"Italian National Conference on Sensors","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Minjae Park,Seok Young Lee,Jin-Seok Hong,N. Kwon","id":"9ec568cb93702a0efa75532e296af22740834843","summary":"The proposed method demonstrated that the HER technique could help mitigate the sparse reward problem; this was further corroborated by the successful autonomous driving results obtained after applying the proposed method to two reward systems, as well as actual experimental results.","score":4},{"url":"https://www.semanticscholar.org/paper/6143e809cd2e9c7a18c3bc8819419fd3b02fbcf2","title":"Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness","venue":"ArXiv","year":2023,"referenceCount":34,"citationCount":3,"influentialCitationCount":0,"publicationDate":"17/01/2023","authors":"Ezgi Korkmaz","id":"6143e809cd2e9c7a18c3bc8819419fd3b02fbcf2","summary":"This work shows that high sensitivity directions do not lie only along particular worst-case directions, but rather are more abundant in the deep neural policy landscape and can be found via more natural means in a black-box set- ting.","score":4},{"url":"https://www.semanticscholar.org/paper/d0b93a2fcfe277edd1d9f59360df1deca5c98f56","title":"SoftTreeMax: Exponential Variance Reduction in Policy Gradient via Tree Search","venue":"ArXiv","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Gal Dalal,Assaf Hallak,Gugan Thoppe,Shie Mannor,Gal Chechik","id":"d0b93a2fcfe277edd1d9f59360df1deca5c98f56","summary":"It is proved that the resulting variance decays exponentially with the planning horizon as a function of the expansion policy, and the closer the resulting state transitions are to uniform, the faster the decay.","score":4},{"url":"https://www.semanticscholar.org/paper/924501c0205218280cb0251c89bda88c5a142b3e","title":"Investigating the role of model-based learning in exploration and transfer","venue":"ArXiv","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Jacob Walker,Eszter V'ertes,Yazhe Li,Gabriel Dulac-Arnold,Ankesh Anand,T. Weber,Jessica B. Hamrick","id":"924501c0205218280cb0251c89bda88c5a142b3e","summary":"It is found that a model-based approach outperforms controlled model-free baselines for transfer learning and intrinsic exploration combined with environment models present a viable direction towards agents that are self-supervised and able to generalize to novel reward functions.","score":4},{"url":"https://www.semanticscholar.org/paper/d15d96517370c9ed0658d176b979bcf92d1373ea","title":"Reason first, then respond: Modular Generation for Knowledge-infused Dialogue","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":46,"citationCount":17,"influentialCitationCount":1,"publicationDate":"09/11/2021","authors":"Leonard Adolphs,Kurt Shuster,Jack Urbanek,Arthur D. Szlam,J. Weston","id":"d15d96517370c9ed0658d176b979bcf92d1373ea","summary":"This work proposes a modular model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps, and finds that such a model hallucinates less in knowledge-grounded dialogue tasks, and has advantages in terms of interpretability and modularity.","score":4},{"url":"https://www.semanticscholar.org/paper/8e7760a3eb57a41124ac9e600df3882d9a7ea4d0","title":"XF2T: Cross-lingual Fact-to-Text Generation for Low-Resource Languages","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/09/2022","authors":"Shivprasad Sagare,Tushar Abhishek,Bhavyajeet Singh,Anubhav Sharma,Manish Gupta,Vasudeva Varma","id":"8e7760a3eb57a41124ac9e600df3882d9a7ea4d0","summary":"An extensive study using popular Transformer-based text generation models on an extended multi-lingual dataset, which is called XA LIGN V2, and investigates the performance of different text generation strategies: multiple variations of pretraining, fact-aware embeddings and structure-aware input encoding.","score":4},{"url":"https://www.semanticscholar.org/paper/bceebd434d7502dcd87004ec7313c2eea2c512fc","title":"A Survey for Efficient Open Domain Question Answering","venue":"ArXiv","year":2022,"referenceCount":86,"citationCount":4,"influentialCitationCount":1,"publicationDate":"15/11/2022","authors":"Qin Zhang,Shan Chen,Dongkuan Xu,Qingqing Cao,Xiaojun Chen,Trevor Cohn,Meng Fang","id":"bceebd434d7502dcd87004ec7313c2eea2c512fc","summary":"This paper walks through the ODQA models and concludes the core techniques on efﬁciency, and Quantitative analysis on memory cost, processing speed, accuracy and overall comparison are given.","score":4},{"url":"https://www.semanticscholar.org/paper/c055a4d03e7ee6647873e8d25ebb7998512507a8","title":"Search-engine-augmented dialogue response generation with cheaply supervised query production","venue":"Artificial Intelligence","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Ante Wang,Linfeng Song,Qi Liu,Haitao Mi,Longyue Wang,Zhaopeng Tu,Jinsong Su,Dong Yu","id":"c055a4d03e7ee6647873e8d25ebb7998512507a8","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/d5fe97309afdf0da633e04b5da4212a054661ecf","title":"Lucid: A Non-intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs","venue":"International Conference on Architectural Support for Programming Languages and Operating Systems","year":2023,"referenceCount":105,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Qi Hu,Mengdie Zhang,Peng Sun,Yonggang Wen,Tianwei Zhang","id":"d5fe97309afdf0da633e04b5da4212a054661ecf","summary":"Lucid is designed and implemented, a non-intrusive deep learning workload scheduler based on interpretable models that reduces the average job completion time and provides explicit system interpretations and excellent scalability for practical deployment.","score":4},{"url":"https://www.semanticscholar.org/paper/615c923b75f2934853889e19d11a2a2514e9f44e","title":"Grounding human-object interaction to affordance behavior in multimodal datasets","venue":"Frontiers in Artificial Intelligence","year":2023,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Alexander Henlein,Anju Gopinath,Nikhil Krishnaswamy,Alexander Mehler,J. Pustejovsky","id":"615c923b75f2934853889e19d11a2a2514e9f44e","summary":"The model, AffordanceUPT, is based on a two-stage adaptation of the Unary-Pairwise Transformer, which is modularize to make affordance detection independent of object detection and can effectively make the Gibsonian/telic distinction.","score":4},{"url":"https://www.semanticscholar.org/paper/f3e80e71aa45d0ecfedf8e18030e353353d6b6f1","title":"MAKE: Product Retrieval with Vision-Language Pre-training in Taobao Search","venue":"ArXiv","year":2023,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Xiaoyang Zheng,Zilong Wang,Sen-Yuan Li,Ke Xu,Tao Zhuang,Qingwen Liu,Xiaoyi Zeng","id":"f3e80e71aa45d0ecfedf8e18030e353353d6b6f1","summary":"This paper presents a novel vision-language (V+L) pre-training methods to exploit the multimodal information of (user query, product title, product image), and demonstrates that the retrieval-specific pre- training model (referred to as MAKE) outperforms existing V+L pre- Training methods on the text-to-multimodal retrieval task.","score":4},{"url":"https://www.semanticscholar.org/paper/adfe0e8eea02b4fff42ab02ecd778ce02c73bf7d","title":"TAP: Accelerating Large-Scale DNN Training Through Tensor Automatic Parallelisation","venue":"ArXiv","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Ziji Shi,Le Jiang,Ang Wang,J. Zhang,Xianyan Jia,Yong Li,Chencan Wu,Jialin Li,Wei Lin","id":"adfe0e8eea02b4fff42ab02ecd778ce02c73bf7d","summary":"A model parallelism framework TAP is presented that automatically searches for the best data and tensor parallel schedules, and the performance of its discovered schedules is competitive with the expert-engineered ones.","score":4},{"url":"https://www.semanticscholar.org/paper/74d0b14ce45c0af16a3bec9406f157009b415414","title":"Deep Class-Incremental Learning: A Survey","venue":"ArXiv","year":2023,"referenceCount":240,"citationCount":3,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Da-Wei Zhou,Qiwen Wang,Zhiyuan Qi,Han-Jia Ye,De-chuan Zhan,Ziwei Liu","id":"74d0b14ce45c0af16a3bec9406f157009b415414","summary":"A rigorous and unified evaluation of 16 methods in benchmark image classification tasks to find out the characteristics of different algorithms empirically and advocate fair comparison by aligning the memory budget in evaluation, as well as several memory-agnostic performance measures.","score":4},{"url":"https://www.semanticscholar.org/paper/45793225c6593db60e9efd95bce1d70bf4844198","title":"Evaluation Paradigms in Question Answering","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":135,"citationCount":5,"influentialCitationCount":0,"publicationDate":2021,"authors":"Pedro Rodriguez,Jordan L. Boyd-Graber","id":"45793225c6593db60e9efd95bce1d70bf4844198","summary":"By better understanding the epistemic heritage of QA, researchers, academia, and industry can more effectively accelerate QA research.","score":4},{"url":"https://www.semanticscholar.org/paper/0b09448f7543453cc066416f547292dc1e4471f6","title":"KILT: a Benchmark for Knowledge Intensive Language Tasks","venue":"North American Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":65,"citationCount":189,"influentialCitationCount":26,"publicationDate":"04/09/2020","authors":"Fabio Petroni,Aleksandra Piktus,Angela Fan,Patrick Lewis,Majid Yazdani,Nicola De Cao,James Thorne,Yacine Jernite,Vassilis Plachouras,Tim Rocktaschel,Sebastian Riedel","id":"0b09448f7543453cc066416f547292dc1e4471f6","summary":"It is found that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text.","score":4},{"url":"https://www.semanticscholar.org/paper/3122a2d7799ba585b993e432b3deb47659b3f3c1","title":"Hurdles to Progress in Long-form Question Answering","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":52,"citationCount":63,"influentialCitationCount":8,"publicationDate":"10/03/2021","authors":"Kalpesh Krishna,Aurko Roy,Mohit Iyyer","id":"3122a2d7799ba585b993e432b3deb47659b3f3c1","summary":"The task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress, and a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset is designed.","score":4},{"url":"https://www.semanticscholar.org/paper/b8c864635656a7ec09c8dcdfc85f600cfba12ccf","title":"Attention-guided Generative Models for Extractive Question Answering","venue":"ArXiv","year":2021,"referenceCount":30,"citationCount":5,"influentialCitationCount":1,"publicationDate":"12/10/2021","authors":"Peng Xu,Davis Liang,Zhiheng Huang,Bing Xiang","id":"b8c864635656a7ec09c8dcdfc85f600cfba12ccf","summary":"A simple strategy to obtain an extractive answer span from the generative model by leveraging the decoder cross-attention patterns, which allows for hallucination-free inference while conferring significant improvements to the model’s ability to rerank relevant passages.","score":4},{"url":"https://www.semanticscholar.org/paper/4919cd4ad287a3f0679846bd95c6805cb8dda4bd","title":"Generating Biographies on Wikipedia: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":88,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Angela Fan,Claire Gardent","id":"4919cd4ad287a3f0679846bd95c6805cb8dda4bd","summary":"A model for English text is developed that uses a retrieval mechanism to identify relevant supporting information on the web and a cache-based pre-trained encoder-decoder to generate long-form biographies section by section, including citation information.","score":4},{"url":"https://www.semanticscholar.org/paper/388513e8e09ad60f619054361f4d2cdf5a146bc8","title":"FeTaQA: Free-form Table Question Answering","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":49,"citationCount":24,"influentialCitationCount":6,"publicationDate":"01/04/2021","authors":"Linyong Nan,Chia-Hsuan Hsieh,Ziming Mao,Xi Victoria Lin,Neha Verma,Rui Zhang,Wojciech Kryscinski,Nick Schoelkopf,Riley Kong,Xiangru Tang,Murori Mutuma,B. Rosand,Isabel Trindade,Renusree Bandaru,Jacob Cunningham,Caiming Xiong,Dragomir R. Radev","id":"388513e8e09ad60f619054361f4d2cdf5a146bc8","summary":"This work introduces FeTaQA, a new dataset with 10K Wikipedia-based table, question, free-form answer, supporting table cells pairs, and provides two benchmark methods for the proposed task: a pipeline method based on semantic parsing-based QA systems and an end-to-end methodBased on large pretrained text generation models, and shows that FeTaZA poses a challenge for both methods.","score":4},{"url":"https://www.semanticscholar.org/paper/a295abfbbbcd92489d5a828d1c25e91e6d4393d4","title":"MixQG: Neural Question Generation with Mixed Answer Types","venue":"NAACL-HLT","year":2021,"referenceCount":51,"citationCount":19,"influentialCitationCount":1,"publicationDate":"15/10/2021","authors":"Lidiya Murakhovs'ka,C. Wu,Philippe Laban,Tong Niu,Wenhao Liu,Caiming Xiong","id":"a295abfbbbcd92489d5a828d1c25e91e6d4393d4","summary":"This paper introduces a neural question generator, MixQG, that outperforms existing work in both seen and unseen domains, and can generate questions with different cognitive levels when conditioned on different answer types.","score":4},{"url":"https://www.semanticscholar.org/paper/37297ac9b85129f8483078bcef6bcb519fb71b1b","title":"Read before Generate! Faithful Long Form Question Answering with Machine Reading","venue":"Findings","year":2022,"referenceCount":60,"citationCount":8,"influentialCitationCount":1,"publicationDate":"01/03/2022","authors":"Dan Su,Xiaoguang Li,Jindi Zhang,Lifeng Shang,Xin Jiang,Qun Liu,Pascale Fung","id":"37297ac9b85129f8483078bcef6bcb519fb71b1b","summary":"A new end-to-end framework is proposed that jointly models answer generation and machine reading with an emphasis on faithful facts and state-of-the-art results on two LFQA datasets demonstrate the effectiveness of the method, in comparison with strong baselines on automatic and human evaluation metrics.","score":4},{"url":"https://www.semanticscholar.org/paper/f7fd184eaa573205dff97d86c836f3038143e87a","title":"An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/10/2022","authors":"Yuxiang Wu,Yu Zhao,Baotian Hu,Pasquale Minervini,Pontus Stenetorp,Sebastian Riedel","id":"f7fd184eaa573205dff97d86c836f3038143e87a","summary":"The Efficient Memory-Augmented Transformer (EMAT) is proposed – it encodes external knowledge into a key-value memory and exploits the fast maximum inner product search for memory querying and produces more accurate results on WoW and ELI5.","score":4},{"url":"https://www.semanticscholar.org/paper/a63f3785611125257a7feefbb9533ca41af59c13","title":"CREPE: Open-Domain Question Answering with False Presuppositions","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Xinyan Velocity Yu,Sewon Min,Luke Zettlemoyer,Hannaneh Hajishirzi","id":"a63f3785611125257a7feefbb9533ca41af59c13","summary":"C REPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums, is introduced, showing that 25% of questions contain false presuppositions, and that adaptations of existing open-domain QA models can be adapted moderately well, but struggle when pre-dicting whether a presupposition is factually correct.","score":4},{"url":"https://www.semanticscholar.org/paper/2a9b5aadf2d79a56edcd60e332dc7b5973cd047d","title":"Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior","venue":"Transactions of the Association for Computational Linguistics","year":2021,"referenceCount":56,"citationCount":9,"influentialCitationCount":1,"publicationDate":"10/08/2021","authors":"Noriyuki Kojima,Alane Suhr,Yoav Artzi","id":"2a9b5aadf2d79a56edcd60e332dc7b5973cd047d","summary":"This work compares user execution of generated instructions to the original system intent as an indication to the system's success communicating its intent, and shows how to use this signal to improve the system’s ability to generate instructions via contextual bandit learning.","score":4},{"url":"https://www.semanticscholar.org/paper/56152658a3bb42e0afd0231e6fd942ea3dc3604f","title":"Simulating Bandit Learning from User Feedback for Extractive Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":53,"citationCount":3,"influentialCitationCount":0,"publicationDate":"18/03/2022","authors":"Ge Gao,Eunsol Choi,Yoav Artzi","id":"56152658a3bb42e0afd0231e6fd942ea3dc3604f","summary":"This work shows that systems initially trained on few examples can dramatically improve given feedback from users on model-predicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation effort, but instead improving the system on-the-fly via user feedback.","score":4},{"url":"https://www.semanticscholar.org/paper/6ba3e4172e5c22c8c3ace05a31e9b119a2e3c33c","title":"Continual Learning for Instruction Following from Realtime Feedback","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Alane Suhr,Yoav Artzi","id":"6ba3e4172e5c22c8c3ace05a31e9b119a2e3c33c","summary":"This work studies the problem of continually training an instruction-following agent from feedback provided by users during collaborative interactions, and shows that the feedback signal is roughly equivalent to the learning signal of supervised demonstration data.","score":4},{"url":"https://www.semanticscholar.org/paper/15ddeb9b812e4063a8b907d50c720e01c753b2b4","title":"Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress","venue":"","year":2022,"referenceCount":88,"citationCount":4,"influentialCitationCount":1,"publicationDate":"03/06/2022","authors":"Rishabh Agarwal,Max Schwarzer,P. S. Castro,Aaron C. Courville,Marc G. Bellemare","id":"15ddeb9b812e4063a8b907d50c720e01c753b2b4","summary":"This work argues for an alternative approach to RL research that could improve real-world RL adoption and help democratize it further, and presents reincarnating RL as an alternative workﬂow or class of problem settings.","score":4},{"url":"https://www.semanticscholar.org/paper/a66e581eca1e6531298563f39d8b4ccdcf9489e2","title":"Fast Inference and Transfer of Compositional Task Structures for Few-shot Task Generalization","venue":"Conference on Uncertainty in Artificial Intelligence","year":2022,"referenceCount":43,"citationCount":2,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"Sungryull Sohn,Hyunjae Woo,Jongwook Choi,lyubing qiang,Izzeddin Gur,Aleksandra Faust,Honglak Lee","id":"a66e581eca1e6531298563f39d8b4ccdcf9489e2","summary":"The proposed multi-task subtask graph inferencer (MTSGI) infers the common high-level task structure in terms of the subtaskgraph graph from the training tasks, and uses it as a prior to improve the task inference in testing.","score":4},{"url":"https://www.semanticscholar.org/paper/20e5561e3e576fbc10b0e97f8f64d8c875e17ad7","title":"A Few More Examples May Be Worth Billions of Parameters","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":65,"citationCount":5,"influentialCitationCount":0,"publicationDate":"08/10/2021","authors":"Yuval Kirstain,Patrick Lewis,S. Riedel,Omer Levy","id":"20e5561e3e576fbc10b0e97f8f64d8c875e17ad7","summary":"This work hypothesizes that unlike open question answering, which involves recalling specific information, solving strategies for tasks with a more restricted output space transfer across examples, and can therefore be learned with small amounts of labeled data, the contribution of additional examples highly depends on the task's format.","score":4},{"url":"https://www.semanticscholar.org/paper/f9ad1fffa1cc76fd5db3ff758c0839492c5147c4","title":"In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yukun Huang,Yanda Chen,Zhou Yu,K. McKeown","id":"f9ad1fffa1cc76fd5db3ff758c0839492c5147c4","summary":"This work proposes to combine in- context learning objectives with language modeling objectives to distill both the ability to read in-context examples and task knowledge to the smaller models and shows consistent improvements for both Meta-ICT and Multitask-ICT on two benchmarks.","score":4},{"url":"https://www.semanticscholar.org/paper/c44120f765fc43994c5cfb4e12e4f62999efeae6","title":"How Context Affects Language Models' Factual Predictions","venue":"Conference on Automated Knowledge Base Construction","year":2020,"referenceCount":56,"citationCount":96,"influentialCitationCount":18,"publicationDate":"14/02/2020","authors":"Fabio Petroni,Patrick Lewis,Aleksandra Piktus,Tim Rocktäschel,Yuxiang Wu,Alexander H. Miller,Sebastian Riedel","id":"c44120f765fc43994c5cfb4e12e4f62999efeae6","summary":"This paper reports that augmenting pre-trained language models in this way dramatically improves performance and that the resulting system, despite being unsupervised, is competitive with a supervised machine reading baseline.","score":4},{"url":"https://www.semanticscholar.org/paper/02d8cfa5bc5f086acc57bb4ac8e4e94515fed7f9","title":"PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them","venue":"Transactions of the Association for Computational Linguistics","year":2021,"referenceCount":73,"citationCount":102,"influentialCitationCount":27,"publicationDate":"13/02/2021","authors":"Patrick Lewis,Yuxiang Wu,Linqing Liu,Pasquale Minervini,Heinrich Kuttler,Aleksandra Piktus,Pontus Stenetorp,Sebastian Riedel","id":"02d8cfa5bc5f086acc57bb4ac8e4e94515fed7f9","summary":"It is found that PAQ preempts and caches test questions, enabling RePAQ to match the accuracy of recent retrieve-and-read models, whilst being significantly faster, and a new QA-pair retriever, RePAZ, is introduced to complement PAQ.","score":4},{"url":"https://www.semanticscholar.org/paper/b58d8579ece27a60432e667bfbdb750590fa65d9","title":"True Few-Shot Learning with Language Models","venue":"Neural Information Processing Systems","year":2021,"referenceCount":101,"citationCount":145,"influentialCitationCount":24,"publicationDate":"24/05/2021","authors":"Ethan Perez,Douwe Kiela,Kyunghyun Cho","id":"b58d8579ece27a60432e667bfbdb750590fa65d9","summary":"This work evaluates the few-shot ability of LMs when such held-out examples are unavailable, a setting the authors call true few- shot learning, and suggests that prior work significantly overestimated thetrue few-shots ability ofLMs given the difficulty of few-Shot model selection.","score":4},{"url":"https://www.semanticscholar.org/paper/537d98e241975dc5c32d9372ae85134dffe45532","title":"DP-KB: Data Programming with Knowledge Bases Improves Transformer Fine Tuning for Answer Sentence Selection","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/03/2022","authors":"Nic Jedema,Thuy Vu,Manish Gupta,Alessandro Moschitti","id":"537d98e241975dc5c32d9372ae85134dffe45532","summary":"This paper implements an efficient, data-programming technique that enriches training data with KB-derived context and improves transformer utilization of encoded knowledge when fine-tuning for a particular QA task, namely answer sentence selection (AS2).","score":4},{"url":"https://www.semanticscholar.org/paper/8ec9d3e29e1db209925f08f505bd5da9777d1dee","title":"Elaboration-Generating Commonsense Question Answering at Scale","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":2,"influentialCitationCount":0,"publicationDate":"02/09/2022","authors":"Wenya Wang,Vivek Srikumar,Hannaneh Hajishirzi,Noah A. Smith","id":"8ec9d3e29e1db209925f08f505bd5da9777d1dee","summary":"This work uses smaller language models to generate useful intermediate context, referred to here as elaborations, and alternates between updating two language models—an elaboration generator and an answer predictor—allowing each to inﬂuence the other.","score":4},{"url":"https://www.semanticscholar.org/paper/3917602c6139811fe3c20bc32070d6cee474fe49","title":"Knowledge Graph Generation From Text","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/11/2022","authors":"Igor Melnyk,Pierre L. Dognin,Payel Das","id":"3917602c6139811fe3c20bc32070d6cee474fe49","summary":"A novel end-to-end multi-stage Knowledge Graph (KG) generation system from textual inputs, separating the overall process into two stages, showing strong overall performance, outperforming the existing baselines.","score":4},{"url":"https://www.semanticscholar.org/paper/b2d783c0ed3bd2c631b99b1487399016a5f00d5f","title":"LabelPrompt: Effective Prompt-based Learning for Relation Classification","venue":"","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"W. Zhang,Xiaoning Song,Zhenhua Feng,Tianyang Xu,Xiaojun Wu","id":"b2d783c0ed3bd2c631b99b1487399016a5f00d5f","summary":"This paper presents a novel prompt-based learning method, namely LabelPrompt, for the relation classification task, and applies an attention query strategy to self-attention layers to resolve two types of tokens, prompt tokens and sequence tokens.","score":4},{"url":"https://www.semanticscholar.org/paper/91eb2e73f8f3574c00dc6787a3e277eedaf33531","title":"CONSISTENT: Open-Ended Question Generation From News Articles","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/10/2022","authors":"Tuhin Chakrabarty,Justin Lewis,S. Muresan","id":"91eb2e73f8f3574c00dc6787a3e277eedaf33531","summary":"This work proposes CONSISTENT, a new end-to-end system for generating open-ended questions that are answerable from and faithful to the input text, and demonstrates its strength over several baselines using both automatic and human=based evaluations.","score":4},{"url":"https://www.semanticscholar.org/paper/2843661ee0d5fa159165beba50c345566cc44c57","title":"Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"David Mueller,Nicholas Andrews,Mark Dredze","id":"2843661ee0d5fa159165beba50c345566cc44c57","summary":"This work studies how certain factors in the shift towards text-to-text models affects multi-task conflict and negative transfer, finding that both directional conflict and transfer are surprisingly constant across architectures.","score":4},{"url":"https://www.semanticscholar.org/paper/f9711475b3b20e349bdb1ee5f474d9aaabd784f1","title":"Zero-shot Triplet Extraction by Template Infilling","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Bosung Kim,Hayate Iso,Nikita Bhutani,Estevam Hruschka,Ndapa Nakashole","id":"f9711475b3b20e349bdb1ee5f474d9aaabd784f1","summary":"It is argued that reducing triplet extraction to a template ﬁlling task over a pre-trained language model can equip the model with zero-shot learning capabilities and enable it to leverage the implicit knowledge in the language model.","score":4},{"url":"https://www.semanticscholar.org/paper/9e42cb2b133bc41600824a1002cb05844c6a46a9","title":"Learning to Generate Task-Specific Adapters from Task Description","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":28,"citationCount":12,"influentialCitationCount":0,"publicationDate":"02/01/2021","authors":"Qinyuan Ye,Xiang Ren","id":"9e42cb2b133bc41600824a1002cb05844c6a46a9","summary":"Hypter is introduced, a framework that improves text-to-text transformer’s generalization ability to unseen tasks by training a hypernetwork to generate task-specific, light-weight adapters from task descriptions.","score":4},{"url":"https://www.semanticscholar.org/paper/6f9fc51102cf49bff4f4e2b336739a45f8389c80","title":"Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?","venue":"Findings","year":2021,"referenceCount":40,"citationCount":15,"influentialCitationCount":1,"publicationDate":"02/06/2021","authors":"Jieyu Zhao,Daniel Khashabi,Tushar Khot,Ashish Sabharwal,Kai-Wei Chang","id":"6f9fc51102cf49bff4f4e2b336739a45f8389c80","summary":"This work proposes a new language understanding task, Linguistic Ethical Interventions (LEI), where the goal is to amend a questionanswering (QA) model’s unethical behavior by communicating context-specific principles of ethics and equity to it.","score":4},{"url":"https://www.semanticscholar.org/paper/771923db144e8b6f908eb93693edef71a8b682ea","title":"What is Not in the Context? Evaluation of Few-shot Learners with Informative Demonstrations","venue":"","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/12/2022","authors":"Michal vStef'anik,Marek Kadlvc'ik","id":"771923db144e8b6f908eb93693edef71a8b682ea","summary":"This work argues that the commonly-used evaluation settings of few-shot models utilizing a random selection of in-context demonstrations is not able to disentangle models’ ability of learning new skills from demonstrations, as most of such-selected demonstrations are commonly not informative for prediction beyond exposing the new task’s input and output distribution.","score":4},{"url":"https://www.semanticscholar.org/paper/8ae9a17c87a4518b513e860683a0ef7824be994d","title":"Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":64,"citationCount":566,"influentialCitationCount":78,"publicationDate":"21/01/2020","authors":"Timo Schick,Hinrich Schütze","id":"8ae9a17c87a4518b513e860683a0ef7824be994d","summary":"This work introduces Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task.","score":4},{"url":"https://www.semanticscholar.org/paper/a6a83754a0d1e9a8e41c1e9bbdbca32d3b9d1fd3","title":"It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners","venue":"North American Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":65,"citationCount":415,"influentialCitationCount":55,"publicationDate":"15/09/2020","authors":"Timo Schick,Hinrich Schütze","id":"a6a83754a0d1e9a8e41c1e9bbdbca32d3b9d1fd3","summary":"This work shows that performance similar to GPT-3 can be obtained with language models that are much “greener” in that their parameter count is several orders of magnitude smaller, and identifies key factors required for successful natural language understanding with small language models.","score":4},{"url":"https://www.semanticscholar.org/paper/ace5e1d171f80181cd3fdecccd88d6a0ab89e0e0","title":"Towards Teachable Autonomous Agents","venue":"ArXiv","year":2021,"referenceCount":184,"citationCount":14,"influentialCitationCount":0,"publicationDate":2021,"authors":"Olivier Sigaud,Hugo Caselles-Dupr'e,Cédric Colas,Ahmed Akakzia,P. Oudeyer,M. Chetouani","id":"ace5e1d171f80181cd3fdecccd88d6a0ab89e0e0","summary":"The purpose of this paper is to elucidate the key obstacles standing in the way towards the design of teachable and autonomous agents and focus on autotelic agents, i.e. agents equipped with forms of intrinsic motivations that enable them to represent, self-generate and pursue their own goals.","score":4},{"url":"https://www.semanticscholar.org/paper/7cbe7c4bad376c9097a17ca4015c309bf1b17993","title":"Co-training Improves Prompt-based Learning for Large Language Models","venue":"International Conference on Machine Learning","year":2022,"referenceCount":48,"citationCount":8,"influentialCitationCount":1,"publicationDate":"02/02/2022","authors":"Hunter Lang,Monica Agrawal,Yoon Kim,D. Sontag","id":"7cbe7c4bad376c9097a17ca4015c309bf1b17993","summary":"Co-training makes it possible to improve the original prompt model and at the same time learn a smaller, downstream task-specific model, and models trained in this manner can significantly improve performance on challenging datasets where there is currently a large gap between prompt-based learning and fully-supervised models.","score":4},{"url":"https://www.semanticscholar.org/paper/c513df38b37b9bae49641a8ad73b01e40fb14f7e","title":"Online learning with binary feedback for multi-class problems","venue":"IEEE Symposium Series on Computational Intelligence","year":2022,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/12/2022","authors":"Evan Lucas,Steven Whitaker,T. Havens","id":"c513df38b37b9bae49641a8ad73b01e40fb14f7e","summary":"This work investigates methods for using and labeling training data in the absence of complete information by attempting to usefully utilize a binary feedback method where the human indicates whether the prediction is correct or incorrect.","score":4},{"url":"https://www.semanticscholar.org/paper/1439777b9e22dca3a10a6c4b9faa712bc74510cc","title":"Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Mingzhu Cai,Siqi Bao,Xin Tian,H. He,Fan Wang,Hua Wu","id":"1439777b9e22dca3a10a6c4b9faa712bc74510cc","summary":"This paper proposes an unsupervised query enhanced approach for knowledge-intensive conversations, namely QKConv, and con-ducted comprehensive experiments on conversational question-answering, task-oriented dialogue, and knowledge-grounded conversation to demonstrate the effectiveness of the proposed method.","score":4},{"url":"https://www.semanticscholar.org/paper/f1e7332a76be8f38091193e6e929d0d653f4867c","title":"On The Fragility of Learned Reward Functions","venue":"ArXiv","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/01/2023","authors":"Lev McKinney,Yawen Duan,David Krueger,A. Gleave","id":"f1e7332a76be8f38091193e6e929d0d653f4867c","summary":"This work demonstrates with experiments in tabular and continuous control environments that the severity of relearning failures can be sensitive to changes in reward model design and the trajectory dataset composition.","score":4},{"url":"https://www.semanticscholar.org/paper/8505170798a21bd7764955f9952cbe346a5d0a50","title":"Molecular Language Model as Multi-task Generator","venue":"ArXiv","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/01/2023","authors":"Yin Fang,Ningyu Zhang,Zhuo Chen,Xiaohui Fan,Huajun Chen","id":"8505170798a21bd7764955f9952cbe346a5d0a50","summary":"This work proposes M OL G EN, a pre-trained molecular language model that effectively learns and shares knowledge across multiple generation tasks and domains, and proposes multi-task molecular preﬁx tuning across several moleculargeneration tasks and different molecular domains with a self-feedback mechanism.","score":4},{"url":"https://www.semanticscholar.org/paper/f4db4a18080e9e4ebba9ba68bb7d34230c84d0c3","title":"Truth Machines: Synthesizing Veracity in AI Language Models","venue":"ArXiv","year":2023,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/01/2023","authors":"Luke Munn,L. Magee,Vanicka Arora","id":"f4db4a18080e9e4ebba9ba68bb7d34230c84d0c3","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/2e96513b803d8d3b0e06373edd33cda4aa72df27","title":"On the portability of extractive Question-Answering systems on scientific papers to real-life application scenarios","venue":"WIESP","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Chyrine Tahri,Xavier Tannier,Patrick Haouat","id":"2e96513b803d8d3b0e06373edd33cda4aa72df27","summary":"This work addresses the portability of extractive Question Answering systems from academic spheres to industries basing their decisions on thorough scientific papers analysis by adopting the pipeline-based retriever-ranker-reader architecture for answering a question on a scientific paper.","score":4},{"url":"https://www.semanticscholar.org/paper/96df989cba0e6cd88b994c4459349fd80b7cf213","title":"Attentional Mixtures of Soft Prompt Tuning for Parameter-efficient Multi-task Knowledge Sharing","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":10,"influentialCitationCount":3,"publicationDate":2022,"authors":"Akari Asai,Mohammadreza Salehi,Matthew E. Peters,Hannaneh Hajishirzi","id":"96df989cba0e6cd88b994c4459349fd80b7cf213","summary":"Experimental results across 17 diverse datasets show that ATTEMPT improves prompt tuning by up to a 22% absolute performance gain and outperforms or matches fully fine-tuned or other parameter-efficient tuning approaches that use over ten times more parameters.","score":4},{"url":"https://www.semanticscholar.org/paper/775c439186b037c09cd9f95b9daf81d23ca21b54","title":"WinoDict: Probing language models for in-context word acquisition","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/09/2022","authors":"Julian Martin Eisenschlos,Jeremy R. Cole,Fangyu Liu,William W. Cohen","id":"775c439186b037c09cd9f95b9daf81d23ca21b54","summary":"A new in-context learning paradigm is introduced to measure Large Language Models’ (LLMs) ability to learn novel words during in-ference, rewriting Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task.","score":4},{"url":"https://www.semanticscholar.org/paper/b4c16b0f26f9f5ad5e12f9bec3f1ad72eaa5491b","title":"Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":130,"citationCount":8,"influentialCitationCount":2,"publicationDate":"24/10/2022","authors":"Maarten Sap,Ronan Le Bras,Daniel Fried,Yejin Choi","id":"b4c16b0f26f9f5ad5e12f9bec3f1ad72eaa5491b","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/cc7462b76d5e8acb28e61ea1f57e17905540a415","title":"TEMPERA: Test-Time Prompting via Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Tianjun Zhang,Xuezhi Wang,Denny Zhou,D. Schuurmans,Joseph Gonzalez","id":"cc7462b76d5e8acb28e61ea1f57e17905540a415","summary":"Theency of the method compared to netuning is compared to TEMPERA, showing that TEMperA can achieve same performance with 5.33x less data.","score":4},{"url":"https://www.semanticscholar.org/paper/7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey","venue":"ACM Computing Surveys","year":2020,"referenceCount":104,"citationCount":478,"influentialCitationCount":49,"publicationDate":"14/09/2020","authors":"Yi Tay,M. Dehghani,Dara Bahri,Donald Metzler","id":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","summary":"This article characterizes a large and thoughtful selection of recent efficiency-flavored “X-former” models, providing an organized and comprehensive overview of existing work and models across multiple domains.","score":4},{"url":"https://www.semanticscholar.org/paper/ec31364e266ac691da29be7d2309fc2a4f8e0ee6","title":"Creative Writing with an AI-Powered Writing Assistant: Perspectives from Professional Writers","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"Daphne Ippolito,Ann Yuan,Andy Coenen,Sehmon Burnam","id":"ec31364e266ac691da29be7d2309fc2a4f8e0ee6","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/428854d9e75f94f0e61f37c6887c77800437d516","title":"MusicLM: Generating Music From Text","venue":"ArXiv","year":2023,"referenceCount":49,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/01/2023","authors":"A. Agostinelli,Timo I. Denk,Zalán Borsos,Jesse Engel,M. Verzetti,Antoine Caillon,Qingqing Huang,A. Jansen,Adam Roberts,M. Tagliasacchi,Matthew Sharifi,Neil Zeghidour,C. Frank","id":"428854d9e75f94f0e61f37c6887c77800437d516","summary":"This work introduces MusicLM, a model for generating high-ﬁdelity music from text descriptions such as “a calming violin melody backed by a distorted guitar riff” and demonstrates that it can transform whistled and hummed melodies according to the style described in a text caption.","score":4},{"url":"https://www.semanticscholar.org/paper/17a8b5e6fef1f69979d57021a8f30a5159e152c7","title":"Commonsense Reasoning for Conversational AI: A Survey of the State of the Art","venue":"","year":2023,"referenceCount":90,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Christopher Richardson,Larry Heck","id":"17a8b5e6fef1f69979d57021a8f30a5159e152c7","summary":"A survey of recent conversational AI research focused on commonsense reasoning, including preliminary observations of the limited commonsense capabilities of two state-of-the-art open dialogue models, BlenderBot3 and LaMDA, and its negative effect on natural interactions.","score":4},{"url":"https://www.semanticscholar.org/paper/46f0064af5557e4a2c9c65d245815bc29050ea62","title":"LegalBench: Prototyping a Collaborative Benchmark for Legal Reasoning","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/09/2022","authors":"Neel Guha,Daniel E. Ho,Julian Nyarko,Christopher R'e","id":"46f0064af5557e4a2c9c65d245815bc29050ea62","summary":"How IRAC—a framework legal scholars use to distinguish types of legal reasoning—can guide the construction of a Foundation Model oriented benchmark is described and a seed set of 44 tasks built according to this framework is presented.","score":4},{"url":"https://www.semanticscholar.org/paper/ad62d710f1854daf372680263f50a4e135e309f2","title":"CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/10/2022","authors":"Steven Y. Feng,Vivek Khetan,Bogdan Sacaleanu,A. Gershman,E. Hovy","id":"ad62d710f1854daf372680263f50a4e135e309f2","summary":"It is shown that while the models can perform decently, CHARD is very challenging with strong potential for further exploration.","score":4},{"url":"https://www.semanticscholar.org/paper/c094cfee1e93cb76f5fc867ae6d89c83ed0d55ef","title":"PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":59,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/10/2022","authors":"Yang Deng,Wenqiang Lei,Wenxuan Zhang,W. Lam,Tat-Seng Chua","id":"c094cfee1e93cb76f5fc867ae6d89c83ed0d55ef","summary":"A novel method is proposed, namely UniPCQA, to adapt a hybrid format of input and output content in PCQA into the Seq2Seq problem, including the reformulation of the numerical reasoning process as code generation.","score":4},{"url":"https://www.semanticscholar.org/paper/1d417bdd331912a458de920459f23fcc7f6e8699","title":"Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":38,"citationCount":3,"influentialCitationCount":0,"publicationDate":"30/10/2022","authors":"Ben Zhou,Kyle Richardson,Xiaodong Yu,D. Roth","id":"1d417bdd331912a458de920459f23fcc7f6e8699","summary":"It is shown that with such intermediate pre-training, developing robust decomposition-based models for a diverse range of tasks becomes more feasible, and the model, DecompT5, improves 20% to 30% on two datasets, Overnight and TORQUE, over the baseline language model.","score":4},{"url":"https://www.semanticscholar.org/paper/18b3ab9763ed3c4633ee68aa6dd75f6377837553","title":"Natural Language Deduction with Incomplete Information","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Zayne Sprague,Kaj Bostrom,Swarat Chaudhuri,Greg Durrett","id":"18b3ab9763ed3c4633ee68aa6dd75f6377837553","summary":"This work proposes a new system that can handle the underspecified setting where not all premises are stated at the outset; that is, additional assumptions need to be materialized to prove a claim.","score":4},{"url":"https://www.semanticscholar.org/paper/1b4c0a28b0c2a30cc3b84a3222e795c90357bc8a","title":"SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images","venue":"ArXiv","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2023","authors":"Ryota Tanaka,Kyosuke Nishida,Kosuke Nishida,Taku Hasegawa,Itsumi Saito,Kuniko Saito","id":"1b4c0a28b0c2a30cc3b84a3222e795c90357bc8a","summary":"A new multi- image document VQA dataset, SlideVQA, containing 2.6k-+ slide decks composed of 52k+ slide images and 14.5k ques- tions about a slide deck is proposed, and a new end-to-end document VZA model is developed that treats evidence selection and question answering in a sequence- to-sequence format.","score":4},{"url":"https://www.semanticscholar.org/paper/4e2e59546a4d67dd25f30ca744feb5717e03d9fc","title":"SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning","venue":"","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jongjin Park,Younggyo Seo,Jinwoo Shin,Honglak Lee,P. Abbeel,Kimin Lee","id":"4e2e59546a4d67dd25f30ca744feb5717e03d9fc","summary":"SURF is presented, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation and improves the feedback-efficiency of the state-ofthe-art preference-based method on a variety of locomotion and robotic manipulation tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/b0726dd009bae5f602a4e71ac5f9e8f53b6e385c","title":"Advances in Preference-based Reinforcement Learning: A Review","venue":"IEEE International Conference on Systems, Man and Cybernetics","year":2022,"referenceCount":56,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/10/2022","authors":"Youssef Abdelkareem,Shady Shehata,F. Karray","id":"b0726dd009bae5f602a4e71ac5f9e8f53b6e385c","summary":"A unified PbRL framework is presented to include the newly emerging approaches that improve the scalability and efficiency of Pb RL.","score":4},{"url":"https://www.semanticscholar.org/paper/24615e613d4551f7aaa0557befa0a8bc403f39cd","title":"Stateful Memory-Augmented Transformers for Dialogue Modeling","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/09/2022","authors":"Qing-yang Wu,Zhou Yu","id":"24615e613d4551f7aaa0557befa0a8bc403f39cd","summary":"A new memory-augmented Transformer is proposed that is compatible with existing pre-trained encoder-decoder models and enables efﬁcient preservation of history information and incorporates a separate memory module alongside the pre- trained Transformer to effectively interchange information between the memory states and the current input context.","score":4},{"url":"https://www.semanticscholar.org/paper/8283064365ae7594d891e8b7daf36fd37ca809b0","title":"Achieving and Understanding Out-of-Distribution Generalization in Systematic Reasoning in Small-Scale Transformers","venue":"","year":2022,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/10/2022","authors":"A. Nam,Mustafa Abdool,Trevor Maxfield,James L. McClelland","id":"8283064365ae7594d891e8b7daf36fd37ca809b0","summary":"It is shown that OODG can occur on a complex problem if the training set includes examples sampled from the whole distribution of simpler component tasks, and that suppressing sensitivity to absolute positions overcomes this limitation.","score":4},{"url":"https://www.semanticscholar.org/paper/b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce","title":"Towards a Mathematics Formalisation Assistant using Large Language Models","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Ayush Agrawal,Siddhartha Gadgil,Navin Goyal,Ashvni Narayanan,Anand Tadipatri","id":"b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce","summary":"The abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover are explored, finding that with careful inputdependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with nearly 75% accuracy for 120 theorem statements.","score":4},{"url":"https://www.semanticscholar.org/paper/5c8cb5d39aa8531fcb5962821ac404e4a0e109d1","title":"Is It Smaller Than a Tennis Ball? Language Models Play the Game of Twenty Questions","venue":"BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans","id":"5c8cb5d39aa8531fcb5962821ac404e4a0e109d1","summary":"It is found that only the largest model has enough world knowledge to play the game of Twenty Questions well, although it still has difficulties with the shape and size of objects.","score":4},{"url":"https://www.semanticscholar.org/paper/f557f3a32d309373e7d31bb93ca1b80b4a6e39e7","title":"Symbolic Math Reasoning with Language Models","venue":"2022 IEEE MIT Undergraduate Research Technology Conference (URTC)","year":2022,"referenceCount":10,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/09/2022","authors":"Vedant Gaur,Nikunj Saunshi","id":"f557f3a32d309373e7d31bb93ca1b80b4a6e39e7","summary":"GPT-3’s davinci-002 model, in addition to having good performance on numerical math word problems, also performs well on the potentially harder symbolic version of the same problems, and adopting a two-step approach leads to better accuracy on the numerical test set in the zero-shot regime.","score":4},{"url":"https://www.semanticscholar.org/paper/66f333c51e2bfa25380069f66500b491218da9c3","title":"Can Pretrained Language Models (Yet) Reason Deductively?","venue":"ArXiv","year":2022,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Zhangdie Yuan,Songbo Hu,Ivan Vulic,A. Korhonen,Zaiqiao Meng","id":"66f333c51e2bfa25380069f66500b491218da9c3","summary":"It is suggested that PLMs cannot yet perform reliable deductive reasoning, demonstrating the importance of controlled examinations and probing of PLMs' reasoning abilities; the results reach beyond (misleading) task performance, revealing thatPLMs are still far from human-level reasoning capabilities, even for simple deductive tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/32635a3daba6cbd7f0dd930aa325254b191c1343","title":"Multilingual Relation Classification via Efficient and Effective Prompting","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Yuxuan Chen,David Harbecke,Leonhard Hennig","id":"32635a3daba6cbd7f0dd930aa325254b191c1343","summary":"This paper presents the first work on prompt-based multilingual relation classification (RC), by introducing an efficient and effective method that constructs prompts from relation triples and involves only minimal translation for the class labels.","score":4},{"url":"https://www.semanticscholar.org/paper/3f4f11b530bd6c3faa1f7fcf6bde9dae95a19673","title":"Zero-Shot Classification by Logical Reasoning on Natural Language Explanations","venue":"ArXiv","year":2022,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Chi Han,Hengzhi Pei,X. Du,Heng Ji","id":"3f4f11b530bd6c3faa1f7fcf6bde9dae95a19673","summary":"This work proposes the framework CLORE (Classification by LOgical Reasoning on Explanations).","score":4},{"url":"https://www.semanticscholar.org/paper/acc61b1a55ace8bba5595d8aee3dc9a14370a4d8","title":"Learning UI Navigation through Demonstrations composed of Macro Actions","venue":"ArXiv","year":2021,"referenceCount":11,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/10/2021","authors":"Wei Li","id":"acc61b1a55ace8bba5595d8aee3dc9a14370a4d8","summary":"This work has developed a framework to reliably build agents capable of UI navigation and made a customization of DQfD to allow demos collected on screenshots to facilitate the demo coverage of rare cases.","score":4},{"url":"https://www.semanticscholar.org/paper/300529850ca8e8ad6633a2b566206bf7f2a38fd9","title":"Evolving Curricula with Regret-Based Environment Design","venue":"International Conference on Machine Learning","year":2022,"referenceCount":99,"citationCount":24,"influentialCitationCount":4,"publicationDate":"02/03/2022","authors":"Jack Parker-Holder,Minqi Jiang,Michael Dennis,Mikayel Samvelyan,J. Foerster,Edward Grefenstette,Tim Rocktaschel","id":"300529850ca8e8ad6633a2b566206bf7f2a38fd9","summary":"This work proposes harnessing the power of evolution in a principled, regret-based curriculum that seeks to constantly produce levels at the frontier of an agent’s capabilities, resulting in curricula that start simple but become increasingly complex.","score":4},{"url":"https://www.semanticscholar.org/paper/549bfdfd9fa718331076810f0d5817adcd79fe69","title":"AutoDIME: Automatic Design of Interesting Multi-Agent Environments","venue":"ArXiv","year":2022,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/03/2022","authors":"I. Kanitscheider,Harrison Edwards","id":"549bfdfd9fa718331076810f0d5817adcd79fe69","summary":"The results suggest that intrinsic teacher rewards, and in particular value disagreement, are a promising approach for automating both single and multi-agent environment design.","score":4},{"url":"https://www.semanticscholar.org/paper/590432f953b6ce1b4b36bf66a2ac65eeee567515","title":"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":77,"citationCount":75,"influentialCitationCount":24,"publicationDate":"02/12/2021","authors":"Keshav Santhanam,O. Khattab,Jon Saad-Falcon,Christopher Potts,M. Zaharia","id":"590432f953b6ce1b4b36bf66a2ac65eeee567515","summary":"Maize is introduced, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction and establishes state-of-the-art quality within and outside the training domain.","score":4},{"url":"https://www.semanticscholar.org/paper/9d40837175577bb0009b138269b422f6d5820d00","title":"Transformer Memory as a Differentiable Search Index","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":39,"influentialCitationCount":9,"publicationDate":"14/02/2022","authors":"Yi Tay,V. Tran,M. Dehghani,Jianmo Ni,Dara Bahri,Harsh Mehta,Zhen Qin,Kai Hui,Zhe Zhao,Jai Gupta,Tal Schuster,William W. Cohen,Donald Metzler","id":"9d40837175577bb0009b138269b422f6d5820d00","summary":"The Differentiable Search Index is introduced, a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process.","score":4},{"url":"https://www.semanticscholar.org/paper/82938e991a4094022bc190714c5033df4c35aaf2","title":"Retrieval-Augmented Reinforcement Learning","venue":"International Conference on Machine Learning","year":2022,"referenceCount":93,"citationCount":12,"influentialCitationCount":0,"publicationDate":"17/02/2022","authors":"Anirudh Goyal,A. Friesen,Andrea Banino,T. Weber,Nan Rosemary Ke,Adrià Puigdomènech Badia,A. Guez,Mehdi Mirza,Ksenia Konyushkova,Michal Valko,Simon Osindero,T. Lillicrap,N. Heess,C. Blundell","id":"82938e991a4094022bc190714c5033df4c35aaf2","summary":"This paper augments an RL agent with a retrieval process (parameterized as a neural network) that has direct access to a dataset of experiences that may be useful in the current context, to help the agent achieve its goal faster and more efﬁciently.","score":4},{"url":"https://www.semanticscholar.org/paper/76453afd79968f2de8356beaca5e0468715feab8","title":"Non-Parametric Temporal Adaptation for Social Media Topic Classification","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":2,"influentialCitationCount":0,"publicationDate":"13/09/2022","authors":"Fatemehsadat Mireshghallah,Nikolai Vogler,Junxian He,Omar U. Florez,Ahmed El-Kishky,Taylor Berg-Kirkpatrick","id":"76453afd79968f2de8356beaca5e0468715feab8","summary":"This paper studies temporal adaptation through the task of longitudinal hash- tag prediction and proposes a non-parametric technique as a simple but effective solution: non- Parametric classiﬁers use datastores which can be updated, either to adapt to test distribution shift or training data deletion, without re-training.","score":4},{"url":"https://www.semanticscholar.org/paper/38b0803b59e4973f09018ce942164b02be4b8bc9","title":"MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":43,"citationCount":6,"influentialCitationCount":3,"publicationDate":"06/10/2022","authors":"Wenhu Chen,Hexiang Hu,Xi Chen,Pat Verga,William W. Cohen","id":"38b0803b59e4973f09018ce942164b02be4b8bc9","summary":"The first Multimodal Retrieval-Augmented Transformer (MuRAG) is proposed, which accesses an external non-parametric multimodal memory to augment language generation and achieves state-of-the-art accuracy.","score":4},{"url":"https://www.semanticscholar.org/paper/135b66a1a3fd887f89c32524139dba915a27f61b","title":"Revision Transformers: Getting RiT of No-Nos","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/10/2022","authors":"Felix Friedrich,Wolfgang Stammer,P. Schramowski,K. Kersting","id":"135b66a1a3fd887f89c32524139dba915a27f61b","summary":"This work proposes the Revision Transformer (RiT), a combination of a large-scale pre-trained LM that inherently but also diffusely encodes world knowledge with a clear-structured revision engine that makes it possible to update the model's knowledge with little effort and the help of user interaction.","score":4},{"url":"https://www.semanticscholar.org/paper/d95b441c2838888d7ac1af73b5f9c800f22fad3a","title":"An Invariant Learning Characterization of Controlled Text Generation","venue":"","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Claudia Shi,Carolina Zheng,Amir Feder,Keyon Vafa,D. Blei","id":"d95b441c2838888d7ac1af73b5f9c800f22fad3a","summary":"This paper shows that the performance of controlled generation may be poor if the target distribution of text differs from the distribution the predictor was trained on, and takes inspiration from causal representation learning and cast controlled generation under distribution shift as an invariant learning problem.","score":4},{"url":"https://www.semanticscholar.org/paper/e5aa2a1e36a2c68fa4aa59afdb8b6e1c419f547c","title":"Natural Language Deduction through Search over Statement Compositions","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":36,"citationCount":16,"influentialCitationCount":3,"publicationDate":"16/01/2022","authors":"Kaj Bostrom,Zayne Sprague,Swarat Chaudhuri,Greg Durrett","id":"e5aa2a1e36a2c68fa4aa59afdb8b6e1c419f547c","summary":"This work proposes a system for doing deductive reasoning in natural language by decomposing the task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system's reasoning process.","score":4},{"url":"https://www.semanticscholar.org/paper/de04aa282f8055cebe86966c592bf37af6aecc99","title":"The Unreliability of Explanations in Few-Shot In-Context Learning","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":18,"influentialCitationCount":2,"publicationDate":2022,"authors":"Xi Ye,Greg Durrett","id":"de04aa282f8055cebe86966c592bf37af6aecc99","summary":"A framework for calibrating model predictions based on the reliability of explanations is presented and it is shown that explanations judged as good by humans—those that are logically consistent with the input and the prediction—usually indicate more accurate predictions.","score":4},{"url":"https://www.semanticscholar.org/paper/a3780e8f45b6c1ab3e27c0ad87c3ea23176dc8a7","title":"The Curious Case of Control","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/05/2022","authors":"Elias Stengel-Eskin,Benjamin Van Durme","id":"a3780e8f45b6c1ab3e27c0ad87c3ea23176dc8a7","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/cdc0c46d510fb70367a084e1bf2ee63155b47569","title":"What does a platypus look like? Generating customized prompts for zero-shot image classification","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":7,"influentialCitationCount":2,"publicationDate":"07/09/2022","authors":"Sarah Pratt,Rosanne Liu,Ali Farhadi","id":"cdc0c46d510fb70367a084e1bf2ee63155b47569","summary":"This work introduces a simple method to generate higher accuracy prompts, without relying on any explicit knowledge of the task domain and with far fewer hand-constructed sentences, and improves accuracy on a range of zero-shot image classiﬁcation benchmarks, including over one percentage point gain on ImageNet.","score":4},{"url":"https://www.semanticscholar.org/paper/b4fcd453c04dc5312ebb5a33f248c9fbd112cf87","title":"Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":1,"influentialCitationCount":0,"publicationDate":"22/09/2022","authors":"Xingdi Yuan,Tong Wang,Yen-Hsiang Wang,Emery Fine,Rania Abdelghani,Pauline Lucas,H'elene Sauz'eon,P. Oudeyer","id":"b4fcd453c04dc5312ebb5a33f248c9fbd112cf87","summary":"This case study framed in the context of question generation proposes two prompt-based approaches to selecting high-quality questions from a set of LLM-generated candidates and empirically demon-strate that the proposed approach can effectively select questions of higher qualities than greedy generation.","score":4},{"url":"https://www.semanticscholar.org/paper/ad3dfb2514cb0c899fcb9a14d229ff2a6018892f","title":"Deep Bidirectional Language-Knowledge Graph Pretraining","venue":"ArXiv","year":2022,"referenceCount":87,"citationCount":6,"influentialCitationCount":0,"publicationDate":"17/10/2022","authors":"Michihiro Yasunaga,Antoine Bosselut,Hongyu Ren,Xikun Zhang,Christopher D. Manning,Percy Liang,J. Leskovec","id":"ad3dfb2514cb0c899fcb9a14d229ff2a6018892f","summary":"D RAGON is proposed, a self-supervised method to pretrain a deeply joint language-knowledge foundation model from text and KG at scale and achieves strong performance on complex reasoning about language and knowledge and low-resource QA and new state-of-the-art results on various BioNLP tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/7a21e72f1f7b3824bf8f33676a99a305ed1558a2","title":"Prompting Language Models for Linguistic Structure","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Terra Blevins,Hila Gonen,Luke Zettlemoyer","id":"7a21e72f1f7b3824bf8f33676a99a305ed1558a2","summary":"This work presents a structured prompting approach that can be used to prompt for linguistic structure prediction tasks, allowing it to perform zero-and few-shot sequence tagging with autoregressive PLMs and shows that structured prompting can retrieve linguistic structure even with arbitrary labels.","score":4},{"url":"https://www.semanticscholar.org/paper/b562be15b076b494023b8ac24fc8c459f4fdf80a","title":"Craft an Iron Sword: Dynamically Generating Interactive Game Characters by Prompting Large Language Models Tuned on Code","venue":"WORDPLAY","year":2022,"referenceCount":21,"citationCount":3,"influentialCitationCount":1,"publicationDate":2022,"authors":"Ryan Volum,Sudha Rao,Michael Xu,Gabriel DesGarennes,Chris Brockett,Benjamin Van Durme,Olivia Deng,Akanksha Malhotra,Bill Dolan","id":"b562be15b076b494023b8ac24fc8c459f4fdf80a","summary":"It is demonstrated that use of a few example conversational prompts can power a conversational agent to generate both natural language and novel code, which can permit development of NPCs with which players can have grounded conversations that are free-form and less repetitive.","score":4},{"url":"https://www.semanticscholar.org/paper/f9d38e03c97562b5f5942f3a0c43bdb751b9dc1c","title":"Wordplay 2022 The 3rd Wordplay: When Language Meets Games Workshop Proceedings of the Workshop","venue":"","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Shrimai Prabhumoye","id":"f9d38e03c97562b5f5942f3a0c43bdb751b9dc1c","summary":"Novel techniques to generate text in a particular style are described, providing an approach of generating engaging naturalistic conversation responses using knowledge generated by pre-trained language models, considering their recent success in a multitude of NLP tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/c6bb04f3d8000b7e800f6359082de39548c7da79","title":"Capturing Structural Locality in Non-parametric Language Models","venue":"International Conference on Learning Representations","year":2021,"referenceCount":47,"citationCount":5,"influentialCitationCount":0,"publicationDate":"06/10/2021","authors":"Frank F. Xu,Junxian He,Graham Neubig,V. Hellendoorn","id":"c6bb04f3d8000b7e800f6359082de39548c7da79","summary":"This paper proposes a simple yet effective approach for adding locality information into non-parametric language models by adding learned parameters that improve the likelihood of retrieving examples from local neighborhoods.","score":4},{"url":"https://www.semanticscholar.org/paper/4a4581003f56e8cb581ad6f383c037964765d3d5","title":"Active Programming by Example with a Natural Language Prior","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":2,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"Ruiqi Zhong,Charles Burton Snell,D. Klein,Jason Eisner","id":"4a4581003f56e8cb581ad6f383c037964765d3d5","summary":"APEL, a new framework that enables non-programmers to indirectly annotate natural language utterances with executable meaning representations, such as SQL programs, is introduced, to reduce effort required from annotators and synthesize simple input databases that nonetheless have high information gain.","score":4},{"url":"https://www.semanticscholar.org/paper/7107d06366b48b3593c8128ed2ca67e0b413628c","title":"Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions","venue":"","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/05/2022","authors":"Ansong Ni,J. Inala,Chenglong Wang,Oleksandr Polozov,Christopher Meek,Dragomir R. Radev,Jianfeng Gao","id":"7107d06366b48b3593c8128ed2ca67e0b413628c","summary":"It is shown that the use of self-sampled correct and partially-correct solutions can benefit learning and help guide the sampling process, leading to more efficient exploration of the solution space and the effectiveness of the method is shown.","score":4},{"url":"https://www.semanticscholar.org/paper/35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":5,"influentialCitationCount":3,"publicationDate":2022,"authors":"Ansong Ni,J. Inala,Chenglong Wang,Oleksandr Polozov,Christopher Meek,Dragomir R. Radev,Jianfeng Gao","id":"35afb74de9660962ebac2843d26de22a6fac2ef6","summary":"This work proposes to let the model perform sampling during training and learn from both self-sampled fully-Correct programs, which yield the gold execution results, as well as partially-correct programs, whose intermediate execution state matches another correct program.","score":4},{"url":"https://www.semanticscholar.org/paper/40edfa97cd02268fccff75eb9c693b11c1a968b2","title":"Formal Specifications from Natural Language","venue":"ArXiv","year":2022,"referenceCount":97,"citationCount":1,"influentialCitationCount":0,"publicationDate":"04/06/2022","authors":"Christopher Hahn,Frederik Schmitt,Julia J. Tillman,Niklas Metzger,Julian Siber,B. Finkbeiner","id":"40edfa97cd02268fccff75eb9c693b11c1a968b2","summary":"These experiments show that language models maintain their generalization capabilities from pre-trained knowledge of natural language to generalize, e.g., to new variable names or operator descriptions, and achieve competitive performance, and even outperform the state-of-the-art for translating into regular expressions.","score":4},{"url":"https://www.semanticscholar.org/paper/075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":7,"influentialCitationCount":2,"publicationDate":"04/06/2022","authors":"J. Inala,Chenglong Wang,Mei Yang,Andrés Codas,Mark Encarnaci'on,Shuvendu K. Lahiri,M. Musuvathi,Jianfeng Gao","id":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","summary":"C ODE R ANKER is a neural ranker that can predict the correctness of a sampled program without executing it and can signiﬁcantly increase the pass@1 accuracy of various code generation models on APPS, HumanEval, and MBPP datasets.","score":4},{"url":"https://www.semanticscholar.org/paper/3ba793e937cb90ea3e82b4a6903ee4a95f307ddf","title":"X-Risk Analysis for AI Research","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":12,"influentialCitationCount":1,"publicationDate":"13/06/2022","authors":"Dan Hendrycks,Mantas Mazeika","id":"3ba793e937cb90ea3e82b4a6903ee4a95f307ddf","summary":"A collection of time-tested concepts from hazard analysis and systems safety, which have been designed to steer large processes in safer directions are reviewed, to discuss how AI researchers can realistically have long-term impacts on the safety of AI systems.","score":4},{"url":"https://www.semanticscholar.org/paper/780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation","venue":"","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/08/2022","authors":"Federico Cassano,John Gouwar,Daniel Nguyen,S. Nguyen,Luna Phipps-Costin,Donald Pinckney,Ming-Ho Yee,Yangtian Zi,Carolyn Jane Anderson,Molly Q. Feldman,Arjun Guha,M. Greenberg,Abhinav Jangda","id":"780f7eebde16b1ae5843df3a79a7772899ef6a71","summary":"This work creates the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages and evaluates the multi-language performance of three state-of-the-art code generation models.","score":4},{"url":"https://www.semanticscholar.org/paper/6b5d1e50894b1f28e4798cf20e9ffa88b9ec011a","title":"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/09/2022","authors":"Hai Dang,Lukas Mecke,Florian Lehmann,Sven Goller,D. Buschek","id":"6b5d1e50894b1f28e4798cf20e9ffa88b9ec011a","summary":"This work discusses the key opportunities and challenges for interactive creative applications that use prompting as a new paradigm for Human-AI interaction and proposes four design goals for user interfaces that support prompting.","score":4},{"url":"https://www.semanticscholar.org/paper/0b8772b7790c69f40897b5eb7f8fd57f24138f3d","title":"ContraGen: Effective Contrastive Learning For Causal Language Model","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/10/2022","authors":"Nihal Jain,Dejiao Zhang,Wasi Uddin Ahmad,Zijian Wang,Feng Nan,Xiaopeng Li,M. Tan,Ramesh Nallapati,Baishakhi Ray,Parminder Bhatia,Xiaofei Ma,Bing Xiang","id":"0b8772b7790c69f40897b5eb7f8fd57f24138f3d","summary":"It is shown that C ONTRA G EN can effectively enhance both uniformity and discrimination of the representations and lead to the desired improvement on various language understanding tasks where discriminative representations are crucial for attaining good performance.","score":4},{"url":"https://www.semanticscholar.org/paper/c140fe515de2f20d0c85c813c7b3ec1defc41f9d","title":"Binding Language Models in Symbolic Languages","venue":"ArXiv","year":2022,"referenceCount":58,"citationCount":17,"influentialCitationCount":1,"publicationDate":"06/10/2022","authors":"Zhoujun Cheng,Tianbao Xie,Peng Shi,Chengzu Li,R.K. Nadkarni,Yushi Hu,Caiming Xiong,Dragomir R. Radev,M. Ostendorf,Luke Zettlemoyer,N. A. Smith,Tao Yu","id":"c140fe515de2f20d0c85c813c7b3ec1defc41f9d","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":38,"citationCount":13,"influentialCitationCount":1,"publicationDate":"13/10/2022","authors":"Aman Madaan,Shuyan Zhou,Uri Alon,Yiming Yang,Graham Neubig","id":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","summary":"This paper shows that when this task is frame as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than L Ms of natural language, even when the downstream task does not involve source code at all.","score":4},{"url":"https://www.semanticscholar.org/paper/6d4a12ea469ff08634eeb24c47b265a7dca2fce2","title":"PADL: Language-Directed Physics-Based Character Control","venue":"ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"Jordan Juravsky,Yunrong Guo,S. Fidler,X. B. Peng","id":"6d4a12ea469ff08634eeb24c47b265a7dca2fce2","summary":"PADL, which leverages recent innovations in NLP in order to take steps towards developing language-directed controllers for physics-based character animation, is presented and it is shown that the framework can be applied to effectively direct a simulated humanoid character to perform a diverse array of complex motor skills.","score":4},{"url":"https://www.semanticscholar.org/paper/d6d90a28b2b4ceb9b81150b5bd498541d5d9aa89","title":"Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments","venue":"ArXiv","year":2022,"referenceCount":132,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Zhivar Sourati,Vishnu Priya Prasanna Venkatesh,D. Deshpande,Himanshu Rawlani,Filip Ilievski,Hong-An Sandlin,Alain Mermoud","id":"d6d90a28b2b4ceb9b81150b5bd498541d5d9aa89","summary":"This paper formalizes prior theoretical work on logical fallacies into a comprehensive three-stage evaluation framework of detection, coarse-grained, and ﬁne-Grained classiﬁcation, and employs three families of robust and explainable methods based on prototype reasoning, instance-based reasoning, and knowledge injection.","score":4},{"url":"https://www.semanticscholar.org/paper/b8d06dd769f89d08bdd9997d7bd363c89ede845b","title":"ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Dheeraj Mekala,J. Wolfe,Subhro Roy","id":"b8d06dd769f89d08bdd9997d7bd363c89ede845b","summary":"This work proposes Z ERO TOP, a zero-shot task-oriented parsing method that decomposes a semantic parsing problem into a set of abstractive and extractive question-answering (QA) problems, enabling us to leverage the ability of LLMs to zero- shot answer reading comprehension questions.","score":4},{"url":"https://www.semanticscholar.org/paper/5435ed7c26f0c250493f244acffb69dd929d116b","title":"Structured Case-based Reasoning for Inference-time Adaptation of Text-to-SQL parsers","venue":"ArXiv","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Abhijeet Awasthi,Soumen Chakrabarti,Sunita Sarawagi","id":"5435ed7c26f0c250493f244acffb69dd929d116b","summary":"StructCBR is posed, a structured case-based reasoning approach, which leverages subtree-level similarity between logical forms of cases and candidate outputs, resulting in better decoder deci-sions and consistent performance improvements over prior inference-time adaptation methods.","score":4},{"url":"https://www.semanticscholar.org/paper/1f22de83d912176cb8857efa1c6d65b14d6a2f5c","title":"ChatGPT is not all you need. A State of the Art Review of large Generative AI models","venue":"ArXiv","year":2023,"referenceCount":34,"citationCount":7,"influentialCitationCount":0,"publicationDate":"11/01/2023","authors":"Roberto Gozalo-Brizuela,E.C. Garrido-Merchán","id":"1f22de83d912176cb8857efa1c6d65b14d6a2f5c","summary":"This work consists on an attempt to describe in a concise way the main models are sectors that aresector that are aﬀected by generative AI and to provide a taxonomy of the main generative models published recently.","score":4},{"url":"https://www.semanticscholar.org/paper/8927db4ee890bf42608752bb840bc9d7db556da1","title":"ChatGPT and Software Testing Education: Promises & Perils","venue":"ArXiv","year":2023,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Sajed Jalil,Suzzana Rafi,Thomas D. LaToza,Kevin Moran,Wing Lam","id":"8927db4ee890bf42608752bb840bc9d7db556da1","summary":"How well ChatGPT performs when tasked with solving common questions in a popular software testing curriculum is examined, and the potential promise, and dangers related to the use ofChatGPT by students and instructors are discussed.","score":4},{"url":"https://www.semanticscholar.org/paper/98207fea68db75d3941577ef87d685944519e09c","title":"Reliable Natural Language Understanding with Large Language Models and Answer Set Programming","venue":"ArXiv","year":2023,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Abhiramon Rajasekharan,Yankai Zeng,Parth Padalkar,Gopal Gupta","id":"98207fea68db75d3941577ef87d685944519e09c","summary":"This work shows how LLMs can be used to effectively extract knowledge -- represented as predicates -- from language, and proposes STAR, a framework that combines LLMs with Answer Set Programming (ASP) to bridge the gap of reasoning in NLU tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/7677e9ead35f8f2578acbf5ad15fb330c83762c4","title":"Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models","venue":"ArXiv","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Hossein Hajipour,Thorsten Holz,Lea Schonherr,Mario Fritz","id":"7677e9ead35f8f2578acbf5ad15fb330c83762c4","summary":"This work proposes a novel black-box inversion approach based on few-shot prompting that automatically and systematically finds 1000s of security vulnerabilities in various code generation models, including the commercial black- box model GitHub Copilot.","score":4},{"url":"https://www.semanticscholar.org/paper/5e2bceb56f116e98baf7e418208057bc0e1c1861","title":"ConceptFusion: Open-set Multimodal 3D Mapping","venue":"","year":2023,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/02/2023","authors":"Krishna Murthy Jatavallabhula,Ali Kuwajerwala,Qiao Gu,Mohd Omama,Tao Chen,Shuang Li,Ganesh Iyer,Soroush Saryazdi,Nikhil Varma Keetha,Ayush Tewari,J. Tenenbaum,Celso M. de Melo,M. Krishna,L. Paull,F. Shkurti,A. Torralba","id":"5e2bceb56f116e98baf7e418208057bc0e1c1861","summary":"ConceptFusion enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU.","score":4},{"url":"https://www.semanticscholar.org/paper/6487a26c6a1afbf3880d87010684094aa856c71c","title":"Train Hard, Fight Easy: Robust Meta Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/01/2023","authors":"Ido Greenberg,Shie Mannor,Gal Chechik,E. Meirom","id":"6487a26c6a1afbf3880d87010684094aa856c71c","summary":"Robust Meta RL algorithm ( RoML) is a meta-algorithm that generates a robust version of any given MRL algorithm, by identifying and over-sampling harder tasks throughout training, and achieves robust returns on several navigation and continuous control benchmarks.","score":4},{"url":"https://www.semanticscholar.org/paper/c715b3b607445998acc869633a4fde32e1933fa2","title":"On the principles of Parsimony and Self-consistency for the emergence of intelligence","venue":"Frontiers of Information Technology & Electronic Engineering","year":2022,"referenceCount":150,"citationCount":17,"influentialCitationCount":1,"publicationDate":"11/07/2022","authors":"Y. Ma,Doris Y. Tsao,H. Shum","id":"c715b3b607445998acc869633a4fde32e1933fa2","summary":"A theoretical framework is proposed that sheds light on understanding deep networks within a bigger picture of intelligence in general and introduces two fundamental principles, Parsimony and Self-consistency, which address two fundamental questions regarding intelligence: what to learn and how to learn, respectively.","score":4},{"url":"https://www.semanticscholar.org/paper/2f440b3771db1adb578044f9cee914aaff328239","title":"A rubric for human-like agents and NeuroAI","venue":"Philosophical Transactions of the Royal Society of London. Biological Sciences","year":2022,"referenceCount":185,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"I. Momennejad","id":"2f440b3771db1adb578044f9cee914aaff328239","summary":"This paper proposes that a closer look at research in recent decades reveals three main dimensions of goals and contributions across these fields: a commitment to achieving agents with human-like (or animal-like) behaviour, neural plausibility, or to solving specific computer science or engineering problems.","score":4},{"url":"https://www.semanticscholar.org/paper/65bad077608a3c2ed8eac242e993aa40aa8c13e9","title":"QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Krishna Srinivasan,K. Raman,Anupam Samanta,Ling-Yen Liao,L. Bertelli,Michael Bendersky","id":"65bad077608a3c2ed8eac242e993aa40aa8c13e9","summary":"It is demonstrated that Retrieval Augmentation of queries provides LLMs with valuable additional context enabling improved understanding, and the proposed approach (QUILL) on a billion-scale, real-world query understanding system resulting in huge gains is demonstrated.","score":4},{"url":"https://www.semanticscholar.org/paper/c7fc4c09a18bf0c26d04fc69f5567bfd3ac0c8f6","title":"Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding","venue":"ArXiv","year":2023,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Yunchang Zhu,Liang Pang,Kangxi Wu,Yanyan Lan,Huawei Shen,Xueqi Cheng","id":"c7fc4c09a18bf0c26d04fc69f5567bfd3ac0c8f6","summary":"This work provides a more general and complete formulation of the comparison principle and comparative loss, and directly uses a unified comparative loss as the final loss being optimized, eliminating the need to set a weighting coefficient between the comparative regularization term and the task-specific losses.","score":4},{"url":"https://www.semanticscholar.org/paper/931e2f94e44f3799612118ac0de54073783d9130","title":"Link the World: Improving Open-domain Conversation with Dynamic Spatiotemporal-aware Knowledge","venue":"","year":2022,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"publicationDate":"28/06/2022","authors":"Han Zhou,Xinchao Xu,Wenquan Wu,Zheng-Yu Niu,Hua Wu,Siqi Bao,Fan Wang,Haifeng Wang","id":"931e2f94e44f3799612118ac0de54073783d9130","summary":"Through automatic and human evaluations, it is found that service information improves the consistency, informativeness, factuality, and engagingness of the dialogue system, making it behave more like a human.","score":4},{"url":"https://www.semanticscholar.org/paper/c1614ab718dad97018ee34fd57864bb58b6ecaba","title":"Knowledge-grounded Dialog State Tracking","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Dian Yu,Mingqiu Wang,Yuan Cao,I. Shafran,Laurent El Shafey,H. Soltau","id":"c1614ab718dad97018ee34fd57864bb58b6ecaba","summary":"This work queries relevant knowledge of various forms based on the dialog context where such information can ground the prediction of dialog states, and demonstrates superior performance of the proposed method over strong baselines, especially in the few-shot learning setting.","score":4},{"url":"https://www.semanticscholar.org/paper/b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval","venue":"International Conference on Learning Representations","year":2021,"referenceCount":45,"citationCount":8,"influentialCitationCount":1,"publicationDate":"18/10/2021","authors":"Sarthak Mittal,Sharath Chandra Raparthy,I. Rish,Yoshua Bengio,Guillaume Lajoie","id":"b8b813111c411ae61881ab9cd25707d9de6444ec","summary":"This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.","score":4},{"url":"https://www.semanticscholar.org/paper/4f451ba06c4c9effd6c4ac0bae222495501a6200","title":"Innovations in Neural Data-to-text Generation","venue":"ArXiv","year":2022,"referenceCount":347,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/07/2022","authors":"Mandar Sharma,Ajay K. Gogineni,Naren Ramakrishnan","id":"4f451ba06c4c9effd6c4ac0bae222495501a6200","summary":"This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella.","score":4},{"url":"https://www.semanticscholar.org/paper/72e6b14c081ad3e6a1c295b60e5c834837e6b304","title":"Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks","venue":"ArXiv","year":2020,"referenceCount":45,"citationCount":10,"influentialCitationCount":1,"publicationDate":"18/05/2020","authors":"Swaroop Mishra,Arindam Mitra,Neeraj Varshney,Bhavdeep Singh Sachdeva,Chitta Baral","id":"72e6b14c081ad3e6a1c295b60e5c834837e6b304","summary":"This work introduces NUMBERGAME, a multifaceted benchmark to evaluate model performance across numerical reasoning tasks of eight diverse formats, and takes forward the recent progress in generic system development, demonstrating the scope of under-explored tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/d9e0b07313a04b033a9f2dcc74c41b2bed8c3614","title":"Explanations for CommonsenseQA: New Dataset and Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":47,"citationCount":31,"influentialCitationCount":8,"publicationDate":2021,"authors":"Shourya Aggarwal,Divyanshu Mandowara,Vishwajeet Agrawal,Dinesh Khandelwal,Parag Singla,Dinesh Garg","id":"d9e0b07313a04b033a9f2dcc74c41b2bed8c3614","summary":"This work human-annotates a first-of-its-kind dataset of positive and negative properties, as well as free-flow explanations, for 11K QA pairs taken from the CQA dataset, and proposes a latent representation based property retrieval model aswell as a GPT-2 based property generation model with a novel two step fine-tuning procedure.","score":4},{"url":"https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":65,"citationCount":91,"influentialCitationCount":20,"publicationDate":"05/03/2021","authors":"Dan Hendrycks,Collin Burns,Saurav Kadavath,Akul Arora,Steven Basart,Eric Tang,D. Song,J. Steinhardt","id":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","summary":"This work introduces MATH, a new dataset of 12, 500 challenging competition mathematics problems which can be used to teach models to generate answer derivations and explanations, and shows that accuracy remains relatively low, even with enormous Transformer models.","score":4},{"url":"https://www.semanticscholar.org/paper/c09ebcb1ca6ad1eced57340f3e81e456416ed185","title":"A Systematic Investigation of Commonsense Knowledge in Large Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":64,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/10/2021","authors":"Xiang Lorraine Li,A. Kuncoro,Jordan Hoffmann,Cyprien de Masson d'Autume,P. Blunsom,Aida Nematzadeh","id":"c09ebcb1ca6ad1eced57340f3e81e456416ed185","summary":"This work conducts a systematic and rigorous zero-shot and few-shot commonsense evaluation of large pre-trained LMs, where it carefully controls for the LMs’ ability to exploit potential surface cues and annotation artefacts and accounts for variations in performance that arise from factors that are not related to commonsense knowledge.","score":4},{"url":"https://www.semanticscholar.org/paper/2b5d234efd26e7377698cf16c901601a3d3c4e56","title":"CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities","venue":"International Conference on Human Factors in Computing Systems","year":2022,"referenceCount":73,"citationCount":38,"influentialCitationCount":2,"publicationDate":"18/01/2022","authors":"Mina Lee,Percy Liang,Qian Yang","id":"2b5d234efd26e7377698cf16c901601a3d3c4e56","summary":"It is argued that by curating and analyzing large interaction datasets, the HCI community can foster more incisive examinations of LMs’ generative capabilities, and presents CoAuthor, a dataset designed for revealing GPT-3’s capabilities in assisting creative and argumentative writing.","score":4},{"url":"https://www.semanticscholar.org/paper/acbe813244e07f32eb034d6c27547d772a995d1d","title":"Uncertainty Estimation for Language Reward Models","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":7,"influentialCitationCount":4,"publicationDate":"14/03/2022","authors":"A. Gleave,Geoffrey Irving","id":"acbe813244e07f32eb034d6c27547d772a995d1d","summary":"It is found that in this setting ensemble active learning does not outperform random sampling, and current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models.","score":4},{"url":"https://www.semanticscholar.org/paper/2aba5bba16dac5cd62683bab9de5d6faaaed0de1","title":"Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":4,"influentialCitationCount":2,"publicationDate":2022,"authors":"Boshi Wang,Xiang Deng,Huan Sun","id":"2aba5bba16dac5cd62683bab9de5d6faaaed0de1","summary":"An iterative prompting framework, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference tasks, and proposes an iterative context-aware prompter, which addresses limitations by learning to dynamically synthesize prompts conditioned on the current step’s contexts.","score":4},{"url":"https://www.semanticscholar.org/paper/7ef9aafc68511afab5b287e62b754576ea37b4ce","title":"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks","venue":"ArXiv","year":2022,"referenceCount":19,"citationCount":3,"influentialCitationCount":1,"publicationDate":"11/05/2022","authors":"K. M. Collins,Catherine Wong,Jiahai Feng,Megan Wei,J. Tenenbaum","id":"7ef9aafc68511afab5b287e62b754576ea37b4ce","summary":"A hybrid Parse-and-Solve model is proposed, which augments distributional LLMs with a symbolic reasoning module, and shows more robust adaptation to out-of-distribution plan- ning problems, demonstrating the promise of hybrid AI models for more human-like reasoning.","score":4},{"url":"https://www.semanticscholar.org/paper/750448e5d852ec0a2e4f7f809f16a1470b2b479b","title":"StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models","venue":"International Conference on Machine Learning","year":2022,"referenceCount":38,"citationCount":6,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Adam Livska,Tom'avs Kovcisk'y,E. Gribovskaya,Tayfun Terzi,Eren Sezener,Devang Agrawal,Cyprien de Masson d'Autume,Tim Scholtes,M. Zaheer,Susannah Young,Ellen Gilsenan-McMahon,Sophia Austin,P. Blunsom,Angeliki Lazaridou","id":"750448e5d852ec0a2e4f7f809f16a1470b2b479b","summary":"It is shown that parametric models can be updated without full retraining, while avoiding catastrophic forgetting, and for semi-parametric models, adding new articles into the search space allows for rapid adaptation, however, models with an outdated underlying LM under-perform those with a retrained LM.","score":4},{"url":"https://www.semanticscholar.org/paper/1a7a24c73521eecf0a2d555e921b27e2c4d8e3c3","title":"What Artificial Neural Networks Can Tell Us About Human Language Acquisition","venue":"ArXiv","year":2022,"referenceCount":174,"citationCount":8,"influentialCitationCount":0,"publicationDate":"17/08/2022","authors":"Alex Warstadt,Samuel R. Bowman","id":"1a7a24c73521eecf0a2d555e921b27e2c4d8e3c3","summary":"Before language learning requires more prior domain-speciﬁc knowledge than current models possess, non-linguistic inputs in the form of multimodal stimuli and multi-agent interaction are explored as ways to make learners morecient at learning from limited linguistic input.","score":4},{"url":"https://www.semanticscholar.org/paper/ace4d199aa72ab0808af0f30a61fc16727c95dec","title":"AudioLM: a Language Modeling Approach to Audio Generation","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":34,"influentialCitationCount":4,"publicationDate":"07/09/2022","authors":"Zalán Borsos,Raphaël Marinier,Damien Vincent,E. Kharitonov,O. Pietquin,Matthew Sharifi,O. Teboul,David Grangier,M. Tagliasacchi,Neil Zeghidour","id":"ace4d199aa72ab0808af0f30a61fc16727c95dec","summary":"The proposed hybrid tokenization scheme leverages the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis.","score":4},{"url":"https://www.semanticscholar.org/paper/4403bb8e839e175142b877222ff736cf949f232e","title":"A Code Centric Evaluation of C/C++ Vulnerability Datasets for Deep Learning Based Vulnerability Detection Techniques","venue":"","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/02/2023","authors":"Ridhi Jain,Nicole Gervasoni,Mthandazo Ndhlovu,Sanjay Rawat","id":"4403bb8e839e175142b877222ff736cf949f232e","summary":"This work aims to propose code-centric features that are relevant to security program analysis tasks like vulnerability detection and a thorough examination of the existing code datasets that demonstrate the main characteristics of the individual datasets to have a clear comparison.","score":4},{"url":"https://www.semanticscholar.org/paper/07d7965b04d8e4bfb21faeb3ffe2524753c7ab7a","title":"Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning","venue":"ArXiv","year":2020,"referenceCount":68,"citationCount":3,"influentialCitationCount":0,"publicationDate":"18/12/2020","authors":"Jerry Zikun Chen,S. Yu,Haoran Wang","id":"07d7965b04d8e4bfb21faeb3ffe2524753c7ab7a","summary":"This work explores methods to generate query reformulations by training reformulators using text-to-text transformers and applies policy-based reinforcement learning algorithms to further encourage reward learning.","score":4},{"url":"https://www.semanticscholar.org/paper/553b3d53dff9a36f1266c5c97ad09e0c0e8b51d7","title":"Generative Context Pair Selection for Multi-hop Question Answering","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":34,"citationCount":2,"influentialCitationCount":0,"publicationDate":"18/04/2021","authors":"Dheeru Dua,C. D. Santos,Patrick Ng,Ben Athiwaratkun,Bing Xiang,Matt Gardner,Sameer Singh","id":"553b3d53dff9a36f1266c5c97ad09e0c0e8b51d7","summary":"This work proposes a generative context selection model for multi-hop QA that reasons about how the given question could have been generated given a context pair and not just independent contexts and shows that on HotpotQA, the proposed generative passage selection model has a better performance than state-of-the-art answering performance.","score":4},{"url":"https://www.semanticscholar.org/paper/723fcade538f71df5fe5d1cde279686240f97b9f","title":"A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models","venue":"ArXiv","year":2022,"referenceCount":203,"citationCount":27,"influentialCitationCount":2,"publicationDate":"14/01/2022","authors":"Han Zhang,Haolin Song,Shaoyu Li,Ming Zhou,Dawei Song","id":"723fcade538f71df5fe5d1cde279686240f97b9f","summary":"This is the first survey paper to summarize CTG techniques from the perspective of PLMs, and it is hoped it can help researchers in related fields to quickly track the academic frontier, providing them with a landscape of the area and a roadmap for future research.","score":4},{"url":"https://www.semanticscholar.org/paper/492a655a67e6ec7423a968cedb70eec0cdbc8e98","title":"A Contrastive Framework for Neural Text Generation","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":31,"influentialCitationCount":8,"publicationDate":"13/02/2022","authors":"Yixuan Su,Tian Lan,Yan Wang,Dani Yogatama,Lingpeng Kong,N. Collier","id":"492a655a67e6ec7423a968cedb70eec0cdbc8e98","summary":"This work shows that an underlying reason for model degeneration is the anisotropic distribution of token representations, and presents a contrastive solution that outperforms state-of-the-art text generation methods as evaluated by both human and automatic metrics.","score":4},{"url":"https://www.semanticscholar.org/paper/75a5a50865b32d600ac18267746d376de7738ce8","title":"Recent Advances in Neural Text Generation: A Task-Agnostic Survey","venue":"ArXiv","year":2022,"referenceCount":262,"citationCount":6,"influentialCitationCount":0,"publicationDate":"06/03/2022","authors":"Cheng Tang,Frank Guerin,Yucheng Li,Chenghua Lin","id":"75a5a50865b32d600ac18267746d376de7738ce8","summary":"A task-agnostic survey of recent advances in neural text generation is presented, which group under the following four headings: data construction, neural frameworks, training and inference strategies, and evaluation metrics.","score":4},{"url":"https://www.semanticscholar.org/paper/c9f48406851954cb098911eccb4124ea5f966675","title":"A Survey on Multi-hop Question Answering and Generation","venue":"ArXiv","year":2022,"referenceCount":183,"citationCount":5,"influentialCitationCount":0,"publicationDate":"19/04/2022","authors":"Vaibhav Mavi,Anubhav Jangra,A. Jatowt","id":"c9f48406851954cb098911eccb4124ea5f966675","summary":"A general and formal definition of MHQA task is provided, the existing attempts to this highly interesting, yet quite challenging task are summarized, and the best methods to createMHQA datasets are outlined.","score":4},{"url":"https://www.semanticscholar.org/paper/47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":47,"citationCount":15,"influentialCitationCount":5,"publicationDate":"25/04/2022","authors":"Freda Shi,Daniel Fried,Marjan Ghazvininejad,Luke Zettlemoyer,Sida I. Wang","id":"47e15941c8b157873c8264e4bf50318d1ba5cd18","summary":"This work introduces execution result–based minimum Bayes risk decoding (MBR-EXEC) for program selection and finds that it consistently improves over all execution-unaware selection methods, suggesting it as an effective approach for natural language to code translation.","score":4},{"url":"https://www.semanticscholar.org/paper/23447f473cd240494b0a20ea008038aaef7e3391","title":"RankGen: Improving Text Generation with Large Ranking Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":126,"citationCount":11,"influentialCitationCount":2,"publicationDate":"19/05/2022","authors":"Kalpesh Krishna,Yapei Chang,J. Wieting,Mohit Iyyer","id":"23447f473cd240494b0a20ea008038aaef7e3391","summary":"RankGen, a 1.2B parameter encoder model for English that scores model generations given a prefix that significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics and human evaluations with English writers.","score":4},{"url":"https://www.semanticscholar.org/paper/32e3ef76772229106747613606021afc5937968a","title":"Multimodal Knowledge Alignment with Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":81,"citationCount":4,"influentialCitationCount":1,"publicationDate":"25/05/2022","authors":"Youngjae Yu,Jiwan Chung,Heeseung Yun,Jack Hessel,J. Park,Ximing Lu,Prithviraj Ammanabrolu,Rowan Zellers,Ronan Le Bras,Gunhee Kim,Yejin Choi","id":"32e3ef76772229106747613606021afc5937968a","summary":"This work proposes ESPER, a novel approach to reinforcement learning which extends language-only zero-shot models to unseen multimodal tasks, like image and audio captioning, and demonstrates that it outperforms baselines and prior work on a variety of zero- shot tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/2098244c530933e92cbb72217e43b918dce25e23","title":"Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":2,"influentialCitationCount":0,"publicationDate":"05/09/2022","authors":"Hezekiah J. Branch,Jonathan Rodriguez Cefalu,Jeremy McHugh,Leyla Hujer,Aditya Bahl,Daniel del Castillo Iglesias,Ron Heichman,Ramesh Darwishi","id":"2098244c530933e92cbb72217e43b918dce25e23","summary":"A major security vulnerability is highlighted in the public release of GPT-3 and this work underscores token distance-minimized perturbations as an effective adversarial approach, bypassing both supervised and unsupervised quality measures.","score":4},{"url":"https://www.semanticscholar.org/paper/891db4adb6d10a51882430ab1266f51e6e3408c5","title":"Augmenting Multi-Turn Text-to-SQL Datasets with Self-Play","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Qi Liu,Zihuiwen Ye,Tao Yu,P. Blunsom,Linfeng Song","id":"891db4adb6d10a51882430ab1266f51e6e3408c5","summary":"Self-play improves the accuracy of a strong baseline on SParC and CoSQL, two widely used cross-domain text-to-SQL datasets, and enhances cross- domain generalization and improves beam-search.","score":4},{"url":"https://www.semanticscholar.org/paper/9e8b29d025cd2718ff61b363ce1c1f422d612303","title":"Learning General World Models in a Handful of Reward-Free Deployments","venue":"ArXiv","year":2022,"referenceCount":121,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"Yingchen Xu,Jack Parker-Holder,Aldo Pacchiano,Philip J. Ball,Oleh Rybkin,Stephen J. Roberts,Tim Rocktaschel,Edward Grefenstette","id":"9e8b29d025cd2718ff61b363ce1c1f422d612303","summary":"This work introduces the reward-free deployment efﬁciency setting, a new paradigm for RL research, and presents CASCADE, a novel approach for self-supervised exploration in this new setting, using an information theoretic objective inspired by Bayesian Active Learning.","score":4},{"url":"https://www.semanticscholar.org/paper/27202f962798d08b39601a36127360c5ccd9c625","title":"Knowledge Transfer from Answer Ranking to Answer Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"Matteo Gabburo,Rik Koncel-Kedziorski,Siddhant Garg,Luca Soldaini,Alessandro Moschitti","id":"27202f962798d08b39601a36127360c5ccd9c625","summary":"This paper proposes to train a GenQA model by transferring knowledge from a trained AS2 model, and proposes to use the As2 model prediction scores for loss weighting and score-conditioned input/output shaping, to aid the knowledge transfer.","score":4},{"url":"https://www.semanticscholar.org/paper/97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":56,"citationCount":2,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"Koustuv Sinha,Amirhossein Kazemnejad,Siva Reddy,J. Pineau,D. Hupkes,Adina Williams","id":"97833e2aa0da5240e62436373b58af988a4ab6ab","summary":"It is observed that models trained with APE over-rely on positional information to the point that they break-down when subjected to sentences with shifted position information, which raises questions about the efficacy of APEs to model the relativity of position information.","score":4},{"url":"https://www.semanticscholar.org/paper/36543b4441c9d88b57b06a7ba887f409afd8141d","title":"Does Self-Rationalization Improve Robustness to Spurious Correlations?","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/10/2022","authors":"Alexis Ross,Matthew E. Peters,Ana Marasović","id":"36543b4441c9d88b57b06a7ba887f409afd8141d","summary":"It is found that while self-rationalization can improve robustness to spurious correlations in low-resource settings, it tends to hurt robustness in higher- resource settings, and appropriate care should be taken when training self- rationalizing models with the goal of creating more trustworthy models.","score":4},{"url":"https://www.semanticscholar.org/paper/6f39c929df030c85f1d5aa029f21a6fedd9f92c8","title":"SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control","venue":"ArXiv","year":2022,"referenceCount":82,"citationCount":4,"influentialCitationCount":1,"publicationDate":"31/10/2022","authors":"Xiaochuang Han,Sachin Kumar,Yulia Tsvetkov","id":"6f39c929df030c85f1d5aa029f21a6fedd9f92c8","summary":"S SD -LM is semi-autoregressive, iteratively generating blocks of text, allowing for ﬂexible output length at decoding time while enabling local bidirectional context updates, and is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing to incorporate classiﬁer guidance and modular control without any adaptation of off-the-shelf classi ﬁers.","score":4},{"url":"https://www.semanticscholar.org/paper/0f7a6c557e376d8c77d684bcda0daee74fc29acf","title":"Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/11/2022","authors":"Yanyang Li,Jianqiao Zhao,M. Lyu,Liwei Wang","id":"0f7a6c557e376d8c77d684bcda0daee74fc29acf","summary":"This work treats the generated knowledge as a noisy knowledge source and proposes the posterior-based reweighing as well as the noisy training strategy to better exploit such generated knowledge in dialogue generation.","score":4},{"url":"https://www.semanticscholar.org/paper/bcec7d17e68aceb91d020dd796ece075694f77c6","title":"COPEN: Probing Conceptual Knowledge in Pre-trained Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/11/2022","authors":"Hao Peng,Xiaozhi Wang,Shengding Hu,Hailong Jin,Lei Hou,Juanzi Li,Zhiyuan Liu,Qun Liu","id":"bcec7d17e68aceb91d020dd796ece075694f77c6","summary":"Inspired by knowledge representation schemata, this work comprehensively evaluates conceptual knowledge of PLMs by designing three tasks to probe whether PLMs organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts, respectively.","score":4},{"url":"https://www.semanticscholar.org/paper/0bacfdef9db436b2fdf686dfc90d780df5a51a15","title":"Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Xiang Fan,Yiwei Lyu,Paul Pu Liang,R. Salakhutdinov,Louis-Philippe Morency","id":"0bacfdef9db436b2fdf686dfc90d780df5a51a15","summary":"This work tackles the problem of generating text following arbitrary distributions by proposing N ANO, a few-shot human-in-the-loop training algorithm that continuously learns from human feedback that achieves state-of- the-art results on single topic/attribute as well as quantiﬁed distribution control compared to previous works.","score":4},{"url":"https://www.semanticscholar.org/paper/0b871a9f12e5c2da1b291a8b166c671256ebe1cd","title":"A Copy Mechanism for Handling Knowledge Base Elements in SPARQL Neural Machine Translation","venue":"AACL/IJCNLP","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/11/2022","authors":"Rose Hirigoyen,A. Zouaq,Samuel Reyd","id":"0b871a9f12e5c2da1b291a8b166c671256ebe1cd","summary":"This work proposes to integrate a copy mechanism for neural SPARQL query generation by adding a copy layer and a dynamic knowledge base vocabulary to two Seq2Seq architectures (CNNs and Transform-ers) that makes the models copy KB elements directly from the questions, instead of generating them.","score":4},{"url":"https://www.semanticscholar.org/paper/0e1f80a3f52c9051026656f00e31c0b9c1428d7a","title":"Can language models automate data wrangling?","venue":"Machine-mediated learning","year":2022,"referenceCount":46,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Gonzalo Jaimovitch-López,C. Ferri,J. Hernández-Orallo,Fernando Martínez-Plumed,M. J. Ramírez-Quintana","id":"0e1f80a3f52c9051026656f00e31c0b9c1428d7a","summary":"A major finding is that language models appear as a powerful tool for a wide range of data wrangling tasks, and some guidelines about how they can be integrated into data processing pipelines are provided, provided the users can take advantage of their flexibility and the diversity of tasks to be addressed.","score":4},{"url":"https://www.semanticscholar.org/paper/91b07210ec07a229e5caf2d5f009a523b39e40ae","title":"Momentum Decoding: Open-ended Text Generation As Graph Exploration","venue":"ArXiv","year":2022,"referenceCount":23,"citationCount":1,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Tian Lan,Yixuan Su,Shuhang Liu,Heyan Huang,Xian-Ling Mao","id":"91b07210ec07a229e5caf2d5f009a523b39e40ae","summary":"This study formulate open-ended text generation from a new perspective, i.e., it is viewed as an exploration process within a directed graph, and proposes a novel decoding method— momentum decoding—which encourages the LM to greedily explore new nodes outside the current graph.","score":4},{"url":"https://www.semanticscholar.org/paper/fee5a9bddaa4d60a3a5bdbe8c2ed0503173a5ab6","title":"Pre-trained Language Models can be Fully Zero-Shot Learners","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/12/2022","authors":"Xuandong Zhao,Siqi Ouyang,Zhiguo Yu,Ming-li Wu,Lei Li","id":"fee5a9bddaa4d60a3a5bdbe8c2ed0503173a5ab6","summary":"This paper proposes nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding, which uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words.","score":4},{"url":"https://www.semanticscholar.org/paper/f9020636530faaf1801f00089ea250b16d2eacd7","title":"Visually-augmented pretrained language models for NLP tasks without images","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"Hangyu Guo,Kun Zhou,Wayne Xin Zhao,Qinyu Zhang,Ji-rong Wen","id":"f9020636530faaf1801f00089ea250b16d2eacd7","summary":"A novel visually-augmented fine-tuning approach that can be generally applied to various PLMs or NLP tasks, without using any retrieved or generated images, namely VAWI is proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/f1b475c6bde80880dd8edfc223c1f20c087201b7","title":"Ethical Issues in Automatic Dialogue Generation for Non-Player Characters in Digital Games","venue":"2022 IEEE International Conference on Big Data (Big Data)","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/12/2022","authors":"Yusuke Mori,Youichiro Miyake","id":"f1b475c6bde80880dd8edfc223c1f20c087201b7","summary":"It is argued that hierarchical ethical considerations for the automatic generation of character utterances in digital games are essential for designing the digital games in the incoming era.","score":4},{"url":"https://www.semanticscholar.org/paper/60e2daaf76a4259e2db955963a544e3a4328856f","title":"Low-Resource Authorship Style Transfer with In-Context Learning","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Ajay Patel,Nicholas Andrews,Chris Callison-Burch","id":"60e2daaf76a4259e2db955963a544e3a4328856f","summary":"This paper proposes a method for automatic evaluation on the low-resource authorship style transfer task utilizing authorship and style representation embeddings and demonstrates that this method, S TYLL, is able to outperform S TRAP and a comprehensive set of baselines.","score":4},{"url":"https://www.semanticscholar.org/paper/23b7cde603b5ec8d5d13d46e1c453dc52d7c3f6c","title":"Latent Diffusion for Language Generation","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Justin Lovelace,Varsha Kishore,Chao-gang Wan,Eliot Shekhtman,Kilian Q. Weinberger","id":"23b7cde603b5ec8d5d13d46e1c453dc52d7c3f6c","summary":"It is demonstrated that continuous diffusion models can be learned in the latent space of a pre-trained encoder-decoder model, enabling them to sample continuous latent representations that can be decoded into natural language with the pre- trained decoder.","score":4},{"url":"https://www.semanticscholar.org/paper/c59129f2cd52aaa02de295baf110cdbb6588f331","title":"TeSS: Zero-Shot Classification via Textual Similarity Comparison with Prompting using Sentence Encoder","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Jimin Hong,Jungsoo Park,Daeyoung Kim,Seongjae Choi,Bokyung Son,Jaewook Kang","id":"c59129f2cd52aaa02de295baf110cdbb6588f331","summary":"TeSS is introduced, a framework for zero-shot classiﬁcation where the assigned label is determined by the embedding similarity between the input text and each candidate label prompt, and a simple interface to assess the quality of sentence encoders.","score":4},{"url":"https://www.semanticscholar.org/paper/ec671e72494c3b1ba9556094ec83706da17a738d","title":"A Brief History of Deep Learning-Based Text Generation","venue":"International Conferences on Computing Advancements","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Anil Bas,M. O. Topal,Ç. Duman,Imke Van Heerden","id":"ec671e72494c3b1ba9556094ec83706da17a738d","summary":"The paper describes deep learning models for a broad audience, focusing on traditional, convolutional, recurrent and generative adversarial networks, as well as transformer architecture.","score":4},{"url":"https://www.semanticscholar.org/paper/0817de9f2bbc4f6db8cd5289d367a8c64aebbebb","title":"Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"R. Wolfe,Yiwei Yang,Billy Howe,Aylin Caliskan","id":"0817de9f2bbc4f6db8cd5289d367a8c64aebbebb","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/cb648d482dbd1e6ad0b0f4da43aca71c06538d4f","title":"Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise","venue":"","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/12/2022","authors":"Zhenghao Lin,Yeyun Gong,Yelong Shen,Tong Wu,Zhihao Fan,Chen Lin,Nan Duan,Weizhu Chen","id":"cb648d482dbd1e6ad0b0f4da43aca71c06538d4f","summary":"To pre-train GENIE on a large-scale language corpus, a new continuous paragraph denoise objective is designed, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version, while preserving the semantic and syntactic coherence.","score":4},{"url":"https://www.semanticscholar.org/paper/40c7b3e24bc6d6fe518a8ff60e58bd4877813b1a","title":"Membership Inference Attacks With Token-Level Deduplication on Korean Language Models","venue":"IEEE Access","year":2023,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Myung Gyo Oh,L. Park,Jaeuk Kim,Jaewoo Park,T.-H. Kwon","id":"40c7b3e24bc6d6fe518a8ff60e58bd4877813b1a","summary":"It is shown that considering both language- and model-specific characteristics is essential to improve the effectiveness of attack strategies, and a deduplication strategy to replace the traditional word-level similarity metric with the BPE token level is proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/165224b81f46fbda2140a4b656991287e96d3548","title":"Is This Abstract Generated by AI? A Research for the Gap between AI-generated Scientific Text and Human-written Scientific Text","venue":"ArXiv","year":2023,"referenceCount":59,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Yongqiang Ma,Jiawei Liu,Fan Yi","id":"165224b81f46fbda2140a4b656991287e96d3548","summary":"There exists a “writing style” gap between AI-generated scientific text and human-written scientific text, which suggests that while AI has the potential to generate scientific content that is as accurate as human- written content, there is still a gap in terms of depth and overall quality.","score":4},{"url":"https://www.semanticscholar.org/paper/34eb9de4a36dbeb47266c53b6e26003acb3523cc","title":"Machine learning and deep learning—A review for ecologists","venue":"Methods in Ecology and Evolution","year":2022,"referenceCount":232,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/04/2022","authors":"Maximilian Pichler,F. Hartig","id":"34eb9de4a36dbeb47266c53b6e26003acb3523cc","summary":"It is concluded that ML and DL are powerful new tools for predictive modeling and data analysis, comparable to other traditional statistical tools.","score":4},{"url":"https://www.semanticscholar.org/paper/0cffb48b8036d62ccc0ebd3bbbfb433860747c83","title":"Learning from flowsheets: A generative transformer model for autocompletion of flowsheets","venue":"Computers &amp; Chemical Engineering","year":2022,"referenceCount":49,"citationCount":1,"influentialCitationCount":1,"publicationDate":"01/08/2022","authors":"Gabriel Vogel,Lukas Schulze Balhorn,Artur M. Schweidtmann","id":"0cffb48b8036d62ccc0ebd3bbbfb433860747c83","summary":"A novel method enabling autocompletion of chemical flowsheets that can provide chemical engineers with recommendations during interactive flowsheet synthesis and demonstrates a high potential for future AI-assisted process synthesis.","score":4},{"url":"https://www.semanticscholar.org/paper/b3ca64d5c579b5209bfe629a698468beebfa6dee","title":"Self Supervision Does Not Help Natural Language Supervision at Scale","venue":"ArXiv","year":2023,"referenceCount":92,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/01/2023","authors":"F.R.T. Weers,Vaishaal Shankar,Angelos Katharopoulos,Yinfei Yang,Tom Gunter","id":"b3ca64d5c579b5209bfe629a698468beebfa6dee","summary":"This work finds that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a beneﬁt over CLIP when trained on a corpus of 11.3M image-text pairs, but lit-tle to no beneﷁt (as evaluated on a suite of common vision tasks) over ClIP when training on a large corpus of 1.4B images.","score":4},{"url":"https://www.semanticscholar.org/paper/121c3831752cb92e1222d8d1d8cc529016845247","title":"Information Retrieval: Recent Advances and Beyond","venue":"ArXiv","year":2023,"referenceCount":229,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/01/2023","authors":"Kailas Hambarde,H. Proença","id":"121c3831752cb92e1222d8d1d8cc529016845247","summary":"A detailed overview of the models used for information retrieval in the first and second stages of the typical processing chain is provided, including methods based on terms, semantic retrieval, and neural.","score":4},{"url":"https://www.semanticscholar.org/paper/de352871a2fb75fc49e4c469d21e92c33701b28d","title":"A Watermark for Large Language Models","venue":"ArXiv","year":2023,"referenceCount":53,"citationCount":6,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"John Kirchenbauer,Jonas Geiping,Yuxin Wen,Jonathan Katz,Ian Miers,T. Goldstein","id":"de352871a2fb75fc49e4c469d21e92c33701b28d","summary":"A statistical test for detecting the watermark with interpretable p-values is proposed, and an information-theoretic framework for analyzing the sensitivity of the watermarks is derived.","score":4},{"url":"https://www.semanticscholar.org/paper/125a4f12bb93a8db8bbc45d77bc001a6a628a47e","title":"AI vs. Human -- Differentiation Analysis of Scientific Content Generation","venue":"","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"Yongqiang Ma,Jiawei Liu,Fan Yi,Qikai Cheng,Yong Huang,Wei Lu,Xiaozhong Liu","id":"125a4f12bb93a8db8bbc45d77bc001a6a628a47e","summary":"It is found that there exists a \"writing style\"gap between AI-generated scientific text and human-written scientific text that contributes to guiding the optimization of AI models to produce high-quality content and addressing related ethical and security concerns.","score":4},{"url":"https://www.semanticscholar.org/paper/76beae98abf567b31219d5fdf2ed4593189b98b0","title":"Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling","venue":"bioRxiv","year":2023,"referenceCount":56,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Hong-Yu Zhou,Yunxiang Fu,Zhicheng Zhang,Cheng Bian,Yizhou Yu","id":"76beae98abf567b31219d5fdf2ed4593189b98b0","summary":"The proposed Knowledge-exploited Auto-encoder for Protein (KeAP), which performs tokenlevel knowledge graph exploration for protein representation learning, and can consistently outperform the previous counterpart on 9 representative downstream applications, sometimes surpassing it by large margins.","score":4},{"url":"https://www.semanticscholar.org/paper/2f43383da215ef757703ee570af296fbe9268246","title":"Dialogue Management and Language Generation for a Robust Conversational Virtual Coach: Validation and User Study","venue":"Italian National Conference on Sensors","year":2023,"referenceCount":87,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"A. Vázquez,Asier López Zorrilla,J. M. Olaso,M. Inés Torres","id":"2f43383da215ef757703ee570af296fbe9268246","summary":"A Dialogue Manager and a Language Generator that are the core modules of a Voice-based Spoken Dialogue System capable of carrying out challenging, long and complex coaching conversations and an efficient integration procedure that will act as an intelligent and robust Virtual Coach are presented.","score":4},{"url":"https://www.semanticscholar.org/paper/8264257f573696fc0a1ef7531c825041832197f8","title":"Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases","venue":"ArXiv","year":2023,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Xiaoxia Wu,Cheng Li,Reza Yazdani Aminabadi,Z. Yao,Yuxiong He","id":"8264257f573696fc0a1ef7531c825041832197f8","summary":"This work fully investigate the feasibility of usingINT4 quantization for language models, and shows that using INT4 introduces no or negligible accuracy degradation for encoder-only and encoder -decoder models, but causes a signiﬁcant accuracy drop for decoder- only models.","score":4},{"url":"https://www.semanticscholar.org/paper/2ca1985816f6670cba0a34ab53bdc73cf82d99af","title":"ST$^{2}$: Spatial-Temporal State Transformer for Crowd-Aware Autonomous Navigation","venue":"IEEE Robotics and Automation Letters","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Yuxiang Yang,Jiahao Jiang,Jing Zhang,Jiye Huang,Mingyu Gao","id":"2ca1985816f6670cba0a34ab53bdc73cf82d99af","summary":"This letter proposes a Spatial-Temporal State Transformer to encode the states while leveraging the deep reinforcement learning method to find the optimal navigation policy accordingly, and extensive experiments demonstrate the superiority of the proposed ST<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex- math></inline- formula> over representative state-of-the-art methods.","score":4},{"url":"https://www.semanticscholar.org/paper/11ffe64c7c0c6ae58016afdf84c3eacfb4e227b8","title":"Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data","venue":"ArXiv","year":2023,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Alon Albalak,Colin Raffel,William Yang Wang","id":"11ffe64c7c0c6ae58016afdf84c3eacfb4e227b8","summary":"This work focuses on automated sampling strategies for FLAD and relates them to the explore-exploit dilemma that is central in multi-armed bandit settings, and proposes two algorithms to train T5 that yield a 9% absolute improvement over the explicitly multi-task pre-trained T0 model.","score":4},{"url":"https://www.semanticscholar.org/paper/512ff5037b28be7415d318ae6e8eeb0abb8c7013","title":"DTATrans: Leveraging Dynamic Token-Based Quantization With Accuracy Compensation Mechanism for Efficient Transformer Architecture","venue":"IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Tao Yang,Fei Ma,Xiaoling Li,Fangxin Liu,Yilong Zhao,Zhezhi He,Li Jiang","id":"512ff5037b28be7415d318ae6e8eeb0abb8c7013","summary":"The transformer accelerator with the variable-speed systolic array (VSSA) is designed and an effective optimization strategy to alleviate the pipeline-stall problem in VSSA without hardware overhead is proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/2a261c6eb6886ba3a0f93319bf7b71939b0176ec","title":"SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling","venue":"ArXiv","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Jiaxiang Dong,Haixu Wu,Haoran Zhang,Li Zhang,Jianmin Wang,Mingsheng Long","id":"2a261c6eb6886ba3a0f93319bf7b71939b0176ec","summary":"SimMTM proposes to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold, which eases the reconstruction task by assembling ruined but complementary temporal variations from multiple masked series.","score":4},{"url":"https://www.semanticscholar.org/paper/2a17c170afd50830276455b297f58b0b413e68dc","title":"Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing","venue":"ArXiv","year":2023,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/02/2023","authors":"Han He,Jinho D. Choi","id":"2a17c170afd50830276455b297f58b0b413e68dc","summary":"A systematic study of S2S modeling using contained decoding on four core tasks: part-of-speech tagging, named entity recognition, constituency and dependency parsing, to develop efficient exploitation methods costing zero extra parameters.","score":4},{"url":"https://www.semanticscholar.org/paper/9c987cba9512a5dfbf141d2981108b9c2b0203c0","title":"Computation vs. Communication Scaling for Future Transformers on Future Hardware","venue":"ArXiv","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/02/2023","authors":"Suchita Pati,Shaizeen Aga,Mahzabeen Islam,N. Jayasena,Matthew D. Sinclair","id":"9c987cba9512a5dfbf141d2981108b9c2b0203c0","summary":"This work provides a comprehensive multi-axial (algorithmic, empirical, hardware evolution) analysis of compute vs. communication (Comp-vs-Comm) scaling for future Transformer models on future hardware and underscores the increasingly large role communication will play as models scale.","score":4},{"url":"https://www.semanticscholar.org/paper/bf6b8d2ed08b1538616caac9da6f0fb463f2077d","title":"Boosting Zero-shot Classification with Synthetic Data Diversity via Stable Diffusion","venue":"ArXiv","year":2023,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Jordan Shipard,A. Wiliem,Kien Nguyen Thanh,Wei Xiang,C. Fookes","id":"bf6b8d2ed08b1538616caac9da6f0fb463f2077d","summary":"This work proposes a $\\textit{bag of tricks}$ to improve diversity and is able to achieve performance on par with one of the vision-language models, CLIP, and allows for endow zero-shot classification capabilities on any classification model.","score":4},{"url":"https://www.semanticscholar.org/paper/31eb3316e4c79f3b2eee3c672c83feceb0893b6b","title":"Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery","venue":"ArXiv","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Yuxin Wen,Neel Jain,John Kirchenbauer,Micah Goldblum,Jonas Geiping,T. Goldstein","id":"31eb3316e4c79f3b2eee3c672c83feceb0893b6b","summary":"This work describes an approach to robustly optimize hard text prompts through efficient gradient-based optimization and shows that hard prompts can be automatically discovered that are effective in tuning LMs for classification.","score":4},{"url":"https://www.semanticscholar.org/paper/09053afc612c6d78aa47971b60bfe6fb152739cf","title":"Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Hyeonho Jeong,Gihyun Kwon,Jong-Chul Ye","id":"09053afc612c6d78aa47971b60bfe6fb152739cf","summary":"A novel neural pipeline for generating a coherent storybook from the plain text of a story using a combination of a pre-trained Large Language Model and a text-guided Latent Diffusion Model to generate coherent images.","score":4},{"url":"https://www.semanticscholar.org/paper/1467ced85b3ae2d695079a1557063a445c43988a","title":"Global Constraints with Prompting for Zero-Shot Event Argument Classification","venue":"ArXiv","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/02/2023","authors":"Zizheng Lin,Hongming Zhang,Yangqiu Song","id":"1467ced85b3ae2d695079a1557063a445c43988a","summary":"This work proposes to use global constraints with prompting to effectively tackles event argument classification without any annotation and task-specific training, exploiting cross-task, cross-argument, and cross-event relations.","score":4},{"url":"https://www.semanticscholar.org/paper/39266f38bdddd04ec89749db1776bfbf63e7f600","title":"Impact of Code Language Models on Automated Program Repair","venue":"ArXiv","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"Nan Jiang,Kevin Liu,Thibaud Lutellier,Lin Tan","id":"39266f38bdddd04ec89749db1776bfbf63e7f600","summary":"This work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques.","score":4},{"url":"https://www.semanticscholar.org/paper/f2909fcd0a1c265097490ce43f5065ef6486310d","title":"Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters","venue":"ArXiv","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/02/2023","authors":"Sébastien Montella,Alexis Nasr,Johannes Heinecke,Frédéric Béchet,L. Rojas-Barahona","id":"f2909fcd0a1c265097490ce43f5065ef6486310d","summary":"Through ablation studies, graph attack and link prediction, it is revealed that RPE might be partially encoding input graphs, and further research regarding the role of RPE will provide valuable insights for Graph-to-Text generation.","score":4},{"url":"https://www.semanticscholar.org/paper/f640a2635f38cbb3dbb83775088c2e27b790ad77","title":"A Unified View of Long-Sequence Models towards Modeling Million-Scale Dependencies","venue":"","year":2023,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Hongyu He,Marko Kabic","id":"f640a2635f38cbb3dbb83775088c2e27b790ad77","summary":"This work takes a step back, studies and compares existing solutions to long-sequence modeling in terms of their pure mathematical formulation, and summarizes them using a unified template, given their shared nature of token mixing, to propose a machine learning system for handling million-scale dependencies.","score":4},{"url":"https://www.semanticscholar.org/paper/30c07037fe40754690af3f873f12f1724b092635","title":"A Uniﬁed View of Long-Sequence Models towards Million-Scale Dependencies","venue":"","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Hongyu He","id":"30c07037fe40754690af3f873f12f1724b092635","summary":"This work study and compare existing solutions to long-sequence modeling in terms of their pure mathematical formulation, and proposes a machine learning system for handling million-scale dependencies, inspired by emerging sparse models of huge capacity.","score":4},{"url":"https://www.semanticscholar.org/paper/a3ff4df653b6970898c04e6b768e58b99786d073","title":"Learning gain differences between ChatGPT and human tutor generated algebra hints","venue":"","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/02/2023","authors":"Z. Pardos,Shreya Bhandari","id":"a3ff4df653b6970898c04e6b768e58b99786d073","summary":"This paper conducts the first learning gain evaluation of ChatGPT by comparing the efficacy of its hints with hints authored by human tutors with 77 participants across two algebra topic areas, Elementary Algebra and Intermediate Algebra.","score":4},{"url":"https://www.semanticscholar.org/paper/7c817fa57edd087820d3b4d16e4d1f40d4cb5200","title":"Generation Probabilities Are Not Enough: Exploring the Effectiveness of Uncertainty Highlighting in AI-Powered Code Completions","venue":"","year":2023,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/02/2023","authors":"Helena Vasconcelos,Gagan Bansal,Adam Fourney,Q. Liao,Jennifer Wortman Vaughan","id":"7c817fa57edd087820d3b4d16e4d1f40d4cb5200","summary":"The question of whether conveying information about uncertainty enables programmers to more quickly and accurately produce code when collaborating with an AI-powered code completion tool, and if so, what measure of uncertainty best fits programmers' needs is explored.","score":4},{"url":"https://www.semanticscholar.org/paper/6ffd19930ab0170f8cfdbf926e8f3c11f85e289a","title":"Decoupled Model Schedule for Deep Learning Training","venue":"","year":2023,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Hongzheng Chen,Cody Hao Yu,Shuai Zheng,Zhen Zhang,Zhiru Zhang,Yida Wang","id":"6ffd19930ab0170f8cfdbf926e8f3c11f85e289a","summary":"A schedule language to decouple model execution from definition is proposed and uses a set of schedule primitives to convert the model for common model training optimizations such as high-performance kernels, effective 3D parallelism, and efficient activation checkpointing, preserving programmability and debuggability for users to a large extent.","score":4},{"url":"https://www.semanticscholar.org/paper/51879aeb001a8397253755247ccd6507d64d2403","title":"Role of Bias Terms in Dot-Product Attention","venue":"","year":2023,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Mahdi Namazifar,Devamanyu Hazarika,Dilek Z. Hakkani-Tür","id":"51879aeb001a8397253755247ccd6507d64d2403","summary":"This work mathematically shows that the bias term of the key linear transformation is redundant and could be omitted without any impact on the attention module, and argues that the biases of the value linear transformation has a more prominent role than that of the bias terms of the query linear transformation.","score":4},{"url":"https://www.semanticscholar.org/paper/b70ae44769fac99295832f5f89f25e9a7b8a5a2c","title":"Multimodal Subtask Graph Generation from Instructional Videos","venue":"","year":2023,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/02/2023","authors":"Y. Jang,Sungryull Sohn,L. Logeswaran,Tiange Luo,Moontae Lee,Ho Hin Lee","id":"b70ae44769fac99295832f5f89f25e9a7b8a5a2c","summary":"This work presents Multimodal Sub task Graph Generation (MSG2), an approach that constructs a Subtask Graph defining the dependency between a task's subtasks relevant to a task from noisy web videos, closer to human-annotated graphs compared to prior approaches.","score":4},{"url":"https://www.semanticscholar.org/paper/9d877509f3fb8fbbf3e3d54eeef3c84bc0e1e3b2","title":"A Simplistic Model of Neural Scaling Laws: Multiperiodic Santa Fe Processes","venue":"","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/02/2023","authors":"Lukasz Dkebowski","id":"9d877509f3fb8fbbf3e3d54eeef3c84bc0e1e3b2","summary":"A model of narration is proposed that has the vanishing entropy rate and applies a randomly chosen deterministic sequence called a multiperiodic sequence that exhibits asymptotic relative frequencies given by Zipf's law.","score":4},{"url":"https://www.semanticscholar.org/paper/194ae9479003351c27192df619c3c789c655e0e7","title":"Black-box Prompt Learning for Pre-trained Language Models","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":14,"influentialCitationCount":1,"publicationDate":"21/01/2022","authors":"Shizhe Diao,Xuechun Li,Yong Lin,Zhichao Huang,Tong Zhang","id":"194ae9479003351c27192df619c3c789c655e0e7","summary":"A Black-box Discrete Prompt Learning (BDPL) is established to resonate with pragmatic interactions between the cloud infrastructure and edge devices and achieves significant improvement on eight benchmarks in a cloud-device collaboration manner.","score":4},{"url":"https://www.semanticscholar.org/paper/6edccbd83a9aae204785d4821f97855677c33866","title":"Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":19,"influentialCitationCount":0,"publicationDate":"21/07/2022","authors":"Yi Tay,M. Dehghani,Samira Abnar,Hyung Won Chung,W. Fedus,J. Rao,Sharan Narang,V. Tran,Dani Yogatama,Donald Metzler","id":"6edccbd83a9aae204785d4821f97855677c33866","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/4f4a80148cb8f328aeaee68b34f9797cfb5ea150","title":"Unpacking Large Language Models with Conceptual Consistency","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/09/2022","authors":"Pritish Sahu,Michael Cogswell,Yunye Gong,Ajay Divakaran","id":"4f4a80148cb8f328aeaee68b34f9797cfb5ea150","summary":"This novel metric measures how well a model can be characterized by finding out how consistent its responses to queries about conceptually relevant background knowledge are, and shows significant variation in conceptual consistency across different kinds of relations, concepts, and prompts.","score":4},{"url":"https://www.semanticscholar.org/paper/17a496e051dd4b8da26a29baeee01f7d072005ce","title":"SQA3D: Situated Question Answering in 3D Scenes","venue":"ArXiv","year":2022,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Xiaojian Ma,Silong Yong,Zilong Zheng,Qing Li,Yitao Liang,Song-Chun Zhu,Siyuan Huang","id":"17a496e051dd4b8da26a29baeee01f7d072005ce","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/fa9312468957ad7d2d39de00cd85c9856ce0d0b0","title":"Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Sadegh Mahdavi,Kevin Swersky,Thomas Kipf,Milad Hashemi,Christos Thrampoulidis,Renjie Liao","id":"fa9312468957ad7d2d39de00cd85c9856ce0d0b0","summary":"An attention-based 2WL-graph neural network (GNN) processor which complements message passing GNNs so their combination outperforms the state-of-the-art model by a 3% margin averaged over all algorithms.","score":4},{"url":"https://www.semanticscholar.org/paper/2c43ef2d8e44d055b61eecddf323a3412007cef8","title":"Deanthropomorphising NLP: Can a Language Model Be Conscious?","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"M. Shardlow,Piotr Przybyła","id":"2c43ef2d8e44d055b61eecddf323a3412007cef8","summary":"It is taken that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it, and the necessary background in language modelling is presented.","score":4},{"url":"https://www.semanticscholar.org/paper/c9ef79d6d47c90722a10c32c64c752eb0343fd61","title":"Progress measures for grokking via mechanistic interpretability","venue":"ArXiv","year":2023,"referenceCount":23,"citationCount":1,"influentialCitationCount":1,"publicationDate":"12/01/2023","authors":"Neel Nanda,Lawrence Chan,Tom Lieberum,Jess Smith,J. Steinhardt","id":"c9ef79d6d47c90722a10c32c64c752eb0343fd61","summary":"This work argues that progress measures can be found via mechanistic interpretability: reverseengineering learned behaviors into their individual components, and defines progress measures that allow to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup.","score":4},{"url":"https://www.semanticscholar.org/paper/e962f95e03a50ff2f3a0fe7840daebac04578c46","title":"Structure-informed Language Models Are Protein Designers","venue":"bioRxiv","year":2023,"referenceCount":83,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"Zaixiang Zheng,Yifan Deng,Dongyu Xue,Yi Zhou,YE Fei,Quanquan Gu","id":"e962f95e03a50ff2f3a0fe7840daebac04578c46","summary":"LM-Design, a generic approach to reprogramming sequence-based protein language models (pLMs), that have learned massive sequential evolutionary knowledge from the universe of natural protein sequences, to acquire an immediate capability to design preferable protein sequences for given folds is presented.","score":4},{"url":"https://www.semanticscholar.org/paper/61e721334296ebfbbf6443b5ed9eb8c83b708c95","title":"Scaling Vision Transformers to 22 Billion Parameters","venue":"ArXiv","year":2023,"referenceCount":121,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"M. Dehghani,Josip Djolonga,Basil Mustafa,Piotr Padlewski,J. Heek,J. Gilmer,A. Steiner,Mathilde Caron,Robert Geirhos,Ibrahim M. Alabdulmohsin,Rodolphe Jenatton,L. Beyer,M. Tschannen,Anurag Arnab,Xiao Wang,C. Riquelme,Matthias Minderer,J. Puigcerver,Utku Evci,Manoj Kumar,Sjoerd van Steenkiste,Gamaleldin F. Elsayed,Aravindh Mahendran,F. Yu,Avital Oliver,Fantine Huot,Jasmijn Bastings,Mark Collier,A. Gritsenko,Vighnesh Birodkar,C. Vasconcelos,Yi Tay,Thomas Mensink,Alexander Kolesnikov,Filip Paveti'c,Dustin Tran,Thomas Kipf,Mario Luvci'c,Xiaohua Zhai,Daniel Keysers,Jeremiah Harmsen,N. Houlsby","id":"61e721334296ebfbbf6443b5ed9eb8c83b708c95","summary":"A recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and a wide variety of experiments on the resulting model, which demonstrates the potential for \"LLM-like\"scaling in vision, and provides key steps towards getting there.","score":4},{"url":"https://www.semanticscholar.org/paper/cbcd19395e4b5ad5e047e0476cb906ca6461df72","title":"Few-Shot Natural Language Inference Generation with PDD: Prompt and Dynamic Demonstration","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/05/2022","authors":"Kaijian Li,Shansan Gong,Kenny Q. Zhu","id":"cbcd19395e4b5ad5e047e0476cb906ca6461df72","summary":"This paper proposes language models with prompt and dynamic demonstration (LM-PDD) to tackle the problem of natural language generation in few-shot settings, and shows that the dynamic demonstration method has good generalizability.","score":4},{"url":"https://www.semanticscholar.org/paper/f0575b2b10e2178f9675a6d92ab7cb80015948d0","title":"DEER: Descriptive Knowledge Graph for Explaining Entity Relationships","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":34,"citationCount":3,"influentialCitationCount":0,"publicationDate":"21/05/2022","authors":"Jie Huang,Kerui Zhu,K. Chang,Jinjun Xiong,W. Hwu","id":"f0575b2b10e2178f9675a6d92ab7cb80015948d0","summary":"Experiments demonstrate that the system can extract and generate high-quality relation descriptions for explaining entity relationships and suggest that it can build an open and informative knowledge graph without human annotation.","score":4},{"url":"https://www.semanticscholar.org/paper/dbeef935bedf509967bf472b183c12bce12f5aea","title":"DKG: A Descriptive Knowledge Graph for Explaining Relationships between Entities","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jie Huang,Kerui Zhu,K. Chang,Jinjun Xiong,W. Hwu","id":"dbeef935bedf509967bf472b183c12bce12f5aea","summary":"To construct DKGs, a self-supervised learning method to extract relation descriptions with the analysis of dependency patterns and a transformer-based relation description synthesizing model to generate relation descriptions are proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/422d8c989adeb904563d0c96d5038f6c8596fa99","title":"Neural Knowledge Bank for Pretrained Transformers","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/07/2022","authors":"Damai Dai,Wen-Jie Jiang,Qingxiu Dong,Yajuan Lyu,Qiaoqiao She,Zhifang Sui","id":"422d8c989adeb904563d0c96d5038f6c8596fa99","summary":"A Neural Knowledge Bank (NKB) and a knowledge in- jection strategy to introduce extra factual knowledge for pretrained Transformers and the interpretability of the NKB is thoroughly analyzed and reveal the meaning of its keys and values in a human-readable way.","score":4},{"url":"https://www.semanticscholar.org/paper/732e3faec4e5be4d144256f2c379b9dc49f0b227","title":"Efficient Long-Text Understanding with Short-Text Models","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":3,"influentialCitationCount":1,"publicationDate":"01/08/2022","authors":"Maor Ivgi,Uri Shaham,Jonathan Berant","id":"732e3faec4e5be4d144256f2c379b9dc49f0b227","summary":"This work proposes SLED, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs and shows that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.","score":4},{"url":"https://www.semanticscholar.org/paper/67f32dec9973d81b9bd3fa9f59dd40263decde91","title":"On Grounded Planning for Embodied Tasks with Language Models","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/08/2022","authors":"Bill Yuchen Lin,Chengsong Huang,Qianchu Liu,Wenda Gu,Sam Sommerer,Xiang Ren","id":"67f32dec9973d81b9bd3fa9f59dd40263decde91","summary":"This paper addresses the question of whether language models have the capacity to generate grounded, executable plans for embodied tasks and demonstrates that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs' ability in grounded planning.","score":4},{"url":"https://www.semanticscholar.org/paper/e88180a9256b3a97849f92d1ca252f8c803681ab","title":"Continuous QA Learning with Structured Prompts","venue":"","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/08/2022","authors":"Yinhe Zheng","id":"e88180a9256b3a97849f92d1ca252f8c803681ab","summary":"Diana is proposed : a dynamic architecture-based lifelong QA model that tries to learn a sequence of QA tasks with a prompt enhanced lan- guage model and outperforms state-of-the-art lifelongQA models, especially in handling unseen tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/b04a2d101532fc7f8cdb1db0e94f408afec990ae","title":"Lifelong Learning for Question Answering with Hierarchical Prompts","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yinpei Dai,Hao Lang,Yinhe Zheng,Fei Huang,Luo Si,Yongbin Li","id":"b04a2d101532fc7f8cdb1db0e94f408afec990ae","summary":"Diana is proposed : a dynamic architecture-based lifelong QA model that tries to learn a sequence of QA tasks with a prompt enhanced lan- guage model and outperforms state-of-the-art lifelongQA models, especially in handling unseen tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/d6f86c2dc340621f0ae2a246cb1d76e7b86969c6","title":"Incorporating Task-specific Concept Knowledge into Script Learning","venue":"ArXiv","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/08/2022","authors":"Chenkai Sun,Tie Xu,ChengXiang Zhai,Heng Ji","id":"d6f86c2dc340621f0ae2a246cb1d76e7b86969c6","summary":"This paper presents T ETRIS, a new task of Goal-Oriented Script Completion, a more realistic and more general setting, where the input includes not only the goal but also additional user context, including preferences and history, to address the problem using a knowledge-based approach.","score":4},{"url":"https://www.semanticscholar.org/paper/891edceb78a274b0c2494d8176bc4d6f6e3f9cbc","title":"Calibrating Sequence likelihood Improves Conditional Language Generation","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":3,"influentialCitationCount":0,"publicationDate":"30/09/2022","authors":"Yao Zhao,Misha Khalman,Rishabh Joshi,Shashi Narayan,Mohammad Saleh,Peter J. Liu","id":"891edceb78a274b0c2494d8176bc4d6f6e3f9cbc","summary":"This work introduces sequence likelihood calibration (SLiC) where the likelihood of model generated sequences are calibrated to better align with reference sequences in the model’s latent space, and presents alternative ways to improve quality with limited training and inference budgets.","score":4},{"url":"https://www.semanticscholar.org/paper/3b622664d44a57280d3a189fa6475e56b96f1add","title":"CIKQA: Learning Commonsense Inference with a Unified Knowledge-in-the-loop QA Paradigm","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Hongming Zhang,Yintong Huo,Yanai Elazar,Yangqiu Song,Yoav Goldberg,D. Roth","id":"3b622664d44a57280d3a189fa6475e56b96f1add","summary":"This work focuses on investigating models’ commonsense inference capabilities from two per-spectives: (1) Whether models can know if the knowledge they have is enough to solve the task; (2) whether models can learn commonsens inference capabilities, that generalize across commonsense tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/2b5e3fbca8b70395eafc1c28bfbe999c8ae7c69a","title":"Converge to the Truth: Factual Error Correction via Iterative Constrained Editing","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Jiangjie Chen,Rui Xu,Wenyuan Zeng,Changzhi Sun,Lei Li,Yanghua Xiao","id":"2b5e3fbca8b70395eafc1c28bfbe999c8ae7c69a","summary":"V EN CE proposes V EN CE, a novel method for factual error correction (FEC) with minimal edits, which formulates the FEC problem as iterative sampling editing actions with respect to a target density function.","score":4},{"url":"https://www.semanticscholar.org/paper/c4995582ef008116b878eaf23c4bc15271680086","title":"Best-k Search Algorithm for Neural Text Generation","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Jiacheng Xu,Caiming Xiong,S. Savarese,Yingbo Zhou","id":"c4995582ef008116b878eaf23c4bc15271680086","summary":"Experiments on four NLG tasks show that best- k search yields more diverse and natural outputs compared to strong baselines, while the proposed algorithm maintains high text quality.","score":4},{"url":"https://www.semanticscholar.org/paper/e25febbcdc351826a174f181ecd83b65084b0146","title":"Neural text summarization for Hungarian","venue":"Acta Linguistica Academica","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"Zijian Győző Yang","id":"e25febbcdc351826a174f181ecd83b65084b0146","summary":"The first Hungarian abstractive summarization tool based on mBART and mT5 models, which gained state-of-the-art results is presented.","score":4},{"url":"https://www.semanticscholar.org/paper/5122b1239af8c259190ff2725a7289ed52c5e879","title":"Implementing Deep Learning-Based Approaches for Article Summarization in Indian Languages","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Rahul Tangsali,Aabha Pingle,Aditya Vyawahare,I. Joshi,Raviraj Joshi","id":"5122b1239af8c259190ff2725a7289ed52c5e879","summary":"This work explores different pre-trained seq2seq models and fine-tune those with the ILSUM 2022 datasets and found the fine-tuned SoTA PEGASUS model worked the best for English, thefine-tuning IndicBART model with augmented data for Hindi, and again fine- Tuned PEGasUS model along with a translation mapping-based approach for Gujarati.","score":4},{"url":"https://www.semanticscholar.org/paper/e29f6d3f72dc57ee08feb04865a490a117a6e270","title":"Meeting Summarization: A Survey of the State of the Art","venue":"ArXiv","year":2022,"referenceCount":100,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"L. P. Kumar,Arman Kabiri","id":"e29f6d3f72dc57ee08feb04865a490a117a6e270","summary":"This survey offers a general overview of text summarization along with datasets and evaluation metrics for meeting summarization, and provides the performance of each summarizer on a leaderboard.","score":4},{"url":"https://www.semanticscholar.org/paper/f8528fd83bcf4edf7e839a7123c3725177faea1c","title":"SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks","venue":"ArXiv","year":2022,"referenceCount":75,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Suwon Shon,Siddhant Arora,Chyi-Jiunn Lin,Ankita Pasad,Felix Wu,Roshan Sharma,Wei Yu Wu,Hung-yi Lee,Karen Livescu,Shinji Watanabe","id":"f8528fd83bcf4edf7e839a7123c3725177faea1c","summary":"This work introduces several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape.","score":4},{"url":"https://www.semanticscholar.org/paper/c372842e46f42d722be433c042bbe21ead604230","title":"SPEC: Summary Preference Decomposition for Low-Resource Abstractive Summarization","venue":"IEEE/ACM Transactions on Audio Speech and Language Processing","year":2023,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Yi-Syuan Chen,Yun-Zhu Song,Hong-Han Shuai","id":"c372842e46f42d722be433c042bbe21ead604230","summary":"This paper proposes a novel decoding method to automatically estimate suitable preferences and generate corresponding summary candidates from the few training examples and proposes a meta learning framework to transfer few-shot learning processes from source corpora to the target corpus.","score":4},{"url":"https://www.semanticscholar.org/paper/0126c13e2674f057b7109baddbafe0bb080d194b","title":"Incorporating Knowledge into Document Summarization: an Application of Prefix-Tuning on GPT-2","venue":"ArXiv","year":2023,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Chen Chen,Wei Emma Zhang,A. Shakeri","id":"0126c13e2674f057b7109baddbafe0bb080d194b","summary":"The improvements on fact preservation in the generated summaries indicates the effectiveness of adopting this preﬁx-tuning-based method in knowledge-enhanced document summarization, and also shows a great potential on other natural language processing tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/064a89270249fb3fa3f95efed1b6f768dc8fa7c3","title":"Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings","venue":"","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/02/2023","authors":"Tohida Rehman,Debarshi Kumar Sanyal,S. Chattopadhyay,Plaban Kumar Bhowmick,Partha Pratim Das","id":"064a89270249fb3fa3f95efed1b6f768dc8fa7c3","summary":"This work aims to automatically construct research highlights given certain segments of the research paper, using a pointer-generator network with coverage mechanism and a contextual embedding layer at the input that encodes the input tokens into SciBERT embeddings.","score":4},{"url":"https://www.semanticscholar.org/paper/04a55a02059d949a7667c4028ad1cd1ee3e5444d","title":"Distribution Aware Metrics for Conditional Natural Language Generation","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":3,"influentialCitationCount":0,"publicationDate":"15/09/2022","authors":"David Chan,Yiming Ni,Austin Myers,Sudheendra Vijayanarasimhan,David A. Ross,J. Canny","id":"04a55a02059d949a7667c4028ad1cd1ee3e5444d","summary":"This work proposes a novel paradigm for multi-candidate evaluation of conditional language generation models, and a new family of metrics that compare the distributions of reference and model-generated caption sets using small sample sets of each.","score":4},{"url":"https://www.semanticscholar.org/paper/a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs","venue":"IEEE Conference on High Performance Extreme Computing","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/09/2022","authors":"Zifan Carl Guo,William S. Moses","id":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","summary":"This work applies transfer learning to low-level (LLVM) programs and study how low- level programs can be made more amenable to Transformer models through various techniques, including preprocessing, infix/prefix operators, and information deduplication.","score":4},{"url":"https://www.semanticscholar.org/paper/5b9aee8d1689ce0452d81ab032d2c5fae0302d70","title":"Multiple-Choice Question Generation: Towards an Automated Assessment Framework","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/09/2022","authors":"Vatsal Raina,M. Gales","id":"5b9aee8d1689ce0452d81ab032d2c5fae0302d70","summary":"This work proposes a set of performance criteria that assess different aspects of the generated multiple-choice questions of interest, including: grammatical correctness, answerability, diversity and complexity.","score":4},{"url":"https://www.semanticscholar.org/paper/8c15556f2292b7db8d87050cb1286db699c0a22c","title":"Paraphrasing Is All You Need for Novel Object Captioning","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/09/2022","authors":"Cheng Yang,Yao-Hung Hubert Tsai,Wanshu Fan,R. Salakhutdinov,Louis-Philippe Morency,Yu-Chiang Frank Wang","id":"8c15556f2292b7db8d87050cb1286db699c0a22c","summary":"Paraphrasing-to-Captioning (P2C) is presented, a two-stage learning framework for NOC which would heuristically optimize the output captions via paraphrasing and leverages cross-modality (image-text) association modules to ensure the above caption characteristics can be properly preserved.","score":4},{"url":"https://www.semanticscholar.org/paper/50b8ba87a90d9e5a76adf082883e9e694d7807ff","title":"UCEpic: Unifying Aspect Planning and Lexical Constraints for Explainable Recommendation","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/09/2022","authors":"Jiacheng Li,Zhankui He,Jingbo Shang,Julian McAuley","id":"50b8ba87a90d9e5a76adf082883e9e694d7807ff","summary":"Compared to previous work controlled by soft constraints, UCE PIC incorporates speciﬁc information from keyphrases and then largely improves the diversity and informativeness of generated explanations.","score":4},{"url":"https://www.semanticscholar.org/paper/752ba6b4da4048b0ce0a34cd03ff84a2861aa2fb","title":"Differentially Private Bias-Term only Fine-tuning of Foundation Models","venue":"ArXiv","year":2022,"referenceCount":70,"citationCount":1,"influentialCitationCount":1,"publicationDate":"30/09/2022","authors":"Zhiqi Bu,Yu-Xiang Wang,Sheng Zha,G. Karypis","id":"752ba6b4da4048b0ce0a34cd03ff84a2861aa2fb","summary":"This work proposes diﬀerentially private bias-term ﬁne-tuning (DP-BiTFiT), which matches the state-of-the-art accuracy for DP algorithms and the eﬃciency of the standard Bi TFiT, and is even faster than the standard full full DP-ne- Tuning.","score":4},{"url":"https://www.semanticscholar.org/paper/adb93f1571748f01a32f76faf395291f820e410e","title":"Multi-Modal Code Summarization with Retrieved Summary","venue":"IEEE Working Conference on Source Code Analysis and Manipulation","year":2022,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/10/2022","authors":"Lile Lin,Zhiqiu Huang,Yaoshen Yu,Ya-Ping Liu","id":"adb93f1571748f01a32f76faf395291f820e410e","summary":"A novel approach based on contrastive learning to build a retrieval model to retrieve semantically similar summaries that incorporates lexical, syntactic and semantic modalities of codes.","score":4},{"url":"https://www.semanticscholar.org/paper/38288e285748b3bace77b1422531da76fdab7b3a","title":"Video Captioning Using Global-Local Representation","venue":"IEEE transactions on circuits and systems for video technology (Print)","year":2022,"referenceCount":88,"citationCount":5,"influentialCitationCount":1,"publicationDate":"01/10/2022","authors":"Liqi Yan,Siqi Ma,Qifan Wang,Yingjie Chen,X. Zhang,A. Savakis,Dongfang Liu","id":"38288e285748b3bace77b1422531da76fdab7b3a","summary":"This work proposes a GLR framework, namely a global-local representation granularity, which exploits extensive vision representations from different video ranges to improve linguistic expression and introduces the progressive training strategy which can effectively organize feature learning to incur optimal captioning behavior.","score":4},{"url":"https://www.semanticscholar.org/paper/ef6002ccc9e72ba483390ee130f17e8f81a2cb11","title":"Just ClozE! A Fast and Simple Method for Evaluating the Factual Consistency in Abstractive Summarization","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"Yiyang Li,Lei Li,Qing Yang,Marina Litvak,N. Vanetik,Dingxing Hu,Yuze Li,Yanquan Zhou,Dongliang Xu,Xuanyu Zhang","id":"ef6002ccc9e72ba483390ee130f17e8f81a2cb11","summary":"This paper demonstrates that ClozE can reduce the evaluation time by nearly 96 % relative to QA-based metrics while retaining their interpretability and performance through experiments on six human-annotated datasets and a meta-evaluation benchmark GO FIGURE (Gabriel et al., 2020).","score":4},{"url":"https://www.semanticscholar.org/paper/591627746d0f8c3b642b7c9415bbc8af66e24a0e","title":"Visualize Before You Write: Imagination-Guided Open-Ended Text Generation","venue":"ArXiv","year":2022,"referenceCount":81,"citationCount":1,"influentialCitationCount":0,"publicationDate":"07/10/2022","authors":"Wanrong Zhu,An Yan,Yujie Lu,Wenda Xu,X. Wang,Miguel Eckstein,William Yang Wang","id":"591627746d0f8c3b642b7c9415bbc8af66e24a0e","summary":"This work proposes iNLG that uses machine-generated images to guide language models in open-ended text generation in both few-shot and full-data scenarios and demonstrates the effectiveness ofiNLG on open-ending text generation tasks, including text completion, story generation, and concept-to-text generation.","score":4},{"url":"https://www.semanticscholar.org/paper/56f0b389ea785efdc8fcad28b76417e134f2a580","title":"QAScore—An Unsupervised Unreferenced Metric for the Question Generation Evaluation","venue":"Entropy","year":2022,"referenceCount":55,"citationCount":3,"influentialCitationCount":0,"publicationDate":"09/10/2022","authors":"Tianbo Ji,Chenyang Lyu,Gareth J. F. Jones,Liting Zhou,Yvette Graham","id":"56f0b389ea785efdc8fcad28b76417e134f2a580","summary":"A new reference-free evaluation metric called QAScore is proposed, which is capable of providing a better mechanism for evaluating QG systems and can obtain a stronger correlation with human judgement according to the human evaluation experiment.","score":4},{"url":"https://www.semanticscholar.org/paper/0e6b7111c1b08b1467e26780613ac0f8053e9ec1","title":"Talk2Face: A Unified Sequence-based Framework for Diverse Face Generation and Analysis Tasks","venue":"ACM Multimedia","year":2022,"referenceCount":62,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Yudong Li,Xianxu Hou,Zhe Zhao,Linlin Shen,Xuefeng Yang,Kimmo Yan","id":"0e6b7111c1b08b1467e26780613ac0f8053e9ec1","summary":"This work proposed a single model, Talk2Face, to simultaneously tackle a large number of face generation and analysis tasks, e.g. text guided face synthesis, face captioning and age estimation, by cast different tasks into a sequence-to-sequence format with the same architecture, parameters and objectives.","score":4},{"url":"https://www.semanticscholar.org/paper/9e06248853f41ab547814fed70e640eba44764cf","title":"DATScore: Evaluating Translation with Data Augmented Translations","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Moussa Kamal Eddine,Guokan Shang,M. Vazirgiannis","id":"9e06248853f41ab547814fed70e640eba44764cf","summary":"This work introduces DATScore, a metric leveraging the BART language model to evaluate the quality of generated text from various aspects and uses data augmentation techniques to improve the evaluation of machine translation.","score":4},{"url":"https://www.semanticscholar.org/paper/6d72e1c59f21ba1c7722cf708c7ada70ad712161","title":"EduQG: A Multi-format Multiple Choice Dataset for the Educational Domain","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Amir Hadifar,Semere Kiros Bitew,Johannes Deleu,Chris Develder,Thomas Demeester","id":"6d72e1c59f21ba1c7722cf708c7ada70ad712161","summary":"A high-quality dataset that contains 3,397 samples comprising multiple choice questions, answers (including distractors), and their source documents, from the educational domain, that can be used for both question and distractor generation, as well as to explore new challenges such as question format conversion.","score":4},{"url":"https://www.semanticscholar.org/paper/969f45a3adf5e0bcf741447b1c67a0f3a386801a","title":"BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Tianxiang Sun,Junliang He,Xipeng Qiu,Xuanjing Huang","id":"969f45a3adf5e0bcf741447b1c67a0f3a386801a","summary":"In-depth analysis suggests that choosing paradigms (matching, regression, or generation) of the metric has a greater impact on fairness than choosing PLMs, and develops debiasing adapters that are injected into PLM layers, mitigating bias in PLM-based metrics while retaining high performance for evaluating text generation.","score":4},{"url":"https://www.semanticscholar.org/paper/361d25efdb55681b3925b38b7e298b54acbc3259","title":"Summary Workbench: Unifying Application and Evaluation of Text Summarization Models","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/10/2022","authors":"S. Syed,Dominik Schwabe,Martin Potthast","id":"361d25efdb55681b3925b38b7e298b54acbc3259","summary":"A new tool for developing and evaluating text summarization models that can be easily integrated as Docker-based plugins, allowing to examine the quality of their summaries against any input and to evaluate them using various evaluation measures.","score":4},{"url":"https://www.semanticscholar.org/paper/0b574244f2ecea75a536106789f08d3c3c2590e0","title":"Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/10/2022","authors":"Yu Zhao,Jianguo Wei,Zhichao Lin,Yueheng Sun,Meishan Zhang,M. Zhang","id":"0b574244f2ecea75a536106789f08d3c3c2590e0","summary":"This work presents Visual Spatial Description (VSD), a new perspective for image-to-text toward spatial semantics, which aims to produce one description focusing on the spatial perspective between the two objects in an image.","score":4},{"url":"https://www.semanticscholar.org/paper/c6e4518dfd687a2a5bed4e78d5d9f999292a1746","title":"Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/10/2022","authors":"Xiao Liu,Yansong Feng,Jizhi Tang,ChenGang Hu,Dongyan Zhao","id":"c6e4518dfd687a2a5bed4e78d5d9f999292a1746","summary":"This paper designs the counterfactual recipe generation task, which asks models to modify a base recipe according to the change of an ingredient, and finetune pretrained language models on the recipe corpus, and uses unsupervised counterfactUAL generation methods to generate modified recipes.","score":4},{"url":"https://www.semanticscholar.org/paper/58b8da3821affc426895a85dbac5556322e6e2a9","title":"EtriCA: Event-Triggered Context-Aware Story Generation Augmented by Cross Attention","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":47,"citationCount":1,"influentialCitationCount":0,"publicationDate":"22/10/2022","authors":"Cheng Tang,Chenghua Lin,Hen-Hsen Huang,Frank Guerin,Zhihao Zhang","id":"58b8da3821affc426895a85dbac5556322e6e2a9","summary":"EtriCA is presented, a novel neural generation model, which improves the relevance and coherence of the generated stories through residually mapping context features to event sequences with a cross-attention mechanism.","score":4},{"url":"https://www.semanticscholar.org/paper/b305c995821c6e9510b9c20e966bb9a6f4658bfe","title":"There Is No Standard Answer: Knowledge-Grounded Dialogue Generation with Adversarial Activated Multi-Reference Learning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/10/2022","authors":"Xueliang Zhao,Tingchen Fu,Chongyang Tao,Rui Yan","id":"b305c995821c6e9510b9c20e966bb9a6f4658bfe","summary":"To extend the hypothesis space of knowledge selection to enhance the mapping relationship between multiple knowledge and multiple responses, a span-based variational model is devised and optimize the model in a wake-sleep style with an ameliorated evidence lower bound objective to learn the one-to-many generalization.","score":4},{"url":"https://www.semanticscholar.org/paper/2ae0c69cac7bb2ba386faf6e9702db54b2ca594d","title":"P3LM: Probabilistically Permuted Prophet Language Modeling for Generative Pre-Training","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/10/2022","authors":"Junwei Bao,Yifan Wang,Jiangyong Ying,Yeyun Gong,Jing Zhao,Youzheng Wu,Xiaodong He","id":"2ae0c69cac7bb2ba386faf6e9702db54b2ca594d","summary":"P$^3$LM, a probabilistically permuted prophet language model, which strengthens the modeling of bidirectional information and long token dependencies for sequence generation, achieves state-of-the-art results compared with strong publicly available generative pre-training methods.","score":4},{"url":"https://www.semanticscholar.org/paper/40b0d9e116c46bd8d6813712f013c54c2790c17c","title":"Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/10/2022","authors":"Dongkyu Lee,K. Cheung,N. Zhang","id":"40b0d9e116c46bd8d6813712f013c54c2790c17c","summary":"This work proposes a regularization scheme that brings dynamic nature into the smoothing parameter by taking model probability distribution into account, thereby varying the parameter per instance.","score":4},{"url":"https://www.semanticscholar.org/paper/be050e69aadd6461ff3bc35dcfa7a551742ef840","title":"Varifocal Question Generation for Fact-checking","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/10/2022","authors":"N. Ousidhoum,Zhangdie Yuan,Andreas Vlachos","id":"be050e69aadd6461ff3bc35dcfa7a551742ef840","summary":"Varifocal is presented, a method that generates questions based on different focal points within a given claim, i.e. different spans of the claim and its metadata, such as its source and date, on a wide range of automatic evaluation metrics.","score":4},{"url":"https://www.semanticscholar.org/paper/0514444cd3564a7d7a561c4e5851c854000adb9f","title":"Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/10/2022","authors":"Dongkyu Lee,Zhiliang Tian,Ying Zhao,K. Cheung,N. Zhang","id":"0514444cd3564a7d7a561c4e5851c854000adb9f","summary":"Empirical comparisons with strong baselines show that hard gate knowledge distillation not only improves model generalization, but also significantly lowers model calibration error.","score":4},{"url":"https://www.semanticscholar.org/paper/9443a623d6e0d423adb9c61b5d225f0077f38767","title":"DEMETR: Diagnosing Evaluation Metrics for Translation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":47,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Marzena Karpinska,N. Raj,Katherine Thai,Yixiao Song,Ankita Gupta,Mohit Iyyer","id":"9443a623d6e0d423adb9c61b5d225f0077f38767","summary":"DEMETR is a diagnostic dataset with 31K English examples for evaluating the sensitivity of MT evaluation metrics to 35 different linguistic perturbations spanning semantic, syntactic, and morphological error categories and it is found that learned metrics perform substantially better than string-based metrics on DEMETR.","score":4},{"url":"https://www.semanticscholar.org/paper/87fed8c2eb9f2b8d3b50e7d4a279fd59ae52ea1e","title":"End-to-End Multimodal Representation Learning for Video Dialog","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Huda AlAmri,Anthony Bilic,Michael Hu,Apoorva Beedu,Irfan Essa","id":"87fed8c2eb9f2b8d3b50e7d4a279fd59ae52ea1e","summary":"This study proposes a new framework that combines 3D-CNN network and transformer-based networks into a single visual encoder to extract more robust semantic representations from videos.","score":4},{"url":"https://www.semanticscholar.org/paper/ffa0032150e2edf21633b6a6867b54ea121cb9d9","title":"MOCHA: A Multi-Task Training Approach for Coherent Text Generation from Cognitive Perspective","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Zhe Hu,Hou Pong Chan,Lifu Huang","id":"ffa0032150e2edf21633b6a6867b54ea121cb9d9","summary":"A novel multi-task training strategy for long text generation grounded on the cognitive theory of writing is proposed, which empowers the model to learn essential subskills needed for writing including planning and reviewing besides end-to-end generation.","score":4},{"url":"https://www.semanticscholar.org/paper/1cc512fc89651dc4e9a1998447a6693d90b73ee0","title":"FaD-VLP: Fashion Vision-and-Language Pre-training towards Unified Retrieval and Captioning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":61,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Suvir Mirchandani,Licheng Yu,Mengjiao MJ Wang,Animesh Sinha,Wen-Jun Jiang,Tao Xiang,Ning Zhang","id":"1cc512fc89651dc4e9a1998447a6693d90b73ee0","summary":"This work proposes a novel fashion-specific pre-training framework based on weakly-supervised triplets constructed from fashion image-text pairs and proposes a flexible decoder-based model architecture capable of both fashion retrieval and captioning tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/dbeb1284c623f09b43ec60655b7ebd7028b5922f","title":"Terminology-aware Medical Dialogue Generation","venue":"ArXiv","year":2022,"referenceCount":15,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Cheng Tang,Hongbo Zhang,Tyler Loakman,Chenghua Lin,Frank Guerin","id":"dbeb1284c623f09b43ec60655b7ebd7028b5922f","summary":"A novel framework to improve medical dialogue generation by considering features centered on domain-speciﬁc terminology is proposed, which outperforms SOTA language models and leverages an attention mechanism to incorporate terminologically centred features.","score":4},{"url":"https://www.semanticscholar.org/paper/cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/10/2022","authors":"Victor C. Dibia,Adam Fourney,Gagan Bansal,Forough Poursabzi-Sangdeh,Han Liu,Saleema Amershi","id":"cba98048f3e85a974c287b271692bf6c197db940","summary":"A simple hybrid metric is proposed, which combines functional correctness and similarity- based metrics to capture different dimensions of what programmers might value and shows that this hybrid metric more accurately captures effort.","score":4},{"url":"https://www.semanticscholar.org/paper/3c5f7e7ee0ab7413ba3bf8ad3400810da542d617","title":"How Far are We from Robust Long Abstractive Summarization?","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":81,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/10/2022","authors":"Huan Yee Koh,Jiaxin Ju,He Zhang,Ming Liu,Shirui Pan","id":"3c5f7e7ee0ab7413ba3bf8ad3400810da542d617","summary":"This work performs fine-grained human annotations to evaluate long document abstractive summarization systems (i.e., models and metrics) with the aim of implementing them to generate reliable summaries and suggests promising directions in the endeavor of developing factual consistency metrics.","score":4},{"url":"https://www.semanticscholar.org/paper/684e0925aa11628a165a6faf2095e45447258769","title":"Towards Inter-character Relationship-driven Story Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":45,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Anvesh Rao Vijjini,Faeze Brahman,Snigdha Chaturvedi","id":"684e0925aa11628a165a6faf2095e45447258769","summary":"This paper proposes Relationships as Latent Variables for Story Generation, (ReLiSt), a model for modeling interpersonal relationships for story generation that is able to generate stories with relationships that are more faithful to desired relationships while maintaining the content quality.","score":4},{"url":"https://www.semanticscholar.org/paper/cfc53d08b5e25a2ecfd94de71e7d23aa3175862f","title":"Medical Text Simplification Using Reinforcement Learning (TESLEA): Deep Learning–Based Text Simplification Approach","venue":"JMIR Medical Informatics","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Atharva Phatak,D. Savage,R. Ohle,Jonathan Smith,Vijay K. Mago","id":"cfc53d08b5e25a2ecfd94de71e7d23aa3175862f","summary":"The proposed TS approach can be applied to automatically generate simplified text for complex medical text data, which would enhance the accessibility of biomedical research to a wider audience.","score":4},{"url":"https://www.semanticscholar.org/paper/4b353d924f656824f9833dfc53a3f41dc1021edd","title":"Semantic space captioner: generating image captions step by step","venue":"Journal of Electronic Imaging (JEI)","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Chenhao Zhu,Xia Ye,Qiduo Lu","id":"4b353d924f656824f9833dfc53a3f41dc1021edd","summary":"The semantic space captioner model is proposed to introduce the concept of dense captioning into image captioning using contrastive language-image pretraining as an encoder for text and images.","score":4},{"url":"https://www.semanticscholar.org/paper/a054e491539f76cc2da4bb7a823c4195b7d90535","title":"Persona-Based Conversational AI: State of the Art and Challenges","venue":"2022 IEEE International Conference on Data Mining Workshops (ICDMW)","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Junfeng Liu,Christopher Symons,R. Vatsavai","id":"a054e491539f76cc2da4bb7a823c4195b7d90535","summary":"This study evaluates two strong baseline methods, the Ranking Profile Memory Network and the Poly-Encoder, on the NeurIPS ConvAI2 benchmark dataset and elucidates the importance of incorporating persona information into conversational systems.","score":4},{"url":"https://www.semanticscholar.org/paper/3a3bcbdcfbf63ccc38c4c7e30a5111885dffd240","title":"OSIC: A New One-Stage Image Captioner Coined","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":1,"influentialCitationCount":0,"publicationDate":"04/11/2022","authors":"Bo Wang,Zhao Zhang,Ming Zhao,Xiaojie Jin,Mingliang Xu,Meng Wang","id":"3a3bcbdcfbf63ccc38c4c7e30a5111885dffd240","summary":"This paper proposes a novel One-Stage Image Captioner (OSIC) with dynamic multi-sight learning, which directly transforms input images into descriptive sentences in one stage and can obtain rich and useful information to improve the image caption task.","score":4},{"url":"https://www.semanticscholar.org/paper/2e5a5769bdc81e1329961e44efad31c67ab4d281","title":"Investigations in Audio Captioning: Addressing Vocabulary Imbalance and Evaluating Suitability of Language-Centric Performance Metrics","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/11/2022","authors":"Sandeep Reddy Kothinti,Dimitra Emmanouilidou","id":"2e5a5769bdc81e1329961e44efad31c67ab4d281","summary":"This work identifies and improves on three main challenges in automated audio captioning: i) data scarcity, ii) imbalance or limitations in the audio captions vocabulary, and iii) the proper performance evaluation metric that can best capture both auditory and semantic characteristics.","score":4},{"url":"https://www.semanticscholar.org/paper/019145218663108e451fc5f3492af960f96df872","title":"AutoTemplate: A Simple Recipe for Lexically Constrained Text Generation","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Hayate Iso","id":"019145218663108e451fc5f3492af960f96df872","summary":"The AutoTemplate is introduced, a simple yet effective lexically constrained text generation framework divided into template generation and lexicalization tasks that outperforms the competitive baselines on both tasks while satisfying the hard lexical constraints.","score":4},{"url":"https://www.semanticscholar.org/paper/7ae089060ecb7ac2788e94ee7e04a3eb7009f6b4","title":"CDialog: A Multi-turn Covid-19 Conversation Dataset for Entity-Aware Dialog Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/11/2022","authors":"Deeksha Varshney,Aizan Zafar,Niranshu Kumar Behra,Asif Ekbal","id":"7ae089060ecb7ac2788e94ee7e04a3eb7009f6b4","summary":"This work makes the very first attempt to release a high-quality multi-turn Medical Dialog dataset relating to Covid-19 disease named CDialog, with over 1K conversations collected from the online medical counselling websites.","score":4},{"url":"https://www.semanticscholar.org/paper/e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models","venue":"DASH","year":2022,"referenceCount":36,"citationCount":2,"influentialCitationCount":2,"publicationDate":"17/11/2022","authors":"Junjie Huang,Chenglong Wang,Jipeng Zhang,Cong Yan,Haotian Cui,J. Inala,Colin B. Clement,Nan Duan,Jianfeng Gao","id":"e402dd77eba504ea93bc38e2a052398bb95db351","summary":"ExeDS is introduced, an evaluation dataset for execution evaluation for data science code generation tasks that contains a set of 534 problems from Jupyter Notebooks, each consisting of code context, task description, reference program, and the desired execution output.","score":4},{"url":"https://www.semanticscholar.org/paper/484b4e96428a7d3ab46330a15b14278ca7bd68ca","title":"GENIUS: Sketch-based Language Model Pre-training via Extreme and Selective Masking for Text Generation and Augmentation","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/11/2022","authors":"Biyang Guo,Yeyun Gong,Yelong Shen,Songqiao Han,Hailiang Huang,Nan Duan,Weizhu Chen","id":"484b4e96428a7d3ab46330a15b14278ca7bd68ca","summary":"This work introduces GENIUS, a conditional text model, which can be used as a strong and ready-to-use data augmentation tool for various natural language processing (NLP) tasks and proposes GeniusAug, which extracts the target-aware sketches from the original training set and then generates new samples based on the sketches.","score":4},{"url":"https://www.semanticscholar.org/paper/ad2149957cd288a5626adcce48f9981a2ab59184","title":"Operationalizing Specifications, In Addition to Test Sets for Evaluating Constrained Generative Models","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/11/2022","authors":"Vikas Raunak,Matt Post,Arul Menezes","id":"ad2149957cd288a5626adcce48f9981a2ab59184","summary":"It is argued that the scale of generative models could be exploited to raise the abstraction level at which evaluation itself is conducted and provided recommendations, based on leveraging specifications as a powerful instrument to evaluate generation quality.","score":4},{"url":"https://www.semanticscholar.org/paper/f991d1622949a2dea285d2ad0cfb2e8157cb84b4","title":"Exploring Discrete Diffusion Models for Image Captioning","venue":"ArXiv","year":2022,"referenceCount":82,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Zixin Zhu,Yixuan Wei,Jianfeng Wang,Zhe Gan,Zheng Zhang,Le Wang,G. Hua,Lijuan Wang,Zicheng Liu,Han Hu","id":"f991d1622949a2dea285d2ad0cfb2e8157cb84b4","summary":"A diffusion-based captioning model, dubbed the name DDCap, is presented to allow more decoding of texts in image captions, and is competitive to the best well-developed auto-regressive frameworks.","score":4},{"url":"https://www.semanticscholar.org/paper/6787e08233ef6716f2a23ed52e28ea8593676810","title":"Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations","venue":"ArXiv","year":2022,"referenceCount":80,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Peng Jin,Jinfa Huang,Fenglin Liu,Xian Wu,Shen Ge,Guoli Song,D. Clifton,Jing Chen","id":"6787e08233ef6716f2a23ed52e28ea8593676810","summary":"Extensive experiments on three benchmark text-video retrieval datasets prove that the proposed EMCL can learn more discriminative video-and-language representations than previous methods, and signiﬁcantly outperform previous state-of-the-art methods across all metrics.","score":4},{"url":"https://www.semanticscholar.org/paper/120f4e798d78c3f6dceed218bee1ca83a5855f55","title":"Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task","venue":"ArXiv","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Shangda Wu,Maosong Sun","id":"120f4e798d78c3f6dceed218bee1ca83a5855f55","summary":"This paper carries out the first study of generating complete and semantically consistent symbolic music scores from text descriptions, and explores theacy of using publicly available checkpoints for natural language processing in the task of text-to-music generation.","score":4},{"url":"https://www.semanticscholar.org/paper/e83241e1c6e7f47fadc39d3506baebf157ad8cae","title":"IvCDS: An End-to-End Driver Simulator for Personal In-Vehicle Conversational Assistant","venue":"International Journal of Environmental Research and Public Health","year":2022,"referenceCount":81,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Tianbo Ji,Xuanhua Yin,Peng Cheng,Liting Zhou,Siyou Liu,Wei Bao,Chenyang Lyu","id":"e83241e1c6e7f47fadc39d3506baebf157ad8cae","summary":"This proposed driver simulator enables one to interact with an in-vehicle assistant like a real person, and the diversity of conversations can be simply controlled by changing the assigned driver profile.","score":4},{"url":"https://www.semanticscholar.org/paper/90c5d0a3566e0616956ff68955b58712c1b2b74a","title":"HaRiM^+: Evaluating Summary Quality with Hallucination Risk","venue":"AACL","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Seonil Son,Junsoo Park,J. Hwang,Junghwa Lee,Hyungjong Noh,Yeonsoo Lee","id":"90c5d0a3566e0616956ff68955b58712c1b2b74a","summary":"This study reinterpret the decoder overconfidence-regularizing objective suggested in (Miao et al., 2021) as a hallucination risk measurement to better estimate the quality of generated summaries, and proposes a reference-free metric, HaRiM+, which only requires an off-the-shelf summarization model to compute the hallucinationrisk based on token likelihoods.","score":4},{"url":"https://www.semanticscholar.org/paper/1382cd1a16b001cbb5a298d4458b788c2f0a6ffa","title":"Human or Machine? Turing Tests for Vision and Language","venue":"ArXiv","year":2022,"referenceCount":78,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/11/2022","authors":"Mengmi Zhang,Giorgia Dellaferrera,Ankur Sikarwar,M. Armendáriz,Noga Mudrik,Prachi Agrawal,Spandan Madan,Andrei Barbu,Haochen Yang,T. Kumar,Meghna Sadwani,Stella Dellaferrera,Michele Pizzochero,H. Pfister,Gabriel Kreiman","id":"1382cd1a16b001cbb5a298d4458b788c2f0a6ffa","summary":"Surprisingly, the results reveal that current AIs are not far from being able to impersonate human judges across different ages, genders, and educational levels.","score":4},{"url":"https://www.semanticscholar.org/paper/8aa23a86603f7dd4eceda3d2e0337ba90dff7f4f","title":"CodeExp: Explanatory Code Document Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/11/2022","authors":"Haotian Cui,Chenglong Wang,Junjie Huang,J. Inala,Todd Mytkowicz,Bolong Wang,Jian Gao,Nan Duan","id":"8aa23a86603f7dd4eceda3d2e0337ba90dff7f4f","summary":"This work first conducted a human study to identify the criteria for high-quality explanatory docstring for code, collected and refined a large-scale code docstring corpus, and formulated automatic evaluation metrics that best match human assessments that can boost future code explanation research.","score":4},{"url":"https://www.semanticscholar.org/paper/d9b26d458f43d204bc0fb2781ab6bea21ce98a22","title":"Controlled Language Generation for Language Learning Items","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"Kevin Stowe,Debanjan Ghosh,Mengxuan Zhao","id":"d9b26d458f43d204bc0fb2781ab6bea21ce98a22","summary":"This work aims to employ natural language generation to rapidly generate items for English language learning applications, and to control the output of the generation to match the requirements of the relevant items.","score":4},{"url":"https://www.semanticscholar.org/paper/5cadbebda3bd266560a7d24e52d5d207049c663a","title":"Automated Generating Natural Language Requirements based on Domain Ontology","venue":"ArXiv","year":2022,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Z. Zhao,Li Zhang,Xiao-qian Gao,Xiaoli Lian,Heyang Lv,Lina Shi","id":"5cadbebda3bd266560a7d24e52d5d207049c663a","summary":"Experiments show that ReqGen outperforms six popular natural language generation approaches with respect to the hard constraint of keywords(phrases) inclusion, BLEU, ROUGE and syntax compliance.","score":4},{"url":"https://www.semanticscholar.org/paper/6d889cea94c76a37a80b4ab6cc77838ce751be4e","title":"Long-Document Cross-Lingual Summarization","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Shaohui Zheng,Zhixu Li,Jiaan Wang,Jianfeng Qu,An Liu,Lei Zhao,Zhigang Chen","id":"6d889cea94c76a37a80b4ab6cc77838ce751be4e","summary":"Per Perseus, the first long-document CLS dataset, is constructed, which collects about 94K Chinese scientific documents paired with English summaries and shows the superiority of the end-to-end baseline, outperforming the strong pipeline models equipped with sophisticated machine translation systems.","score":4},{"url":"https://www.semanticscholar.org/paper/6e0087571449c002d412e283e2db6dbef7fc3b3f","title":"Code Question Answering via Task-Adaptive Sequence-to-Sequence Pre-training","venue":"Asia-Pacific Software Engineering Conference","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Tingrui Yu,Xiaodong Gu,Beijun Shen","id":"6e0087571449c002d412e283e2db6dbef7fc3b3f","summary":"The proposed CodeMaster is a novel pre-training based approach for automatically answering code questions via task adaptation, and achieves state-of-the-art performance, and highlights the effectiveness of the approach.","score":4},{"url":"https://www.semanticscholar.org/paper/32b3b990b00e890c44d769ac297889a7c75a394f","title":"Grounded Keys-to-Text Generation: Towards Factual Open-Ended Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":56,"citationCount":1,"influentialCitationCount":0,"publicationDate":"04/12/2022","authors":"Faeze Brahman,Baolin Peng,Michel Galley,Sudha Rao,Bill Dolan,Snigdha Chaturvedi,Jianfeng Gao","id":"32b3b990b00e890c44d769ac297889a7c75a394f","summary":"A new grounded keys-to-text generation task is proposed, to generate a factual description about an entity given a set of guiding keys, and grounding passages, and an automatic metric, MAFE, is proposed for factual correctness of generated descriptions.","score":4},{"url":"https://www.semanticscholar.org/paper/72d7f6e605879976d7fbda1b656ffead7befe56a","title":"Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"Björn Plüster,Jakob Ambsdorf,Lukas Braach,Jae Hee Lee,S. Wermter","id":"72d7f6e605879976d7fbda1b656ffead7befe56a","summary":"This work proposes to evade limitations by applying recent advances in large-scale multi-task pretraining of generative Transformer models to the problem of VL-NLE tasks and shows that jointly training on multiple tasks can increase the explanation quality.","score":4},{"url":"https://www.semanticscholar.org/paper/7cd7ed841ef4d82604ad2a65411299e7edd109e1","title":"Momentum Calibration for Text Generation","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"Xingxing Zhang,Yiran Liu,Xun Wang,Pengcheng He,Yang Yu,Si-Qing Chen,Wayne Xiong,Furu Wei","id":"7cd7ed841ef4d82604ad2a65411299e7edd109e1","summary":"MoCa is an online method that dynamically generates slowly evolving samples using a momentum moving average generator with beam search and MoCa learns to align its model scores of these samples with their actual qualities.","score":4},{"url":"https://www.semanticscholar.org/paper/4a43a773f0725c998c65f2d51ddd49d5919183f7","title":"Real-World Compositional Generalization with Disentangled Sequence-to-Sequence Learning","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Hao Zheng,Mirella Lapata","id":"4a43a773f0725c998c65f2d51ddd49d5919183f7","summary":"A new architecture is introduced to Dangle which leads to better generalization performance across existing tasks and datasets, and a new machine translation benchmark which is created by detecting naturally occurring compositional patterns in relation to a training set.","score":4},{"url":"https://www.semanticscholar.org/paper/c6784d18a77176f58b75e639d0b99d797a0b4f84","title":"Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Sunjae Yoon,Eunseop Yoon,Hee Suk Yoon,Junyeong Kim,Changdong Yoo","id":"c6784d18a77176f58b75e639d0b99d797a0b4f84","summary":"Text Hallucination Mitigating (THAM) framework is designed, which incorporates Text Hallucinations Regularization (THR) loss derived from the proposed information-theoretic text hallucination measurement approach, which validates the effectiveness on VGD benchmarks and shows enhanced interpretability.","score":4},{"url":"https://www.semanticscholar.org/paper/2dd4bdb71e8d1a1bb6f7f2cb500972f082aa91fd","title":"NLIP: Noise-robust Language-Image Pre-training","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/12/2022","authors":"Runhu Huang,Yanxin Long,Jianhua Han,Hang Xu,Xiwen Liang,Chunjing Xu,Xiaodan Liang","id":"2dd4bdb71e8d1a1bb6f7f2cb500972f082aa91fd","summary":"By collaboratively optimiz- ing noise-harmonization and noise-completion schemes, the NLIP can alleviate the common noise effects during image- text pre-training in a more efﬁcient way.","score":4},{"url":"https://www.semanticscholar.org/paper/c1bc6168ee5e8b943d3b904266fc445b08f3aa3d","title":"Injecting Domain Knowledge in Language Models for Task-oriented Dialogue Systems","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":25,"citationCount":2,"influentialCitationCount":1,"publicationDate":"15/12/2022","authors":"Denis Emelin,Daniele Bonadiman,Sawsan Alqahtani,Yi Zhang,Saab Mansour","id":"c1bc6168ee5e8b943d3b904266fc445b08f3aa3d","summary":"This paper utilizes light-weight adapters that can be easily integrated with PLMs and serve as a repository for facts learned from different KBs and introduces Knowledge Probing using Response Selection (KPRS) – a probe designed specifically for TOD models.","score":4},{"url":"https://www.semanticscholar.org/paper/8b6e5c63078e53800807d27a0b3e0686633c67cb","title":"Swing Distillation: A Privacy-Preserving Knowledge Distillation Framework","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Junzhuo Li,Xinwei Wu,Weilong Dong,Shuangzhi Wu,Chao Bian,Deyi Xiong","id":"8b6e5c63078e53800807d27a0b3e0686633c67cb","summary":"Experiments demonstrate that the proposed swing distillation can signiﬁcantly reduce (by over 80% in terms of canary exposure) the risk of privacy leakage in comparison to KD with competitive or better performance.","score":4},{"url":"https://www.semanticscholar.org/paper/8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Haau-Sing Li,Mohsen Mesgar,André F. T. Martins,Iryna Gurevych","id":"8210cef990b8e5cddbc95000e46309bdd25337f7","summary":"The empirical results support the hypothesis that clariﬁcations result in more precise generated code, as shown by an improve-ment of 17.52 in BLEU, 12.72 in CodeBLEU, and 7.7% in the exact match.","score":4},{"url":"https://www.semanticscholar.org/paper/71fe1a42838bedb0ce774d7df46ecb4413650688","title":"LENS: A Learnable Evaluation Metric for Text Simplification","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Mounica Maddela,Yao Dou,David Heineman,Wei Xu","id":"71fe1a42838bedb0ce774d7df46ecb4413650688","summary":"This work introduces R ANK & R ATE , a human evaluation framework that rates simpliﬁcations from several models in a list-wise manner by leveraging an interactive interface, which ensures both consistency and accuracy in the evaluation process.","score":4},{"url":"https://www.semanticscholar.org/paper/fa427cf7371bd0ef9801004b1f062cffc41f0283","title":"GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Jian Yang,Shuming Ma,Li Dong,Shaohan Huang,Haoyang Huang,Yuwei Yin,Dongdong Zhang,Liqun Yang,Zhoujun Li,Furu Wei","id":"fa427cf7371bd0ef9801004b1f062cffc41f0283","summary":"A GAN-style model for encoder-decoder pre-training by introducing an auxiliary discriminator, unifying the ability of language understanding and generation in a single model and achieves state-of-the-art performance.","score":4},{"url":"https://www.semanticscholar.org/paper/bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yichen Xu,Yanqiao Zhu","id":"bae76e1d13abe54f66dc140be53538b864578ba8","summary":"This paper presents a comprehensive survey of the NCI domain, including a thorough review of pretraining techniques, tasks, datasets, and model architectures, and hopes it will serve as a bridge between the natural language and programming language communities.","score":4},{"url":"https://www.semanticscholar.org/paper/bc9d103493d93a9ad8e6b60af4d9a900e4470146","title":"CausalDialogue: Modeling Utterance-level Causality in Conversations","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yi-Lin Tuan,Alon Albalak,Wenda Xu,Michael Stephen Saxon,Connor Pryor,L. Getoor,William Yang Wang","id":"bc9d103493d93a9ad8e6b60af4d9a900e4470146","summary":"This research examines user utterances as causes and generated responses as effects and proposes a causality-enhanced method called Exponential Maximum Average Treatment Effect (ExMATE) to enhance the impact of causality at the utterance level in training neural conversation models.","score":4},{"url":"https://www.semanticscholar.org/paper/8e79d63d9747d05bed2c1e851f6f7d14c0fbbdd2","title":"SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Hongyi Yuan,Zheng Yuan,Chuanqi Tan,Fei Huang,Songfang Huang","id":"8e79d63d9747d05bed2c1e851f6f7d14c0fbbdd2","summary":"This work proposes SeqDiffuSeq, a text diffusion model for sequence-to-sequence generation that uses an encoder-decoder Transformers architecture to model denoising function and combines the self-conditioning technique and a newly proposed adaptive noise schedule technique.","score":4},{"url":"https://www.semanticscholar.org/paper/d184b066aca3e5a5a0bab8da6edd1c6f58bce9a8","title":"TegFormer: Topic-to-Essay Generation with Good Topic Coverage and High Text Coherence","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/12/2022","authors":"Wang Qi,R. Liu,Y. Zuo,Yong Chen,Dell Zhang","id":"d184b066aca3e5a5a0bab8da6edd1c6f58bce9a8","summary":"A novel approach called TegFormer is proposed which utilizes the Transformer architecture where the encoder is enriched with domain-speciﬁc contexts while the decoder is enhanced by a large-scale pre-trained language model.","score":4},{"url":"https://www.semanticscholar.org/paper/808245dbe81f988fbcf5143d0762462500e705c0","title":"Graph-based Keyword Planning for Legal Clause Generation from Topics","venue":"NLLP","year":2023,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/01/2023","authors":"Sagar Joshi,Sumanth Balaji,Aparna Garimella,Vasudeva Varma","id":"808245dbe81f988fbcf5143d0762462500e705c0","summary":"This paper proposes a controllable graph-based mechanism that can generate legal clauses using only the topic or type of the legal clauses, and illustrates the effectiveness of this two-stage approach on a broad set of clause topics in contracts.","score":4},{"url":"https://www.semanticscholar.org/paper/0bc2753f59e653de718b5c7a2a0a7e00d13778c7","title":"NL2CMD: An Updated Workflow for Natural Language to Bash Commands Translation","venue":"","year":2023,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Quchen Fu,Zhongwei Teng,Marco Georgaklis,Jules White,Douglas,C. Schmidt","id":"0bc2753f59e653de718b5c7a2a0a7e00d13778c7","summary":"A state-of-the-art translation model used to generate Bash Commands from the corresponding English text is described and a new NL2CMD dataset is introduced that is automatically generated, involves minimal human intervention, and is over six times larger than prior datasets.","score":4},{"url":"https://www.semanticscholar.org/paper/294c1bd38f4d18e5ace4c8d1a57739902cf3a6e8","title":"Automated Audio Captioning With Topic Modeling","venue":"IEEE Access","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Aysegül Özkaya Eren,M. Sert","id":"294c1bd38f4d18e5ace4c8d1a57739902cf3a6e8","summary":"This study proposes a framework that integrates audio embeddings with audio topics in a transformer-based encoder-decoder architecture and believes that the topic modeling can be used to extract semantic content in the AAC task.","score":4},{"url":"https://www.semanticscholar.org/paper/658a912984d6a4899d1369ca674b06c7aafd45d0","title":"DIRECT: Toward Dialogue-Based Reading Comprehension Tutoring","venue":"IEEE Access","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Jin-Xia Huang,Yohan Lee,Oh-Woog Kwon","id":"658a912984d6a4899d1369ca674b06c7aafd45d0","summary":"A dialogue-based intelligent tutoring system (ITS) that imitates human expert tutors that asks questions, assesses student answers, provides hints, and even chats to encourage student engagement is developed.","score":4},{"url":"https://www.semanticscholar.org/paper/906dba49de4b320eefeea04d206ef4d8f4e5d583","title":"Multi-Hop Reasoning Question Generation and Its Application","venue":"IEEE Transactions on Knowledge and Data Engineering","year":2021,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/04/2021","authors":"Jianxing Yu,Qinliang Su,Xiaojun Quan,Jian Yin","id":"906dba49de4b320eefeea04d206ef4d8f4e5d583","summary":"This article proposes a new adaptive meta-learner to optimize the basic QG model according to the specific characteristic of the evaluated case and proposes a data-driven multi-level recognizer to measure the similarity of such structured inputs.","score":4},{"url":"https://www.semanticscholar.org/paper/69e330037afd18e5546fdcacbc9a9f7deb69bba9","title":"Memorization and Generalization in Neural Code Intelligence Models","venue":"Information and Software Technology","year":2021,"referenceCount":93,"citationCount":8,"influentialCitationCount":0,"publicationDate":"16/06/2021","authors":"Md Rafiqul Islam Rabin,Aftab Hussain,V. Hellendoorn,Mohammad Amin Alipour","id":"69e330037afd18e5546fdcacbc9a9f7deb69bba9","summary":"This work evaluates the memorization and generalization tendencies in neural code intelligence models through a case study across several benchmarks and model families by leveraging established approaches from other fields that use DNNs, such as introducing targeted noise into the training dataset.","score":4},{"url":"https://www.semanticscholar.org/paper/15f26375e2c02a4817be2da347638eeef9b9ecbc","title":"Bayesian Active Summarization","venue":"SSRN Electronic Journal","year":2021,"referenceCount":34,"citationCount":2,"influentialCitationCount":0,"publicationDate":"09/10/2021","authors":"Alexios Gidiotis,Grigorios Tsoumakas","id":"15f26375e2c02a4817be2da347638eeef9b9ecbc","summary":"This work introduces Bayesian Active Summarization (BAS), as a method of combining active learning methods with state-of-the-art summarization models, and suggests that BAS achieves better and more robust performance, compared to random selection, particularly for small and very small data annotation budgets.","score":4},{"url":"https://www.semanticscholar.org/paper/26128a9bea65c0ed48c96ebb97140e3dd69d1dd0","title":"COMPASS: a Creative Support System that Alerts Novelists to the Unnoticed Missing Contents","venue":"Computer Speech &amp; Language","year":2022,"referenceCount":86,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/02/2022","authors":"Yusuke Mori,Hiroaki Yamane,Ryohei Shimizu,Yusuke Mukuta,Tatsuya Harada","id":"26128a9bea65c0ed48c96ebb97140e3dd69d1dd0","summary":"Variable Number MPP (VN-MPP), a new MPP task that removes the restriction to predict multiple missing sentences or to judge whether there are no missing sentences in the first place, is proposed and a creative writing support system, COMPASS, is developed.","score":4},{"url":"https://www.semanticscholar.org/paper/dfdb6f57c4e9a070a1a98c583e63e08e66fd2105","title":"On Distinctive Image Captioning via Comparing and Reweighting","venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence","year":2022,"referenceCount":73,"citationCount":4,"influentialCitationCount":1,"publicationDate":"16/03/2022","authors":"Jiuniu Wang,Wenjia Xu,Qingzhong Wang,Antoni B. Chan","id":"dfdb6f57c4e9a070a1a98c583e63e08e66fd2105","summary":"A distinctiveness metric—between-set CIDEr (CIDErBtw) is proposed to evaluate the distinctiveness of a caption with respect to those of similar images and reveals that the human annotations of each image in the MSCOCO dataset are not equivalent based on distinctiveness.","score":4},{"url":"https://www.semanticscholar.org/paper/672524688552788d319da3917dd7c7540cabc926","title":"How to Approach Ambiguous Queries in Conversational Search: A Survey of Techniques, Approaches, Tools, and Challenges","venue":"ACM Computing Surveys","year":2022,"referenceCount":192,"citationCount":5,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"Kimiya Keyvan,J. Huang","id":"672524688552788d319da3917dd7c7540cabc926","summary":"This work provides an overview of characteristics of ambiguous queries and contributes to better understanding of the existing technologies and challenges in CSS focus on disambiguation of unclear queries from various dimensions.","score":4},{"url":"https://www.semanticscholar.org/paper/5421a2058de7d8d39a2d060a53265e600d8f053a","title":"Knowledge-grounded dialogue modelling with dialogue-state tracking, domain tracking, and entity extraction","venue":"Computer Speech and Language","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/09/2022","authors":"Taesuk Hong,Junhee Cho,Haeun Yu,Youngjoong Ko,Jungyun Seo","id":"5421a2058de7d8d39a2d060a53265e600d8f053a","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/26325fbf09743fcd17ff9cbcd2bca22b05b1d4f1","title":"WikiDes: A Wikipedia-Based Dataset for Generating Short Descriptions from Paragraphs","venue":"Information Fusion","year":2022,"referenceCount":111,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/09/2022","authors":"Hoang Thang Ta,Abu Bakar Siddiqur Rahman,Navonil Majumder,A. Hussain,Lotfollah Najjar,N. Howard,Soujanya Poria,Alexander Gelbukh","id":"26325fbf09743fcd17ff9cbcd2bca22b05b1d4f1","summary":"WikiDes, a novel dataset to generate short descriptions of Wikipedia articles for the problem of text summarization, is introduced and a practical impact on Wikipedia and Wikidata is shown since there are thousands of missing descriptions.","score":4},{"url":"https://www.semanticscholar.org/paper/641b906b4069661122af7a26ff27921982211b31","title":"CORGI-PM: A Chinese Corpus For Gender Bias Probing and Mitigation","venue":"ArXiv","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Ge Zhang,Yizhi Li,Yao-Hwa Wu,Linyuan Zhang,Chenghua Lin,Jiayi Geng,Shizhuo Wang,Jie Fu","id":"641b906b4069661122af7a26ff27921982211b31","summary":"This work proposes a Chinese corpus, CORGI-PM, which contains 32.9k sentences with high-quality labels derived by following an annotation scheme speciﬁcally developed for gender bias in the Chinese context, and addresses three challenges for automatic textual gender bias mitigation.","score":4},{"url":"https://www.semanticscholar.org/paper/220ddeb4dc43bc922289fec8b1b60d7226068b20","title":"Parameter-Efficient Fine-Tuning Design Spaces","venue":"ArXiv","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/01/2023","authors":"Jiaao Chen,Aston Zhang,Xingjian Shi,Mu Li,Alexander J. Smola,Diyi Yang","id":"220ddeb4dc43bc922289fec8b1b60d7226068b20","summary":"This work presents a parameter-efficient fine-tuning design paradigm and discovers design patterns that are applicable to different experimental settings and shows experimentally that these methods consistently and significantly outperform investigated parameter- efficient fine- Tuning strategies across different backbone models and different tasks in natural language processing.","score":4},{"url":"https://www.semanticscholar.org/paper/4c8461d82880037ed81dd1340c4e8e6d7456964b","title":"GenPADS: Reinforcing politeness in an end-to-end dialogue system","venue":"PLoS ONE","year":2023,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/01/2023","authors":"Kshitij Mishra,Mauajama Firdaus,Asif Ekbal","id":"4c8461d82880037ed81dd1340c4e8e6d7456964b","summary":"A novel end-to-end dialogue system that combines a politeness classifier to extract polite information present in user’s and agent's utterances and a generation model to generate varying but semantically correct responses, which performs better than two considered baselines.","score":4},{"url":"https://www.semanticscholar.org/paper/8da051282822dd6c54234099d4ecade697ba7166","title":"ReqGen: Keywords-Driven Software Requirements Generation","venue":"Mathematics","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/01/2023","authors":"Z. Zhao,Li Zhang,Xiaoli Lian,Xiao-qian Gao,Heyang Lv,Lina Shi","id":"8da051282822dd6c54234099d4ecade697ba7166","summary":"Experiments show that ReqGen outperforms six popular natural language generation approaches with respect to the hard constraint of keywords’ (phrases’) inclusion, BLEU, ROUGE, and syntax compliance.","score":4},{"url":"https://www.semanticscholar.org/paper/590b7a6da46d28fdd2b268d6cefcb09e7a70de5d","title":"TikTalk: A Multi-Modal Dialogue Dataset for Real-World Chitchat","venue":"ArXiv","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/01/2023","authors":"Hongpeng Lin,Ludan Ruan,Wenke Xia,Peiyu Liu,Jing Wen,Yixin Xu,Di Hu,Ruihua Song,Wayne Xin Zhao,Qin Jin,Zhiwu Lu","id":"590b7a6da46d28fdd2b268d6cefcb09e7a70de5d","summary":"Experimental results demonstrate that integrat-ing vision and knowledge in response generation performs the best, although there is still a large room for future improvement.","score":4},{"url":"https://www.semanticscholar.org/paper/8ea84f2f7ad6edc6efc4dc256d6958f4920dc1c8","title":"Bottom-up and Top-down Object Inference Networks for Image Captioning","venue":"ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)","year":2023,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/01/2023","authors":"Yingwei Pan,Yehao Li,Ting Yao,Tao Mei","id":"8ea84f2f7ad6edc6efc4dc256d6958f4920dc1c8","summary":"This work presents Bottom-up and Top-down Object inference Networks (BTO-Net), that novelly exploits the object sequence of interest as top-down signals to guide image captioning and obtains competitive performances on COCO benchmark.","score":4},{"url":"https://www.semanticscholar.org/paper/b3628b58bbed1d427182bf766cbf454b7c10a43a","title":"Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences","venue":"ArXiv","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/01/2023","authors":"Xudong Hong,A. Sayeed,K. Mehra,V. Demberg,B. Schiele","id":"b3628b58bbed1d427182bf766cbf454b7c10a43a","summary":"A character-based story generation model driven by coherence as a strong baseline is proposed and generated stories are more coherent, visually grounded, and more diverse than stories generated with the current state-of-the-art model.","score":4},{"url":"https://www.semanticscholar.org/paper/fab84ef4a2cee4d496d761a677efcbd290dab4ac","title":"Byte Pair Encoding for Symbolic Music","venue":"ArXiv","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Nathan Fradet,Jean-Pierre Briot,F. Chhel,A. E. Seghrouchni,Nicolas Gutowski","id":"fab84ef4a2cee4d496d761a677efcbd290dab4ac","summary":"This paper shows how Byte Pair Encoding (BPE) can improve the results of deep learning models while improving its performances, and studies the impact of BPE on how models learn the embeddings, and shows that it can help to increase their isotropy, i.e., the uniformity of the variance of their positions in the space.","score":4},{"url":"https://www.semanticscholar.org/paper/754e0d146b7969d06f3feded6e14d92b44c15337","title":"Can an AI Win Ghana's National Science and Maths Quiz? An AI Grand Challenge for Education","venue":"ArXiv","year":2023,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"George Boateng,V. Kumbol,E. E. Kaufmann","id":"754e0d146b7969d06f3feded6e14d92b44c15337","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/ef68ac57708ab2745ea71e066868e0d4f1da8d6c","title":"Zero-shot Clarifying Question Generation for Conversational Search","venue":"ArXiv","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Zhenduo Wang,Yuancheng Tu,Corby Rosset,Nick Craswell,Ming Wu,Qingyao Ai","id":"ef68ac57708ab2745ea71e066868e0d4f1da8d6c","summary":"This work innovatively explore generating clarifying questions in a zero-shot setting to overcome the cold start problem and proposes a constrained clarifying question generation system which uses both question templates and query facets to guide the effective and precise question generation.","score":4},{"url":"https://www.semanticscholar.org/paper/595c5b22354bcab17f0534a41b9c19bbc3c6b9ad","title":"Generative Adversarial Neural Machine Translation for Phonetic Languages via Reinforcement Learning","venue":"IEEE Transactions on Emerging Topics in Computational Intelligence","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Amit Kumar,A. Pratap,Anil Kumar Singh","id":"595c5b22354bcab17f0534a41b9c19bbc3c6b9ad","summary":"This work proposes a novel improvement in Generative Adversarial Networks (GAN)-NMT by incorporating deep reinforcement learning-based optimised attention in generator and convolutional neural network in discriminator and creates the novel joint embedding of subwords and sub-phonetic representation of sentences as input to GAN.","score":4},{"url":"https://www.semanticscholar.org/paper/eb53979c96eacec543620af2b899a3772eb6d32f","title":"Commonsense-Aware Prompting for Controllable Empathetic Dialogue Generation","venue":"ArXiv","year":2023,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Yiren Liu,H. Kilicoglu","id":"eb53979c96eacec543620af2b899a3772eb6d32f","summary":"A novel framework that improves empathetic dialogue generation using pre-trained language models by incorporating commonsense knowledge through prompt verbalization, and controlling dialoguegeneration using a strategy-driven future discriminator is proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/257d7c8ab9d18d3cb93b9ec6d87aa66b28e610f4","title":"DEVICE: DEpth and VIsual ConcEpts Aware Transformer for TextCaps","venue":"ArXiv","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"Dongsheng Xu,Qingbao Huang,Yiru Cai","id":"257d7c8ab9d18d3cb93b9ec6d87aa66b28e610f4","summary":"To construct three-dimensional geometric relations, depth information is introduced and a depth-enhanced feature updating module is proposed to ameliorate OCR token features and generate more precise and comprehensive captions.","score":4},{"url":"https://www.semanticscholar.org/paper/8856fef5cefdd97f2835b960ef657b4a757fe191","title":"Transform, Contrast and Tell: Coherent Entity-Aware Multi-Image Captioning","venue":"ArXiv","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/02/2023","authors":"Jingqiang Chen","id":"8856fef5cefdd97f2835b960ef657b4a757fe191","summary":"Experiments on three datasets show the proposed captioning model outperforms 6 baselines according to single-image captioning evaluations, and the generated captions are more coherent than that of baseline according to coherence evaluations and human evaluations.","score":4},{"url":"https://www.semanticscholar.org/paper/7fdfe07e7e6d6dd38efdab12a3e81d47325ad575","title":"Learning Translation Quality Evaluation on Low Resource Languages from Large Language Models","venue":"ArXiv","year":2023,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Amirkeivan Mohtashami,M. Verzetti,Paul K. Rubenstein","id":"7fdfe07e7e6d6dd38efdab12a3e81d47325ad575","summary":"It is shown how knowledge can be distilled from Large Language Models (LLMs) to improve upon learned metrics without requiring human annotators, by creating synthetic datasets which can be mixed into existing datasets, requiring only a corpus of text in the target language.","score":4},{"url":"https://www.semanticscholar.org/paper/1d74875aa4f415cb2c60b17fd1eb3e4ae543bfe1","title":"Keeping Pace with Ever-Increasing Data: Towards Continual Learning of Code Intelligence Models","venue":"ArXiv","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Shuzheng Gao,Hongyu Zhang,Cuiyun Gao,Chaozheng Wang","id":"1d74875aa4f415cb2c60b17fd1eb3e4ae543bfe1","summary":"This paper proposes REPEAT, a novel method for continual learning of code intelligence models that addresses the catastrophic forgetting problem with representative exemplars replay and adaptive parameter regularization.","score":4},{"url":"https://www.semanticscholar.org/paper/44f0eb7be111747f54a8669f74be0bb0dad96cff","title":"An Empirical Comparison of Pre-Trained Models of Source Code","venue":"ArXiv","year":2023,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Changan Niu,Chuanyi Li,Vincent Ng,Dongxiao Chen,Jidong Ge,B. Luo","id":"44f0eb7be111747f54a8669f74be0bb0dad96cff","summary":"The first systematic empirical comparison of 19 recently- developed pre-trained models of source code on 13 SE tasks is performed, and a recently-developed 4-dimensional categorization of pre- trained models is adopted to gain additional insights into these models.","score":4},{"url":"https://www.semanticscholar.org/paper/5d896fb2f0da16060f22ed43e582464605237f28","title":"Training-free Lexical Backdoor Attacks on Language Models","venue":"ArXiv","year":2023,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Yujin Huang,Terry Yue Zhuo,Qiongkai Xu,Han Hu,Xingliang Yuan,Chunyang Chen","id":"5d896fb2f0da16060f22ed43e582464605237f28","summary":"This work proposes Training-Free Lexical Backdoor Attack (TFLexAttack) as the first training-free backdoor attack on language models by injecting lexical triggers into the tokenizer of a language model via manipulating its embedding dictionary using carefully designed rules.","score":4},{"url":"https://www.semanticscholar.org/paper/75e44b9e4c084085008edb59ae7c3cad23f5fbe5","title":"Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization","venue":"ArXiv","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/02/2023","authors":"Zhixin Guo,Minyxuan Yan,Jiexing Qi,Jianping Zhou,Ziwei He,Zhouhan Lin,Guanjie Zheng,Xinbing Wang","id":"75e44b9e4c084085008edb59ae7c3cad23f5fbe5","summary":"Inspired by how humans descript tabular data with prior knowledge, a new framework is suggested: PromptMize, which targets table-to-text generation under few-shot settings and achieves remarkable performance in generating quality as judged by human and automatic evaluations.","score":4},{"url":"https://www.semanticscholar.org/paper/535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code","venue":"ArXiv","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"Shuyan Zhou,Uri Alon,Sumit Agarwal,Graham Neubig","id":"535a3d95a743e1f4b591b5b2af3e778a6347158a","summary":"This paper proposes CodeBERTScore, an automatic evaluation metric for code generation, which builds on BERTScore and achieves a higher correlation with human preference and with functional correctness than all existing metrics.","score":4},{"url":"https://www.semanticscholar.org/paper/3561b5b7a1982740c7127b8d2ab14d7e07c9c0cd","title":"Enhancing User Personalization in Conversational Recommenders","venue":"","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Allen Lin,Ziwei Zhu,Jianling Wang,James Caverlee","id":"3561b5b7a1982740c7127b8d2ab14d7e07c9c0cd","summary":"This paper proposes a novel conversational recommendation framework with two unique features: a greedy NDCG attribute selector, to enhance user personalization in the interactive preference elicitation process by prioritizing attributes that most effectively represent the actual preference space of the user.","score":4},{"url":"https://www.semanticscholar.org/paper/229578bcf5cbc33e502e41d3001aedfc548ace74","title":"EmoKbGAN: Emotion controlled response generation using Generative Adversarial Network for knowledge grounded conversation","venue":"PLoS ONE","year":2023,"referenceCount":76,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Deeksha Varshney,Asif Ekbal,Mrigank Tiwari,Ganesh Nagaraja","id":"229578bcf5cbc33e502e41d3001aedfc548ace74","summary":"This paper presents a method named EmoKbGAN for automatic response generation that makes use of the Generative Adversarial Network (GAN) in multiple-discriminator settings involving joint minimization of the losses provided by each attribute specific discriminator model (knowledge and emotion discriminator).","score":4},{"url":"https://www.semanticscholar.org/paper/0a9364ca06771d2b85da147a453bca0d02f8e248","title":"Multimodal Machines from a perspective of humans Ph.D Thesis Proposal","venue":"","year":2022,"referenceCount":152,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sunit Bhattacharya","id":"0a9364ca06771d2b85da147a453bca0d02f8e248","summary":"This thesis proposal focuses on highlighting the differences in the learning mechanisms of humans and deep learning systems and explores yet how recent work has established similarities between representations learnt by deeplearning systems and cognitive data collected from the human brain.","score":4},{"url":"https://www.semanticscholar.org/paper/5840bf765be8c3bcedab63f43f5982ddba26eaf9","title":"SPRINT: S CALABLE S EMANTIC P OLICY P RE T RAINING VIA L ANGUAGE I NSTRUCTION R ELABELING","venue":"","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"5840bf765be8c3bcedab63f43f5982ddba26eaf9","summary":"Experiments using a realistic household simulator show that agents pre-trained with SPRINT learn new long-horizon household tasks substantially faster than with previous pre-training approaches.","score":4},{"url":"https://www.semanticscholar.org/paper/c2536182c010c41941e8a031071a1880c34cec60","title":"Unified Scaling Laws for Routed Language Models","venue":"International Conference on Machine Learning","year":2022,"referenceCount":68,"citationCount":30,"influentialCitationCount":5,"publicationDate":"02/02/2022","authors":"Aidan Clark,Diego de Las Casas,Aurelia Guy,A. Mensch,Michela Paganini,Jordan Hoffmann,Bogdan Damoc,Blake A. Hechtman,Trevor Cai,Sebastian Borgeaud,George van den Driessche,Eliza Rutherford,T. Hennigan,Matthew G. Johnson,Katie Millican,Albin Cassirer,Chris Jones,Elena Buchatskaya,D. Budden,L. Sifre,Simon Osindero,Oriol Vinyals,Jack W. Rae,Erich Elsen,K. Kavukcuoglu,K. Simonyan","id":"c2536182c010c41941e8a031071a1880c34cec60","summary":"This work derives and justifies scaling laws defined on parameter count and computational requirement which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques.","score":4},{"url":"https://www.semanticscholar.org/paper/aa4d9972af3264d032dbee58501ed4ac49477103","title":"Scaling Laws and Interpretability of Learning from Repeated Data","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":6,"influentialCitationCount":0,"publicationDate":"21/05/2022","authors":"Danny Hernandez,Tom B. Brown,Tom Conerly,Nova DasSarma,Dawn Drain,S. El-Showk,Nelson Elhage,Zac Hatfield-Dodds,T. Henighan,Tristan Hume,Scott Johnston,Benjamin Mann,C. Olah,Catherine Olsson,Dario Amodei,Nicholas Joseph,Jared Kaplan,Sam McCandlish","id":"aa4d9972af3264d032dbee58501ed4ac49477103","summary":"It is shown that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization.","score":4},{"url":"https://www.semanticscholar.org/paper/18b0b58449120abc819aaaa175cb8af92009ff18","title":"Know your audience: specializing grounded language models with the game of Dixit","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/06/2022","authors":"Aaditya K Singh,David Ding,Andrew M. Saxe,Felix Hill,Andrew Kyle Lampinen","id":"18b0b58449120abc819aaaa175cb8af92009ff18","summary":"This work forms a round of Dixit as a multi-agent image reference game where a (trained) speaker model is rewarded for describing a target image such that one (pretrained) listener model can correctly identify it from a pool of distractors, but another listener cannot.","score":4},{"url":"https://www.semanticscholar.org/paper/eaef083b9d661f42cc0d89d9d8156218f33a91d9","title":"Long Range Language Modeling via Gated State Spaces","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":7,"influentialCitationCount":2,"publicationDate":"27/06/2022","authors":"Harsh Mehta,Ankit Gupta,Ashok Cutkosky,Behnam Neyshabur","id":"eaef083b9d661f42cc0d89d9d8156218f33a91d9","summary":"This work proposes a new layer named Gated State Space (GSS) and shows that it trains signiﬁcantly faster than the diagonal version of S4 on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement.","score":4},{"url":"https://www.semanticscholar.org/paper/63c670ba8018da0a7e33c34ffd84c2a3ca54b894","title":"Language Models Can Teach Themselves to Program Better","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":3,"influentialCitationCount":1,"publicationDate":"29/07/2022","authors":"Patrick M. Haluptzok,Matthew Bowers,A. Kalai","id":"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","summary":"This work shows how generating synthetic programming puzzles and solutions, veriﬁed for correctness by a Python interpreter, can be used to improve performance in solving test puzzles from P3, a public benchmark set of Python Programming Puzzles.","score":4},{"url":"https://www.semanticscholar.org/paper/e221c769957a8eef2836a64cede88401129d86e9","title":"Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP","venue":"ArXiv","year":2022,"referenceCount":87,"citationCount":9,"influentialCitationCount":1,"publicationDate":"10/08/2022","authors":"T. Nguyen,Gabriel Ilharco,Mitchell Wortsman,Sewoong Oh,Ludwig Schmidt","id":"e221c769957a8eef2836a64cede88401129d86e9","summary":"It is demonstrated that simply gathering a large amount of data from the web is not the most effective way to build a pre-training dataset for robust generalization, necessitating further study into dataset design.","score":4},{"url":"https://www.semanticscholar.org/paper/8bd182f01c99e643c9a5a96832dc1a16b9cd10d0","title":"Domain-Specific Text Generation for Machine Translation","venue":"Conference of the Association for Machine Translation in the Americas","year":2022,"referenceCount":73,"citationCount":2,"influentialCitationCount":0,"publicationDate":"11/08/2022","authors":"Yasmin Moslem,Rejwanul Haque,John D. Kelleher,Andy Way","id":"8bd182f01c99e643c9a5a96832dc1a16b9cd10d0","summary":"This work proposes leveraging state-of-the-art pretrained language models (LMs) for domain-specific data augmentation for MT, simulating the domain characteristics of either a small bilingual dataset, or the monolingual source text to be translated, to generate huge amounts of synthetic bilingual in-domain data.","score":4},{"url":"https://www.semanticscholar.org/paper/d17bddf9dc329bb9ff7883642699b84055db06fc","title":"PLATO-K: Internal and External Knowledge Enhanced Dialogue Generation","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/11/2022","authors":"Siqi Bao,H. He,Jun Xu,Hua Lu,Fan Wang,Hua Wu,Han Zhou,Wenquan Wu,Zheng-Yu Niu,Haifeng Wang","id":"d17bddf9dc329bb9ff7883642699b84055db06fc","summary":"Compared to the existing state-of-the-art Chinese dialogue model, the overall engagingness of PLATO-K is improved remarkably by 36.2% and 49.2%, and the knowledge issue is alleviated significantly in PLATo-K with such comprehensive internal and external knowledge enhancement.","score":4},{"url":"https://www.semanticscholar.org/paper/ef58e99dbb90bebbdc6187b5cba94dd5973dbb16","title":"Retrieval-Augmented Multimodal Language Modeling","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":3,"influentialCitationCount":1,"publicationDate":2022,"authors":"Michihiro Yasunaga,Armen Aghajanyan,Weijia Shi,Rich James,J. Leskovec,Percy Liang,M. Lewis,Luke Zettlemoyer,Wen-tau Yih","id":"ef58e99dbb90bebbdc6187b5cba94dd5973dbb16","summary":"Retrieval-Augmented CM3, the first retrieval-augmented multimodal model that can retrieve and generate mixtures of text and images, is proposed and shows that RA-CM3 exhibits novel capabilities such as knowledge-intensive image generation and multimodals in-context.","score":4},{"url":"https://www.semanticscholar.org/paper/22775e58932cdfbd273a2a835a22c5d86800a458","title":"Continuous diffusion for categorical data","venue":"ArXiv","year":2022,"referenceCount":99,"citationCount":6,"influentialCitationCount":4,"publicationDate":"28/11/2022","authors":"S. Dieleman,Laurent Sartran,Arman Roshannai,Nikolay Savinov,Yaroslav Ganin,Pierre H. Richemond,A. Doucet,Robin Strudel,Chris Dyer,Conor Durkan,Curtis Hawthorne,Rémi Leblond,Will Grathwohl,J. Adler","id":"22775e58932cdfbd273a2a835a22c5d86800a458","summary":"CD is proposed, a framework for modelling categorical data with diﬀusion models that are continuous both in time and input space and demonstrates its eﬃcacy on several language modelling tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/7c6fcb3f1577b2385342d054b94df7924a5cdd13","title":"ClueWeb22: 10 Billion Web Documents with Visual and Semantic Information","venue":"","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"Arnold Overwijk,Chenyan Xiong,X. Liu,Cameron VandenBerg,Jamie Callan","id":"7c6fcb3f1577b2385342d054b94df7924a5cdd13","summary":"The newest iteration of the ClueWeb22 corpus is larger, more varied, of higher-quality, and aligned with the document distributions in commercial web search.","score":4},{"url":"https://www.semanticscholar.org/paper/7928253927fc11ba7da64b428b6b97dce096672c","title":"DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing","venue":"ArXiv","year":2022,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/12/2022","authors":"Conglong Li,Z. Yao,Xiaoxia Wu,Minjia Zhang,Yuxiong He","id":"7928253927fc11ba7da64b428b6b97dce096672c","summary":"DeepSpeed Data Efficiency is presented, a framework that makes better use of data, increases training efficiency, and improves model quality, and takes extensibility, flexibility and composability into consideration, so that users can easily utilize the framework to compose multiple techniques and apply customized strategies.","score":4},{"url":"https://www.semanticscholar.org/paper/94c69cf028c27ee360c7cb8b0bdba6027ee52dbb","title":"Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":2,"influentialCitationCount":1,"publicationDate":"14/12/2022","authors":"Alexei Baevski,Arun Babu,Wei-Ning Hsu,Michael Auli","id":"94c69cf028c27ee360c7cb8b0bdba6027ee52dbb","summary":"Data2vec 2.0 beneﬁts from the rich contextualized target representations introduced in data2vec which enable a fast self-supervised learner.","score":4},{"url":"https://www.semanticscholar.org/paper/ab972a92dd5ac31f8b8b026a64707bfeb3149397","title":"Do DALL-E and Flamingo Understand Each Other?","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/12/2022","authors":"Hang Li,Jindong Gu,Rajat Koner,Sahand Sharifzadeh,Volker Tresp","id":"ab972a92dd5ac31f8b8b026a64707bfeb3149397","summary":"It is argued that the best text or caption for a given image is the text which would generate the image which is the most similar to that image, which results in the caption which is best aligned with the original text.","score":4},{"url":"https://www.semanticscholar.org/paper/b118283afc5d8652de52cd13a5e287d76c5ec91f","title":"Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text","venue":"ArXiv","year":2022,"referenceCount":24,"citationCount":2,"influentialCitationCount":0,"publicationDate":"24/12/2022","authors":"Liam Dugan,Daphne Ippolito,Arun Kirubarajan,Sherry Shi,Chris Callison-Burch","id":"b118283afc5d8652de52cd13a5e287d76c5ec91f","summary":"There is substantial variance in annotator skill and that given proper incentives, annotators can improve at this task over time, and the RoFT dataset is released: a collection of over 21,000 human annotations paired with error classiﬁcations to encourage future work in human detection and evaluation of generated text.","score":4},{"url":"https://www.semanticscholar.org/paper/4b308ba40e67b0b4b25c6fde17195d5a456a2f41","title":"Cramming: Training a Language Model on a Single GPU in One Day","venue":"ArXiv","year":2022,"referenceCount":125,"citationCount":3,"influentialCitationCount":1,"publicationDate":"28/12/2022","authors":"Jonas Geiping,T. Goldstein","id":"4b308ba40e67b0b4b25c6fde17195d5a456a2f41","summary":"This work investigates the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU and investigates why scaling down is hard, and which modifications actually improve performance in this scenario.","score":4},{"url":"https://www.semanticscholar.org/paper/b369b05f5386ce22dc7fa33c6f912d5c1cd27f14","title":"The Power of External Memory in Increasing Predictive Model Capacity","venue":"ArXiv","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Cenk Baykal,D. Cutler,Nishanth Dikkala,Nikhil Ghosh,R. Panigrahy,Xin Wang","id":"b369b05f5386ce22dc7fa33c6f912d5c1cd27f14","summary":"This work introduces a new method, alternating updates, that enables access to an increased token dimension without increasing the computation time, and demonstrates its effectiveness in language modeling.","score":4},{"url":"https://www.semanticscholar.org/paper/d5bf40a7aff69dcdf9317c8374012c08fa66f87b","title":"Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning","venue":"","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"A. Luccioni,Alex Hernández-García","id":"d5bf40a7aff69dcdf9317c8374012c08fa66f87b","summary":"A survey of the carbon emissions of 95 ML models across time and different tasks in natural language processing and computer vision is presented, in terms of the energy sources used, the amount of CO2 emissions produced, how these emissions evolve acrosstime and how they relate to model performance.","score":4},{"url":"https://www.semanticscholar.org/paper/71e7624755c6a6c024d733220918aa2b1a04f4ed","title":"Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":2,"influentialCitationCount":0,"publicationDate":"16/09/2022","authors":"Ben Prystawski,P. Thibodeau,Noah D. Goodman","id":"71e7624755c6a6c024d733220918aa2b1a04f4ed","summary":"Chain-of-thought prompts are used to intro-duce structures from probabilistic models into large language models and show that they can improve paraphrase selection.","score":4},{"url":"https://www.semanticscholar.org/paper/8b97b25a0825b84feebe1b34968a8144bf5acdf0","title":"Knowledge-enhanced Neural Machine Reasoning: A Review","venue":"ArXiv","year":2023,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/02/2023","authors":"Tanmoy Chowdhury,Chen Ling,Xuchao Zhang,Xujiang Zhao,Guang-ying Bai,Jian Pei,Haifeng Chen,Liang Zhao","id":"8b97b25a0825b84feebe1b34968a8144bf5acdf0","summary":"This survey provides an in-depth examination of recent advancements in the field, introducing a novel taxonomy that categorizes existing knowledge-enhanced methods into two primary categories and four subcategories and elucidate the current application domains.","score":4},{"url":"https://www.semanticscholar.org/paper/9d81bc8bebf1beb936427c224afb219b54a64f1e","title":"Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":65,"citationCount":71,"influentialCitationCount":13,"publicationDate":"16/04/2021","authors":"Ari Holtzman,Peter West,Vered Schwartz,Yejin Choi,Luke Zettlemoyer","id":"9d81bc8bebf1beb936427c224afb219b54a64f1e","summary":"Domain Conditional Pointwise Mutual Information is introduced, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task.","score":4},{"url":"https://www.semanticscholar.org/paper/91806ece16c0e5cfde5a653277a219ddd9bfe5a7","title":"CheckDST: Measuring Real-World Generalization of Dialogue State Tracking Performance","venue":"ArXiv","year":2021,"referenceCount":45,"citationCount":4,"influentialCitationCount":1,"publicationDate":2021,"authors":"Hyundong Justin Cho,Chinnadhurai Sankar,Christopher Lin,Kaushik Ram Sadagopan,Shahin Shayandeh,Asli Celikyilmaz,Jonathan May,A. Beirami","id":"91806ece16c0e5cfde5a653277a219ddd9bfe5a7","summary":"This work introduces CheckDST, an instantiation of CheckList for DST that quantifies robustness with test set augmenta- 013 tions and new metrics that measure consistency, and compares state-of-the-art DST models.","score":4},{"url":"https://www.semanticscholar.org/paper/c586320be1b7178893e7fe4d87e4527b73f0f250","title":"Know Thy Strengths: Comprehensive Dialogue State Tracking Diagnostics","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":52,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/12/2021","authors":"Hyundong Justin Cho,Chinnadhurai Sankar,Christopher Lin,Kaushik Ram Sadagopan,Shahin Shayandeh,Asli Celikyilmaz,Jonathan May,A. Beirami","id":"c586320be1b7178893e7fe4d87e4527b73f0f250","summary":"It is discovered that different classes of DST models have clear strengths and weaknesses, where generation models are more promising for handling language variety while span-based classification models are less robust to unseen entities and each model class has distinct patterns of failure.","score":4},{"url":"https://www.semanticscholar.org/paper/4724ebee34ca2cd0a19c3a1ddb83d6d870dd7904","title":"Few-shot Learning with Multilingual Language Models","venue":"ArXiv","year":2021,"referenceCount":88,"citationCount":50,"influentialCitationCount":3,"publicationDate":"20/12/2021","authors":"Xi Victoria Lin,Todor Mihaylov,Mikel Artetxe,Tianlu Wang,Shuohui Chen,Daniel Simig,Myle Ott,Naman Goyal,Shruti Bhosale,Jingfei Du,Ramakanth Pasunuru,Sam Shleifer,Punit Singh Koura,Vishrav Chaudhary,Brian O'Horo,Jeff Wang,Luke Zettlemoyer,Zornitsa Kozareva,Mona Diab,V. Stoyanov,Xian Li","id":"4724ebee34ca2cd0a19c3a1ddb83d6d870dd7904","summary":"This work trains multilingual generative language models on a corpus covering a diverse set of languages, and conducts an in-depth analysis of different multilingual prompting approaches, showing in particular that strong in-context few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples.","score":4},{"url":"https://www.semanticscholar.org/paper/265d3e6c09816f744500924931aa977f0a8ba288","title":"WikiOmnia: filtration and evaluation of the generated QA corpus on the whole Russian Wikipedia","venue":"IEEE Games Entertainment Media Conference","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"D. Pisarevskaya,Tatiana Shavrina","id":"265d3e6c09816f744500924931aa977f0a8ba288","summary":"The WikiOmnia dataset is presented, a new publicly available set of QA pairs and corresponding Russian Wikipedia article summary sections, composed with a fully automated generation and filtration pipeline.","score":4},{"url":"https://www.semanticscholar.org/paper/cbdb45fc16b0885905b91d84281c310e6cb49e9c","title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":54,"citationCount":116,"influentialCitationCount":15,"publicationDate":"18/04/2021","authors":"Swaroop Mishra,Daniel Khashabi,Chitta Baral,Hannaneh Hajishirzi","id":"cbdb45fc16b0885905b91d84281c310e6cb49e9c","summary":"This work introduces NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances, and adopts generative pre-trained language models to encode task-specific instructions along with input and generate task output.","score":4},{"url":"https://www.semanticscholar.org/paper/6bd91a3183ddb844641acb9f3fe9faec6a9ff617","title":"Meta-learning via Language Model In-context Tuning","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":60,"citationCount":30,"influentialCitationCount":4,"publicationDate":"15/10/2021","authors":"Yanda Chen,Ruiqi Zhong,Sheng Zha,G. Karypis,He He","id":"6bd91a3183ddb844641acb9f3fe9faec6a9ff617","summary":"ICT leverages the inductive bias of pre-trained LMs to perform pattern matching, and outperforms MAML by an absolute 6% average AUC-ROC score on BinaryClfs, gaining more advantage with increasing model size.","score":4},{"url":"https://www.semanticscholar.org/paper/a0b777b25cdf0fc992568ca52a5c7bebf1ee987f","title":"Deep Transfer Learning & Beyond: Transformer Language Models in Information Systems Research","venue":"ACM Computing Surveys","year":2021,"referenceCount":213,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/10/2021","authors":"Ross Gruetzemacher,D. Paradice","id":"a0b777b25cdf0fc992568ca52a5c7bebf1ee987f","summary":"A review of existing IS literature reveals that suboptimal text mining techniques are prevalent and that the more advanced TLMs could be applied to enhance and increase IS research involving text data, and to enable new IS research topics, thus creating more value for the research community.","score":4},{"url":"https://www.semanticscholar.org/paper/7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model","venue":"ArXiv","year":2022,"referenceCount":69,"citationCount":194,"influentialCitationCount":15,"publicationDate":"28/01/2022","authors":"Shaden Smith,M. Patwary,Brandon Norick,P. LeGresley,Samyam Rajbhandari,J. Casper,Zhun Liu,Shrimai Prabhumoye,George Zerveas,V. Korthikanti,Elton Zhang,Rewon Child,Reza Yazdani Aminabadi,J. Bernauer,Xia Song,M. Shoeybi,Yuxiong He,Michael Houston,Saurabh Tiwary,Bryan Catanzaro","id":"7cbc2a7843411a1768ab762930707af0a3c33a19","summary":"The infrastructure as well as the 3D parallelism methodology used to train the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters is presented.","score":4},{"url":"https://www.semanticscholar.org/paper/1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","title":"Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework","venue":"International Conference on Machine Learning","year":2022,"referenceCount":106,"citationCount":148,"influentialCitationCount":41,"publicationDate":"07/02/2022","authors":"Peng Wang,An Yang,Rui Men,Junyang Lin,Shuai Bai,Zhikang Li,Jianxin Ma,Chang Zhou,Jingren Zhou,Hongxia Yang","id":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","summary":"Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni- modal tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/18e27c61ccc227fefc8a82b5732f08b6b41e0c84","title":"Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)","venue":"ACM Conference on Recommender Systems","year":2022,"referenceCount":85,"citationCount":20,"influentialCitationCount":1,"publicationDate":"24/03/2022","authors":"Shijie Geng,Shuchang Liu,Zuohui Fu,Yingqiang Ge,Yongfeng Zhang","id":"18e27c61ccc227fefc8a82b5732f08b6b41e0c84","summary":"A flexible and unified text-to-text paradigm called “Pretrain, Personalized Prompt, and Predict Paradigm” (P5) for recommendation, which unifies various recommendation tasks in a shared framework and will revolutionize the technical form of recommender systems towards universal recommendation engine.","score":4},{"url":"https://www.semanticscholar.org/paper/b399dcb2ce41a8389b7d1ad8689ee7641c4d12f3","title":"WikiOmnia: generative QA corpus on the whole Russian Wikipedia","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/04/2022","authors":"D. Pisarevskaya,Tatiana Shavrina","id":"b399dcb2ce41a8389b7d1ad8689ee7641c4d12f3","summary":"The WikiOmnia dataset is presented, a new publicly available set of QA-pairs and corresponding Russian Wikipedia article summary sections, composed with a fully automated generative pipeline, which includes every available article from Wikipedia for the Russian language.","score":4},{"url":"https://www.semanticscholar.org/paper/cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":30,"citationCount":4,"influentialCitationCount":1,"publicationDate":"19/04/2022","authors":"Matthew Finlayson,Kyle Richardson,Ashish Sabharwal,Peter Clark","id":"cb16b85891172572cd856142880b503db0c2bc61","summary":"This work uses the task of deciding whether a given string matches a regular expression to identify properties of tasks, instructions, and instances that make instruction learning challenging, and proposes Hard RegSet as a challenging instruction learning dataset and a controlled environment for studying instruction learning.","score":4},{"url":"https://www.semanticscholar.org/paper/c211f7516abf28bb751dce63043d09edec14b4b4","title":"TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":46,"citationCount":13,"influentialCitationCount":1,"publicationDate":"29/04/2022","authors":"Joel Jang,Seonghyeon Ye,C. Lee,Sohee Yang,Joongbo Shin,Janghoon Han,Gyeonghun Kim,Minjoon Seo","id":"c211f7516abf28bb751dce63043d09edec14b4b4","summary":"TemporalWiki is introduced, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.","score":4},{"url":"https://www.semanticscholar.org/paper/07c70ca55793984ffdf31582a05170ef3d62381a","title":"Prompt Consistency for Zero-Shot Task Generalization","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":54,"citationCount":13,"influentialCitationCount":1,"publicationDate":"29/04/2022","authors":"Chunting Zhou,Junxian He,Xuezhe Ma,Taylor Berg-Kirkpatrick,Graham Neubig","id":"07c70ca55793984ffdf31582a05170ef3d62381a","summary":"This work takes advantage of the fact that multiple prompts can be used to specify a single task, and proposes to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts, to improve zero-shot performance.","score":4},{"url":"https://www.semanticscholar.org/paper/1bcde55995a957b3e8a595d536b816cb8989cf1d","title":"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":4,"influentialCitationCount":0,"publicationDate":"01/05/2022","authors":"E. Karpas,Omri Abend,Yonatan Belinkov,Barak Lenz,Opher Lieber,Nir Ratner,Y. Shoham,Hofit Bata,Yoav Levine,K. Leyton-Brown,Dor Muhlgay,N. Rozen,Erez Schwartz,Gal Shachaf,S. Shalev-Shwartz,A. Shashua,Moshe Tenenholtz","id":"1bcde55995a957b3e8a595d536b816cb8989cf1d","summary":"This work describes a neuro-symbolic architecture with multiple neural models, complemented by discrete knowledge and reasoning modules, dubbed the Modular Reasoning, Knowledge and Language (MRKL), and some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs’ MRKL system implementation.","score":4},{"url":"https://www.semanticscholar.org/paper/2f291b0b59483e9c3c4a3391f34e6b29aff848a1","title":"DeepStruct: Pretraining of Language Models for Structure Prediction","venue":"Findings","year":2022,"referenceCount":97,"citationCount":6,"influentialCitationCount":3,"publicationDate":"21/05/2022","authors":"Chenguang Wang,Xiao Liu,Zui Chen,Haoyun Hong,Jie Tang,Dawn Song","id":"2f291b0b59483e9c3c4a3391f34e6b29aff848a1","summary":"It is shown that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that are evaluated.","score":4},{"url":"https://www.semanticscholar.org/paper/e6296cf7c2c7b4578f1ae644edae4ceee5a5faea","title":"On Measuring Social Biases in Prompt-Based Multi-Task Learning","venue":"NAACL-HLT","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Afra Feyza Akyurek,Sejin Paik,Muhammed Yusuf Kocyigit,S. Akbiyik,cSerife Leman Runyun,D. Wijaya","id":"e6296cf7c2c7b4578f1ae644edae4ceee5a5faea","summary":"This paper uses an existing bias benchmark for the former BBQ and creates the first bias benchmark in natural language inference BBNLI with hand-written hypotheses while also converting each benchmark into the other form, suggesting that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form.","score":4},{"url":"https://www.semanticscholar.org/paper/5c78831b77d176f00679ae0654cc9eca0113b1a1","title":"Few-shot Reranking for Multi-hop QA via Language Model Prompting","venue":"","year":2022,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"Muhammad Khalifa,L. Logeswaran,Moontae Lee,Ho Hin Lee,Lu Wang","id":"5c78831b77d176f00679ae0654cc9eca0113b1a1","summary":"PromptRank, which relies on large language models prompting for multi-hop path reranking, first constructs an instruction-based prompt that includes a candidate document path and then computes the relevance score between a given question and the path based on the conditional likelihood of the question given the path prompt according to a language model.","score":4},{"url":"https://www.semanticscholar.org/paper/1791972b162aea075dbd07a8b5ba8760d9264f02","title":"Billions of Parameters Are Worth More Than In-domain Training Data: A case study in the Legal Case Entailment Task","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":2,"influentialCitationCount":0,"publicationDate":"30/05/2022","authors":"G. Rosa,L. Bonifacio,Vitor Jeronymo,H. Abonizio,R. Lotufo,Rodrigo Nogueira","id":"1791972b162aea075dbd07a8b5ba8760d9264f02","summary":"The 3B-parameter zero-shot model outperforms all models, including ensembles, in the COLIEE 2021 test set and also achieves the best performance of a single model in theCOLIEE 2022 competition, second only to the ensemble composed of the 3B model itself and a smaller version of the same model.","score":4},{"url":"https://www.semanticscholar.org/paper/fbb832181ebcdd45d7527502b860b7e421a65a1c","title":"InPars: Unsupervised Dataset Generation for Information Retrieval","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2022,"referenceCount":56,"citationCount":7,"influentialCitationCount":2,"publicationDate":"06/07/2022","authors":"L. Bonifacio,H. Abonizio,Marzieh Fadaee,Rodrigo Nogueira","id":"fbb832181ebcdd45d7527502b860b7e421a65a1c","summary":"This work harnesses the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks and shows that models finetuned solely on these synthetic datasets outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods.","score":4},{"url":"https://www.semanticscholar.org/paper/2d3b6058370b804009fd7866098f4fca2d1894ca","title":"Reducing Retraining by Recycling Parameter-Efficient Prompts","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/08/2022","authors":"Brian Lester,Joshua Yurtsever,Siamak Shakeri,Noah Constant","id":"2d3b6058370b804009fd7866098f4fca2d1894ca","summary":"This work proposes and investigates several approaches to “Prompt Re-cycling”, where a prompt trained on a source model is transformed to work with the new target model, and shows that recycling between models is possible.","score":4},{"url":"https://www.semanticscholar.org/paper/12941d1b35059a85b3a135a8aa8f6cc3dd630e39","title":"Changes from Classical Statistics to Modern Statistics and Data Science","venue":"ArXiv","year":2022,"referenceCount":136,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/10/2022","authors":"Kai Zhang,Shan-Yu Liu,M. Xiong","id":"12941d1b35059a85b3a135a8aa8f6cc3dd630e39","summary":"This perspective addresses the urgent need to overcome those fundamental limitations and encourages extensions of classical probability theory, hypothesis testing and regression, Brownian motion, diffusion equations and stochastic differential equations from Euclidean space to non-Euclidan space.","score":4},{"url":"https://www.semanticscholar.org/paper/37f0f1f55f44bff84aac27a346dd47d0c6c136e3","title":"GPS: Genetic Prompt Search for Efficient Few-Shot Learning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Hanwei Xu,Yujun Chen,Yulun Du,Nan Shao,Yang Wang,Haiyu Li,Zhilin Yang","id":"37f0f1f55f44bff84aac27a346dd47d0c6c136e3","summary":"GPS is gradient-free and requires no update of model parameters but only a small validation set, which outperforms manual prompts by a large margin and is better than other parameter-efficient tuning methods such as prompt tuning.","score":4},{"url":"https://www.semanticscholar.org/paper/b4e3ce1c8d263cf028bec5ca0b837a88ff96a475","title":"Multi-Vector Retrieval as Sparse Alignment","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":3,"influentialCitationCount":0,"publicationDate":"02/11/2022","authors":"Yujie Qian,Jinhyuk Lee,Sai Meher Karthik Duddu,Zhuyun Dai,Siddhartha Brahma,Iftekhar Naim,Tao Lei,Vincent Zhao","id":"b4e3ce1c8d263cf028bec5ca0b837a88ff96a475","summary":"This paper proposes A LIGNE R, a novel multi-vector retrieval model that learns sparsiﬁed pairwise alignments between query and document tokens and learns the sparse unary saliences with entropy-regularized linear programming, which outperforms other methods to achieve sparsity.","score":4},{"url":"https://www.semanticscholar.org/paper/748a2700ec11f51560a69ec05c67ca9f97014be7","title":"EvEntS ReaLM: Event Reasoning of Entity States via Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Evangelia Spiliopoulou,Artidoro Pagnoni,Yonatan Bisk,E. Hovy","id":"748a2700ec11f51560a69ec05c67ca9f97014be7","summary":"The results indicate that the prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/419146161a6ab37c3111593631b6c657f5da13ae","title":"TCBERT: A Technical Report for Chinese Topic Classification BERT","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Ting Han,Kunhao Pan,Xinyu Chen,Dingjie Song,Yuchen Fan,Xinyu Gao,Ruyi Gan,Jiaxing Zhang","id":"419146161a6ab37c3111593631b6c657f5da13ae","summary":"This work investigates supervised continued pre- training on BERT for Chinese topic classiﬁcation task and incorporates prompt-based learning and contrastive learning into the pre-training.","score":4},{"url":"https://www.semanticscholar.org/paper/cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3","title":"Legal Prompting: Teaching a Language Model to Think Like a Lawyer","venue":"ArXiv","year":2022,"referenceCount":23,"citationCount":1,"influentialCitationCount":0,"publicationDate":"02/12/2022","authors":"Fang Yu,Lee Quartey,Frank Schilder","id":"cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3","summary":"This work takes the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot prompting approaches and shows that the best results are produced by prompts that are derived from legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion).","score":4},{"url":"https://www.semanticscholar.org/paper/bbe93c90b7b87939cd064c805858feca61a3234d","title":"Self-Instruct: Aligning Language Model with Self Generated Instructions","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":6,"influentialCitationCount":3,"publicationDate":"20/12/2022","authors":"Yizhong Wang,Yeganeh Kordi,Swaroop Mishra,Alisa Liu,Noah A. Smith,Daniel Khashabi,Hannaneh Hajishirzi","id":"bbe93c90b7b87939cd064c805858feca61a3234d","summary":"S ELF -I NSTRUCT provides an almost annotation-free method for aligning pre-trained language models with instructions, and is released to facili-tate future studies on instruction tuning.","score":4},{"url":"https://www.semanticscholar.org/paper/c49a0912595a1cc70aab63524f64ed08c92194a8","title":"Evolutionary-scale prediction of atomic level protein structure with a language model","venue":"bioRxiv","year":2022,"referenceCount":73,"citationCount":48,"influentialCitationCount":7,"publicationDate":"21/12/2022","authors":"Zeming Lin,Halil Akin,Roshan Rao,B. Hie,Zhongkai Zhu,Wenting Lu,Nikita Smetanin,Robert Verkuil,Ori Kabeli,Y. Shmueli,Allan dos Santos Costa,M. Fazel-Zarandi,Tom Sercu,Salvatore Candido,Alexander Rives","id":"c49a0912595a1cc70aab63524f64ed08c92194a8","summary":"It is shown that direct inference of structure from primary sequence using a large language model enables an order of magnitude speed-up in high resolution structure prediction, which results in prediction that is up to 60x faster than state-of-the-art while maintaining resolution and accuracy.","score":4},{"url":"https://www.semanticscholar.org/paper/2eb0e52354b7bbee820905189985877700651108","title":"Multitask Instruction-based Prompting for Fallacy Recognition","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"Tariq Alhindi,Tuhin Chakrabarty,Elena Musi,S. Muresan","id":"2eb0e52354b7bbee820905189985877700651108","summary":"This work shows how instruction-based prompting in a multitask setup based on the T5 model improves the results against approaches built for a specific dataset such as T5, BERT or GPT-3 and examines the effect of model size and prompt choice on model performance.","score":4},{"url":"https://www.semanticscholar.org/paper/b7270830eb423d6970490e7ab20c14732e9d7ae2","title":"Efficient Language Model Training through Cross-Lingual and Progressive Transfer Learning","venue":"ArXiv","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2023","authors":"Malte Ostendorff,G. Rehm","id":"b7270830eb423d6970490e7ab20c14732e9d7ae2","summary":"A cross-lingual and progressive transfer learning approach that transfers models from a source language, for which pre-trained models are publicly available, like English, to a new target language, called CLP-Transfer is introduced.","score":4},{"url":"https://www.semanticscholar.org/paper/4043a936960de8e149dc208178fe1bcb157c7fa4","title":"Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches","venue":"","year":2019,"referenceCount":313,"citationCount":62,"influentialCitationCount":3,"publicationDate":"02/04/2019","authors":"Shane Storks,Qiaozi Gao,J. Chai","id":"4043a936960de8e149dc208178fe1bcb157c7fa4","summary":"An overview of recent benchmarks, relevant knowledge resources, and state-of-the-art learning and inference approaches in order to support a better understanding of this growing field of NLP is provided.","score":4},{"url":"https://www.semanticscholar.org/paper/53d5930ecd9dcc3eb79ef576f61fea248602e850","title":"Grounded Compositional Outputs for Adaptive Language Modeling","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":52,"citationCount":3,"influentialCitationCount":0,"publicationDate":"24/09/2020","authors":"Nikolaos Pappas,Phoebe Mulcaire,Noah A. Smith","id":"53d5930ecd9dcc3eb79ef576f61fea248602e850","summary":"This work proposes a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions, and is the first word-level language model with a size that does not depend on the training vocabulary.","score":4},{"url":"https://www.semanticscholar.org/paper/907a77639069bb7dd270f017068745706133cffc","title":"Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2023","authors":"Patrick Perrine","id":"907a77639069bb7dd270f017068745706133cffc","summary":"This work argues that this lack of accessibility could instill a nativist bias in researchers new to computational linguistics, and calls upon researchers to open source their LLM code wherever possible to allow both empircist and hybrid approaches to remain accessible.","score":4},{"url":"https://www.semanticscholar.org/paper/c1ace33daf974d3d16752c7a8565f32a63b09c49","title":"Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners","venue":"ArXiv","year":2022,"referenceCount":74,"citationCount":9,"influentialCitationCount":2,"publicationDate":"22/05/2022","authors":"Zhenhailong Wang,Manling Li,Ruochen Xu,Luowei Zhou,Jie Lei,Xudong Lin,Shuohang Wang,Ziyi Yang,Chenguang Zhu,Derek Hoiem,Shih-Fu Chang,Mohit Bansal,Heng Ji","id":"c1ace33daf974d3d16752c7a8565f32a63b09c49","summary":"The goal of this work is to build ﬂexible video-language models that can generalize to various video-to-text tasks from few examples, such as domain-speciﬁc captioning, question answering, and future event prediction, which outperforms state-of-the-art supervised models trained on any video datasets.","score":4},{"url":"https://www.semanticscholar.org/paper/ace0745f4449f20e4f4297476941fcd7dc7ab05c","title":"Higher Cognition: A Mechanical Perspective","venue":"Encyclopedia","year":2022,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/08/2022","authors":"Robert Friedman","id":"ace0745f4449f20e4f4297476941fcd7dc7ab05c","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/dd12f12015220d4beb2967fe23860d575e7a9e53","title":"Grounding Language with Visual Affordances over Unstructured Data","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":2,"influentialCitationCount":0,"publicationDate":"04/10/2022","authors":"Oier Mees,Jessica Borja-Diaz,W. Burgard","id":"dd12f12015220d4beb2967fe23860d575e7a9e53","summary":"This work proposes a novel approach to learn general-purpose language-conditioned robot skills from unstructured, of-ine and reset-free data in the real world by exploiting a self-supervised visuo-lingual affordance model, which requires annotating as little as 1% of the total data with language.","score":4},{"url":"https://www.semanticscholar.org/paper/f763cd0e3a9e14e1bf7ab0afacb9bbff30bbba29","title":"Using Both Demonstrations and Language Instructions to Efficiently Learn Robotic Tasks","venue":"ArXiv","year":2022,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Albert Yu,R. Mooney","id":"f763cd0e3a9e14e1bf7ab0afacb9bbff30bbba29","summary":"This is the first work to show that simultaneously conditioning a multi-task robotic manipulation policy on both demonstration and language embeddings improves sample efﬁciency and generalization over conditioning on either modality alone.","score":4},{"url":"https://www.semanticscholar.org/paper/1fa589c76e14492dca7a544d58628c2a4dc55264","title":"Instruction-Following Agents with Jointly Pre-Trained Vision-Language Models","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Hao Liu,Lisa Lee,Kimin Lee,P. Abbeel","id":"1fa589c76e14492dca7a544d58628c2a4dc55264","summary":"This work proposes a simple yet effective model for robots to solve instruction-following tasks in vision-based environments that outperforms all state-of-the-art pre-trained or trained-from-scratch methods in both single-task and multi-task settings.","score":4},{"url":"https://www.semanticscholar.org/paper/af3a6deac815c7e41a58b15be74e5e7a1066b899","title":"InstructRL: Simple yet Effective Instruction-Following Agents with Multimodal Transformer","venue":"","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/10/2022","authors":"Hao Liu,Lisa Lee,Kimin Lee,P. Abbeel","id":"af3a6deac815c7e41a58b15be74e5e7a1066b899","summary":"This work proposes a simple yet effective model for robots to solve instruction-following tasks in vision-based environments that outperforms all state-of-the-art pre-trained or trained-from-scratch methods in both single-task and multi-task settings.","score":4},{"url":"https://www.semanticscholar.org/paper/df065b3cd3621211320d9900186243aa1d086067","title":"Open-vocabulary Semantic Segmentation with Frozen Vision-Language Models","venue":"British Machine Vision Conference","year":2022,"referenceCount":55,"citationCount":3,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Chao Ma,Yu-Hao Yang,Yanfeng Wang,Ya Zhang,Weidi Xie","id":"df065b3cd3621211320d9900186243aa1d086067","summary":"This paper introduces Fusioner, with a lightweight, transformer-based fusion module, that pairs the frozen visual representation with language concept through a handful of image segmentation data, and demonstrates superior performance over previous models.","score":4},{"url":"https://www.semanticscholar.org/paper/d7d6701a0c6a252c950f1d046c277157da5a463d","title":"Learning to Solve Voxel Building Embodied Tasks from Pixels and Natural Language Instructions","venue":"ArXiv","year":2022,"referenceCount":15,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Alexey Skrynnik,Zoya Volovikova,Marc-Alexandre Côté,Anton Voronov,Artem Zholus,Negar Arabzadeh,Shrestha Mohanty,Milagro Teruel,A. Awadallah,A. Panov,M. Burtsev,Julia Kiseleva","id":"d7d6701a0c6a252c950f1d046c277157da5a463d","summary":"A new method is proposed that combines a language model and reinforcement learning for the task of building objects in a Minecraft-like environment according to the natural language instructions and generates a set of consistently achievable sub-goals from the instructions and then completes associated sub-tasks with a pre-trained RL policy.","score":4},{"url":"https://www.semanticscholar.org/paper/5aaf780c63c87db6c90c7f96308cf90296c5b226","title":"Zero-shot Image Captioning by Anchor-augmented Vision-Language Space Alignment","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Junyan Wang,Yi Zhang,Ming Yan,J. Zhang,J. Sang","id":"5aaf780c63c87db6c90c7f96308cf90296c5b226","summary":"This work proposes Cross-modal Language Models (CLMs) to facilitate unsupervised cross- modal learning and proposes Anchor Augment to guide the generative model’s attention to the ﬁne-grained information in the representation of CLIP.","score":4},{"url":"https://www.semanticscholar.org/paper/d9c7e401ae1dce2c5836ad561adb5a4504a42176","title":"Doubly Right Object Recognition: A Why Prompt for Visual Rationales","venue":"ArXiv","year":2022,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Chengzhi Mao,Revant Teotia,Amrutha Sundar,Sachit Menon,Junfeng Yang,Xin Eric Wang,Carl Vondrick","id":"d9c7e401ae1dce2c5836ad561adb5a4504a42176","summary":"By trans-ferring the rationales from language models into visual representations through a tailored dataset, it is shown that a “why prompt,” which adapts large visual representations to produce correct rationales, can be learned.","score":4},{"url":"https://www.semanticscholar.org/paper/ae766548699f27e669932de14e1c0f47b2828536","title":"Prompting for Multimodal Hateful Meme Classification","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Rui Cao,R. Lee,Wen-Haw Chong,Jing Jiang","id":"ae766548699f27e669932de14e1c0f47b2828536","summary":"This work proposes PromptHate, a simple yet effective prompt-based model that prompts pre-trained language models (PLMs) for hateful meme classification, and constructs simple prompts and provides a few in-context examples to exploit the implicit knowledge in the pre- trained RoBERTa language model for hateful memes classification.","score":4},{"url":"https://www.semanticscholar.org/paper/6c194ca1bcee1fab2dfb289dadcdd9a829b736df","title":"Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis","venue":"ArXiv","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/02/2023","authors":"Zhu Wang,Sourav Medya,S. Ravi","id":"6c194ca1bcee1fab2dfb289dadcdd9a829b736df","summary":"This work proposes an end-to-end vision and language model incorporating explicit knowledge graphs and introduces an interactive out-of-distribution (OOD) layer using implicit network operator that is used to filter noise that is brought by external knowledge base.","score":4},{"url":"https://www.semanticscholar.org/paper/66eae7128c34dd7967d79224eb9dbc978773c3d0","title":"I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Chandra Bhagavatula,Jena D. Hwang,Doug Downey,Ronan Le Bras,Ximing Lu,Keisuke Sakaguchi,Swabha Swayamdipta,Peter West,Yejin Choi","id":"66eae7128c34dd7967d79224eb9dbc978773c3d0","summary":"The key intellectual question is whether it is possible, if at all, to design a learning algorithm that does not beneﬁt from scale, yet leads to a competitive level of commonsense acquisition.","score":4},{"url":"https://www.semanticscholar.org/paper/044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences","venue":"Neural Information Processing Systems","year":2020,"referenceCount":117,"citationCount":828,"influentialCitationCount":139,"publicationDate":"28/07/2020","authors":"M. Zaheer,Guru Guruganesh,Kumar Avinava Dubey,J. Ainslie,Chris Alberti,Santiago Ontañón,Philip Pham,Anirudh Ravula,Qifan Wang,Li Yang,Amr Ahmed","id":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","summary":"It is shown that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model.","score":3},{"url":"https://www.semanticscholar.org/paper/30eff53e981695c7296d258b8dc44b4c7b482a0c","title":"A Discrete Hard EM Approach for Weakly Supervised Question Answering","venue":"Conference on Empirical Methods in Natural Language Processing","year":2019,"referenceCount":46,"citationCount":122,"influentialCitationCount":20,"publicationDate":"11/09/2019","authors":"Sewon Min,Danqi Chen,Hannaneh Hajishirzi,Luke Zettlemoyer","id":"30eff53e981695c7296d258b8dc44b4c7b482a0c","summary":"This paper develops a hard EM learning scheme that computes gradients relative to the most likely solution at each update and significantly outperforms previous methods on six QA tasks, including absolute gains of 2–10%, and achieves the state-of-the-art on five of them.","score":3},{"url":"https://www.semanticscholar.org/paper/3fd92feb43d61645429ec4a2c80b1304a3c8bd69","title":"A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":37,"citationCount":5,"influentialCitationCount":0,"publicationDate":"14/06/2021","authors":"Zhihong Shao,Lifeng Shang,Qun Liu,Minlie Huang","id":"3fd92feb43d61645429ec4a2c80b1304a3c8bd69","summary":"Extensive experiments show that this method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions.","score":3},{"url":"https://www.semanticscholar.org/paper/b9fa682deb66997ed27f3f47d8976c64f093be27","title":"MSReNet: Multi-step Reformulation for Open-Domain Question Answering","venue":"Natural Language Processing and Chinese Computing","year":2020,"referenceCount":25,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/10/2020","authors":"Weiguang Han,Min Peng,Qianqian Xie,Xiuzhen Zhang,Hua Wang","id":"b9fa682deb66997ed27f3f47d8976c64f093be27","summary":"A new framework MSReNet is introduced for open-domain question answering where the question reformulator interacts with the term-based retrieval system, which can improve retrieval precision and QA performance.","score":3},{"url":"https://www.semanticscholar.org/paper/7434fb6c79a50bfedd4ad1c872f9e4fb1f25571a","title":"ERNIE-Doc: A Retrospective Long-Document Modeling Transformer","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":49,"citationCount":15,"influentialCitationCount":0,"publicationDate":"01/01/2021","authors":"Siyu Ding,Junyuan Shang,Shuohuan Wang,Yu Sun,Hao Tian,Hua Wu,Haifeng Wang","id":"7434fb6c79a50bfedd4ad1c872f9e4fb1f25571a","summary":"Two welldesigned techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-DOC 1, which has a much longer effective context length, to capture the contextual information of a complete document.","score":3},{"url":"https://www.semanticscholar.org/paper/b53c510769760d2fbcb674ed14bacfe251ae0ebc","title":"Select and Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/01/2023","authors":"Weiguang Han,Boyi Zhang,Qianqian Xie,Min Peng,Yanzhao Lai,Jimin Huang","id":"b53c510769760d2fbcb674ed14bacfe251ae0ebc","summary":"A paradigm for automatic pair trading is proposed as a unified task rather than a two-step pipeline and a hierarchical reinforcement learning framework is designed to jointly learn and optimize two subtasks.","score":3},{"url":"https://www.semanticscholar.org/paper/f01c433acfaef63d7dfc168a507fdb09cd48032e","title":"Narrative Question Answering with Cutting-Edge Open-Domain QA Techniques: A Comprehensive Study","venue":"Transactions of the Association for Computational Linguistics","year":2021,"referenceCount":42,"citationCount":6,"influentialCitationCount":0,"publicationDate":"07/06/2021","authors":"Xiangyang Mou,Chenghao Yang,Mo Yu,Bingsheng Yao,Xiaoxiao Guo,Saloni Potdar,Hui Su","id":"f01c433acfaef63d7dfc168a507fdb09cd48032e","summary":"The findings indicate that the event-centric questions dominate this task, which exemplifies the inability of existing QA models to handle event-oriented scenarios.","score":3},{"url":"https://www.semanticscholar.org/paper/3467b97c35d7d9e8d94b0a490751880ccdbcc479","title":"Neural Code Summarization: How Far Are We?","venue":"ArXiv","year":2021,"referenceCount":60,"citationCount":11,"influentialCitationCount":4,"publicationDate":2021,"authors":"Ensheng Shi,Yanlin Wang,Lun Du,Junjie Chen,Shi Han,Hongyu Zhang,Dongmei Zhang,Hongbin Sun","id":"3467b97c35d7d9e8d94b0a490751880ccdbcc479","summary":"A systematic and in-depth analysis of five state-of-the-art neural source code summarization models on three widely used datasets suggests that the BLEU metric has many variants and code pre-processing choices can have a large impact on the summarization performance.","score":3},{"url":"https://www.semanticscholar.org/paper/343594d16840c3841e70ca603f500a79c433848b","title":"On the Evaluation of Neural Code Summarization","venue":"International Conference on Software Engineering","year":2021,"referenceCount":71,"citationCount":12,"influentialCitationCount":1,"publicationDate":"15/07/2021","authors":"Ensheng Shi,Yanlin Wang,Lun Du,Junjie Chen,Shi Han,Hongyu Zhang,Dongmei Zhang,Hongbin Sun","id":"343594d16840c3841e70ca603f500a79c433848b","summary":"A systematic and in-depth analysis of 5 state-of-the-art neural code summarization models on 6 widely used BLEU variants, 4 pre-processing operations and their combinations, and 3 widely used datasets shows that some important factors have a great influence on the model evaluation, especially on the performance of models and the ranking among the models.","score":3},{"url":"https://www.semanticscholar.org/paper/1e6345a73372f593a0bab78e66ee6c98a44feaed","title":"EventNarrative: A large-scale Event-centric Dataset for Knowledge Graph-to-Text Generation","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":56,"citationCount":3,"influentialCitationCount":0,"publicationDate":"30/10/2021","authors":"A. Colas,A. Sadeghian,Yue Wang,D. Wang","id":"1e6345a73372f593a0bab78e66ee6c98a44feaed","summary":"The aim of this paper is to help break new ground in event-centric research where data is lacking and to give researchers a well-deﬁned, large-scale dataset in order to better evaluate existing and future knowledge graph-to-text models.","score":3},{"url":"https://www.semanticscholar.org/paper/eca8c79f2a0c469e26af514481e72edb97a26a35","title":"A review on COLREGs-compliant navigation of autonomous surface vehicles: From traditional to learning-based approaches","venue":"Journal of Automation and Intelligence","year":2022,"referenceCount":99,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Liangmou Hu,Huosheng Hu,W. Naeem,Zidong Wang","id":"eca8c79f2a0c469e26af514481e72edb97a26a35","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/b1281b87f45b533d275c3cb262cdac8c52ef936a","title":"Value Activation for Bias Alleviation: Generalized-activated Deep Double Deterministic Policy Gradients","venue":"Neurocomputing","year":2021,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2021","authors":"Jiafei Lyu,Yu Yang,Jiangpeng Yan,Xiu Li","id":"b1281b87f45b533d275c3cb262cdac8c52ef936a","summary":"This work theoretically and experimentally shows that the distance between the max operator and the generalized-activated weighting operator can be bounded and shows that simple activation functions are enough for amazing performance without any tricks and special design for activation function.","score":3},{"url":"https://www.semanticscholar.org/paper/5df17c261a663dbc2ccabc212d75a03828e65fd0","title":"Solving Full <math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"M1\">\n                     <mi>N</mi>\n                     <mo>×</mo>\n                     <mi>N</mi>\n                     <mo>×</mo>\n                     <mi>N</mi>\n                  </math> Rubik’s Supercube Using Genetic Algorithm","venue":"International Journal of Computer Games Technology","year":2023,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/01/2023","authors":"R. Świta,Z. Suszyński","id":"5df17c261a663dbc2ccabc212d75a03828e65fd0","summary":"An algorithm that uses an evolutionary approach to the problem of solving the Full Rubik, i.e., the orientation of all cubies, including the internal ones, not only according to face colors but to the same orientation in 3D space is presented.","score":3},{"url":"https://www.semanticscholar.org/paper/521b4e26df0f1cf5763dece14cbb218df152dc59","title":"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering","venue":"International Conference on Learning Representations","year":2019,"referenceCount":46,"citationCount":203,"influentialCitationCount":33,"publicationDate":"24/11/2019","authors":"Akari Asai,Kazuma Hashimoto,Hannaneh Hajishirzi,R. Socher,Caiming Xiong","id":"521b4e26df0f1cf5763dece14cbb218df152dc59","summary":"A new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions and achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.","score":3},{"url":"https://www.semanticscholar.org/paper/a9c2c295853fb35461b7fbbb60071d29238cb334","title":"Complex Factoid Question Answering with a Free-Text Knowledge Graph","venue":"The Web Conference","year":2020,"referenceCount":61,"citationCount":30,"influentialCitationCount":6,"publicationDate":"19/04/2020","authors":"Chen Zhao","id":"a9c2c295853fb35461b7fbbb60071d29238cb334","summary":"Experiments show delft can answer entity-rich questions better than machine reading based models, bert-based answer ranking and memory networks and the novel graph neural network which reasons on the rich but noisy free-text evidence.","score":3},{"url":"https://www.semanticscholar.org/paper/e88b18cde9f0b0be5dbf85c373720bc7d952f960","title":"F1 Is Not Enough! Models and Evaluation towards User-Centered Explainable Question Answering","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":45,"citationCount":10,"influentialCitationCount":1,"publicationDate":"13/10/2020","authors":"Hendrik Schuff,Heike Adel,Ngoc Thang Vu","id":"e88b18cde9f0b0be5dbf85c373720bc7d952f960","summary":"A hierarchical model and a new regularization term are proposed to strengthen the answer-explanation coupling as well as two evaluation scores to quantify the coupling.","score":3},{"url":"https://www.semanticscholar.org/paper/fac6e5ab3eb90b3651757b9d7a7927ed9367ae5f","title":"Latent Reasoning for Low-Resource Question Generation","venue":"Findings","year":2021,"referenceCount":70,"citationCount":4,"influentialCitationCount":0,"publicationDate":2021,"authors":"Xinting Huang,Jianzhong Qi,Yu Sun,Rui Zhang","id":"fac6e5ab3eb90b3651757b9d7a7927ed9367ae5f","summary":"A novel generative approach that optimizes the two-phase model without question decomposition data is proposed and outperforms competitive baselines on HOTPOTQA, a benchmark multi-hop question answering dataset.","score":3},{"url":"https://www.semanticscholar.org/paper/21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures","venue":"ArXiv","year":2020,"referenceCount":73,"citationCount":67,"influentialCitationCount":14,"publicationDate":"17/07/2020","authors":"Daniel Furrer,Marc van Zee,Nathan Scales,Nathanael Scharli","id":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","summary":"It is shown that masked language model (MLM) pre-training rivals SCAN-inspired architectures on primitive holdout splits and establishes a new state of the art on the CFQ compositional generalization benchmark using MLM pre- training together with an intermediate representation.","score":3},{"url":"https://www.semanticscholar.org/paper/986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language","venue":"Neural Information Processing Systems","year":2020,"referenceCount":39,"citationCount":18,"influentialCitationCount":5,"publicationDate":"15/10/2020","authors":"Yinuo Guo,Zeqi Lin,Jian-Guang Lou,Dongmei Zhang","id":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","summary":"A novel hierarchical poset decoding paradigm for compositional generalization in language that enforces partial permutation invariance in semantics, thus avoiding overfitting to bias ordering information and results show that it outperforms current decoders.","score":3},{"url":"https://www.semanticscholar.org/paper/cc0fc60e9b9d036acf53899c259574c8ce2aedfc","title":"Compositional Generalization via Semantic Tagging","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":36,"citationCount":19,"influentialCitationCount":1,"publicationDate":"22/10/2020","authors":"Hao Zheng,Mirella Lapata","id":"cc0fc60e9b9d036acf53899c259574c8ce2aedfc","summary":"This work decomposes decoding into two phases where an input utterance is first tagged with semantic symbols representing the meanings of its individual words, and then a sequence-to-sequence model is used to predict the final meaning representation conditioning on the utterance and the predicted tag sequence.","score":3},{"url":"https://www.semanticscholar.org/paper/7344ed64d1717780422fd1d58fae85edc544d180","title":"Iterative Utterance Segmentation for Neural Semantic Parsing","venue":"AAAI Conference on Artificial Intelligence","year":2020,"referenceCount":40,"citationCount":2,"influentialCitationCount":0,"publicationDate":"13/12/2020","authors":"Yinuo Guo,Zeqi Lin,Jian-Guang Lou,Dongmei Zhang","id":"7344ed64d1717780422fd1d58fae85edc544d180","summary":"A novel framework for boosting neural semantic parsers via iterative utterance segmentation that iterates between two neural modules: a segmenter for segmenting a span from the utterance, and a parser for mapping the span into a partial meaning representation.","score":3},{"url":"https://www.semanticscholar.org/paper/f0787aace6a6d892140d9da85b49e76cb06f6862","title":"Automatic Knowledge Augmentation for Generative Commonsense Reasoning","venue":"ArXiv","year":2021,"referenceCount":17,"citationCount":3,"influentialCitationCount":0,"publicationDate":"30/10/2021","authors":"Jaehyung Seo,Chanjun Park,Sugyeong Eo,Hyeonseok Moon,Heuiseok Lim","id":"f0787aace6a6d892140d9da85b49e76cb06f6862","summary":"A data-centric method that uses automatic knowledge augmentation to extend commonsense knowledge using a machine knowledge generator that can generate semi-golden sentences that improve the generative commonsense reasoning of a language model without architecture modiﬁcations.","score":3},{"url":"https://www.semanticscholar.org/paper/a576512a7562597fd30719a834d5866d010ef6ab","title":"Compositional Generalization for Natural Language Interfaces to Web APIs","venue":"ArXiv","year":2021,"referenceCount":33,"citationCount":1,"influentialCitationCount":1,"publicationDate":"09/12/2021","authors":"Saghar Hosseini,A. Awadallah,Yu Su","id":"a576512a7562597fd30719a834d5866d010ef6ab","summary":"New compositional generalization tasks for NL2API are defined which explore the models’ ability to extrapolate from simple API calls in the training set to new and more complex API Calls in the inference phase.","score":3},{"url":"https://www.semanticscholar.org/paper/474c954231413f2a249f04272dbeda19cd8ff09b","title":"Bridging the Generalization Gap in Text-to-SQL Parsing with Schema Expansion","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":40,"citationCount":5,"influentialCitationCount":2,"publicationDate":2022,"authors":"Chen Zhao,Yu Su,Adam Pauls,Emmanouil Antonios Platanios","id":"474c954231413f2a249f04272dbeda19cd8ff09b","summary":"This work proposes to address the out-of-domain generalization problem of domain specific phrases to composite operation over columns by incorporating prior domain knowledge by preprocessing table schemas, and design a method that consists of two components: schema expansion and schema pruning.","score":3},{"url":"https://www.semanticscholar.org/paper/2aa1d4350e80613feed88d5a6337e79693f7aa57","title":"KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":58,"citationCount":14,"influentialCitationCount":2,"publicationDate":"08/07/2020","authors":"S. Cao,Jiaxin Shi,Liangming Pan,L. Nie,Yutong Xiang,Lei Hou,Juanzi Li,Bin He,Hanwang Zhang","id":"2aa1d4350e80613feed88d5a6337e79693f7aa57","summary":"Experimental results show that state-of-the-art KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that K QA Pro is challenging and Complex KBZA requires further research efforts.","score":3},{"url":"https://www.semanticscholar.org/paper/997410e2bf80f25f73752dd6fd7122227385ed2d","title":"Measuring Compositional Consistency for Video Question Answering","venue":"Computer Vision and Pattern Recognition","year":2022,"referenceCount":57,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/04/2022","authors":"Mona Gandhi,Mustafa Omer Gul,Eva Prakash,Madeleine Grunde-McLaughlin,Ranjay Krishna,Maneesh Agrawala","id":"997410e2bf80f25f73752dd6fd7122227385ed2d","summary":"A question decomposition engine that programmatically deconstructs a compositional question into a directed acyclic graph of sub-questions and finds that models either cannot reason correctly through most compositions or are reliant on incorrect reasoning to reach answers, frequently contradicting themselves or achieving high accuracies when failing at intermediate reasoning steps.","score":3},{"url":"https://www.semanticscholar.org/paper/8969ea3d254e149aebcfd1ffc8f46910d7cb160e","title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":3,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Najoung Kim,Tal Linzen,P. Smolensky","id":"8969ea3d254e149aebcfd1ffc8f46910d7cb160e","summary":"It is argued that exposure to pre-training data may break distributional control across training and test to gauge compositional generalization, where certain lexical items only occur in limited contexts during training.","score":3},{"url":"https://www.semanticscholar.org/paper/a14505553eda5ae26380d4317d6358491f238e41","title":"Generative adversarial interactive imitation learning for path following of autonomous underwater vehicle","venue":"Ocean Engineering","year":2022,"referenceCount":40,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/09/2022","authors":"Dongqian Jiang,Jie Huang,Zheng Fang,C. Cheng,Q. Sha,Bo He,Guangliang Li","id":"a14505553eda5ae26380d4317d6358491f238e41","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/ae7def75a40073ed119fc9e93a9ecd6d58d3bab0","title":"Knowledge- and ambiguity-aware robot learning from corrective and evaluative feedback","venue":"Neural computing & applications (Print)","year":2023,"referenceCount":16,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/01/2023","authors":"C. Celemin,J. Kober","id":"ae7def75a40073ed119fc9e93a9ecd6d58d3bab0","summary":"An IIL method is proposed that improves the human–robot interaction for non-expert and imperfect teachers in two directions and enables the teachers to train with the flexibility of using corrective demonstrations, evaluative reinforcements, and implicit positive feedback.","score":3},{"url":"https://www.semanticscholar.org/paper/aea6e0d7f5baa18887fff60c5ff375a8654a4646","title":"Deep Reinforcement Learning for the Design of Musical Interaction","venue":"","year":2018,"referenceCount":84,"citationCount":0,"influentialCitationCount":0,"publicationDate":2018,"authors":"","id":"aea6e0d7f5baa18887fff60c5ff375a8654a4646","summary":"Overall, it was shown that the use of interactive machine learning techniques such as interactive deep reinforcement learning provide exciting new directions for research and have an added and complementary value next to standard ways of manual exploration.","score":3},{"url":"https://www.semanticscholar.org/paper/1b577cafcbda916ccf32a5e0291210afde5c663b","title":"THE IMPLICIT PREFERENCE INFORMATION","venue":"","year":2018,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":2018,"authors":"AN In","id":"1b577cafcbda916ccf32a5e0291210afde5c663b","summary":"An algorithm based on Maximum Causal Entropy IRL is developed and it is found that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized.","score":3},{"url":"https://www.semanticscholar.org/paper/3e6cde685fdf321d7edf9319f7b07c01ff79c11a","title":"Reward learning from human preferences and demonstrations in Atari","venue":"","year":2018,"referenceCount":55,"citationCount":2,"influentialCitationCount":0,"publicationDate":2018,"authors":"Dandelion Mané","id":"3e6cde685fdf321d7edf9319f7b07c01ff79c11a","summary":"This work trains a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games and achieves strictly superhuman performance on 2 games without using game rewards.","score":3},{"url":"https://www.semanticscholar.org/paper/01ee79fa125f532893ba50ad3c77cd4bf29de0ca","title":"Directed Policy Gradient for Safe Reinforcement Learning with Human Advice","venue":"ArXiv","year":2018,"referenceCount":16,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/08/2018","authors":"Hélène Plisnier,Denis Steckelmacher,T. Brys,Diederik M. Roijers,A. Nowé","id":"01ee79fa125f532893ba50ad3c77cd4bf29de0ca","summary":"The technique, Directed Policy Gradient (DPG), allows a teacher or backup policy to override the agent before it acts undesirably, while allowing the agent to leverage human advice or directives to learn faster.","score":3},{"url":"https://www.semanticscholar.org/paper/e911cb25e9ce6cc71fa7c854c69c6347279bcb71","title":"Cycle-of-Learning for Autonomous Systems from Human Interaction","venue":"ArXiv","year":2018,"referenceCount":11,"citationCount":13,"influentialCitationCount":0,"publicationDate":"28/08/2018","authors":"Nicholas R. Waytowich,Vinicius G. Goecks,V. Lawhern","id":"e911cb25e9ce6cc71fa7c854c69c6347279bcb71","summary":"Two key concepts provided by the Cycle-of-Learning framework are how it handles the integration of the different human-interaction modalities (demonstration, intervention, and evaluation) and how to define the switching criteria between them.","score":3},{"url":"https://www.semanticscholar.org/paper/325e1b7cec684c22bb3c2cf65205c77eaf55114f","title":"Reward learning from human preferences and demonstrations in Atari","venue":"Neural Information Processing Systems","year":2018,"referenceCount":58,"citationCount":157,"influentialCitationCount":19,"publicationDate":"15/11/2018","authors":"Borja Ibarz,J. Leike,Tobias Pohlen,Geoffrey Irving,S. Legg,Dario Amodei","id":"325e1b7cec684c22bb3c2cf65205c77eaf55114f","summary":"This work trains a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games and achieves strictly superhuman performance on 2 games without using game rewards.","score":3},{"url":"https://www.semanticscholar.org/paper/c6f913e4baa7f2c85363c0625c87003ad3b3a14c","title":"Scalable agent alignment via reward modeling: a research direction","venue":"ArXiv","year":2018,"referenceCount":182,"citationCount":137,"influentialCitationCount":15,"publicationDate":"19/11/2018","authors":"J. Leike,David Krueger,Tom Everitt,Miljan Martic,Vishal Maini,S. Legg","id":"c6f913e4baa7f2c85363c0625c87003ad3b3a14c","summary":"This work outlines a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning.","score":3},{"url":"https://www.semanticscholar.org/paper/4494395f9e8550e5516c44ef7ce2cb71b217374f","title":"Learning from Human Feedback: A Comparison of Interactive Reinforcement Learning Algorithms","venue":"","year":2019,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2019,"authors":"Von Menschlichem,Dorothea Koert,Tag der Einreichung,Erklärung zur Master-Thesis","id":"4494395f9e8550e5516c44ef7ce2cb71b217374f","summary":"This thesis presents a general framework that can incorporate different kinds of human interaction and the environmental reward in an Interactive Reinforcement Learning algorithm and simplifies the combination of several, interactive and not interactive, algorithms.","score":3},{"url":"https://www.semanticscholar.org/paper/d35f9c78fc6d656d530aac2ed9f2aae6137b9041","title":"I MPLICIT IN THE S TATE OF THE W ORLD","venue":"","year":2019,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":2019,"authors":"Rohin Shah","id":"d35f9c78fc6d656d530aac2ed9f2aae6137b9041","summary":"An algorithm based on Maximum Causal Entropy IRL is developed and it is found that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized.","score":3},{"url":"https://www.semanticscholar.org/paper/5d60613c15de600dc321788b1953bf49f8eced50","title":"Preferences Implicit in the State of the World","venue":"International Conference on Learning Representations","year":2018,"referenceCount":31,"citationCount":40,"influentialCitationCount":2,"publicationDate":"27/09/2018","authors":"Rohin Shah,Dmitrii Krasheninnikov,Jordan Alexander,P. Abbeel,A. Dragan","id":"5d60613c15de600dc321788b1953bf49f8eced50","summary":"An algorithm based on Maximum Causal Entropy IRL is developed and it is found that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized.","score":3},{"url":"https://www.semanticscholar.org/paper/78959e2e613bffabd11896a67a3cfc71793dde07","title":"Yesterday's Reward is Today's Punishment: Contrast Effects in Human Feedback to Reinforcement Learning Agents","venue":"Adaptive Agents and Multi-Agent Systems","year":2020,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":2020,"authors":"Divya Ramesh,Anthony Z. Liu,A. J. Echeverria,Jean Y. Song,Nicholas R. Waytowich,Walter S. Lasecki","id":"78959e2e613bffabd11896a67a3cfc71793dde07","summary":"This work presents empirical evidence to show that human perception affected by contrast effects distorts their feedback to Reinforcement Learning agents, and provides a conceptual understanding of a source of inconsistency in human feedback, thus informing the design of human-agent interactions.","score":3},{"url":"https://www.semanticscholar.org/paper/096758be2b50762474757b38c18f51938cad4f4e","title":"Learning Human Objectives by Evaluating Hypothetical Behavior","venue":"International Conference on Machine Learning","year":2019,"referenceCount":54,"citationCount":57,"influentialCitationCount":1,"publicationDate":"05/12/2019","authors":"S. Reddy,A. Dragan,S. Levine,S. Legg,J. Leike","id":"096758be2b50762474757b38c18f51938cad4f4e","summary":"An algorithm that safely and interactively learns a model of the user's reward function and actively synthesizes hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment.","score":3},{"url":"https://www.semanticscholar.org/paper/14b7d4a34ba869e106d82d658e2973163a11eb12","title":"FRESH: Interactive Reward Shaping in High-Dimensional State Spaces using Human Feedback","venue":"Adaptive Agents and Multi-Agent Systems","year":2020,"referenceCount":51,"citationCount":16,"influentialCitationCount":2,"publicationDate":"19/01/2020","authors":"Baicen Xiao,Qifan Lu,B. Ramasubramanian,Andrew Clark,L. Bushnell,R. Poovendran","id":"14b7d4a34ba869e106d82d658e2973163a11eb12","summary":"This paper seeks to effectively integrate feedback signals supplied by a human operator with deep reinforcement learning algorithms in high-dimensional state spaces and uses an ensemble of neural networks with a shared network architecture to represent model uncertainty and the confidence of the neural network in its output.","score":3},{"url":"https://www.semanticscholar.org/paper/9f31b5dd1d9407de3ae8aa0f88bc8cfe9392f398","title":"Human-in-the-Loop Methods for Data-Driven and Reinforcement Learning Systems","venue":"ArXiv","year":2020,"referenceCount":213,"citationCount":8,"influentialCitationCount":0,"publicationDate":"17/03/2020","authors":"Vinicius G. Goecks","id":"9f31b5dd1d9407de3ae8aa0f88bc8cfe9392f398","summary":"Results presented in this work show that the reward signal that is learned based upon human interaction accelerates the rate of learning of reinforcement learning algorithms and that learning from a combination of human demonstrations and interventions is faster and more sample efficient when compared to traditional supervised learning algorithms.","score":3},{"url":"https://www.semanticscholar.org/paper/08bfe0db80f150bc2f8372b9a9817b7c844c1bca","title":"Explanation Augmented Feedback in Human-in-the-Loop Reinforcement Learning","venue":"ArXiv","year":2020,"referenceCount":63,"citationCount":12,"influentialCitationCount":0,"publicationDate":"26/06/2020","authors":"L. Guan,Mudit Verma,S. Kambhampati","id":"08bfe0db80f150bc2f8372b9a9817b7c844c1bca","summary":"Explanation Augmented Feedback allows for explanatory information to be given as saliency maps from the human in addition to the binary feedback, and an ablation study is presented to confirm the hypothesis that augmenting binary feedback with state salient information gives a boost in performance.","score":3},{"url":"https://www.semanticscholar.org/paper/1e273fb74ebe3d0cb860a301292cee066d058d12","title":"A Survey on Interactive Reinforcement Learning: Design Principles and Open Challenges","venue":"Conference on Designing Interactive Systems","year":2020,"referenceCount":115,"citationCount":45,"influentialCitationCount":4,"publicationDate":"03/07/2020","authors":"Christian Arzate Cruz,T. Igarashi","id":"1e273fb74ebe3d0cb860a301292cee066d058d12","summary":"This paper elucidate the roles played by HCI researchers in interactive RL, identifying ideas and promising research directions, and proposes generic design principles that will provide researchers with a guide to effectively implement interactive RL applications.","score":3},{"url":"https://www.semanticscholar.org/paper/8c6dc25db6c2e5e90b03d0ecd328b74e3f4cd0c4","title":"X 2 T : T RAINING AN X-TOT EXT T YPING I NTERFACE WITH O NLINE L EARNING FROM U SER F EEDBACK","venue":"","year":2021,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Jensen Gao,S. Reddy","id":"8c6dc25db6c2e5e90b03d0ecd328b74e3f4cd0c4","summary":"This work proposes an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters.","score":3},{"url":"https://www.semanticscholar.org/paper/364deb949f6ad1abee7f10079b48b61da9203579","title":"Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation","venue":"Neural Information Processing Systems","year":2020,"referenceCount":52,"citationCount":18,"influentialCitationCount":4,"publicationDate":"26/06/2020","authors":"L. Guan,Mudit Verma,Sihang Guo,Ruohan Zhang,Subbarao Kambhampati","id":"364deb949f6ad1abee7f10079b48b61da9203579","summary":"This paper focuses on the task of learning from feedback, in which the human trainer not only gives binary evaluative “good\" or “bad\" feedback for queried state-action pairs, but also provides a visual explanation by annotating relevant features in images.","score":3},{"url":"https://www.semanticscholar.org/paper/f9f340c8bd0712780148d0f431cd4914a515f4b1","title":"Recent advances in leveraging human guidance for sequential decision-making tasks","venue":"Autonomous Agents and Multi-Agent Systems","year":2021,"referenceCount":232,"citationCount":13,"influentialCitationCount":4,"publicationDate":"23/06/2021","authors":"Ruohan Zhang,F. Torabi,Garrett Warnell,P. Stone","id":"f9f340c8bd0712780148d0f431cd4914a515f4b1","summary":"This survey provides a high-level overview of five recent machine learning frameworks that primarily rely on human guidance apart from pre-specified reward functions or conventional, step-by-step action demonstrations.","score":3},{"url":"https://www.semanticscholar.org/paper/9c4badd7f983cc37aa4794433893649cc7440fdc","title":"Uncertainties based queries for Interactive policy learning with evaluations and corrections","venue":"ICMI Companion","year":2021,"referenceCount":9,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/10/2021","authors":"C. Celemin,J. Kober","id":"9c4badd7f983cc37aa4794433893649cc7440fdc","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/2a66fda232180a1701016ff5c15d1761ddfa2004","title":"X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback","venue":"International Conference on Learning Representations","year":2022,"referenceCount":56,"citationCount":5,"influentialCitationCount":0,"publicationDate":"04/03/2022","authors":"Jensen Gao,S. Reddy,G. Berseth,Nicholas Hardy,N. Natraj,K. Ganguly,A. Dragan,S. Levine","id":"2a66fda232180a1701016ff5c15d1761ddfa2004","summary":"This work proposes an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters.","score":3},{"url":"https://www.semanticscholar.org/paper/40c099d10be5a7ff429ee13f4c3f8b49142b1587","title":"Human-in-the-Loop Reinforcement Learning for Adaptive Assistive Interfaces","venue":"","year":2022,"referenceCount":101,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jensen Gao","id":"40c099d10be5a7ff429ee13f4c3f8b49142b1587","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/d3d1f85fe0f9461a570c6040e94adcb530dd5325","title":"A Dual Representation Framework for Robot Learning with Human Guidance","venue":"","year":2022,"referenceCount":80,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Ruohan Zhang,Dhruva Bansal,Yilun Hao,Ayano Hiranaka,Jialu Gao,Chen Wang,Li Fei-Fei,Jiajun Wu","id":"d3d1f85fe0f9461a570c6040e94adcb530dd5325","summary":"It is argued that learning is more efﬁcient if the agent is equipped with a high-level, symbolic representation and two novel learning algorithms based on this framework for learning from human evaluative feedback and from preference are proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/fa5943a641a35d5a7afd545f5bdfd9e71cc56e1a","title":"Combining Learning from Human Feedback and Knowledge Engineering to Solve Hierarchical Tasks in Minecraft","venue":"Make","year":2021,"referenceCount":52,"citationCount":5,"influentialCitationCount":1,"publicationDate":"07/12/2021","authors":"Vinicius G. Goecks,Nicholas R. Waytowich,David Watkins,Bharat Prakash","id":"fa5943a641a35d5a7afd545f5bdfd9e71cc56e1a","summary":"This work presents the solution that won first place and was awarded the most human-like agent in the 2021 NeurIPS Competition MineRL BASALT Challenge: Learning from Human Feedback in Minecraft, which challenged participants to use human data to solve four tasks defined only by a natural language description and no reward function.","score":3},{"url":"https://www.semanticscholar.org/paper/a1189ba5d86d32bc5fecd32ee905f8ff4767cbdb","title":"ASHA: Assistive Teleoperation via Human-in-the-Loop Reinforcement Learning","venue":"IEEE International Conference on Robotics and Automation","year":2022,"referenceCount":66,"citationCount":3,"influentialCitationCount":0,"publicationDate":"05/02/2022","authors":"S. Chen,Jensen Gao,S. Reddy,G. Berseth,A. Dragan,S. Levine","id":"a1189ba5d86d32bc5fecd32ee905f8ff4767cbdb","summary":"The results show that the method successfully learns to map 128-dimensional gaze features to 7-dimensional joint torques from sparse rewards in under 10 minutes of online training, and seamlessly helps users who employ different gaze strategies, while adapting to distributional shift in webcam inputs, tasks, and environments.","score":3},{"url":"https://www.semanticscholar.org/paper/2b46e8f2f2339e4dfbc8c3db33b7a6fb65ee62ad","title":"Deep Reinforcement Learning from Policy-Dependent Human Feedback","venue":"ArXiv","year":2019,"referenceCount":41,"citationCount":51,"influentialCitationCount":6,"publicationDate":"12/02/2019","authors":"Dilip Arumugam,Jun Ki Lee,S. Saskin,M. Littman","id":"2b46e8f2f2339e4dfbc8c3db33b7a6fb65ee62ad","summary":"The effectiveness of the Deep COACH algorithm is demonstrated in the rich 3D world of Minecraft with an agent that learns to complete tasks by mapping from raw pixels to actions using only real-time human feedback in 10-15 minutes of interaction.","score":3},{"url":"https://www.semanticscholar.org/paper/69b68b2d83989680854ffa2d12321028bfbdb722","title":"Robot Learning via Human Adversarial Games","venue":"IEEE/RJS International Conference on Intelligent RObots and Systems","year":2019,"referenceCount":37,"citationCount":7,"influentialCitationCount":1,"publicationDate":"02/03/2019","authors":"Jiali Duan,Qian Wang,Lerrel Pinto,C.-C. Jay Kuo,S. Nikolaidis","id":"69b68b2d83989680854ffa2d12321028bfbdb722","summary":"In a manipulation task, it is shown that grasping success improves significantly when the robot trains with a human adversary as compared to training in a self-supervised manner.","score":3},{"url":"https://www.semanticscholar.org/paper/bd6f723c0f38e3a55ccbff305a34b8deff68a615","title":"Learning Behaviors from a Single Video Demonstration Using Human Feedback","venue":"Adaptive Agents and Multi-Agent Systems","year":2019,"referenceCount":16,"citationCount":5,"influentialCitationCount":0,"publicationDate":"08/05/2019","authors":"S. Gandhi,T. Oates,T. Mohsenin,Nicholas R. Waytowich","id":"bd6f723c0f38e3a55ccbff305a34b8deff68a615","summary":"This paper uses human feedback to construct a mapping between the internal state representation of the agent and the visual representation from the video, and shows the effectiveness of this method by teaching a hopper agent in the MuJoCo simulator to perform a backflip.","score":3},{"url":"https://www.semanticscholar.org/paper/894d12eb96d05976954dcd1a1e9b6270ebb54b92","title":"Learning from Observations Using a Single Video Demonstration and Human Feedback","venue":"ArXiv","year":2019,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/09/2019","authors":"S. Gandhi,T. Oates,T. Mohsenin,Nicholas R. Waytowich","id":"894d12eb96d05976954dcd1a1e9b6270ebb54b92","summary":"This paper trains an autonomous agent using a single video demonstration and uses human feedback (using numerical similarity rating) to map the standard representation of the agent to the visual representation with a neural network.","score":3},{"url":"https://www.semanticscholar.org/paper/a87257e5ec7d4e20aed56b1dc726ab1dee1b2535","title":"Accelerating Human-Agent Collaborative Reinforcement Learning","venue":"Petra","year":2021,"referenceCount":17,"citationCount":3,"influentialCitationCount":0,"publicationDate":"29/06/2021","authors":"Fotios Lygerakis,M. Dagioglou,V. Karkaletsis","id":"a87257e5ec7d4e20aed56b1dc726ab1dee1b2535","summary":"This work uses a discrete Soft Actor-Critic agent on a real-time collaborative game with humans to examine how different allocations of on-line and off-line gradient updates impact the game performance and the total training time.","score":3},{"url":"https://www.semanticscholar.org/paper/6778d6a0f959cdcc42718ee9fc279fd1f00f3d88","title":"Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning","venue":"Journal of Artificial Intelligence Research","year":2020,"referenceCount":109,"citationCount":53,"influentialCitationCount":9,"publicationDate":"06/10/2020","authors":"Rodrigo Toro Icarte,Toryn Q. Klassen,R. Valenzano,Sheila A. McIlraith","id":"6778d6a0f959cdcc42718ee9fc279fd1f00f3d88","summary":"This paper proposes reward machines, a type of finite state machine that supports the specification of reward functions while exposing reward function structure, and describes different methodologies to exploit this structure to support learning, including automated reward shaping, task decomposition, and counterfactual reasoning with off-policy learning.","score":3},{"url":"https://www.semanticscholar.org/paper/243abd03e6fa267219d5afd82f387dddc0db26a3","title":"Accelerating the Learning of TAMER with Counterfactual Explanations","venue":"International Conference on Development and Learning","year":2021,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/08/2021","authors":"Jakob Karalus,F. Lindner","id":"243abd03e6fa267219d5afd82f387dddc0db26a3","summary":"This work extends the HRL framework TAMER for evaluative feedback with the possibility to enhance human feedback with two different types of counterfactual explanations (action and state based) to improve the speed of learning.","score":3},{"url":"https://www.semanticscholar.org/paper/18d750263f1dfe43374e8791cefa580a511c2098","title":"Few-Shot Preference Learning for Human-in-the-Loop RL","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":3,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Joey Hejna,Dorsa Sadigh","id":"18d750263f1dfe43374e8791cefa580a511c2098","summary":"Motivated by the success of metalearning, pre-train preference models on prior task data and quickly adapt them for new tasks using only a handful of queries, reducing the amount of online feedback needed to train manipulation policies in Meta-World by 20 ×, and demonstrating the effectiveness of this method on a real Franka Panda Robot.","score":3},{"url":"https://www.semanticscholar.org/paper/ecdba129b97570c03c83fd6ab5037d9fe771f1b7","title":"When is Realizability Sufficient for Off-Policy Reinforcement Learning?","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":1,"influentialCitationCount":1,"publicationDate":"10/11/2022","authors":"A. Zanette","id":"ecdba129b97570c03c83fd6ab5037d9fe771f1b7","summary":"These error bounds establish that oﬀ-policy reinforcement learning remains statistically viable even in absence of Bellman completeness, and characterize the intermediate situation between the favorable Bellman complete setting and the worst-case scenario where exponential lower bounds are in force.","score":3},{"url":"https://www.semanticscholar.org/paper/9fa8733e68ad7e6e9135800a66606eb07503ba37","title":"Deep Reinforcement Learning for Tehran Stock Trading","venue":"Journal of Novel Engineering Science and Technology","year":2022,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/11/2022","authors":"Neda Yousefi","id":"9fa8733e68ad7e6e9135800a66606eb07503ba37","summary":"Single stock trading models are presented based on the fine-tuned state-of-the-art deep reinforcement learning algorithms (Deep Deterministic Policy Gradient and Advantage Actor Critic) and shows that the agent designed based on both algorithms is able to make intelligent decisions on historical data.","score":3},{"url":"https://www.semanticscholar.org/paper/c64565f8deca0fad78dc5687a9ce7717e0f4fd22","title":"Applying Deep Reinforcement Learning to the HP Model for Protein Structure Prediction","venue":"Physica A: Statistical Mechanics and its Applications","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/11/2022","authors":"Kaiyuan Yang,Houjing Huang,Olafs Vandans,A. Murali,Fujia Tian,R. Yap,Liang Dai","id":"c64565f8deca0fad78dc5687a9ce7717e0f4fd22","summary":"Deep reinforcement learning is applied to the HP model for protein folding and it is demonstrated that a DQN based on long short-term memory (LSTM) architecture greatly enhances the RL learning ability and significantly improves the search process.","score":3},{"url":"https://www.semanticscholar.org/paper/8ab4acd34bebd1a901fa99ab0167f49be2a3ba74","title":"Real-time Bidding Strategy in Display Advertising: An Empirical Analysis","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Mengjuan Liu,Zhengning Hu,Zhi Lai,Daiwei Zheng,Xuyun Nie","id":"8ab4acd34bebd1a901fa99ab0167f49be2a3ba74","summary":"The problem and challenges of optimizing bidding strategies for individual advertisers in real-time bidding display advertising are described and several representative bidding strategies are introduced, especially the research advances andChallenges of reinforcement learning-based bidding strategies.","score":3},{"url":"https://www.semanticscholar.org/paper/0e872b44bf1d3abd03f3c882cc0f14a6b8c9374e","title":"A Dynamic Deep Reinforcement Learning-Bayesian Framework for Anomaly Detection","venue":"IEEE transactions on intelligent transportation systems (Print)","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Jeremy Watts,F. van Wyk,Shahrbanoo Rezaei,Yiyang Wang,Neda Masoud,Anahita Khojandi","id":"0e872b44bf1d3abd03f3c882cc0f14a6b8c9374e","summary":"A mathematical framework which utilizes a dynamic threshold for an anomaly classification algorithm in real-time in order to maximize the safety of a trip and shows that the POMDP model outperforms state-of-the-art benchmarks, especially under more difficult to detect anomaly profiles.","score":3},{"url":"https://www.semanticscholar.org/paper/2c8428e63bd02fae15dd72985678dd14e2982e48","title":"Learning to Transmit Fresh Information in Energy Harvesting Networks","venue":"IEEE Transactions on Green Communications and Networking","year":2022,"referenceCount":54,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Shiyang Leng,A. Yener","id":"2c8428e63bd02fae15dd72985678dd14e2982e48","summary":"Comparable AoI to the optimal is demonstrated and faster runtime of learning solvers is observed, verifying the efficacy of learning in terms of both optimality and computational energy efficiency for AoI-focused scheduling and resource allocation problems in wireless networks.","score":3},{"url":"https://www.semanticscholar.org/paper/0aec8ed21d4b34112679a39d7070f4503d61b9d8","title":"STL-Based Synthesis of Feedback Controllers Using Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/12/2022","authors":"Nikhil Kumar Singh,I. Saha","id":"0aec8ed21d4b34112679a39d7070f4503d61b9d8","summary":"This work proposes a new quantitative semantics for STL having several desirable properties, making it suitable for reward generation, and establishes its new semantics to be the most suitable for synthesizing feedback con- trollers for complex continuous dynamical systems through reinforcement learning.","score":3},{"url":"https://www.semanticscholar.org/paper/43d20b615cd65229d467ecb24e3f1eaba561d04e","title":"Smoothing Policy Iteration for Zero-sum Markov Games","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/12/2022","authors":"Yangang Ren,Yao Lyu,Wenxuan Wang,Sheng Li,Zeyang Li,Jingliang Duan","id":"43d20b615cd65229d467ecb24e3f1eaba561d04e","summary":"Results show that SPI can approximate the worst-case value function with a high accuracy and SaAC can stabilize the training process and improve the adversarial robustness in a large margin.","score":3},{"url":"https://www.semanticscholar.org/paper/52055da99bb0fdad22f8c11b96ca69f0d3d7f9e1","title":"Reinforced Approximate Exploratory Data Analysis","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Shaddy Garg,S. Mitra,Tong Yu,Yash Gadhia,Arjun Kashettiwar","id":"52055da99bb0fdad22f8c11b96ca69f0d3d7f9e1","summary":"A Deep Reinforcement Learning (DRL) based framework which can optimize the sample selection in order to keep the analysis and insight generation intact and can preserve the original insight generation while improving the interaction latency, compared to baseline methods.","score":3},{"url":"https://www.semanticscholar.org/paper/f0a0fb744c4764432062ad6305c97bfcc21e4f30","title":"On the Sample Complexity of Actor-Critic Method for Reinforcement Learning with Function Approximation","venue":"Machine-mediated learning","year":2019,"referenceCount":88,"citationCount":56,"influentialCitationCount":9,"publicationDate":"18/10/2019","authors":"Harshat Kumar,Alec Koppel,Alejandro Ribeiro","id":"f0a0fb744c4764432062ad6305c97bfcc21e4f30","summary":"This work puts forth a new variant of actor-critic that employs Monte Carlo rollouts during the policy search updates, which results in controllable bias that depends on the number of critic evaluations, providing insight into the interplay between optimization and generalization in reinforcement learning.","score":3},{"url":"https://www.semanticscholar.org/paper/3d45ef7217501a4e325809665d3d47949e041fa7","title":"DeepAPP: A Deep Reinforcement Learning Framework for Mobile Application Usage Prediction","venue":"IEEE Transactions on Mobile Computing","year":2019,"referenceCount":64,"citationCount":30,"influentialCitationCount":0,"publicationDate":"10/11/2019","authors":"Zhihao Shen,Kang Yang,Xi Zhao,Jianhua Zou,Wan Du","id":"3d45ef7217501a4e325809665d3d47949e041fa7","summary":"A deep reinforcement learning framework, named as DeepAPP, which learns a model-free predictive neural network from historical app usage data and reduces the prediction time of the state-of-the-art by 6.58× is presented.","score":3},{"url":"https://www.semanticscholar.org/paper/27ded4d32a8643b80545ef739a04528eeecf48fd","title":"Modelling penetration testing with reinforcement learning using capture‐the‐flag challenges: Trade‐offs between model‐free learning and a priori knowledge","venue":"IET Information Security","year":2020,"referenceCount":27,"citationCount":8,"influentialCitationCount":0,"publicationDate":"26/05/2020","authors":"Fabio Massimo Zennaro,L. Erdődi","id":"27ded4d32a8643b80545ef739a04528eeecf48fd","summary":"This paper focuses its attention on simpliﬁed penetration testing problems expressed in the form of capture the ﬂag hacking challenges, and analyzes how model-free reinforcement learning algorithms may help to solve them.","score":3},{"url":"https://www.semanticscholar.org/paper/8b452de8224ffadc5882da8de81c9dc7e16452fa","title":"Using Distributed Reinforcement Learning for Resource Orchestration in a Network Slicing Scenario","venue":"IEEE/ACM Transactions on Networking","year":2021,"referenceCount":53,"citationCount":3,"influentialCitationCount":0,"publicationDate":"17/05/2021","authors":"Federico Mason,G. Nencioni,A. Zanella","id":"8b452de8224ffadc5882da8de81c9dc7e16452fa","summary":"This paper attacks the Network Slicing problem by exploiting a Deep Reinforcement Learning approach, based on a distributed architecture, and shows that this approach yields better performance than both a static allocation of system resources and an efficient empirical strategy.","score":3},{"url":"https://www.semanticscholar.org/paper/288e5a79e73742f9ead3bfa9463891717414f6fd","title":"Addressing Hindsight Bias in Multigoal Reinforcement Learning","venue":"IEEE Transactions on Cybernetics","year":2021,"referenceCount":67,"citationCount":5,"influentialCitationCount":0,"publicationDate":"08/09/2021","authors":"Chenjia Bai,Lingxiao Wang,Yixin Wang,Zhaoran Wang,Rui Zhao,Chenyao Bai,Peng Liu","id":"288e5a79e73742f9ead3bfa9463891717414f6fd","summary":"The bias-corrected HER (BHER), an efficient algorithm that corrects the hindsight bias in training and outperforms several state-of-the-art multigoal RL approaches in challenging robotics tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a","title":"An information-theoretic perspective on intrinsic motivation in reinforcement learning: a survey","venue":"Entropy","year":2022,"referenceCount":193,"citationCount":3,"influentialCitationCount":0,"publicationDate":"19/09/2022","authors":"A. Aubret,L. Matignon,S. Hassas","id":"c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a","summary":"This work computationally revisit the notions of surprise, novelty, and skill-learning, and suggests that novelty and surprise can assist the building of a hierarchy of transferable skills which abstracts dynamics and makes the exploration process more robust.","score":3},{"url":"https://www.semanticscholar.org/paper/cb4bae3382fbccda5dc6eba7822feda858f5673f","title":"Design and Planning of Flexible Mobile Micro-Grids Using Deep Reinforcement Learning","venue":"Applied Energy","year":2022,"referenceCount":139,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"Cesare Caputo,M. Cardin,Pudong Ge,Fei Teng,A. Korre,E. A. Rio-Chanona","id":"cb4bae3382fbccda5dc6eba7822feda858f5673f","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/6e97544840a37042c55c53f2808614d39a066f93","title":"Deep Spectral Q-learning with Application to Mobile Health","venue":"ArXiv","year":2023,"referenceCount":84,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/01/2023","authors":"Yuhe Gao,C. Shi,R. Song","id":"6e97544840a37042c55c53f2808614d39a066f93","summary":"A deep spectral Q- learning algorithm is proposed, which integrates principal component analysis (PCA) with deep Q-learning to handle the mixed frequency data and proves that the mean return under the estimated optimal policy converges to that under the optimal one and establishes its rate of convergence.","score":3},{"url":"https://www.semanticscholar.org/paper/675f12b6b4e70995af040127fe3171c6405d8c95","title":"Why People Skip Music? On Predicting Music Skips using Deep Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Francesco Meggetto,C. Revie,J. Levine,Yashar Moshfeghi","id":"675f12b6b4e70995af040127fe3171c6405d8c95","summary":"Findings indicate that, overall, users’ behaviour features are the most discriminative in how the proposed DRL model predicts music skips, which suggests that a limited amount of user data should be collected and leveraged to predict skipping behaviour.","score":3},{"url":"https://www.semanticscholar.org/paper/fa6786bd22a1eb67a4e1f28719103ba0e0c2398d","title":"Planning for Learning Object Properties","venue":"ArXiv","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/01/2023","authors":"Leonardo Lamanna,L. Serafini,Mohamadreza Faridghasemnia,A. Saffiotti,A. Saetti,A. Gerevini,P. Traverso","id":"fa6786bd22a1eb67a4e1f28719103ba0e0c2398d","summary":"This paper formalizes the problem of automatically training a neural network to recognize object properties as a symbolic planning problem (using PDDL), and uses planning techniques to produce a strategy for automating the training dataset creation and the learning process.","score":3},{"url":"https://www.semanticscholar.org/paper/7235d95473b81585b4fc13e72fc77b2b7beca060","title":"Area-Driven FPGA Logic Synthesis Using Reinforcement Learning","venue":"Asia and South Pacific Design Automation Conference","year":2023,"referenceCount":13,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/01/2023","authors":"Guanglei Zhou,J. Anderson","id":"7235d95473b81585b4fc13e72fc77b2b7beca060","summary":"This work applies reinforcement learning (RL) to determine a unique recipe of algorithms for each circuit, and shows that the RL agent is able to generalize, and perform beneficial logic synthesis optimizations across a variety of circuits.","score":3},{"url":"https://www.semanticscholar.org/paper/ef751de5c8d41aedb39f931190bdaaff061c7803","title":"Toward Efficient Gradient-Based Value Estimation","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Arsalan Sharifnassab,R. Sutton","id":"ef751de5c8d41aedb39f931190bdaaff061c7803","summary":"To resolve the adverse effect of poor conditioning of MSBE on gradient based methods, a low complexity batch-free proximal method that approximately follows the Gauss-Newton direction and is asymptotically robust to parameterization is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/b6b89f6b99da3bab6cf5f6d376a140fd63c13efb","title":"Better Training of GFlowNets with Local Credit and Incomplete Trajectories","venue":"ArXiv","year":2023,"referenceCount":44,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"L. Pan,Nikolay Malkin,Dinghuai Zhang,Y. Bengio","id":"b6b89f6b99da3bab6cf5f6d376a140fd63c13efb","summary":"This paper considers the case where the energy function can be applied not just to terminal states but also to intermediate states, and shows how to reparameterize the GFlowNet state flow function to take advantage of the partial reward already accrued at each state.","score":3},{"url":"https://www.semanticscholar.org/paper/e0f98307096ac8bff1c3302096d5c0771b9a7a9f","title":"Deep Reinforcement Learning for Traffic Light Control in Intelligent Transportation Systems","venue":"ArXiv","year":2023,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/02/2023","authors":"Xiao-Yang Liu,Ming Zhu,S. Borst,A. Elwalid","id":"e0f98307096ac8bff1c3302096d5c0771b9a7a9f","summary":"This paper investigates deep reinforcement learning to control traffic lights, and both theoretical analysis and numerical experiments show that the intelligent behavior ``greenwave\" emerges naturally a grid road network, which is proved to be the optimal policy in an avenue with multiple cross streets.","score":3},{"url":"https://www.semanticscholar.org/paper/b75726d152584403d44a81d4de39ff8070736660","title":"Reinforcement learning approach to control an inverted pendulum: A general framework for educational purposes","venue":"PLoS ONE","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Sardor Israilov,Li Fu,J. Sánchez-Rodríguez,F. Fusco,Guillaume Allibert,C. Raufaste,M. Argentina","id":"b75726d152584403d44a81d4de39ff8070736660","summary":"This article proposes a general framework to reproduce successful experiments and simulations based on the inverted pendulum, a classic problem often used as a benchmark to evaluate control strategies and introduces two algorithms to give a comprehensive understanding of the approach and discuss its implementation on real systems.","score":3},{"url":"https://www.semanticscholar.org/paper/576a178c730490c4efdb4fc57049a683f6f5b8e3","title":"Quantum Computing Provides Exponential Regret Improvement in Episodic Reinforcement Learning","venue":"","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Bhargav Ganguly,Yulian Wu,Di Wang,V. Aggarwal","id":"576a178c730490c4efdb4fc57049a683f6f5b8e3","summary":"An UCB based quantum algorithmic framework to facilitate learning of a finite-horizon MDP and achieves an exponential improvement in regret as compared to the classical counterparts, key to the significant regret improvement in quantum reinforcement learning.","score":3},{"url":"https://www.semanticscholar.org/paper/c458282071810e433c841d572958113e4acdcca6","title":"RLQ: Workload Allocation With Reinforcement Learning in Distributed Queues","venue":"IEEE Transactions on Parallel and Distributed Systems","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/03/2023","authors":"Alessandro Staffolani,Victor-Alexandru Darvariu,P. Bellavista,Mirco Musolesi","id":"c458282071810e433c841d572958113e4acdcca6","summary":"This work formulate the task allocation problem in the Markov Decision Process framework, in which an agent assigns tasks to an available resource, and receives a numerical reward signal upon task completion, and implements and integrated with the popular Celery task queuing system for Python.","score":3},{"url":"https://www.semanticscholar.org/paper/9e6763597414d865237fdd065eed50bbc5ff14f5","title":"Limitations of Autoregressive Models and Their Alternatives","venue":"North American Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":53,"citationCount":10,"influentialCitationCount":2,"publicationDate":"23/10/2020","authors":"Chu-Cheng Lin,Aaron Jaech,Xin Li,M. Gormley,Jason Eisner","id":"9e6763597414d865237fdd065eed50bbc5ff14f5","summary":"Standard autoregressive language models perform only polynomial-time computation to compute the probability of the next symbol, which means they cannot model distributions whose next-symbol probability is hard to compute.","score":3},{"url":"https://www.semanticscholar.org/paper/46c585ee9abf76779ea4b863d2da4358efd0d1d3","title":"Adaptive Semiparametric Language Models","venue":"Transactions of the Association for Computational Linguistics","year":2021,"referenceCount":40,"citationCount":43,"influentialCitationCount":1,"publicationDate":"04/02/2021","authors":"Dani Yogatama,Cyprien de Masson d'Autume,Lingpeng Kong","id":"46c585ee9abf76779ea4b863d2da4358efd0d1d3","summary":"A language model that combines a large parametric neural network with a non-parametric episodic memory component in an integrated architecture is presented and a gating function to adaptively combine multiple information sources to make a prediction is designed.","score":3},{"url":"https://www.semanticscholar.org/paper/b61de520bc1ae57abde895601b62b4f92d82c0b4","title":"Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization","venue":"ArXiv","year":2021,"referenceCount":51,"citationCount":6,"influentialCitationCount":1,"publicationDate":"27/07/2021","authors":"Chiyuan Zhang,M. Raghu,J. Kleinberg,Samy Bengio","id":"b61de520bc1ae57abde895601b62b4f92d82c0b4","summary":"This paper introduces a novel benchmark, Pointer Value Retrieval (PVR) tasks, that explore the limits of neural network generalization, and demonstrates that this task structure provides a rich testbed for understanding generalization.","score":3},{"url":"https://www.semanticscholar.org/paper/4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80","title":"How BPE Affects Memorization in Transformers","venue":"ArXiv","year":2021,"referenceCount":67,"citationCount":10,"influentialCitationCount":0,"publicationDate":"06/10/2021","authors":"E. Kharitonov,Marco Baroni,D. Hupkes","id":"4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80","summary":"It is demonstrated that the size of the subword vocabulary learned by Byte-Pair Encoding greatly affects both ability and tendency of standard Transformer models to memorize training data, even when the authors control for the number of learned parameters.","score":3},{"url":"https://www.semanticscholar.org/paper/7a33cbd2381a9b353fb2006b1a759e884ff24cab","title":"∞-former: Infinite Memory Transformer","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":38,"citationCount":11,"influentialCitationCount":2,"publicationDate":2022,"authors":"Pedro Henrique Martins,Zita Marinho,André F. T. Martins","id":"7a33cbd2381a9b353fb2006b1a759e884ff24cab","summary":"The ∞-former is proposed, which extends the vanilla transformer with an unbounded long-term memory and is able to model arbitrarily long contexts and maintain “sticky memories” while keeping a fixed computation budget.","score":3},{"url":"https://www.semanticscholar.org/paper/b987eaf762f3126358b1a5ab6c381c9b1839fe9d","title":"Better Language Model with Hypernym Class Prediction","venue":"","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"He Bai","id":"b987eaf762f3126358b1a5ab6c381c9b1839fe9d","summary":"This study hypothesizes that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words and train large neural LMs by gradually annealing from predicting the class to token prediction during training.","score":3},{"url":"https://www.semanticscholar.org/paper/609e1c196fced582caf9113aa6a003b64d3cdcd6","title":"Better Language Model with Hypernym Class Prediction","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":39,"citationCount":2,"influentialCitationCount":0,"publicationDate":"21/03/2022","authors":"He Bai,Tong Wang,Alessandro Sordoni,Peng Shi","id":"609e1c196fced582caf9113aa6a003b64d3cdcd6","summary":"This study hypothesizes that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words and train large neural LMs by gradually annealing from predicting the class to token prediction during training.","score":3},{"url":"https://www.semanticscholar.org/paper/8ccd6ea80e3fb165213b95d5ff7e71b497befc0a","title":"A BERT-based Language Modeling Framework","venue":"Interspeech","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/09/2022","authors":"Chin-Yueh Chien,K. Chen","id":"8ccd6ea80e3fb165213b95d5ff7e71b497befc0a","summary":"A set of BERT-based language models are proposed, and a neural-based dynamic adaptation method is also introduced to combine these language models systematically and methodically to explore novel uses of a pre-trained model for language modeling.","score":3},{"url":"https://www.semanticscholar.org/paper/54abaf3108b7ca92e829f6797d5e73264c2350c1","title":"Nearest Neighbor Language Models for Stylistic Controllable Generation","venue":"IEEE Games Entertainment Media Conference","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Severino Trotta,Lucie Flek,Charles F Welch","id":"54abaf3108b7ca92e829f6797d5e73264c2350c1","summary":"It is found that style-specific datastores improve generation performance, though results vary greatly across styles, and the effect of pretraining data and specific styles should be explored in future work.","score":3},{"url":"https://www.semanticscholar.org/paper/37ba9c33025fb31f25436010e12c65a0bafc0e1f","title":"Meta-Learning Fast Weight Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Kevin Clark,Kelvin Guu,Ming-Wei Chang,Panupong Pasupat,G. Hinton,Mohammad Norouzi","id":"37ba9c33025fb31f25436010e12c65a0bafc0e1f","summary":"Fast Weight Layers are presented, a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention and can also be applied at training time, so the model learns to make good use of gradient updates.","score":3},{"url":"https://www.semanticscholar.org/paper/13388109ea0d1e332f11647f2d24655bfe08a5f3","title":"Robust Meta-Representation Learning via Global Label Inference and Classification","venue":"ArXiv","year":2022,"referenceCount":76,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/12/2022","authors":"Ruohan Wang,Isak Falk,M. Pontil,C. Ciliberto","id":"13388109ea0d1e332f11647f2d24655bfe08a5f3","summary":"This work discusses why pre-training yields more robust meta-representation and connects the theoretical analysis to existing works and empirical results, and introduces Meta Label Learning (MeLa), a novel meta-learning algorithm that learns task relations by inferring global labels across tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/ac608a4a6b19b3208e560eee5daadb3cc18638a2","title":"Efficient Attention via Control Variates","venue":"ArXiv","year":2023,"referenceCount":97,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/02/2023","authors":"Lin Zheng,Jianbo Yuan,Chong Wang,Lingpeng Kong","id":"ac608a4a6b19b3208e560eee5daadb3cc18638a2","summary":"This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity.","score":3},{"url":"https://www.semanticscholar.org/paper/755de63517a62ffdaeca33e3946e33354ea15920","title":"Open data and algorithms for open science in AI-driven molecular informatics","venue":"Current Opinion in Structural Biology","year":2023,"referenceCount":124,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/04/2023","authors":"Henning Otto Brinkhaus,Kohulan Rajan,Jonas Schaub,Achim Zielesny,Christoph Steinbeck","id":"755de63517a62ffdaeca33e3946e33354ea15920","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/8a7df164c4687e3c402b1cbf0ab404de49cfb75e","title":"Rissanen Data Analysis: Examining Dataset Characteristics via Description Length","venue":"International Conference on Machine Learning","year":2021,"referenceCount":107,"citationCount":11,"influentialCitationCount":2,"publicationDate":"05/03/2021","authors":"Ethan Perez,Douwe Kiela,Kyunghyun Cho","id":"8a7df164c4687e3c402b1cbf0ab404de49cfb75e","summary":"The method is called Rissanen Data Analysis (RDA) after the father of MDL, and its applicability on a wide variety of settings in NLP is showcased, ranging from evaluating the utility of generating subquestions before answering a question, to analyzing the value of rationales and explanations, to investigating the importance of different parts of speech, and uncovering dataset gender bias.","score":3},{"url":"https://www.semanticscholar.org/paper/1525c819ecaed99dad8117da1206eff3d470756d","title":"Benchmarks for Automated Commonsense Reasoning: A Survey","venue":"ArXiv","year":2023,"referenceCount":168,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/02/2023","authors":"E. Davis","id":"1525c819ecaed99dad8117da1206eff3d470756d","summary":"A survey of the development and uses of AI commonsense benchmarks, and it is argued that it is worthwhile to invest the work needed ensure that benchmark examples are consistently high quality.","score":3},{"url":"https://www.semanticscholar.org/paper/da7d67f9d3341754942a4c3f5da6eface2adf68e","title":"Template Grounded Examples High level advice Key Elements VariablesSelector Reactive LearnerScriptLesson","venue":"","year":2018,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":2018,"authors":"Eric,Yeh","id":"da7d67f9d3341754942a4c3f5da6eface2adf68e","summary":"This work presents experimental results for a deep reinforcement learning agent in a Minecraft-based game environment that show how such synthetic experiences improve performance, enabling the agent to achieve faster learning and higher rates of success.","score":3},{"url":"https://www.semanticscholar.org/paper/a5b66ee341cb990f7f70a124b5fab3316d3b7e27","title":"ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension","venue":"ArXiv","year":2018,"referenceCount":48,"citationCount":192,"influentialCitationCount":39,"publicationDate":"30/10/2018","authors":"Sheng Zhang,Xiaodong Liu,Jingjing Liu,Jianfeng Gao,Kevin Duh,Benjamin Van Durme","id":"a5b66ee341cb990f7f70a124b5fab3316d3b7e27","summary":"This work presents a large-scale dataset, ReCoRD, for machine reading comprehension requiring commonsense reasoning, and demonstrates that the performance of state-of-the-art MRC systems fall far behind human performance.","score":3},{"url":"https://www.semanticscholar.org/paper/c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge","venue":"North American Chapter of the Association for Computational Linguistics","year":2019,"referenceCount":43,"citationCount":517,"influentialCitationCount":144,"publicationDate":2019,"authors":"Alon Talmor,Jonathan Herzig,Nicholas Lourie,Jonathan Berant","id":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","summary":"This work presents CommonsenseQA: a challenging new dataset for commonsense question answering, which extracts from ConceptNet multiple target concepts that have the same semantic relation to a single source concept.","score":3},{"url":"https://www.semanticscholar.org/paper/303edbd86773b68432fa2ccb7c223aa22abe08b3","title":"Bridging the Gap: Converting Human Advice into Imagined Examples","venue":"","year":2019,"referenceCount":64,"citationCount":4,"influentialCitationCount":0,"publicationDate":2019,"authors":"Eric,Yeh","id":"303edbd86773b68432fa2ccb7c223aa22abe08b3","summary":"This work presents experimental results for a deep reinforcement learning agent in a Minecraft-based game environment that show how such synthetic experiences improve performance, enabling the agent to achieve faster learning and higher rates of success.","score":3},{"url":"https://www.semanticscholar.org/paper/570163353e0a954d9e5829f6ae92ae09c3225041","title":"Reinforcement Learning With Human Advice: A Survey","venue":"Frontiers in Robotics and AI","year":2020,"referenceCount":158,"citationCount":20,"influentialCitationCount":0,"publicationDate":"22/05/2020","authors":"Anis Najar,M. Chetouani","id":"570163353e0a954d9e5829f6ae92ae09c3225041","summary":"An overview of the existing methods for integrating human advice into a reinforcement learning process is provided and a taxonomy of the different forms of advice that can be provided to a learning agent is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/8c8fe7c49ad0c3fc513c1dbf282ad91c49d1616f","title":"Answer Complex Questions: Path Ranker Is All You Need","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2021,"referenceCount":54,"citationCount":7,"influentialCitationCount":2,"publicationDate":"11/07/2021","authors":"Xinyu Zhang,Ke Zhan,Enrui Hu,Chengzhen Fu,Lan Luo,Hao Jiang,Yantao Jia,Fan Yu,Zhicheng Dou,Zhao Cao,Lei Chen","id":"8c8fe7c49ad0c3fc513c1dbf282ad91c49d1616f","summary":"A purely rank-based framework Thinking Path Re-Ranker (TPRR), which is comprised of Thinking Path Ranker ( TPR) for generating document sequences called \"a path\" and External Path Reranker (E PR) for selecting the best path from candidate paths generated by TPR.","score":3},{"url":"https://www.semanticscholar.org/paper/f162b64756f01cdf04bc59c7592a77e4c8981656","title":"Question Answering over Knowledge Bases by Leveraging Semantic Parsing and Neuro-Symbolic Reasoning","venue":"ArXiv","year":2020,"referenceCount":46,"citationCount":22,"influentialCitationCount":0,"publicationDate":"03/12/2020","authors":"Pavan Kapanipathi,I. Abdelaziz,Srinivas Ravishankar,S. Roukos,Alexander G. Gray,Ramón Fernández Astudillo,Maria Chang,Cristina Cornelio,Saswati Dana,Achille Fokoue,Dinesh Garg,A. Gliozzo,Sairam Gurajada,Hima P. Karanam,Naweed Khan,Dinesh Khandelwal,Young-suk Lee,Yunyao Li,F. Luus,Ndivhuwo Makondo,Nandana Mihindukulasooriya,Tahira Naseem,S. Neelam,Lucian Popa,R. Reddy,Ryan Riegel,Gaetano Rossiello,Udit Sharma,G P Shrivatsa Bhargav,Mo Yu","id":"f162b64756f01cdf04bc59c7592a77e4c8981656","summary":"A semantic parsing and reasoning-based Neuro-Symbolic Question Answering system that achieves state-of-the-art performance on QALD-9 and LC-QuAD 1.0 and integrates multiple, reusable modules that are trained specifically for their individual tasks and do not require end-to-end training data.","score":3},{"url":"https://www.semanticscholar.org/paper/274b9cce94bc5ab6d30a62ecaeb424cce829af60","title":"Understanding Hard Negatives in Noise Contrastive Estimation","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":37,"citationCount":21,"influentialCitationCount":4,"publicationDate":"13/04/2021","authors":"Wenzheng Zhang,K. Stratos","id":"274b9cce94bc5ab6d30a62ecaeb424cce829af60","summary":"The contrastive loss is viewed as a biased estimator of the gradient of the cross-entropy loss, and it is shown both theoretically and empirically that setting the negative distribution to be the model distribution results in bias reduction.","score":3},{"url":"https://www.semanticscholar.org/paper/f5b64b831b2e8ef9143c94c4fd967061fc523b84","title":"BiBL: AMR Parsing and Generation with Bidirectional Bayesian Learning","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Ziming Cheng,Z. Li,Hai Zhao","id":"f5b64b831b2e8ef9143c94c4fd967061fc523b84","summary":"This work proposes data-efficient Bidirectional Bayesian learning (BiBL) to facilitate bidirectional information transition by adopting a single-stage multitasking strategy so that the resulting model may enjoy much lighter training at the same time.","score":3},{"url":"https://www.semanticscholar.org/paper/fad50ee368dac1a12c1b207f75bb3b647c896531","title":"Reasoning over Hybrid Chain for Table-and-Text Open Domain QA","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":7,"influentialCitationCount":2,"publicationDate":"15/01/2022","authors":"Wanjun Zhong,Junjie Huang,Qianchu Liu,Ming Zhou,Jiahai Wang,Jian Yin,Nan Duan","id":"fad50ee368dac1a12c1b207f75bb3b647c896531","summary":"A novel chain-centric pretraining method is proposed, to enhance the pre-trained model in identifying the cross-modality reasoning process and alleviating the data sparsity problem.","score":3},{"url":"https://www.semanticscholar.org/paper/5e793ce1e3003cd42144e45b7d8db06a60b58ca2","title":"Generative Multi-hop Retrieval","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":44,"citationCount":4,"influentialCitationCount":0,"publicationDate":"27/04/2022","authors":"Hyunji Lee,Sohee Yang,Hanseok Oh,Minjoon Seo","id":"5e793ce1e3003cd42144e45b7d8db06a60b58ca2","summary":"This paper proposes an encoder-decoder model that performs multi-hop retrieval by simply generating the entire text sequences of the retrieval targets, which means the query and the documents interact in the language model’s parametric space rather than L2 or inner product space as in the bi-encoder approach.","score":3},{"url":"https://www.semanticscholar.org/paper/1715aa36ccc851310308630d4db61dcecf49a50d","title":"Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering","venue":"ArXiv","year":2019,"referenceCount":41,"citationCount":72,"influentialCitationCount":8,"publicationDate":"10/11/2019","authors":"Sewon Min,Danqi Chen,Luke Zettlemoyer,Hannaneh Hajishirzi","id":"1715aa36ccc851310308630d4db61dcecf49a50d","summary":"This work introduces an approach for open-domain question answering (QA) that retrieves and reads a passage graph, where vertices are passages of text and edges represent relationships that are derived from an external knowledge base or co-occurrence in the same article.","score":3},{"url":"https://www.semanticscholar.org/paper/d703742740f8b6bec5173b6f0a8954f603cccd17","title":"Experimental IR Meets Multilinguality, Multimodality, and Interaction: 11th International Conference of the CLEF Association, CLEF 2020, Thessaloniki, Greece, September 22–25, 2020, Proceedings","venue":"Conference and Labs of the Evaluation Forum","year":2020,"referenceCount":509,"citationCount":1,"influentialCitationCount":0,"publicationDate":2020,"authors":"A. Arampatzis,E. Kanoulas,T. Tsikrika,S. Vrochidis,Hideo Joho,C. Lioma,Carsten Eickhoff,Aurélie Névéol,L. Cappellato,N. Ferro","id":"d703742740f8b6bec5173b6f0a8954f603cccd17","summary":"This paper presents SberQuAD – a large Russian reading comprehension (RC) dataset created similarly to English SQuAD, which contains about 50K question-paragraph-answer triples and is seven times larger compared to the next competitor.","score":3},{"url":"https://www.semanticscholar.org/paper/79cd9f77e5258f62c0e15d11534aea6393ef73fe","title":"Dense Passage Retrieval for Open-Domain Question Answering","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":55,"citationCount":1143,"influentialCitationCount":412,"publicationDate":"10/04/2020","authors":"Vladimir Karpukhin,Barlas Oğuz,Sewon Min,Patrick Lewis,Ledell Yu Wu,Sergey Edunov,Danqi Chen,Wen-tau Yih","id":"79cd9f77e5258f62c0e15d11534aea6393ef73fe","summary":"This work shows that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework.","score":3},{"url":"https://www.semanticscholar.org/paper/379b94a0df51bd3725312ce7b69c6fa57afd8da2","title":"Meta Adaptive Neural Ranking with Contrastive Synthetic Supervision","venue":"ArXiv","year":2020,"referenceCount":53,"citationCount":4,"influentialCitationCount":0,"publicationDate":2020,"authors":"Si Sun,Yingzhuo Qian,Zhenghao Liu,Chenyan Xiong,Kaitao Zhang,Jie Bao,Zhiyuan Liu,Paul Bennett","id":"379b94a0df51bd3725312ce7b69c6fa57afd8da2","summary":"This paper presents a meta-analyses of the reinforcement learning system called reinforcement learning, which automates the very labor-intensive and therefore time-heavy and expensive process of training reinforcement learning systems.","score":3},{"url":"https://www.semanticscholar.org/paper/1c8e2d3c887692ce0cb78b09685ea46be5cb2611","title":"An In-depth Analysis of Passage-Level Label Transfer for Contextual Document Ranking","venue":"ArXiv","year":2021,"referenceCount":65,"citationCount":3,"influentialCitationCount":0,"publicationDate":"30/03/2021","authors":"Koustav Rudra,Zeon Trevor Fernando,Avishek Anand","id":"1c8e2d3c887692ce0cb78b09685ea46be5cb2611","summary":"It is found that direct transfer of relevance labels from documents to passages introduces label noise that strongly affects retrieval effectiveness for large training datasets and query processing times are adversely affected by fine-grained splitting schemes.","score":3},{"url":"https://www.semanticscholar.org/paper/23a7260e39b1a1638053dd40d57c560850622bea","title":"Hybrid Encoder: Towards Efficient and Precise Native AdsRecommendation via Hybrid Transformer Encoding Networks","venue":"ArXiv","year":2021,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/04/2021","authors":"Junhan Yang,Zheng Liu,Bowen Jin,Jianxun Lian,Defu Lian,Akshay Soni,Eun Yong Kang,Yajun Wang,Guang-zhong Sun,Xing Xie","id":"23a7260e39b1a1638053dd40d57c560850622bea","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/fe935caed47ef090a306d6d09240f76adc43a420","title":"Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins","venue":"Proceedings of the VLDB Endowment","year":2021,"referenceCount":81,"citationCount":7,"influentialCitationCount":1,"publicationDate":"02/06/2021","authors":"S. Suri,I. Ilyas,Christopher R'e,Theodoros Rekatsinas","id":"fe935caed47ef090a306d6d09240f76adc43a420","summary":"Ember is proposed, a system that abstracts and automates keyless joins to generalize context enrichment and allows users to develop nocode pipelines for five domains, including search, recommendation and question answering, and can exceed alternatives by up to 39% recall, with as little as a single line configuration change.","score":3},{"url":"https://www.semanticscholar.org/paper/098354d529a26bd28a4371d02b110891f960a2ea","title":"Multi-level retrieval with semantic Axiomatic Fuzzy Set clustering for question answering","venue":"Applied Soft Computing","year":2021,"referenceCount":54,"citationCount":4,"influentialCitationCount":0,"publicationDate":"03/09/2021","authors":"Qi Lang,Xiaodong Liu,Yingjie Deng","id":"098354d529a26bd28a4371d02b110891f960a2ea","summary":"This paper proposes a coarse-to-fine unsupervised evidence sentences retrieval model based on the Axiomatic Fuzzy Sets clustering with both reasoning ability and interpretability for open-domain and multi-hop reading comprehension tasks that require complex multi-step reasoning processes.","score":3},{"url":"https://www.semanticscholar.org/paper/7a025a90cc36e668fb69f980ee809cd7d752854e","title":"Adaptive Information Seeking for Open-Domain Question Answering","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":39,"citationCount":17,"influentialCitationCount":1,"publicationDate":"14/09/2021","authors":"Yunchang Zhu,Liang Pang,Yanyan Lan,Huawei Shen,Xueqi Cheng","id":"7a025a90cc36e668fb69f980ee809cd7d752854e","summary":"A novel adaptive information-seeking strategy for open-domain question answering, namely AISO is proposed, which outperforms all baseline methods with predefined strategies in terms of both retrieval and answer evaluations.","score":3},{"url":"https://www.semanticscholar.org/paper/08433ddb8c799c00008bc71a6252ee473585f7e3","title":"Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":48,"citationCount":21,"influentialCitationCount":7,"publicationDate":"16/03/2022","authors":"Shuo Wang,Yichong Xu,Yuwei Fang,Yang Liu,S. Sun,Ruochen Xu,Chenguang Zhu,Michael Zeng","id":"08433ddb8c799c00008bc71a6252ee473585f7e3","summary":"Experimental results show that this simple method can achieve significantly better performance on a variety of NLU and NLG tasks, including summarization, machine translation, language modeling, and question answering tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/125b7393c02cbb61e77ce23673830e9523c1d5e3","title":"Clickbait Spoiling via Question Answering and Passage Retrieval","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/03/2022","authors":"Matthias Hagen,Maik Frobe,Artur Jurk,Martin Potthast","id":"125b7393c02cbb61e77ce23673830e9523c1d5e3","summary":"A large-scale evaluation and error analysis on a new corpus of 5,000 manually spoiled clickbait posts shows that the spoiler type classifier achieves an accuracy of 80%, while the question answering model DeBERTa-large outperforms all others in generating spoilers for both types.","score":3},{"url":"https://www.semanticscholar.org/paper/ea1cc2e43cd3d0622381f8b0fe5e84d989c2350e","title":"Multifaceted Improvements for Conversational Open-Domain Question Answering","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":1,"influentialCitationCount":1,"publicationDate":"01/04/2022","authors":"Tingting Liang,Yixuan Jiang,Congying Xia,Ziqiang Zhao,Yuyu Yin,Philip S. Yu","id":"ea1cc2e43cd3d0622381f8b0fe5e84d989c2350e","summary":"A framework with Multifaceted Improvements for Conversational open-domain Question Answering (MICQA), which has three significant advantages: first, the proposed KL-divergence based regularization is able to lead to a better question understanding for retrieval and answer reading, and second, the added post-ranker module can push more relevant passages to the top placements and be selected for reader with a two-aspect constrains.","score":3},{"url":"https://www.semanticscholar.org/paper/832aa77be74d7d3a8b33b9abc0f48b8b7babac12","title":"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval","venue":"International Conference on Machine Learning","year":2022,"referenceCount":111,"citationCount":18,"influentialCitationCount":3,"publicationDate":"27/05/2022","authors":"Pascal Notin,M. Dias,J. Frazer,Javier Marchena-Hurtado,Aidan N. Gomez,D. Marks,Y. Gal","id":"832aa77be74d7d3a8b33b9abc0f48b8b7babac12","summary":"Tranception is introduced, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art protein prediction performance, and ProteinGym is developed – an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks.","score":3},{"url":"https://www.semanticscholar.org/paper/ba41674cec00fb383663151a5809ae051b959e0d","title":"Improving Wikipedia Verifiability with AI","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/07/2022","authors":"Fabio Petroni,Samuel Broscheit,Aleksandra Piktus,Patrick Lewis,Gautier Izacard,Lucas Hosseini,Jane Dwivedi-Yu,M. Lomeli,Timo Schick,Pierre-Emmanuel Mazar'e,Armand Joulin,Edouard Grave,Sebastian Riedel","id":"ba41674cec00fb383663151a5809ae051b959e0d","summary":"A neural network based system, called Side, is developed to identify Wikipedia citations that are unlikely to support their claims, and subsequently recommend better ones from the web, and indicates that an AI-based system could be used, in tandem with humans, to improve the veriﬁability of Wikipedia.","score":3},{"url":"https://www.semanticscholar.org/paper/2959cb37db73490f75c678e8f8a93fa742ab868e","title":"LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":2,"influentialCitationCount":0,"publicationDate":"29/08/2022","authors":"Kai Zhang,Chongyang Tao,Tao Shen,Can Xu,Xiubo Geng,Binxing Jiao,Daxin Jiang","id":"2959cb37db73490f75c678e8f8a93fa742ab868e","summary":"This work proposes to make a dense retriever align a well-performing lexicon-aware representation model and finds its improve-ment is complementary to the standard ranker distillation, which can further lift state-of-the-art performance.","score":3},{"url":"https://www.semanticscholar.org/paper/0aadf207efbbc37e1c46ce22f859785b1cff86c4","title":"Pre-trained Language Model based Retrieval and Ranking for Web Search","venue":"ACM Transactions on the Web","year":2022,"referenceCount":112,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/10/2022","authors":"Lixin Zou,Weixue Lu,Yiding Liu,Hengyi Cai,Xiaokai Chu,Dehong Ma,Daiting Shi,Yu Sun,Zhicong Cheng,Simiu Gu,Shuaiqiang Wang,Dawei Yin","id":"0aadf207efbbc37e1c46ce22f859785b1cff86c4","summary":"Novel practices to perform expressive PLM-based semantic retrieval with a flexible poly-interaction scheme and cost-efficiently contextualize and rank web documents with a cheap yet powerful Pyramid-ERNIE architecture are presented.","score":3},{"url":"https://www.semanticscholar.org/paper/741a5536c2e58e1595e7396345f1e5d40a3aa775","title":"CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":3,"influentialCitationCount":0,"publicationDate":"18/11/2022","authors":"Minghan Li,Sheng-Chieh Lin,Barlas Oğuz,Asish Ghoshal,Jimmy Lin,Yashar Mehdad,Wen-tau Yih,Xilun Chen","id":"741a5536c2e58e1595e7396345f1e5d40a3aa775","summary":"CITADEL learns to route different token vectors to the predicted lexical “keys” such that a query token vector only interacts with document token vectors routed to the same key.","score":3},{"url":"https://www.semanticscholar.org/paper/929ecaf5e6f8f706c736e64222e06a552aea0934","title":"Adam: Dense Retrieval Distillation with Adaptive Dark Examples","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Chang Liu,Chongyang Tao,Xiubo Geng,Tao Shen,Dongyan Zhao,Can Xu,Binxing Jiao,Daxin Jiang","id":"929ecaf5e6f8f706c736e64222e06a552aea0934","summary":"A DAM, a knowledge distillation framework that can better transfer the dark knowledge held in the teacher with A daptive D ark ex AM ples, is proposed and a self-paced distillation strategy that adaptively con-centrates on a subset of high-quality instances to conduct the authors' dark-example-based knowledgedistillation to help the student learn better is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/f6e82e8fa03874817adc69f7d42a7b8d2022bb1a","title":"CONCRETE: Improving Cross-lingual Fact-checking with Cross-lingual Retrieval","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":39,"citationCount":2,"influentialCitationCount":0,"publicationDate":"05/09/2022","authors":"Kung-Hsiang Huang,ChengXiang Zhai,Heng Ji","id":"f6e82e8fa03874817adc69f7d42a7b8d2022bb1a","summary":"The proposed Cross-lingual Inverse Cloze Task (X-ICT), a self-supervised algorithm that creates training instances by translating the title of a passage, is presented, a first fact-checking framework augmented with cross-lingually retrieval that aggregates evidence retrieved from multiple languages through a cross-lingsual retriever.","score":3},{"url":"https://www.semanticscholar.org/paper/da4409b332cfec82229649d7fc69cb87209bddb6","title":"CatAlyst: Domain-Extensible Intervention for Preventing Task Procrastination Using Large Generative Models","venue":"ArXiv","year":2023,"referenceCount":86,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/02/2023","authors":"Riku Arakawa,Hiromu Yakura,Masataka Goto","id":"da4409b332cfec82229649d7fc69cb87209bddb6","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/492987781038027ccf869b6992f48eb14022bac2","title":"\"No, to the Right\" - Online Language Corrections for Robotic Manipulation via Shared Autonomy","venue":"ArXiv","year":2023,"referenceCount":51,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/01/2023","authors":"Yuchen Cui,Siddharth Karamcheti,Raj Palleti,Nidhya Shivakumar,Percy Liang,Dorsa Sadigh","id":"492987781038027ccf869b6992f48eb14022bac2","summary":"This work presents a framework for incorporating and adapting to natural language corrections – “to the right”, or “no, towards the book” – as the robot executes, and shows that this corrections-aware approach obtains higher task completion rates, and is subjectively preferred by users because of its reliability, precision, and ease of use.","score":3},{"url":"https://www.semanticscholar.org/paper/1076327ad3849b234abce6da9066dc61d05cdd90","title":"Multiview Compressive Coding for 3D Reconstruction","venue":"ArXiv","year":2023,"referenceCount":83,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/01/2023","authors":"Chaozheng Wu,Justin Johnson,J. Malik,Christoph Feichtenhofer,Georgia Gkioxari","id":"1076327ad3849b234abce6da9066dc61d05cdd90","summary":"This work introduces a simple framework that operates on 3D points of single objects or whole scenes coupled with category-agnostic large-scale training from diverse RGB-D videos and introduces a model that learns to compress the input appearance and geometry to predict the 3D structure by querying a 3D-aware decoder.","score":3},{"url":"https://www.semanticscholar.org/paper/becc067db5fc1e4d7f34bf680945c94335f51c3a","title":"Regeneration Learning: A Learning Paradigm for Data Generation","venue":"ArXiv","year":2023,"referenceCount":91,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/01/2023","authors":"Xu Tan,Tao Qin,Jiang Bian,Tie-Yan Liu,Y. Bengio","id":"becc067db5fc1e4d7f34bf680945c94335f51c3a","summary":"Regeneration learning can be a widely-used paradigm for data generation (e.g., text generation, speech recognition, speech synthesis, music composition, image generation, and video generation) and can provide valuable insights into developing data generation methods.","score":3},{"url":"https://www.semanticscholar.org/paper/0824e6f75e18325a79b11e3e4a118409e3297f97","title":"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts","venue":"ArXiv","year":2023,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/01/2023","authors":"Minghao Xu,Xinyu Yuan,Santiago Miret,Jian Tang","id":"0824e6f75e18325a79b11e3e4a118409e3297f97","summary":"The ProtDescribe dataset is built to augment protein sequences with text descriptions of their functions and other important properties, and the ProtST framework is proposed to enhance pre-training and understanding by biomedical scientists.","score":3},{"url":"https://www.semanticscholar.org/paper/a07b92339321bdc7604467333d6bdbeea69147a8","title":"InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt","venue":"ArXiv","year":2023,"referenceCount":75,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Dongchao Yang,Songxiang Liu,Rongjie Huang,Guangzhi Lei,Chao Weng,H. Meng,Dong Yu","id":"a07b92339321bdc7604467333d6bdbeea69147a8","summary":"This study attempts to use natural language as style prompt to control the styles in the synthetic speech, e.g. “Sigh tone in full of sad mood with some helpless feeling”, and proposes an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects.","score":3},{"url":"https://www.semanticscholar.org/paper/600d3ad507c581cd143f363d8d55042da53ef142","title":"UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers","venue":"ArXiv","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Dachuan Shi,Chaofan Tao,Ying Jin,Zhendong Yang,Chun Yuan,Jiaqi Wang","id":"600d3ad507c581cd143f363d8d55042da53ef142","summary":"Experiments on multiple generative and discriminative vision-language tasks, including Visual Reasoning, Image Caption, Visual Question Answer, Image-Text Retrieval, Text-Image Retrival, and Image Classiﬁcation, demonstrate the effectiveness and versatility of the proposed UPop framework.","score":3},{"url":"https://www.semanticscholar.org/paper/be148b81922706c831cfb0d48ea528af47c7acb4","title":"The geometry of hidden representations of large transformer models","venue":"ArXiv","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"L. Valeriani,Diego Doimo,F. Cuturello,A. Laio,A. Ansuini,A. Cazzaniga","id":"be148b81922706c831cfb0d48ea528af47c7acb4","summary":"It is shown that the semantic complexity of the dataset emerges at the end of the first peak, and is suggested to use the ID profile as an unsupervised proxy to identify the layers which are more suitable for downstream learning tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/ddce9cfee35ee4f36f2720c095cb33293f11f62e","title":"Learning to Scale Temperature in Masked Self-Attention for Image Inpainting","venue":"ArXiv","year":2023,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Xiang Zhou,Yuan Zeng,Yi Gong","id":"ddce9cfee35ee4f36f2720c095cb33293f11f62e","summary":"An image inpainting framework with a multi-head temperature masked self-attention mechanism, which provides stable and efficient temperature learning and uses multiple distant contextual information for high quality image inPainting is presented.","score":3},{"url":"https://www.semanticscholar.org/paper/f367ce68505e01d0452fe4be113e27f7a98c9d6d","title":"LAD: Language Augmented Diffusion for Reinforcement Learning","venue":"","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Pamela Mishkin","id":"f367ce68505e01d0452fe4be113e27f7a98c9d6d","summary":"This paper demonstrates the comparable performance of LAD with the state-of-the-art on the CALVIN language robotics benchmark with a much simpler architecture that contains no inductive biases special-ized to robotics, achieving an average success rate of 72% compared to the best performance of 76%.","score":3},{"url":"https://www.semanticscholar.org/paper/bbd181612924aaf94b12d3c74383538f1c80e68f","title":"Tackling AlfWorld with Action Attention and Common Sense from Pretrained LMs","venue":"","year":2022,"referenceCount":14,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yue Wu,So Yeon Min,Yonatan Bisk,R. Salakhutdinov,Shrimai Prabhumoye","id":"bbd181612924aaf94b12d3c74383538f1c80e68f","summary":"A novel question answering framework to simplify observations and an agent that handles arbitrary roll-out length and action space size based on action attention is implemented and achieved on the Alfworld benchmark for indoor instruction following.","score":3},{"url":"https://www.semanticscholar.org/paper/b7d27c5af2d314f6ec45b6d88984fb45220eb379","title":"Bootstrapped Transformer for Offline Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":5,"influentialCitationCount":2,"publicationDate":"17/06/2022","authors":"Kerong Wang,Hanye Zhao,Xufang Luo,Kan Ren,Weinan Zhang,Dongsheng Li","id":"b7d27c5af2d314f6ec45b6d88984fb45220eb379","summary":"This paper proposes a novel algorithm named Bootstrapped Transformer, which incorporates the idea of bootstrapping and leverages the learned model to self-generate more ofﬂine data to further boost the sequence model training.","score":3},{"url":"https://www.semanticscholar.org/paper/a9e6380ec7e0a9c336606e94e07705dee9391fef","title":"Robot Task Planning and Situation Handling in Open Worlds","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"04/10/2022","authors":"Yan Ding,Xiaohan Zhang,S. Amiri,Nieqing Cao,Hao Yang,Chad Esselink,Shiqi Zhang","id":"a9e6380ec7e0a9c336606e94e07705dee9391fef","summary":"A novel algorithm (COWP) for open-world task planning and situation handling that dynamically augments the robot’s action knowledge with task-oriented common sense based on the current task at hand and robot skills is introduced.","score":3},{"url":"https://www.semanticscholar.org/paper/a02ab0f0a5b00f506de5b826209a99fe33236c1b","title":"Learning Automata-Based Task Knowledge Representation from Large-Scale Generative Language Models","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/12/2022","authors":"Yunhao Yang,Jean-Raphael Gaglione,U. Topcu","id":"a02ab0f0a5b00f506de5b826209a99fe33236c1b","summary":"A novel algorithm named GLM2FSA is proposed, which obtains high-level task knowledge represented in a finite state automaton (FSA) from a given brief description of the task goal and can be directly utilized in formal verification.","score":3},{"url":"https://www.semanticscholar.org/paper/364f0dd472a79af2c647da70d3d3f8c1c4128f36","title":"A Song of Ice and Fire: Analyzing Textual Autotelic Agents in ScienceWorld","venue":"ArXiv","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"Laetitia Teodorescu,Eric Yuan,Marc-Alexandre Côté,P. Oudeyer","id":"364f0dd472a79af2c647da70d3d3f8c1c4128f36","summary":"The importance of selectivity from the social peer's feedback is shown; that experience replay needs to over-sample examples of rare goals; and that following self-generated goal sequences where the agent's competence is intermediate leads to significant improvements in final performance.","score":3},{"url":"https://www.semanticscholar.org/paper/bcaf0a460e5a2f1ac48902071652d570023219f4","title":"ON MEMORY IN HUMAN AND ARTIFICIAL LANGUAGE PROCESSING SYSTEMS","venue":"","year":2020,"referenceCount":51,"citationCount":13,"influentialCitationCount":0,"publicationDate":2020,"authors":"Aida Nematzadeh,Sebastian Ruder,Dani Yogatama","id":"bcaf0a460e5a2f1ac48902071652d570023219f4","summary":"The separation of computation and storage is considered as necessary, desired properties of the storage system are suggested, and the benefit of integrating different types of human memory into next-generation language processing systems are discussed.","score":3},{"url":"https://www.semanticscholar.org/paper/3410e91c55ec78e4ec94b6a56897945e136d7cd8","title":"Progress in Neural NLP: Modeling, Learning, and Reasoning","venue":"Engineering","year":2020,"referenceCount":110,"citationCount":60,"influentialCitationCount":2,"publicationDate":"01/03/2020","authors":"Ming Zhou,Nan Duan,Shujie Liu,H. Shum","id":"3410e91c55ec78e4ec94b6a56897945e136d7cd8","summary":"The importance of reasoning is emphasized in this paper because it is important for building interpretable and knowledge-driven neural NLP models to handle complex tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions","venue":"Neural Information Processing Systems","year":2020,"referenceCount":44,"citationCount":44,"influentialCitationCount":8,"publicationDate":"18/06/2020","authors":"Qian Liu,Shengnan An,Jian-Guang Lou,Bei Chen,Zeqi Lin,Yan Gao,Bin Zhou,Nanning Zheng,Dongmei Zhang","id":"336ee50043b916c9e932338c02fd1abc87a6e849","summary":"A refreshing view that connects a memory-augmented neural model with analytical expressions, to achieve compositional generalization is presented, fitting well with the cognitive argument while still being trained in an end-to-end manner via a hierarchical reinforcement learning algorithm.","score":3},{"url":"https://www.semanticscholar.org/paper/ec31d9dcdd984f9546a6d0ad3cecf33699247140","title":"with KNN-Based Composite Memory for Dialog","venue":"","year":2020,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/12/2020","authors":"Angela Fan,Claire Gardent,Chloé Braud,Antoine Bordes","id":"ec31d9dcdd984f9546a6d0ad3cecf33699247140","summary":"This work proposes augmenting generative Transformer neural networks with KNN-based Information Fetching modules, and applies these modules to generative dialog modeling, a challenging task where information must be flexibly retrieved and incorporated to maintain the topic and flow of conversation.","score":3},{"url":"https://www.semanticscholar.org/paper/d642868ce4325ebf3026c0aa0c497a079f112a8d","title":"On the Binding Problem in Artificial Neural Networks","venue":"ArXiv","year":2020,"referenceCount":393,"citationCount":110,"influentialCitationCount":13,"publicationDate":"09/12/2020","authors":"Klaus Greff,Sjoerd van Steenkiste,J. Schmidhuber","id":"d642868ce4325ebf3026c0aa0c497a079f112a8d","summary":"This paper proposes a unifying framework that revolves around forming meaningful entities from unstructured sensory inputs, maintaining this separation of information at a representational level (representation), and using these entities to construct new inferences, predictions, and behaviors (composition).","score":3},{"url":"https://www.semanticscholar.org/paper/8905f3dcd215fbc3d56839b6f52a43d77ac59fe8","title":"Augmenting Transformers with KNN-Based Composite Memory for Dialog","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2020,"referenceCount":51,"citationCount":47,"influentialCitationCount":6,"publicationDate":"27/04/2020","authors":"Angela Fan,Claire Gardent,Chloé Braud,Antoine Bordes","id":"8905f3dcd215fbc3d56839b6f52a43d77ac59fe8","summary":"This work proposes augmenting generative Transformer neural networks with KNN-based Information Fetching modules, and applies these modules to generative dialog modeling, a challenging task where information must be flexibly retrieved and incorporated to maintain the topic and flow of conversation.","score":3},{"url":"https://www.semanticscholar.org/paper/2c0a266f9cb88bb914c138ece0deaab8cf528f78","title":"Neural Sequence-to-grid Module for Learning Symbolic Rules","venue":"AAAI Conference on Artificial Intelligence","year":2021,"referenceCount":36,"citationCount":4,"influentialCitationCount":1,"publicationDate":"13/01/2021","authors":"Segwang Kim,Hyoungwook Nam,Joonyoung Kim,Kyomin Jung","id":"2c0a266f9cb88bb914c138ece0deaab8cf528f78","summary":"A neural sequence-to-grid (seq2grid) module, an input preprocessor that automatically segments and aligns an input sequence into a grid and enhances TextCNN to solve the bAbI QA tasks without external memory is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/64a29bee2e1ad29547d590a3cc26274f4c537145","title":"Not All Memories are Created Equal: Learning to Forget by Expiring","venue":"International Conference on Machine Learning","year":2021,"referenceCount":50,"citationCount":16,"influentialCitationCount":1,"publicationDate":"13/05/2021","authors":"Sainbayar Sukhbaatar,Da Ju,Spencer Poff,Stephen Roller,Arthur D. Szlam,J. Weston,Angela Fan","id":"64a29bee2e1ad29547d590a3cc26274f4c537145","summary":"It is demonstrated that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality, and it is shown that it trains faster and uses less memory.","score":3},{"url":"https://www.semanticscholar.org/paper/b1a49877c4f8636f0c51e09cccbf2777a6bf8c96","title":"Mutation Testing of Deep Reinforcement Learning Based on Real Faults","venue":"ArXiv","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/01/2023","authors":"Florian Tambon,Vahid Majdinasab,Amin Nikanjam,F. Khomh,G. Antoniol","id":"b1a49877c4f8636f0c51e09cccbf2777a6bf8c96","summary":"This paper builds on the existing approach of MT in order to propose a framework, RLMutation, for MT applied to RL and finds that even with a relatively small number of test cases and operators, it can generate HOM with interesting properties which can enhance testing capability in RL systems.","score":3},{"url":"https://www.semanticscholar.org/paper/d08a6a41e2b16928a1dc93b259bffbe37dae021d","title":"Synthesizing Adversarial Negative Responses for Robust Response Ranking and Evaluation","venue":"Findings","year":2021,"referenceCount":93,"citationCount":12,"influentialCitationCount":2,"publicationDate":"10/06/2021","authors":"Prakhar Gupta,Yulia Tsvetkov,Jeffrey P. Bigham","id":"d08a6a41e2b16928a1dc93b259bffbe37dae021d","summary":"This work proposes mask-and-fill and keyword-guided approaches that generate negative examples for training more robust dialogue systems and proposes approaches for automatically creating adversarial negative training data to help ranking and evaluation models learn features beyond content similarity.","score":3},{"url":"https://www.semanticscholar.org/paper/392b2698f147470348e40cb2426edbcd629c5037","title":"Narrative Incoherence Detection","venue":"ArXiv","year":2020,"referenceCount":53,"citationCount":3,"influentialCitationCount":0,"publicationDate":"21/12/2020","authors":"Deng Cai,Yizhe Zhang,Yichen Huang,Wai Lam,Bill Dolan","id":"392b2698f147470348e40cb2426edbcd629c5037","summary":"With pre-training on large-scale data and cycle-consistent sentence embedding, the extended sentence-level model can achieve comparable detection accuracy to the tokenlevel model and enables simultaneous incoherence detection and infilling/modification suggestions.","score":3},{"url":"https://www.semanticscholar.org/paper/da5d78b3e3a1544fde98fba86088e1215e97cbe8","title":"All NLP Tasks Are Generation Tasks: A General Pretraining Framework","venue":"ArXiv","year":2021,"referenceCount":37,"citationCount":29,"influentialCitationCount":3,"publicationDate":2021,"authors":"Zhengxiao Du,Yujie Qian,Xiao Liu,Ming Ding,J. Qiu,Zhilin Yang,Jie Tang","id":"da5d78b3e3a1544fde98fba86088e1215e97cbe8","summary":"This architecture has three major benefits: it performs well on classification, unconditional generation, and conditional generation tasks with one single pretrained model; it outperforms BERT-like models on classification due to improved pretrain-finetune consistency; and it naturally handles variable-length blank filling which is crucial for many downstream tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/ea45423c8f44025a276f6e88aafffbf87a898c89","title":"Language Model Augmented Relevance Score","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":67,"citationCount":7,"influentialCitationCount":2,"publicationDate":"19/08/2021","authors":"Ruibo Liu,Jason Wei,Soroush Vosoughi","id":"ea45423c8f44025a276f6e88aafffbf87a898c89","summary":"This paper proposes Language Model Augmented Relevance Score (MARS), a new context-aware metric for NLG evaluation that leverages off-the-shelf language models, guided by reinforcement learning, to create augmented references that consider both the generation context and available human references.","score":3},{"url":"https://www.semanticscholar.org/paper/3156551194f47dc1bd40c06ed3e716e2fb3a816c","title":"Visual Explanation of Deep Q-Network for Robot Navigation by Fine-tuning Attention Branch","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/08/2022","authors":"Yuya Maruyama,Hiroshi Fukui,Tsubasa Hirakawa,Takayoshi Yamashita,H. Fujiyoshi,K. Sugiura","id":"3156551194f47dc1bd40c06ed3e716e2fb3a816c","summary":"Experimental results with robot navigation task show that the proposed method can generate interpretable attention maps for a visual explanation of deep RL models.","score":3},{"url":"https://www.semanticscholar.org/paper/402f0232ed92ae6c3384767064352306db471db8","title":"Bridging Scenarios in Reinforcement Learning with Continuously Generated Relaying Predictive Models","venue":"2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","year":2022,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/08/2022","authors":"Kuo Li,Qing-Shan Jia","id":"402f0232ed92ae6c3384767064352306db471db8","summary":"Experimental results show that CRPM helps to avoid sub-optimal policies and outperforms other algorithms in both the source and target scenarios and helps to improve the classical model-free algorithm by considering it as a particular case of transfer learning in the same domain.","score":3},{"url":"https://www.semanticscholar.org/paper/c9c1864f29e9ce1f0ad22389694998e53c78df62","title":"Implementation of Quantum Deep Reinforcement Learning Using Variational Quantum Circuits","venue":"2022 International Conference on Trends in Quantum Computing and Emerging Business Technologies (TQCEBT)","year":2022,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"S. Lokes,C. S. J. Mahenthar,S. Kumaran,Palaniyappan Sathyaprakash,Vaithiyashankar Jayakumar","id":"c9c1864f29e9ce1f0ad22389694998e53c78df62","summary":"Variational Quantum Circuits (VQC) is explored for Deep Q network-based Reinforcement Learning by remodeling the target network and experience replay into a representation of VQC, and the reduction in model parameters is reduced to achieve better results than classical neural networks.","score":3},{"url":"https://www.semanticscholar.org/paper/8b4d4d0602ad72f26f3c16c03251240f80f3c83c","title":"DanZero: Mastering GuanDan Game with Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Yudong Lu,Jian Zhao,Youpeng Zhao,Wen-gang Zhou,Houqiang Li","id":"8b4d4d0602ad72f26f3c16c03251240f80f3c83c","summary":"This paper proposes the first AI program DanZero for GuanDan, whose rules are similar to DouDizhu but much more complicated, and trains it using a distributed framework and reveals the outstanding performance of DanZero.","score":3},{"url":"https://www.semanticscholar.org/paper/8cb7e8c8c95b03085ebe0447322481c71810baa5","title":"Approximate Optimal Filter Design for Vehicle System through Actor-Critic Reinforcement Learning","venue":"Automotive Innovation","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Yuming Yin,S. Li,Kaiming Tang,Wenhan Cao,Wei Wu,Hongbo Li","id":"8cb7e8c8c95b03085ebe0447322481c71810baa5","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/768c0c51da0b7627334e460c78f2f408722ca9bf","title":"Virtual Network Embedding with Virtual Nodes Ranking and Multi Points Sampling","venue":"International Conference on Advanced Cloud and Big Data","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Ying Yuan,Yichen Yang,Cong Wang","id":"768c0c51da0b7627334e460c78f2f408722ca9bf","summary":"A trainable virtual network node ranking method leveraging a graph neural network to provide a more reasonable virtual node mapping sequence and a multi-record point sampling strategy that can collect samples from multiple record points is designed to reduce the correlation of samples in the training set and obtain the global optimal solution in the embedding process.","score":3},{"url":"https://www.semanticscholar.org/paper/ba56b43c34d6161effed557880b10b5db074afca","title":"Variants of Bellman equation on reinforcement learning problems","venue":"Other Conferences","year":2022,"referenceCount":11,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Zhen-Yi Zhao","id":"ba56b43c34d6161effed557880b10b5db074afca","summary":"These algorithms include MonteCarlo method, Temporal-Difference learning, Sarsa algorithm, Q-learning algorithm, Deep Q-Networks, Hamilton-Jacobi-Bellman equation and others are reviewed and the complexity of each algorithm is compared.","score":3},{"url":"https://www.semanticscholar.org/paper/5b0f149e28d2373e77dc4686b74833c94360438b","title":"Roadmap of AlphaGo to AlphaStar: Problems and challenges","venue":"Other Conferences","year":2022,"referenceCount":13,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Yuhan Rong","id":"5b0f149e28d2373e77dc4686b74833c94360438b","summary":"This paper focuses on the in-depth analysis of the internal connections of Alpha series from the perspective of the problems and challenges solved to give an insight for the future development of reinforcement learning.","score":3},{"url":"https://www.semanticscholar.org/paper/a520a24ce27684e1fd87b3fe96e9b05a3e727dd5","title":"Applying Artificial Intelligence in Cryptocurrency Markets: A Survey","venue":"Algorithms","year":2022,"referenceCount":115,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"R. Amirzadeh,A. Nazari,D. Thiruvady","id":"a520a24ce27684e1fd87b3fe96e9b05a3e727dd5","summary":"This survey paper aims to review the current research trends in applications of supervised and reinforcement learning models in cryptocurrency price prediction and highlights potential research gaps and possible areas for improvement.","score":3},{"url":"https://www.semanticscholar.org/paper/2e7e6a00bdc3f8c11ab3b29314ff8180c57d245b","title":"Credit-cognisant reinforcement learning for multi-agent cooperation","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/11/2022","authors":"F. Bredell,H. Engelbrecht,J. C. Schoeman","id":"2e7e6a00bdc3f8c11ab3b29314ff8180c57d245b","summary":"This paper introduces the concept of credit-cognisant rewards (CCRs), which allows an agent to perceive the effect its actions had on the environment as well as on its co-agents, and shows that by manipulating these experiences and constructing the reward contained within them to include the rewards received by all the agents within the same action sequence, this paper is able to improve the performance of independent deep Q-learning aswell as deep recurrent Q- learning.","score":3},{"url":"https://www.semanticscholar.org/paper/f1574b6146699cf5000fda0619a625eb1cdc4ddd","title":"A Low Latency Adaptive Coding Spiking Framework for Deep Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Lang Qin,Rui Yan,Huajin Tang","id":"f1574b6146699cf5000fda0619a625eb1cdc4ddd","summary":"This work proposes the Adaptive Coding Spiking Framework (ACSF) for SNN-based DRL and achieves low latency and great energy efﬁciency at the same time.","score":3},{"url":"https://www.semanticscholar.org/paper/8ef263292d9863df2130182aeb13d1d5ed3ea199","title":"Explainable and Safe Reinforcement Learning for Autonomous Air Mobility","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/11/2022","authors":"Leihao Wang,Hongyu Yang,Yi Lin,S. Yin,Yuankai Wu","id":"8ef263292d9863df2130182aeb13d1d5ed3ea199","summary":"A fully explainable DRL framework wherein the coupled Q value learning model is decompose into a safety-awareness and efﬁciency one and the safety Q learning module provides rich information about the safety situation of environments.","score":3},{"url":"https://www.semanticscholar.org/paper/cb37811b39fe829f1e2691a15eaa6a9253ecc42a","title":"Continuous Episodic Control","venue":"ArXiv","year":2022,"referenceCount":16,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"Zhao Yang,Thomas M. Moerland,M. Preuss,Aske Plaat","id":"cb37811b39fe829f1e2691a15eaa6a9253ecc42a","summary":"Results on several sparse-reward continuous control environments show that the proposed CEC method learns faster than state-of-the-art model- free RL and memory-augmented RL algorithms, while main-taining good long-run performance as well.","score":3},{"url":"https://www.semanticscholar.org/paper/d141ccecf969606ea62ad286b97a233cd0147b94","title":"A Policy Optimization Algorithm Based on Sample Adaptive Reuse and Dual-Clipping for Robotic Action Control","venue":"SSRN Electronic Journal","year":2022,"referenceCount":15,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Li-yang Zhao,Tianqing Chang,J. Zhang,Lei Zhang,Kaixuan Chu,Libin Guo,D. Kong","id":"d141ccecf969606ea62ad286b97a233cd0147b94","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/f93e975281750006b9d12f483d1c7d1121830e08","title":"Kick-motion Training with DQN in AI Soccer Environment","venue":"ArXiv","year":2022,"referenceCount":14,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"B. Park,J. Lee,Taeyoung Kim,Dongsoo Har","id":"f93e975281750006b9d12f483d1c7d1121830e08","summary":"Using the RCS eliminates the necessity for the agent to know all the (state) information of entire soccer information and reduces the dimension of the state that the agent needs to know to perform kick-motion, and consequently alleviates COD.","score":3},{"url":"https://www.semanticscholar.org/paper/63689edecc451d08571f02603eb113a9477a2587","title":"Artificial intelligence meets radar resource management: A comprehensive background and literature review","venue":"IET Radar, Sonar &amp; Navigation","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"U. Hashmi,Sunila Akbar,R. Adve,P. Moo,Jack Ding","id":"63689edecc451d08571f02603eb113a9477a2587","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/73d12fae628e897a6c855f23ef1e5bb4b352839b","title":"Accelerating Self-Imitation Learning from Demonstrations via Policy Constraints and Q-Ensemble","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/12/2022","authors":"C. Li","id":"73d12fae628e897a6c855f23ef1e5bb4b352839b","summary":"A learning from demonstrations method named A-SILfD is proposed, which treats expert demonstrations as the agent’s successful experiences and uses experiences to constrain policy improvement and prevents performance degradation due to large estimation errors in the Q-function by the ensemble Q-functions.","score":3},{"url":"https://www.semanticscholar.org/paper/2e90470adf2f4d8ce4659602d6db8a191eeaf3b5","title":"Q-learning–based practical disturbance compensation control for hypersonic flight vehicle","venue":"Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Xu Li,Ziyi Zhang,Yuehui Ji,Junjie Liu,Qiang Gao","id":"2e90470adf2f4d8ce4659602d6db8a191eeaf3b5","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/c15794a8b381da90a46f137aab3387b66a215617","title":"Off-Policy Reinforcement Learning with Loss Function Weighted by Temporal Difference Error","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/12/2022","authors":"Bumgeun Park,Taeyoung Kim,Woohyeon Moon,L. Vecchietti,Dongsoo Har","id":"c15794a8b381da90a46f137aab3387b66a215617","summary":"A novel method is proposed that introduces a weighting factor for each experience when calculating the loss function at the learning stage and can be combined with prioritization methods for non-uniform sampling to improve sampling efficiency and increase the performance of TD-based off-policy RL algorithms.","score":3},{"url":"https://www.semanticscholar.org/paper/a7692a6748be022dfd9534310b339c15919a9f56","title":"Twin attentive deep reinforcement learning for multi-agent defensive convoy","venue":"International Journal of Machine Learning and Cybernetics","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/12/2022","authors":"Dongyu Fan,Haikuo Shen,Lijing Dong","id":"a7692a6748be022dfd9534310b339c15919a9f56","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/2b49156cf855dbb39768ae0ba7d7cb9263d17e5c","title":"Informed Machine Learning – A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems","venue":"IEEE Transactions on Knowledge and Data Engineering","year":2019,"referenceCount":198,"citationCount":208,"influentialCitationCount":11,"publicationDate":"29/03/2019","authors":"Laura von Rueden,S. Mayer,Katharina Beckh,B. Georgiev,Sven Giesselbach,R. Heese,Birgit Kirsch,Julius Pfrommer,Annika Pick,Rajkumar Ramamurthy,Michal Walczak,J. Garcke,C. Bauckhage,Jannis Schuecker","id":"2b49156cf855dbb39768ae0ba7d7cb9263d17e5c","summary":"A taxonomy is introduced that serves as a classification framework for informed machine learning approaches and considers the source of knowledge, its representation, and its integration into the machine learning pipeline.","score":3},{"url":"https://www.semanticscholar.org/paper/6110486edc5a9268cdde48c7c4f67a7782a7d61b","title":"Dynamic Programming Principles for Mean-Field Controls with Learning","venue":"Operational Research","year":2019,"referenceCount":54,"citationCount":8,"influentialCitationCount":0,"publicationDate":"17/11/2019","authors":"Haotian Gu,Xin Guo,Xiaoli Wei,Renyuan Xu","id":"6110486edc5a9268cdde48c7c4f67a7782a7d61b","summary":"It is shown that multiagent systems with mean-field approximation and learning can be recast as general forms of reinforcement learning problems, where the state variable is replaced by the probability distribution.","score":3},{"url":"https://www.semanticscholar.org/paper/51cc21e3e0b18171c40dcb886684ddc97a06ae20","title":"Self reward design with fine-grained interpretability","venue":"Scientific Reports","year":2021,"referenceCount":31,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/12/2021","authors":"Erico Tjoa,G. Cuntai","id":"51cc21e3e0b18171c40dcb886684ddc97a06ae20","summary":"The framework introduced in this paper is called the Self Reward Design (SRD), inspired by the Inverse Reward Design, and this interpretable design can solve the problem by pure design (although imperfectly) and be optimized like a standard DNN.","score":3},{"url":"https://www.semanticscholar.org/paper/5912f6344c91f27cb6b9384099b18b5b7d65903c","title":"Deep Reinforcement Learning for Preparation of Thermal and Prethermal Quantum States","venue":"Physical Review Applied","year":2022,"referenceCount":102,"citationCount":1,"influentialCitationCount":1,"publicationDate":"26/07/2022","authors":"Shotaro Z. Baba,N. Yoshioka,Y. Ashida,T. Sagawa","id":"5912f6344c91f27cb6b9384099b18b5b7d65903c","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/85b7b2cc34275d9eccb2518ff573b12ad1f537d5","title":"Inference and dynamic decision-making for deteriorating systems with probabilistic dependencies through Bayesian networks and deep reinforcement learning","venue":"Reliability Engineering &amp; System Safety","year":2022,"referenceCount":63,"citationCount":2,"influentialCitationCount":0,"publicationDate":"02/09/2022","authors":"P. G. Morato,C. Andriotis,K. Papakonstantinou,P. Rigo","id":"85b7b2cc34275d9eccb2518ff573b12ad1f537d5","summary":"A deep decentralized multi-agent actor-critic reinforcement learning approach, in which the policies are approximated by actor neural networks guided by a critic network, providing optimal management strategies directly at the system level.","score":3},{"url":"https://www.semanticscholar.org/paper/43ac57f2d339912e8cedd40ff7522c519ddadae8","title":"Goals, usefulness and abstraction in value-based choice","venue":"Trends in Cognitive Sciences","year":2022,"referenceCount":137,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"B. De Martino,A. Cortese","id":"43ac57f2d339912e8cedd40ff7522c519ddadae8","summary":"The computational and biological principles that enable the brain to compute the usefulness of an option or action by creating abstractions that flexibly adapt to changing goals are outlined.","score":3},{"url":"https://www.semanticscholar.org/paper/300d7d9f2c785ffc81f5eb1c9f7458a985929d05","title":"Transfer reinforcement learning method with multi-label learning for compound fault recognition","venue":"Advanced Engineering Informatics","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Zisheng Wang,Qing Zhang,Lv Tang,Tielin Shi,Jianping Xuan","id":"300d7d9f2c785ffc81f5eb1c9f7458a985929d05","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/9ee43603374933e597f04a34141d423cd7b8e92d","title":"Data Valuation Without Training of a Model","venue":"ArXiv","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/01/2023","authors":"Nohyun Ki,Hoyong Choi,Hye Won Chung","id":"9ee43603374933e597f04a34141d423cd7b8e92d","summary":"A training-free data valuation score, called complexity-gap score, is provided, which is a data-centric score to quantify the inﬂuence of individual instances in generalization of two-layer overparameterized neural networks and can quantify irregularity of the instances.","score":3},{"url":"https://www.semanticscholar.org/paper/d2dfa69439bab536c2355d9d80227ee7d0c8cb83","title":"Nondeterministic efficient cooling with a near-unit probability","venue":"","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/01/2023","authors":"Jia-shun Yan,J. Jing","id":"d2dfa69439bab536c2355d9d80227ee7d0c8cb83","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/ffacd7cdf3ed40dcfe24aa6cb5b9a72898301a70","title":"Value Enhancement of Reinforcement Learning via Efficient and Robust Trust Region Optimization","venue":"ArXiv","year":2023,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/01/2023","authors":"C. Shi,Zhengling Qi,Jianing Wang,Fan Zhou","id":"ffacd7cdf3ed40dcfe24aa6cb5b9a72898301a70","summary":"This paper proposes a novel value enhancement method to improve the performance of a given initial policy computed by existing state-of-the-art RL algorithms, and is generally applicable to any parametrized policy that belongs to certain pre-specified function class (e.g., deep neural networks).","score":3},{"url":"https://www.semanticscholar.org/paper/120b5bd6444cdbc8b91cfb16d304cfce59150dfe","title":"Planning Automated Driving with Accident Experience Referencing and Common-sense Inferencing","venue":"ArXiv","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/01/2023","authors":"Shaobo Qiu,Ji Li,Guoxi Chen,Hong Wang,Boqi Li","id":"120b5bd6444cdbc8b91cfb16d304cfce59150dfe","summary":"This work presents the concept of an Automated Driving Strategical Brain (ADSB): a framework of a scene perception and scene safety evaluation system that works at a higher abstraction level, incorporating experience referencing, common-sense inferring and goal-and-value judging capabilities, to provide a contextual perspective for decision making within automated driving planning.","score":3},{"url":"https://www.semanticscholar.org/paper/af069d9670df16a01c3d5df77baebdf42d9e256f","title":"Off-the-Grid MARL: a Framework for Dataset Generation with Baselines for Cooperative Offline Multi-Agent Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Claude Formanek,Asad Jeewa,J. Shock,Arnu Pretorius","id":"af069d9670df16a01c3d5df77baebdf42d9e256f","summary":"OG-MARL is released, a framework for generating offline multi-agent reinforcement learning datasets and algorithms that provide settings that are characteristic of real-world systems, including complex dynamics, non-stationarity, partial observability, suboptimality and sparse rewards.","score":3},{"url":"https://www.semanticscholar.org/paper/b3d09c2d9384dd78364d53e9b3411d6a99363648","title":"An integrated solution of deep reinforcement learning for automatic IMRT treatment planning in non-small-cell lung cancer","venue":"Frontiers in Oncology","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"Hanlin Wang,X. Bai,Yajuan Wang,Yanfei Lu,Binbing Wang","id":"b3d09c2d9384dd78364d53e9b3411d6a99363648","summary":"The feasibility of this integrated solution in automatic treatment planning based on the Eclipse TPS is demonstrated and contributes to improving the efficiency of the overall planning workflow and reducing the variation of plan quality in different regions and treatment centers.","score":3},{"url":"https://www.semanticscholar.org/paper/19c9c9393b6cd38ad800ef3369bcc544740ed9ea","title":"Helixer–de novo Prediction of Primary Eukaryotic Gene Models Combining Deep Learning and a Hidden Markov Model","venue":"bioRxiv","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/02/2023","authors":"Felix Holst,Anthony M. Bolger,Christopher Günther,Janina Maß,Sebastian Triesch,Felicitas Kindel,Niklas Kiel,N. Saadat,O. Ebenhöh,B. Usadel,R. Schwacke,Marie E. Bolger,A. Weber,Alisandra K. Denton","id":"19c9c9393b6cd38ad800ef3369bcc544740ed9ea","summary":"Helixer is a fully applicable, fast and user friendly tool for predicting primary gene models from DNA sequence alone and its quality is state-of-the-art, with predictions scoring closer by most measures to the references than to predictions from other de novo tools.","score":3},{"url":"https://www.semanticscholar.org/paper/b4023250016e424ec5df5207d8823a035e02daa0","title":"Temporal Video-Language Alignment Network for Reward Shaping in Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Ziyuan Cao,Reshma A. Ramachandra,K. Yu","id":"b4023250016e424ec5df5207d8823a035e02daa0","summary":"This work proposes a natural language-based reward shaping approach that maps trajectories from the Montezuma's Revenge game environment to corresponding natural language instructions using an extension of the LanguagE-Action Reward Network (LEARN) framework.","score":3},{"url":"https://www.semanticscholar.org/paper/6bbf2b090d3741068728d6cc30bc4cab15271360","title":"COACH: Cooperative Robot Teaching","venue":"","year":2023,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Cunjun Yu,Yiqing Xu,Linfeng Li,David Hsu","id":"6bbf2b090d3741068728d6cc30bc4cab15271360","summary":"This work formalizes cooperative robot teaching as a Markov game, consisting of four key elements: the target task, the student model, the teacher model, and the interactive teaching-learning process.","score":3},{"url":"https://www.semanticscholar.org/paper/e5e477903869e6dee6e0de5d10c7fb463bf4b471","title":"When neuro-robots go wrong: A review","venue":"Frontiers in Neurorobotics","year":2023,"referenceCount":160,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"M. S. Khan,J. Olds","id":"e5e477903869e6dee6e0de5d10c7fb463bf4b471","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/961e8d410bd263849a2f58c8028bed3d0fdf68c7","title":"Questions to Guide the Future of Artificial Intelligence Research","venue":"ArXiv","year":2019,"referenceCount":115,"citationCount":3,"influentialCitationCount":0,"publicationDate":"21/12/2019","authors":"J. Ott","id":"961e8d410bd263849a2f58c8028bed3d0fdf68c7","summary":"This article proposes leading questions to guide the future of artificial intelligence research and states that there are clear computational principles on which the brain operates but the problem is finding these computational needles in a haystack of biological complexity.","score":3},{"url":"https://www.semanticscholar.org/paper/778620f68c2dadfa6154fe20a2e771945fda0ec5","title":"Embedding Synthetic Off-Policy Experience for Autonomous Driving via Zero-Shot Curricula","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":1,"influentialCitationCount":0,"publicationDate":"02/12/2022","authors":"Eli Bronstein,S. Srinivasan,Supratik Paul,Aman Sinha,Matthew O'Kelly,Payam Nikdel,Shimon Whiteson","id":"778620f68c2dadfa6154fe20a2e771945fda0ec5","summary":"A method to predict the inherent difﬁculty of a driving situation given data collected from a set of autonomous vehicles deployed on public roads is presented and can be used in a zero-shot transfer to generate curricula for an imitation-learning based planning agent.","score":3},{"url":"https://www.semanticscholar.org/paper/5bb567f9b6ecdac86345495871636fde48adc410","title":"Off-Policy Deep Reinforcement Learning Algorithms for Handling Various Robotic Manipulator Tasks","venue":"ArXiv","year":2022,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/12/2022","authors":"Altun Rzayev,Vahid Tavakol Aghaei","id":"5bb567f9b6ecdac86345495871636fde48adc410","summary":"Three reinforcement learning algorithms; DDPG, TD3 and SAC have been used to train Fetch robotic manipulator for four diﬀerent tasks in MuJoCo simulation environment and the e-ciency and the speed of these three algorithms are analyzed in a controlled environment.","score":3},{"url":"https://www.semanticscholar.org/paper/6af2b4051ae785501674c26f3bf89313e8849814","title":"How to Train your Decision-Making AIs","venue":"","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Ruohan Zhang,Dhruva Bansal","id":"6af2b4051ae785501674c26f3bf89313e8849814","summary":"These are examples of sequential decision tasks, in which the AI agent needs to make a sequence of decisions to achieve its goal.","score":3},{"url":"https://www.semanticscholar.org/paper/7b6dd8542906a8014ba4924efdc7213d5e0a0711","title":"How to Train Your Agent: Active Learning from Human Preferences and Justifications in Safety-critical Environments","venue":"Adaptive Agents and Multi-Agent Systems","year":2022,"referenceCount":23,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Ilias Kazantzidis,T. Norman,Yali Du,Christopher T. Freeman","id":"7b6dd8542906a8014ba4924efdc7213d5e0a0711","summary":"This model, JPAL-HA, proposes an efficient mechanism to harness human preferences and justifications to significantly improve safety during the learning process without increasing the number of interactions with a user.","score":3},{"url":"https://www.semanticscholar.org/paper/4ec50cee2f70e97c00d294985a924a56081a8573","title":"Democratizing RL Research by Reusing Prior Computation","venue":"","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"4ec50cee2f70e97c00d294985a924a56081a8573","summary":"As deep RL research move towards more complex and challenging benchmarks, the computational barrier to entry in RL research would be even substantially higher, due to the inefficiency of tabula rasa RL.","score":3},{"url":"https://www.semanticscholar.org/paper/7cfa93c3d646d57c4a1f52538549db474877df0c","title":"What Can Knowledge Bring to Machine Learning?—A Survey of Low-shot Learning for Structured Data","venue":"ACM Transactions on Intelligent Systems and Technology","year":2021,"referenceCount":343,"citationCount":6,"influentialCitationCount":2,"publicationDate":"11/06/2021","authors":"Yang Hu,Adriane P. Chapman,Guihua Wen,Dame Wendy Hall","id":"7cfa93c3d646d57c4a1f52538549db474877df0c","summary":"The fundamental factors of low-shot learning technologies are reviewed, with a focus on the operation of structured knowledge under different low- shot conditions, and the prospects and gaps of industrial applications and future research directions are pointed out.","score":3},{"url":"https://www.semanticscholar.org/paper/1c35807e1a4c24e2013fa0a090cee9cc4716a5f5","title":"Reinforcement Learning with Action-Free Pre-Training from Videos","venue":"International Conference on Machine Learning","year":2022,"referenceCount":117,"citationCount":20,"influentialCitationCount":0,"publicationDate":"25/03/2022","authors":"Younggyo Seo,Kimin Lee,Stephen James,P. Abbeel","id":"1c35807e1a4c24e2013fa0a090cee9cc4716a5f5","summary":"A framework that learns representations useful for understanding the dynamics via generative pretraining on videos that improves both performances and sample-efﬁciency of vision-based RL in a variety of manipulation and locomotion tasks is introduced.","score":3},{"url":"https://www.semanticscholar.org/paper/ed04974e0ba9556291d9645221d4905fb0e32838","title":"DouZero+: Improving DouDizhu AI by Opponent Modeling and Coach-guided Learning","venue":"2022 IEEE Conference on Games (CoG)","year":2022,"referenceCount":45,"citationCount":2,"influentialCitationCount":0,"publicationDate":"06/04/2022","authors":"Youpeng Zhao,Jian Zhao,Xu Hu,Wen-gang Zhou,Houqiang Li","id":"ed04974e0ba9556291d9645221d4905fb0e32838","summary":"The integration of the above two techniques into DouZero, the DouDizhu AI system achieves better performance and ranks top in the Botzone leaderboard among more than 400 AI agents, including DouZero.","score":3},{"url":"https://www.semanticscholar.org/paper/b1362b8a11b1984378b91162195e10e369f6b012","title":"Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation","venue":"International Conference on Machine Learning","year":2022,"referenceCount":53,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Xiaoyu Chen,Han Zhong,Zhuoran Yang,Zhaoran Wang,Liwei Wang","id":"b1362b8a11b1984378b91162195e10e369f6b012","summary":"This paper proposes the ﬁrst optimistic model-based algorithm for PbRL with general function approximation, which estimates the model using value-targeted regression and calculates the exploratory policies by solving an optimistic planning problem.","score":3},{"url":"https://www.semanticscholar.org/paper/470bd6813d303d4ea776def788d215caaa8e772f","title":"History Compression via Language Models in Reinforcement Learning","venue":"International Conference on Machine Learning","year":2022,"referenceCount":103,"citationCount":6,"influentialCitationCount":0,"publicationDate":"24/05/2022","authors":"Fabian Paischer,Thomas Adler,Vihang Patil,Angela Bitto-Nemling,Markus Holzleitner,S. Lehner,Hamid Eghbal-zadeh,S. Hochreiter","id":"470bd6813d303d4ea776def788d215caaa8e772f","summary":"The new method, HELM, enables actor-critic network architectures that contain a pretrained language Transformer for history representation as a memory module that is much more sample efﬁcient than competitors.","score":3},{"url":"https://www.semanticscholar.org/paper/3dfa5b1cc516cb0a8d8f3e435f42edd38c9ae1ad","title":"Performative Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":58,"citationCount":2,"influentialCitationCount":0,"publicationDate":"30/06/2022","authors":"Debmalya Mandal,Stelios Triantafyllou,Goran Radanovic","id":"3dfa5b1cc516cb0a8d8f3e435f42edd38c9ae1ad","summary":"This work considers a regularized version of the reinforcement learning problem and shows that repeatedly optimizing this objective converges to a performatively stable policy under reasonable assumptions on the transition dynamics.","score":3},{"url":"https://www.semanticscholar.org/paper/5718a058c096adb0fd92829d202adffde30df771","title":"Learning Task Embeddings for Teamwork Adaptation in Multi-Agent Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":2,"influentialCitationCount":0,"publicationDate":"05/07/2022","authors":"Lukas Schäfer,Filippos Christianos,A. Storkey,Stefano V. Albrecht","id":"5718a058c096adb0fd92829d202adffde30df771","summary":"This work discusses the problem of teamwork adaptation in which a team of agents needs to adapt their policies to solve novel tasks with limited limitedtuning and proposes three MATE training paradigms: independent MATE, centralised MATES, and mixed MATE which vary in the information used for the task encoding.","score":3},{"url":"https://www.semanticscholar.org/paper/02cf7fa57f11add4a45f27e549ded4449c24553c","title":"Ablation Study of How Run Time Assurance Impacts the Training and Performance of Reinforcement Learning Agents","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/07/2022","authors":"Nathaniel Hamilton,Kyle Dunlap,Taylor T. Johnson,Kerianne L. Hobbs","id":"02cf7fa57f11add4a45f27e549ded4449c24553c","summary":"By studying multiple RTA approaches in both on-policy and o-policy RL algorithms, this work seeks to understand which RTA methods are most e-ective, whether the agents become dependent on the RTA, and the importance of reward shaping versus safe exploration in RL agent training.","score":3},{"url":"https://www.semanticscholar.org/paper/420a3be9533a232cab0dfb28185b4619fae919f5","title":"The applicability of reinforcement learning for the automatic generation of state preparation circuits","venue":"GECCO Companion","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/07/2022","authors":"Thomas Gabor,Maximilian Zorn,C. Linnhoff-Popien","id":"420a3be9533a232cab0dfb28185b4619fae919f5","summary":"This work approaches the problem of state preparation using reinforcement learning on an agent that is trained to only prepare a single fixed quantum state, and then trains a single network to prepare arbitrary quantum states to some degree of success, despite a complete lack of structure in the training data set.","score":3},{"url":"https://www.semanticscholar.org/paper/1fc6c125450e6ee549e8617da0cdc216dacdc7d0","title":"The Game of Hidden Rules: A New Kind of Benchmark Challenge for Machine Learning","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/07/2022","authors":"Eric Pulick,S. Bharti,Yiding Chen,V. Menkov,Yonatan Dov Mintz,Paul Kantor,Vicki M. Bier","id":"1fc6c125450e6ee549e8617da0cdc216dacdc7d0","summary":"The Game of Hidden Rules (GOHR) is introduced, a novel benchmark environment that offers an enormous range of ML challenges and enables precise examination of how task elements inﬂuence practical difﬁculty contribute to overall performance for the machine learner.","score":3},{"url":"https://www.semanticscholar.org/paper/046ba0b7c12126e0a4e75dac6eacdcc864907d28","title":"Flowsheet synthesis through hierarchical reinforcement learning and graph neural networks","venue":"ArXiv","year":2022,"referenceCount":68,"citationCount":5,"influentialCitationCount":0,"publicationDate":"25/07/2022","authors":"Laura Stops,Roel Leenhouts,Qitong Gao,Artur M. Schweidtmann","id":"046ba0b7c12126e0a4e75dac6eacdcc864907d28","summary":"A reinforcement learning algorithm for chemical process design based on a state-of-the-art actor-critic logic that represents chemical processes as graphs and uses graph convolutional neural networks to learn from process graphs is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/c5d49a31eede6a10ff95e55468701afe4b977d81","title":"Autonomous Vehicles Roundup Strategy by Reinforcement Learning with Prediction Trajectory","venue":"Cybersecurity and Cyberforensics Conference","year":2022,"referenceCount":10,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/07/2022","authors":"Jiayang Ni,Rubing Ma,Hua Zhong,Bo Wang","id":"c5d49a31eede6a10ff95e55468701afe4b977d81","summary":"Simulation experiments show that the convergence speed and win rate of MADDPG algorithm based on trajectory prediction and artificial potential field is significantly improved, and it also has strong adaptability to various task scenarios.","score":3},{"url":"https://www.semanticscholar.org/paper/becdb3a4ec34089ac8f34b647aa92365ba901db4","title":"Contrastive UCB: Provably Efficient Contrastive Self-Supervised Learning in Online Reinforcement Learning","venue":"International Conference on Machine Learning","year":2022,"referenceCount":57,"citationCount":4,"influentialCitationCount":0,"publicationDate":"29/07/2022","authors":"Shuang Qiu,Lingxiao Wang,Chenjia Bai,Zhuoran Yang,Zhaoran Wang","id":"becdb3a4ec34089ac8f34b647aa92365ba901db4","summary":"This work provides the first provably efficient online RL algorithm that incorporates contrastive learning for representation learning, and theoretically proves that the algorithm recovers the true representations and simultaneously achieves sample efficiency in learning the optimal policy and Nash equilibrium in MDPs and MGs.","score":3},{"url":"https://www.semanticscholar.org/paper/a3ebbc1349916276812e5ded51211184a4013f7b","title":"Hierarchical Multi-agent Model for Reinforced MedicalResource Allocation with Imperfect Information","venue":"ACM Transactions on Intelligent Systems and Technology","year":2022,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/07/2022","authors":"Qianyue Hao,Fengli Xu,Lin Chen,Pan Hui,Yong Li","id":"a3ebbc1349916276812e5ded51211184a4013f7b","summary":"A hierarchical reinforcement learning framework with several specially designed components is proposed, which design an recurrent neural network based framework to utilize the imperfect information obtained from the environment and a multi-agents voting method, which modifies the decision making process considering the randomness during model training and thus improves the performance.","score":3},{"url":"https://www.semanticscholar.org/paper/cdb4a04a095eb7e2c2aa67aec17602aa599fe3c5","title":"Optimistic MLE - A Generic Model-based Algorithm for Partially Observable Sequential Decision Making","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":3,"influentialCitationCount":2,"publicationDate":"29/09/2022","authors":"Qinghua Liu,Praneeth Netrapalli,Csaba Szepesvari,Chi Jin","id":"cdb4a04a095eb7e2c2aa67aec17602aa599fe3c5","summary":"It is proved that OMLE learns the near-optimal policies of an enormously rich class of sequential decision making problems in a polynomial number of samples, which unifies the existing understandings of model-based RL in both fully observable and partially observable settings.","score":3},{"url":"https://www.semanticscholar.org/paper/3997273c28eda260784e1a35c763f74de892c166","title":"Skill-Based Reinforcement Learning with Intrinsic Reward Matching","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Ademi Adeniji,Amber Xie,P. Abbeel","id":"3997273c28eda260784e1a35c763f74de892c166","summary":"Intrinsic Reward Matching (IRM), which uniﬁes these two phases of learning via the skill discriminator, is presented, and is demonstrated that IRM is competitive with previous skill selection methods on the Unsupervised Reinforcement Learning Benchmark and enables to utilize pretrained skills far more effectively on challenging tabletop manipulation tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/b6035d63a6c26faca2be4f6f4a040bb95ce11705","title":"CUP: Critic-Guided Policy Reuse","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/10/2022","authors":"Jin Zhang,Siyuan Li,Chongjie Zhang","id":"b6035d63a6c26faca2be4f6f4a040bb95ce11705","summary":"A novel policy reuse algorithm called Critic-gUided Policy reuse (CUP), which avoids training any extra components and efﬁciently reuses source policies and outperforms baseline algorithms.","score":3},{"url":"https://www.semanticscholar.org/paper/203a1c0e5025489a52c030adbbc102a787685ee6","title":"Unpacking Reward Shaping: Understanding the Benefits of Reward Engineering on Sample Complexity","venue":"ArXiv","year":2022,"referenceCount":52,"citationCount":3,"influentialCitationCount":1,"publicationDate":"18/10/2022","authors":"Abhishek Gupta,Aldo Pacchiano,Yuexiang Zhai,S. Kakade,S. Levine","id":"203a1c0e5025489a52c030adbbc102a787685ee6","summary":"An insight into the mechanisms through which reward shaping can signiﬁcantly improve the complexity of reinforcement learning while retaining asymptotic performance is provided.","score":3},{"url":"https://www.semanticscholar.org/paper/11aead34bac158d61eff1e78f23350f3e7ffb461","title":"Evaluation of Neural Network Verification Methods for Air-to-Air Collision Avoidance","venue":"Journal of Air Transportation","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Diego Manzanas Lopez,Taylor T. Johnson,Stanley Bak,Hoang-Dung Tran,Kerianne L. Hobbs","id":"11aead34bac158d61eff1e78f23350f3e7ffb461","summary":"A new closed loop extension of this benchmark, which consists of a set of ten closed loop properties selected to evaluate the safety of an ownship aircraft in the presence of a co-altitude intruder aircraft, is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/cd4bb728ba816c8e3721c5f45e784399db2eb699","title":"Adversarial Policies Beat Superhuman Go AIs","venue":"","year":2022,"referenceCount":50,"citationCount":1,"influentialCitationCount":1,"publicationDate":"01/11/2022","authors":"Tony Tong Wang,A. Gleave,Nora Belrose,Tom Tseng,Joseph Miller,Kellin Pelrine,Michael Dennis,Yawen Duan,V. Pogrebniak,S. Levine,Stuart Russell","id":"cd4bb728ba816c8e3721c5f45e784399db2eb699","summary":"This work attacks the state-of-the-art Go-playing AI system, Kata go, by training adversarial policies that play against frozen KataGo victims, demonstrating that even superhuman AI systems may harbor surprising failure modes.","score":3},{"url":"https://www.semanticscholar.org/paper/96fa28ba8b6282f443eda96a24a709bdadcabd7e","title":"Complex relationship graph abstraction for autonomous air combat collaboration: A learning and expert knowledge hybrid approach","venue":"Expert systems with applications","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Haiyin Piao,Yue Han,Hechang Chen,Xuanqi Peng,Songyuan Fan,Yang Sun,Chen Liang,Zhimin Liu,Zhixiao Sun,Deyun Zhou","id":"96fa28ba8b6282f443eda96a24a709bdadcabd7e","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/080f2ef96cf82a379c5db91ed270cb37d7f82bcf","title":"Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation","venue":"ArXiv","year":2022,"referenceCount":63,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/11/2022","authors":"Siddharth Nayak,Kenneth M. F. Choi,Wenqi Ding,Sydney I. Dolan,Karthik Gopalakrishnan,H. Balakrishnan","id":"080f2ef96cf82a379c5db91ed270cb37d7f82bcf","summary":"InforMARL is proposed, a novel architecture for multi-agent reinforcement learning (MARL) which uses local information intelligently to compute paths for all the agents in a decentralized manner and can be used in conjunction with any standard MARL algorithm.","score":3},{"url":"https://www.semanticscholar.org/paper/83caa4b447b816526587927e2b15a9c260dd2398","title":"Achieving mouse-level strategic evasion performance using real-time computational planning","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/11/2022","authors":"German Espinosa,Gabrielle E. Wink,Alexander T. Lai,D. Dombeck,M. A. MacIver","id":"83caa4b447b816526587927e2b15a9c260dd2398","summary":"This algorithm allows us to achieve mouse- level predator evasion performance with orders of magnitude less computation than a widespread algorithm for planning in the situations of partial observability that typify predator-prey interactions.","score":3},{"url":"https://www.semanticscholar.org/paper/db0a787b6c42b993d7fec9f27408b2a85db1ccaf","title":"Wasserstein gradient flows policy optimization via input convex neural networks","venue":"Other Conferences","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Yixuan Wang","id":"db0a787b6c42b993d7fec9f27408b2a85db1ccaf","summary":"A large-scale Wasserstein gradient flow RL method is got by introducing input convex neural networks (ICNNs) to improve the Jordan-Kinderlehrer-otto (JKO) scheme.","score":3},{"url":"https://www.semanticscholar.org/paper/ebdbdfdc68a7dc0581154e951920327167700c98","title":"Agent-State Construction with Auxiliary Inputs","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Ruo Yu Tao,Adam White,Marlos C. Machado","id":"ebdbdfdc68a7dc0581154e951920327167700c98","summary":"This work presents a series of ex-amples illustrating the diﬀerent ways of using auxiliary inputs for reinforcement learning, and shows that these auxiliary inputs can be used to discriminate between observations that would otherwise be aliased, leading to more expressive features that smoothly interpolate between dif-ferent states.","score":3},{"url":"https://www.semanticscholar.org/paper/37b7c079efcf5f4d77b0957c3d72c198d785e138","title":"Value-based CTDE Methods in Symmetric Two-team Markov Game: from Cooperation to Team Competition","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Pascal Leroy,J. Pisane,D. Ernst","id":"37b7c079efcf5f4d77b0957c3d72c198d785e138","summary":"The results suggest that training against multiple evolving strategies achieves the best results when, for scoring their performances, teams are faced with several strategies.","score":3},{"url":"https://www.semanticscholar.org/paper/627785cd6b768feda87d3f43e457c1824e3d9a8c","title":"The Impact of Batch Deep Reinforcement Learning on Student Performance: A Simple Act of Explanation Can Go A Long Way","venue":"International Journal of Artificial Intelligence in Education","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"Markel Sanz Ausin,Mehak Maniktala,T. Barnes,Min Chi","id":"627785cd6b768feda87d3f43e457c1824e3d9a8c","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/ed7801b29273ec3208fcbddd04a712cd80e86cb6","title":"Automatic Discovery of Multi-perspective Process Model using Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Sunghyun Sim,Ling Liu,Hyerim Bae","id":"ed7801b29273ec3208fcbddd04a712cd80e86cb6","summary":"An automatic discovery framework of a multi-perspective process model based on deep Q-Learning that can automatically perform process model discovery steps, conformance check steps, and enhancements steps and a new method that further optimizes the experience replay (ER) method to improve the learning performance of reinforcement learning agents.","score":3},{"url":"https://www.semanticscholar.org/paper/87361e44f253a3044b02b947c0a2370ff4f1e3e7","title":"Towards Improving Exploration in Self-Imitation Learning using Intrinsic Motivation","venue":"IEEE Symposium Series on Computational Intelligence","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Alain Andres,Esther Villar-Rodriguez,J. Ser","id":"87361e44f253a3044b02b947c0a2370ff4f1e3e7","summary":"In this work intrinsic motivation is used to encourage the agent to explore the environment based on its curiosity, whereas imitation learning allows repeating the most promising experiences to accelerate the learning process.","score":3},{"url":"https://www.semanticscholar.org/paper/a905c861cae2b128ac4dc24614714987cbcab130","title":"DeepRepair: Style-Guided Repairing for Deep Neural Networks in the Real-World Operational Environment","venue":"IEEE Transactions on Reliability","year":2022,"referenceCount":106,"citationCount":10,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Bing Yu,Hua Qi,Guo Qing,Felix Juefei-Xu,Xiaofei Xie,L. Ma,Jianjun Zhao","id":"a905c861cae2b128ac4dc24614714987cbcab130","summary":"A style-guided data augmentation for repairing DNN in the operational environment is proposed, which learns and introduces the unknown failure patterns within the failure samples into the training data via the style transfer.","score":3},{"url":"https://www.semanticscholar.org/paper/0102c2b64cedd427ac003df1a22daa097f94567e","title":"Negotiation and honesty in artificial intelligence methods for the board game of Diplomacy","venue":"Nature Communications","year":2022,"referenceCount":66,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"János Kramár,Tom Eccles,I. Gemp,A. Tacchetti,Kevin R. McKee,Mateusz Malinowski,T. Graepel,Yoram Bachrach","id":"0102c2b64cedd427ac003df1a22daa097f94567e","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/0a0399f30ae2b2c9b27de5fbdd180a3374b70c37","title":"Towards New Generation, Biologically Plausible Deep Neural Network Learning","venue":"The Scientist","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Anirudh Apparaju,Ognjen Arandjelovíc","id":"0a0399f30ae2b2c9b27de5fbdd180a3374b70c37","summary":"A biologically plausible learning method that takes advantage of various biological processes, such as Hebbian synaptic plasticity, and includes both supervised and unsupervised elements is described and its lesser resilience to additive, zero mean Gaussian noise is found.","score":3},{"url":"https://www.semanticscholar.org/paper/eea6b8c0d500df2a8fce7f23d5c6988de6b9749d","title":"A Hierarchical Deep Reinforcement Learning Framework for 6-DOF UCAV Air-to-Air Combat","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Jiajun Chai,Wenzhang Chen,Yuanheng Zhu,Zonggui Yao,Dongbin Zhao","id":"eea6b8c0d500df2a8fce7f23d5c6988de6b9749d","summary":"A general hierarchical framework to resolve the within-vision-range (WVR) air-to-air combat problem under 6 dimensions of degree (6-DOF) dynamics is proposed and an effective reward function is designed to accurately track various macro behavior.","score":3},{"url":"https://www.semanticscholar.org/paper/b65118061650446bdd79087def417e6a4de7ecdf","title":"PrefRec: Preference-based Recommender Systems for Reinforcing Long-term User Engagement","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Wanqi Xue,Qingpeng Cai,Zhenghai Xue,Shuo Sun,Shuchang Liu,Dong Zheng,Peng Jiang,Bo An","id":"b65118061650446bdd79087def417e6a4de7ecdf","summary":"This work proposes a novel paradigm, Pre ference-based Rec ommender systems (PrefRec), which allows RL recommender systems to learn from preferences about users’ historical behaviors rather than explicitly defined rewards, and designs an effective optimization method for PrefRec, which uses an additional value function, expectile regression and reward model pre-training to improve the performance.","score":3},{"url":"https://www.semanticscholar.org/paper/4f248a4292976fd5451cc282226addfb369f5c82","title":"AI-Based Military Decision Support Using Natural Language","venue":"Online World Conference on Soft Computing in Industrial Applications","year":2022,"referenceCount":12,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/12/2022","authors":"Michael Möbius,Daniel Kallfass,Thomas Doll,Dietmar Kunde","id":"4f248a4292976fd5451cc282226addfb369f5c82","summary":"A combined approach where human knowledge and responsibility collaborate with an AI system is proposed where the orders and actions imposed by AI are given in natural language in order to validate and evaluate the reasoning of AI.","score":3},{"url":"https://www.semanticscholar.org/paper/b58ef62a34bdbcd357824c9e58d89cbbcc0e2375","title":"Evaluating Model-free Reinforcement Learning toward Safety-critical Tasks","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":1,"influentialCitationCount":1,"publicationDate":"12/12/2022","authors":"Linrui Zhang,Q. Zhang,Li Shen,Bo Yuan,Xueqian Wang,Dacheng Tao","id":"b58ef62a34bdbcd357824c9e58d89cbbcc0e2375","summary":"This paper proposes Unrolling Safety Layer (USL), a joint method that combines safety optimization and safety pro- jection and explicitly enforces hard constraints via the deep unrolling architecture and enjoys struc- tural advantages in navigating the trade-off between reward improvement and constraint satisfaction.","score":3},{"url":"https://www.semanticscholar.org/paper/cbe7ace2d80209f1bfb62af318cae24ff147cb52","title":"An Adaptive Updating Method of Target Network Based on Moment Estimates for Deep Reinforcement Learning","venue":"Neural Processing Letters","year":2022,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"Miaoping Sun,Zequan Yang,Xunhua Dai,X. Nian,Hongyun Xiong,Haibo Wang","id":"cbe7ace2d80209f1bfb62af318cae24ff147cb52","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/5c41258c2e90372e363f6bb3519b27a76b66ab48","title":"A Data-Efficient Training Method for Deep Reinforcement Learning","venue":"Electronics","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Wenhui Feng,Chongzhao Han,Feng Lian,Xia Liu","id":"5c41258c2e90372e363f6bb3519b27a76b66ab48","summary":"A data-efficient training method is proposed in which a DQN is used as a base algorithm, and an elaborate curriculum is designed for the agent in the simulation scenario to accelerate the training process.","score":3},{"url":"https://www.semanticscholar.org/paper/9fd98c158005a3770b6c378f7b97b754937b5d54","title":"Investigation of reinforcement learning for shape optimization of profile extrusion dies","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/12/2022","authors":"C. Fricke,D. Wolff,Marco Kemmerling,S. Elgeti","id":"9fd98c158005a3770b6c378f7b97b754937b5d54","summary":"This work investigates the impact of utilizingerent agents on the training progress and the potential of wall time saving by utilizing multiple environments during training and the utilization of Reinforcement Learning as a learning-based optimization algorithm.","score":3},{"url":"https://www.semanticscholar.org/paper/ab8674f284e7ce8fe08433003b7d641bcb9f88f5","title":"Certificates of quantum many-body properties assisted by machine learning","venue":"Physical Review Research","year":2021,"referenceCount":120,"citationCount":2,"influentialCitationCount":0,"publicationDate":"05/03/2021","authors":"Borja Requena,G. Muñoz-Gil,M. Lewenstein,V. Dunjko,Jordi Tura i Brugués","id":"ab8674f284e7ce8fe08433003b7d641bcb9f88f5","summary":"This work proposes a novel approach combining the power of relaxation techniques with deep reinforcement learning in order to find the best possible bounds within a limited computational budget, and provides tools to generalize the approach to other common applications in the field of quantum information processing.","score":3},{"url":"https://www.semanticscholar.org/paper/85750b05dc461645d756ff4839475e0ae319449b","title":"Machine and quantum learning for diamond-based quantum applications","venue":"Materials for Quantum Technology","year":2022,"referenceCount":150,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/07/2022","authors":"Dylan G. Stone,C. Bradac","id":"85750b05dc461645d756ff4839475e0ae319449b","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/83434a8e9796fb78472358b8b4d4444db61d19db","title":"Artificial Intelligence and Machine Learning for Quantum Technologies","venue":"Physical Review A","year":2022,"referenceCount":154,"citationCount":4,"influentialCitationCount":0,"publicationDate":"07/08/2022","authors":"M. Krenn,Jonas Landgraf,T. Foesel,F. Marquardt","id":"83434a8e9796fb78472358b8b4d4444db61d19db","summary":"This perspective article showcases in illustrative examples how scientists in the past few years have started to use machine learning and more broadly methods of artiﬁcial intelligence to analyze quantum measurements, estimate the parameters of quantum devices, discover new quantum experimental setups, protocols, and feedback strategies, and generally improve aspects of quantum computing, quantum communication, and quantum simulation.","score":3},{"url":"https://www.semanticscholar.org/paper/aa6fbabb6c4fe2058615462243a89fe40568e262","title":"Scalable Planning and Learning Framework Development for Swarm-to-Swarm Engagement Problems","venue":"AIAA SCITECH 2023 Forum","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Umut Demir,A. Satir,Gulay Goktas Sever,Cansu Yikilmaz,N. K. Ure","id":"aa6fbabb6c4fe2058615462243a89fe40568e262","summary":"This work proposes a reinforcement learning (RL) based framework to decompose to large-scale swarm engagement problems into a number of independent multi-agent pursuit-evasion games and verifies the approach in large- scale swarm-to-swarm engagement simulations.","score":3},{"url":"https://www.semanticscholar.org/paper/f2b2981f7d9439d734acbd0edbd8ad6e7af77a2b","title":"Mystique: Accurate and Scalable Production AI Benchmarks Generation","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Mingyu Liang,Wenyin Fu,Louis Feng,Zhongyi Lin,P. Panakanti,Srinivas Sridharan,Christina Delimitrou","id":"f2b2981f7d9439d734acbd0edbd8ad6e7af77a2b","summary":"Mystique is an accurate and scalable framework for production AI benchmark generation that leverages the PyTorch execution graph (EG), a new feature that captures the runtime information of AI models at the granularity of operators, in a graph format, together with their metadata.","score":3},{"url":"https://www.semanticscholar.org/paper/8a03a51b01eddf21bbe2a0a71ec8186c6e65ace1","title":"MERLIN: Multi-agent offline and transfer learning for occupant-centric energy flexible operation of grid-interactive communities using smart meter data and CityLearn","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/12/2022","authors":"K. Nweye,S. Sankaranarayanan,Z. Nagy","id":"8a03a51b01eddf21bbe2a0a71ec8186c6e65ace1","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/1cceaba216f0392e11269883e5cbd24b1ef732d7","title":"Robust Lane Change Decision Making for Autonomous Vehicles: An Observation Adversarial Reinforcement Learning Approach","venue":"IEEE Transactions on Intelligent Vehicles","year":2023,"referenceCount":38,"citationCount":11,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Xiangkun He,Haohan Yang,Zhongxu Hu,Chen Lv","id":"1cceaba216f0392e11269883e5cbd24b1ef732d7","summary":"The proposed observation adversarial reinforcement learning approach for robust lane change decision making of autonomous vehicles can not only enhance the performance of an autonomous vehicle but also improve the robustness of lane change policies against adversarial observation perturbations.","score":3},{"url":"https://www.semanticscholar.org/paper/b7823997fb185f208b6a6723b60413ff179d2639","title":"Standing on the Shoulders of AI Giants","venue":"Computer","year":2023,"referenceCount":12,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Hsiao-Ying Lin","id":"b7823997fb185f208b6a6723b60413ff179d2639","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/49adfa7a7031446f8b58a913f1c777b9819ba922","title":"Achieving efficient interpretability of reinforcement learning via policy distillation and selective input gradient regularization.","venue":"Neural Networks","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Jinwei Xing,Takashi Nagata,Xinyun Zou,E. Neftci,J. Krichmar","id":"49adfa7a7031446f8b58a913f1c777b9819ba922","summary":"This work proposes an approach of Distillation with selective Input Gradient Regularization (DIGR) which uses policy distillation and input gradient regularization to produce new policies that achieve both high interpretability and computation efficiency in generating saliency maps.","score":3},{"url":"https://www.semanticscholar.org/paper/cf3bd470c716ddca24f0a26b85b4bdccd5bc2797","title":"Deep reinforcement learning empowers automated inverse design and optimization of photonic crystals for nanoscale laser cavities","venue":"Nanophotonics","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/01/2023","authors":"Renjie Li,Ceyao Zhang,Wentao Xie,Yuanhao Gong,Feilong Ding,Hui Dai,Zihan Chen,F. Yin,Zhaoyu Zhang","id":"cf3bd470c716ddca24f0a26b85b4bdccd5bc2797","summary":"L2DO can achieve over two orders of magnitude higher sample-efficiency without suffering from the three issues above and confirms the potential of deep RL algorithms to surpass human designs and marks a solid step towards a fully automated AI framework for photonics inverse design.","score":3},{"url":"https://www.semanticscholar.org/paper/72fdfb106c47e27e0ccd42760dd67af720dc255f","title":"Learning to Participate through Trading of Reward Shares","venue":"ArXiv","year":2023,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/01/2023","authors":"M. Kölle,Tim Matheis,Philipp Altmann,Kyrill Schmid","id":"72fdfb106c47e27e0ccd42760dd67af720dc255f","summary":"This paper proposes a method inspired by the stock market, where agents have the opportunity to participate in other agents’ returns by acquiring reward shares, and shows that this mechanism promotes cooperative policies among independently trained agents in social dilemma situations.","score":3},{"url":"https://www.semanticscholar.org/paper/cb0912e8d780b7b5e8651df0baaa20b56f4fd6ff","title":"Single-Trajectory Distributionally Robust Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/01/2023","authors":"Zhipeng Liang,Xiaoteng Ma,J. Blanchet,Jiheng Zhang,Zhengyuan Zhou","id":"cb0912e8d780b7b5e8651df0baaa20b56f4fd6ff","summary":"This work proposes distributionally robust Q-learning with single trajectory (DRQ) , and its average-reward variant named differential DRQ, and provides asymptotic convergence guarantees and experiments for both settings, demonstrating their superiority in the perturbed environments against the non-robust ones.","score":3},{"url":"https://www.semanticscholar.org/paper/b05345d5b1593f383c544ac77c079ab2641ec542","title":"Turbulence control in plane Couette flow using low-dimensional neural ODE-based models and deep reinforcement learning","venue":"ArXiv","year":2023,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/01/2023","authors":"Alec J. Linot,Kevin Zeng,M. Graham","id":"b05345d5b1593f383c544ac77c079ab2641ec542","summary":"This work obtains a 25-dimensional DManD model of the dynamics by combining an autoencoder and neural ordinary diﬀerential equation, and trains an RL control agent, yielding a 440-fold speedup over training on the DNS, with equivalent control performance.","score":3},{"url":"https://www.semanticscholar.org/paper/8dbd7e096e0338b7470acab318786a6756ece6f7","title":"Intelligent Decision-Making and Human Language Communication Based on Deep Reinforcement Learning in a Wargame Environment","venue":"IEEE Transactions on Human-Machine Systems","year":2023,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Yuxiang Sun,Bo Yuan,Qinlin Xiang,Jiawei Zhou,Jiahui Yu,Di Dai,Xianzhong Zhou","id":"8dbd7e096e0338b7470acab318786a6756ece6f7","summary":"This article creatively incorporates deep learning and natural language processing technologies in the warg game field to transform game context situation maps into textual suggestions in wargame confrontation.","score":3},{"url":"https://www.semanticscholar.org/paper/086a7c3cbd9992923f626efda1f412f8694f33a8","title":"DCM: Deep complementary energy method based on the principle of minimum complementary energy","venue":"ArXiv","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"Yizheng Wang,Jia Sun,Zaiyuan Lu,Pipi Hu,Yinghua Liu","id":"086a7c3cbd9992923f626efda1f412f8694f33a8","summary":"The result shows the advantage of the proposed DCM is suitable for dealing with problems of dominated displacement boundary conditions, which is proved by mathematical derivations, as well as with numerical experiments.","score":3},{"url":"https://www.semanticscholar.org/paper/45f32b26dc754ed293ac1add3b233f5ad6822518","title":"Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation","venue":"ArXiv","year":2023,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Qiwen Cui,K. Zhang,S. Du","id":"45f32b26dc754ed293ac1add3b233f5ad6822518","summary":"An iterative-best-response type algorithm that can learn pure Markov Nash equilibria in independent linear Markov potential games and an algorithm with $\\widetilde$ sample complexity to learn Markov CCE, which improves the state-of-the-art result in Daskalakis et al. 2022.","score":3},{"url":"https://www.semanticscholar.org/paper/6e1b7a851ecdff764b066bcc28d6130779eb098e","title":"TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play","venue":"","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Fanqing Lin,Shiyu Huang,Tim Pearce,Wenze Chen,Weijuan Tu","id":"6e1b7a851ecdff764b066bcc28d6130779eb098e","summary":"This paper develops a multi-agent system to play the full 11 vs. 11 game mode, without demonstrations, and introduces several innovations, including adaptive curriculum learning, a novel self-play strategy, and an objective that optimizes the policies of multiple agents jointly.","score":3},{"url":"https://www.semanticscholar.org/paper/b832551eca18725cfd982aaece5711878099824b","title":"GAIL-PT: An intelligent penetration testing framework with generative adversarial imitation learning","venue":"Computers &amp; Security","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/03/2023","authors":"Jinyin Chen,Shulong Hu,Haibin Zheng,Changyou Xing,Guomin Zhang","id":"b832551eca18725cfd982aaece5711878099824b","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/410e793147cc3787a135b9e8c2a3250d8e2a2f66","title":"Modeling collective motion for fish schooling via multi-agent reinforcement learning","venue":"Ecological Modelling","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/03/2023","authors":"X. Wang,Shuo Liu,Yifan Yu,Shengzhi Yue,Yingru Liu,Fumin Zhang,Yuanshan Lin","id":"410e793147cc3787a135b9e8c2a3250d8e2a2f66","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/9125b3f152d792257a51745e7af6d191d05b54a0","title":"Interactive Learning with Corrective Feedback for Policies based on Deep Neural Networks","venue":"International Symposium on Experimental Robotics","year":2018,"referenceCount":14,"citationCount":12,"influentialCitationCount":1,"publicationDate":"30/09/2018","authors":"Rodrigo Pérez-Dattari,C. Celemin,Javier Ruiz-del-Solar,J. Kober","id":"9125b3f152d792257a51745e7af6d191d05b54a0","summary":"This work approaches an alternative Interactive Machine Learning strategy for training DNN policies based on human corrective feedback, with a method called Deep COACH (D-COACH), which takes advantage of the knowledge and insights of human teachers as well as the power of DNNs, but also has no need of a reward function.","score":3},{"url":"https://www.semanticscholar.org/paper/4b61c25a86083c20730c9b12737ac6ac4178c364","title":"An Introduction to Deep Reinforcement Learning","venue":"Found. Trends Mach. Learn.","year":2018,"referenceCount":355,"citationCount":679,"influentialCitationCount":42,"publicationDate":"30/11/2018","authors":"Vincent François-Lavet,Peter Henderson,Riashat Islam,Marc G. Bellemare,Joelle Pineau","id":"4b61c25a86083c20730c9b12737ac6ac4178c364","summary":"This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques and particular focus is on the aspects related to generalization and how deep RL can be used for practical applications.","score":3},{"url":"https://www.semanticscholar.org/paper/f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751","title":"Deep Reinforcement Learning","venue":"Reinforcement Learning for Cyber-Physical Systems","year":2018,"referenceCount":890,"citationCount":297,"influentialCitationCount":19,"publicationDate":"15/10/2018","authors":"Yuxi Li","id":"f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751","summary":"This work discusses deep reinforcement learning in an overview style, focusing on contemporary work, and in historical contexts, with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources.","score":3},{"url":"https://www.semanticscholar.org/paper/156a84996911a2116bdddd89aade6921e3e68d22","title":"Model-Free Reinforcement Learning for Real-World Robots","venue":"","year":2020,"referenceCount":200,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/11/2020","authors":"Denis Steckelmacher","id":"156a84996911a2116bdddd89aade6921e3e68d22","summary":"This thesis considers real-world tasks that may benefit from Reinforcement Learning, and for which there is no simulator available, and introduces the model-free actor-critic Bootstrapped Dual Policy Iteration algorithm (BDPI), a formalism that allows an agent to learn partially-observable tasks in a sample-efficient way.","score":3},{"url":"https://www.semanticscholar.org/paper/40848b41ed8c9c255ecd8a920006877691b52d03","title":"WILDS: A Benchmark of in-the-Wild Distribution Shifts","venue":"International Conference on Machine Learning","year":2020,"referenceCount":424,"citationCount":496,"influentialCitationCount":87,"publicationDate":"14/12/2020","authors":"P. W. Koh,Shiori Sagawa,H. Marklund,Sang Michael Xie,Marvin Zhang,Akshay Balsubramani,Weihua Hu,Michihiro Yasunaga,Richard L. Phillips,Sara Beery,J. Leskovec,A. Kundaje,E. Pierson,S. Levine,Chelsea Finn,Percy Liang","id":"40848b41ed8c9c255ecd8a920006877691b52d03","summary":"WILDS is presented, a benchmark of in-the-wild distribution shifts spanning diverse data modalities and applications, and is hoped to encourage the development of general-purpose methods that are anchored to real-world distribution shifts and that work well across different applications and problem settings.","score":3},{"url":"https://www.semanticscholar.org/paper/266cc7ff4856b6a2ce9cc0a3e5f6c155ecc448a2","title":"Can Wikipedia Help Offline Reinforcement Learning?","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":42,"influentialCitationCount":8,"publicationDate":"28/01/2022","authors":"Machel Reid,Yutaro Yamada,S. Gu","id":"266cc7ff4856b6a2ce9cc0a3e5f6c155ecc448a2","summary":"This work looks to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains when ﬁnetuned on ofﬂine RL tasks (control, games), and proposes techniques to improve transfer between these domains.","score":3},{"url":"https://www.semanticscholar.org/paper/0bda4b0246e2ad07e83da9fd904b92bc8fe831d9","title":"Structured Output Feedback Control for Linear Quadratic Regulator Using Policy Gradient Method","venue":"","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/03/2022","authors":"Shokichi Takakura,Kazuhiro Sato","id":"0bda4b0246e2ad07e83da9fd904b92bc8fe831d9","summary":"To solve the static output feedback control for Linear Quadratic Regulator problems with structured constraints under the assumption that system parameters are un- known, the policy gradient algorithm based on the gradient projection method is proposed and its global convergence to ε -stationary points is shown.","score":3},{"url":"https://www.semanticscholar.org/paper/30d51a26086a6632d2ed0edd882054b46e8519a9","title":"Automating reinforcement learning architecture design for code optimization","venue":"International Conference on Compiler Construction","year":2022,"referenceCount":89,"citationCount":4,"influentialCitationCount":0,"publicationDate":"18/03/2022","authors":"Huanting Wang,Zhanyong Tang,Chen Zhang,Jiaqi Zhao,Chris Cummins,H. Leather,Z. Wang","id":"30d51a26086a6632d2ed0edd882054b46e8519a9","summary":"SuperSonic is a new open-source framework to allow compiler developers to integrate RL into compilers easily, regardless of their RL expertise, and shows that SuperSonic consistently improves hand-tuned methods by delivering better overall performance.","score":3},{"url":"https://www.semanticscholar.org/paper/651468a69da74dab716cebbd179a5cbb8e672c14","title":"Self-Imitation Learning from Demonstrations","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":4,"influentialCitationCount":2,"publicationDate":"21/03/2022","authors":"Georgiy Pshikhachev,Dmitry Ivanov,Vladimir Egorov,A. Shpilman","id":"651468a69da74dab716cebbd179a5cbb8e672c14","summary":"Self-Imitation Learning (SIL), a recent RL algorithm that exploits agent’s past good experience, is extended to the LfD setup by initializing its replay buffer with demonstrations and shows the superiority of SIL over existing L fD algorithms in settings of suboptimal demonstrations and sparse rewards.","score":3},{"url":"https://www.semanticscholar.org/paper/0b3ca2a700085a877c560e20558566536f18d5a2","title":"Modular Lifelong Reinforcement Learning via Neural Composition","venue":"International Conference on Learning Representations","year":2022,"referenceCount":55,"citationCount":7,"influentialCitationCount":1,"publicationDate":"01/07/2022","authors":"Jorge Armando Mendez Mendez,H. V. Seijen,Eric Eaton","id":"0b3ca2a700085a877c560e20558566536f18d5a2","summary":"This work explores a particular form of composition based on neural modules and presents a set of RL problems that intuitively admit compositional solutions and demonstrates that neural composition indeed captures the underlying structure of this space of problems.","score":3},{"url":"https://www.semanticscholar.org/paper/96cd3bf5094e26ba6c367c8486d57fb21e570770","title":"Efficiently Detecting Non-Stationary Opponents: A Bayesian Policy Reuse Approach under Partial Observability","venue":"Applied Sciences","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/07/2022","authors":"Yu Wang,Ke Fu,Hao Chen,Quan Liu,Jian Huang,Zhongjie Zhang","id":"96cd3bf5094e26ba6c367c8486d57fb21e570770","summary":"It is demonstrated that Bayes-Lab outperforms existing state-of-the-art methods in terms of detection accuracy, accumulative rewards, and episodic rewards in a predator–prey scenario.","score":3},{"url":"https://www.semanticscholar.org/paper/649e713a760143f49c20ed4019fbf5434f89b596","title":"Safety-informed mutations for evolutionary deep reinforcement learning","venue":"GECCO Companion","year":2022,"referenceCount":39,"citationCount":2,"influentialCitationCount":0,"publicationDate":"09/07/2022","authors":"Enrico Marchesini,Chris Amato","id":"649e713a760143f49c20ed4019fbf5434f89b596","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/95aa4c864c05bcb942fa7fab4611b43ebad30c3a","title":"Learning to Utilize Curiosity: A New Approach of Automatic Curriculum Learning for Deep RL","venue":"Mathematics","year":2022,"referenceCount":25,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/07/2022","authors":"Zeyang Lin,Jun Lai,Xi-liang Chen,Lei Cao,Jun Wang","id":"95aa4c864c05bcb942fa7fab4611b43ebad30c3a","summary":"The CMCL algorithm is compared with other baseline algorithms in cooperative-competitive environments, and the experimental simulation results show that the CMCL method can improve the training performance and robustness of multi-agent deep reinforcement learning algorithms.","score":3},{"url":"https://www.semanticscholar.org/paper/ab94fcda889acf9a0f3fd4901723ac11e9587c3a","title":"Decentralized Coordination in Partially Observable Queueing Networks","venue":"Global Communications Conference","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/08/2022","authors":"Jiekai Jia,Anam Tahir,H. Koeppl","id":"ab94fcda889acf9a0f3fd4901723ac11e9587c3a","summary":"This work implemented a communication channel for agents to share their information in order to reduce the packet drop rate in a discrete-time queueing network and shows empirically that ATVC is able to infer the true state of the queues and leads to a policy which outperforms existing baselines.","score":3},{"url":"https://www.semanticscholar.org/paper/a5398f5186bf45be39dbeb25d3635d7e10ac3831","title":"Longitudinal deep truck: Deep longitudinal model with application to sim2real deep reinforcement learning for heavy‐duty truck control in the field","venue":"Journal of Field Robotics","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/11/2022","authors":"Saleh Albeaik,Trevor Wu,Ganesh Vurimi,Fang-Chieh Chou,Xiao-Yun Lu,A. Bayen","id":"a5398f5186bf45be39dbeb25d3635d7e10ac3831","summary":"A two‐layer gray‐box deep learning model is developed to capture longitudinal dynamics of heavy‐duty trucks while abstracting their complexity and an approach to properly break the nested feedback loops in the model for training is presented.","score":3},{"url":"https://www.semanticscholar.org/paper/85dea9ee99e7d11bee0c82c857ab355e36638c13","title":"Evaluation Beyond Task Performance: Analyzing Concepts in AlphaZero in Hex","venue":"ArXiv","year":2022,"referenceCount":83,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/11/2022","authors":"Charles Lovering,J. Forde,G. Konidaris,Elizabeth-Jane Pavlick,M. Littman","id":"85dea9ee99e7d11bee0c82c857ab355e36638c13","summary":"This work investigates AlphaZero’s internal representations in the game of Hex using two evaluation techniques from natural language processing (NLP): model probing and behavioral tests, and finds that MCTS discovers concepts before the neural network learns to encode them.","score":3},{"url":"https://www.semanticscholar.org/paper/fdb655f285562ff144ad8eff27c30e332d626ee7","title":"RL-Recruiter+: Mobility-Predictability-Aware Participant Selection Learning for From-Scratch Mobile Crowdsensing","venue":"IEEE Transactions on Mobile Computing","year":2022,"referenceCount":52,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Yunfan Hu,Jiangtao Wang,Bo Wu,S. Helal","id":"fdb655f285562ff144ad8eff27c30e332d626ee7","summary":"A novel framework based on reinforcement learning, named RL-Recruiter+, which is able to make a good sequence of participant selection decisions for each sensing slot with the gradual accumulation of mobility trajectories over time, and outperforms the baseline approaches under various settings.","score":3},{"url":"https://www.semanticscholar.org/paper/193c40c22c2a727a0fcc11486c42b61285e92ee3","title":"A review on reinforcement learning for contact-rich robotic manipulation tasks","venue":"Robotics and Computer-Integrated Manufacturing","year":2023,"referenceCount":152,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Íñigo Elguea-Aguinaco,Antonio Serrano-Muñoz,D. Chrysostomou,Ibai Inziarte-Hidalgo,S. Bøgh,N. Arana-Arexolaleiba","id":"193c40c22c2a727a0fcc11486c42b61285e92ee3","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/5e78af2f7da1228d650c35105fec76ef4ef9dcc9","title":"Intelligent Computing: The Latest Advances, Challenges and Future","venue":"Intelligent Computing","year":2022,"referenceCount":358,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Shiqiang Zhu,Ting Yu,Tao Xu,Hongyang Chen,S. Dustdar,S. Gigan,D. Gunduz,E. Hossain,Yaochu Jin,Feng Lin,Bo Liu,Zhiguo Wan,Ji Zhang,Zhifeng Zhao,Wentao Zhu,Zuoning Chen,T. Durrani,Huaimin Wang,Jiangxing Wu,Tongyi Zhang,Yunhe Pan","id":"5e78af2f7da1228d650c35105fec76ef4ef9dcc9","summary":"The first comprehensive survey of literature on intelligent computing is presented, covering its theory fundamentals, the technological fusion of intelligence and computing, important applications, challenges, and future perspectives, which will provide a comprehensive reference and cast valuable insights into intelligent computing for academic and industrial researchers and practitioners.","score":3},{"url":"https://www.semanticscholar.org/paper/5312086d943766689b4ff52760878828c7ce2f9b","title":"GrAVITree: Graph-based Approximate Value Function In a Tree","venue":"ArXiv","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/01/2023","authors":"Patrick H. Washington,David Fridovich-Keil,M. Schwager","id":"5312086d943766689b4ff52760878828c7ce2f9b","summary":"GrAVITree is introduced, a tree- and sampling-based algorithm to compute a near-optimal value function and corresponding feedback policy for indeﬁnite time- horizon, terminal state-constrained nonlinear optimal control problems, suitable for arbitrary nonlinear control systems with both state and input constraints.","score":3},{"url":"https://www.semanticscholar.org/paper/4a5255dbdaba6d36dca55289f2ab299e78cab09e","title":"How to train a self-driving vehicle: On the added value (or lack thereof) of curriculum learning and replay buffers","venue":"Frontiers in Artificial Intelligence","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/01/2023","authors":"S. Mahmoud,E. Billing,Henrik Svensson,Serge Thill","id":"4a5255dbdaba6d36dca55289f2ab299e78cab09e","summary":"The main results show that curriculum learning indeed offers an additional benefit over a vanilla reinforcement learning approach (using Deep-Q Learning), but the replay buffer actually has a detrimental effect in most (but not all) combinations of data generation approaches the authors considered here.","score":3},{"url":"https://www.semanticscholar.org/paper/017844bd5429a27c46b434903dc0e60ebf215f90","title":"Visual Imitation Learning with Patch Rewards","venue":"ArXiv","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Minghuan Liu,Tairan He,Weinan Zhang,Shuicheng Yan,Zhongwen Xu","id":"017844bd5429a27c46b434903dc0e60ebf215f90","summary":"This work proposes Adversarial Imitation Learning with Patch Rewards (PatchAIL), which employs a patch-based discriminator to measure the expertise of different local parts from given images and provide patch rewards.","score":3},{"url":"https://www.semanticscholar.org/paper/d0d7c8fe3452c16218b6989015467f345eca8790","title":"Offline Learning of Closed-Loop Deep Brain Stimulation Controllers for Parkinson Disease Treatment","venue":"ArXiv","year":2023,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/02/2023","authors":"Qitong Gao,Stephen L. Schimdt,Afsana Chowdhury,Guangyu Feng,Jennifer J. Peters,Katherine Genty,W. Grill,Dennis A. Turner,M. Pajic","id":"d0d7c8fe3452c16218b6989015467f345eca8790","summary":"An offline reinforcement learning framework is introduced, allowing the use of past clinical data to train an RL policy to adjust the stimulation amplitude in real time, with the goal of reducing energy use while maintaining the same level of treatment (i.e., control) efficacy as cDBS.","score":3},{"url":"https://www.semanticscholar.org/paper/7346e08cb0e22a3b3c34aeff594d6c82595b1eb0","title":"Sequence generation for multi-task scheduling in cloud manufacturing with deep reinforcement learning","venue":"Journal of manufacturing systems","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/04/2023","authors":"Yaoyao Ping,Yongkui Liu,Lin Zhang,Lihui Wang,Xun Xu","id":"7346e08cb0e22a3b3c34aeff594d6c82595b1eb0","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/320959db43db08185cfbabe655e044c2a378ed39","title":"Virtual to Real-World Transfer Learning: A Systematic Review","venue":"Electronics","year":2021,"referenceCount":78,"citationCount":7,"influentialCitationCount":0,"publicationDate":"21/06/2021","authors":"Mahesh Ranaweera,Q. Mahmoud","id":"320959db43db08185cfbabe655e044c2a378ed39","summary":"This systematic review defines (a) transfer learning; (b) discusses the recent research conducted; (c) the current status of transfer learning and finally, (d) discusses how transfer learning can bridge the gap between the virtual and the real.","score":3},{"url":"https://www.semanticscholar.org/paper/3032844d6ac6882ccb03e7a2c22a0026b210ac05","title":"What Matters in Learning from Offline Human Demonstrations for Robot Manipulation","venue":"Conference on Robot Learning","year":2021,"referenceCount":90,"citationCount":78,"influentialCitationCount":16,"publicationDate":"06/08/2021","authors":"Ajay Mandlekar,Danfei Xu,J. Wong,Soroush Nasiriany,Chen Wang,Rohun Kulkarni,Li Fei-Fei,S. Savarese,Yuke Zhu,Roberto Mart'in-Mart'in","id":"3032844d6ac6882ccb03e7a2c22a0026b210ac05","summary":"This study analyzes the most critical challenges when learning from offline human data for manipulation and highlights opportunities for learning from human datasets, such as the ability to learn proficient policies on challenging, multi-stage tasks beyond the scope of current reinforcement learning methods.","score":3},{"url":"https://www.semanticscholar.org/paper/ca6096142016a2ba8133f6cb2c04ad30f5eae730","title":"Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning","venue":"ArXiv","year":2021,"referenceCount":32,"citationCount":99,"influentialCitationCount":16,"publicationDate":"24/09/2021","authors":"N. Rudin,David Hoeller,Philipp Reist,M. Hutter","id":"ca6096142016a2ba8133f6cb2c04ad30f5eae730","summary":"A training set-up that achieves fast policy generation for real-world robotic tasks by using massive parallelism on a single workstation GPU is presented and a novel game-inspired curriculum that is well suited for training with thousands of simulated robots in parallel is presented.","score":3},{"url":"https://www.semanticscholar.org/paper/41e43d9c766128cdd715c64fbd30e0c9fdf14652","title":"From Motor Control to Team Play in Simulated Humanoid Football","venue":"Sci. Robotics","year":2021,"referenceCount":165,"citationCount":38,"influentialCitationCount":2,"publicationDate":"25/05/2021","authors":"Siqi Liu,Guy Lever,Zhe Wang,J. Merel,S. Eslami,Daniel Hennes,Wojciech M. Czarnecki,Yuval Tassa,Shayegan Omidshafiei,A. Abdolmaleki,Noah Siegel,Leonard Hasenclever,Luke Marris,S. Tunyasuvunakool,H. F. Song,Markus Wulfmeier,Paul Muller,Tuomas Haarnoja,Brendan D. Tracey,K. Tuyls,T. Graepel,N. Heess","id":"41e43d9c766128cdd715c64fbd30e0c9fdf14652","summary":"This work optimized teams of agents to play simulated football via reinforcement learning, constraining the solution space to that of plausible movements learned using human motion capture data, resulting in a team of coordinated humanoid football players that exhibit complex behavior at different scales, quantified by a range of analysis and statistics.","score":3},{"url":"https://www.semanticscholar.org/paper/efbc2c6306ff1f3bfa282fc62f8467764fd41c25","title":"Accelerated Policy Learning with Parallel Differentiable Simulation","venue":"International Conference on Learning Representations","year":2022,"referenceCount":56,"citationCount":12,"influentialCitationCount":2,"publicationDate":"14/04/2022","authors":"Jie Xu,Viktor Makoviychuk,Yashraj S. Narang,Fabio Ramos,W. Matusik,Animesh Garg,M. Macklin","id":"efbc2c6306ff1f3bfa282fc62f8467764fd41c25","summary":"A high-performance differentiable simulator and a new policy learning algorithm (SHAC) that can effectively leverage simulation gradients, even in the presence of non-smoothness are presented.","score":3},{"url":"https://www.semanticscholar.org/paper/0c845688166c07cb095ed0dbd55da817f34a4cfb","title":"Learning to guide multiple heterogeneous actors from a single human demonstration via automatic curriculum learning in StarCraft II","venue":"Defense + Commercial Sensing","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/05/2022","authors":"Nicholas R. Waytowich,J. Hare,Vinicius G. Goecks,Mark R. Mittrick,J. Richardson,Anjon Basak,Derrik E. Asher","id":"0c845688166c07cb095ed0dbd55da817f34a4cfb","summary":"The results show that an agent trained via automated curriculum learning can outperform state-of- the-art deep reinforcement learning baselines and match the performance of the human expert in a simulated command and control task in StarCraft II modeled over a real military scenario.","score":3},{"url":"https://www.semanticscholar.org/paper/d30b069c54b06c7749b83fbdf050c17fdea374d9","title":"AACC: Asymmetric Actor-Critic in Contextual Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/08/2022","authors":"Wangyang Yue,Yuan Zhou,Xiaochuan Zhang,Yuchen Hua,Zhiyuan Wang,Guang Kou","id":"d30b069c54b06c7749b83fbdf050c17fdea374d9","summary":"This paper formalizes the task of adapting to changing environmental dynamics in RL as a generalization problem using Contextual Markov Decision Processes (CMDPs) and proposes the Asymmetric Actor-Critic in Contextual RL (AACC) as an end-to-end actor-critic method to deal with such generalization tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/8a7dbe1a807c948eb1b6496c22e12f8ba03c94ff","title":"Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment","venue":"ArXiv","year":2022,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Huihan Liu,Soroush Nasiriany,Lance Zhang,Zhiyao Bao,Yuke Zhu","id":"8a7dbe1a807c948eb1b6496c22e12f8ba03c94ff","summary":"Sirius is presented, a principled framework for humans and robots to collaborate through a division of work where partially autonomous robots are tasked with handling a major portion of decision-making where they work reliably; meanwhile, human operators monitor the process and intervene in challenging situations.","score":3},{"url":"https://www.semanticscholar.org/paper/174bf19aa934e9ba2016a1837cfe667b13a05953","title":"Adaptive Actuation of Magnetic Soft Robots Using Deep Reinforcement Learning","venue":"Advanced Intelligent Systems","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/04/2022","authors":"Jianpeng Yao,Quanliang Cao,Yuwei Ju,Yuxuan Sun,Ruiqi Liu,Xiaotao Han,Liang Li","id":"174bf19aa934e9ba2016a1837cfe667b13a05953","summary":"The first case of using strategies entirely generated by reinforcement learning to control real MSRs is presented, which can be used to establish a route for the creation of highly adaptive design framework.","score":3},{"url":"https://www.semanticscholar.org/paper/8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning","venue":"Conference on Automated Knowledge Base Construction","year":2021,"referenceCount":95,"citationCount":15,"influentialCitationCount":4,"publicationDate":2021,"authors":"Chadi Helwe,C. Clavel,Fabian M. Suchanek","id":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","summary":"This survey paper discusses the performance of transformers on different reasoning tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning.","score":3},{"url":"https://www.semanticscholar.org/paper/21ec9c0f869bdb33b06c7dbc8880169db0397d08","title":"UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark","venue":"AAAI Conference on Artificial Intelligence","year":2021,"referenceCount":77,"citationCount":64,"influentialCitationCount":14,"publicationDate":"24/03/2021","authors":"Nicholas Lourie,Ronan Le Bras,Chandra Bhagavatula,Yejin Choi","id":"21ec9c0f869bdb33b06c7dbc8880169db0397d08","summary":"Two new ways to evaluate commonsense models are proposed, emphasizing their generality on new tasks and building on diverse, recently introduced benchmarks, and a novel evaluation, the cost equivalent curve, is proposed, that sheds new insight on how the choice of source datasets, pretrained language models, and transfer learning methods impacts performance and data efficiency.","score":3},{"url":"https://www.semanticscholar.org/paper/50a00638b8fb2037bf8d06ef7b1f52ae4299f8a1","title":"Question Answering for the Curated Web: Tasks and Methods in QA over Knowledge Bases and Text Collections","venue":"Synthesis Lectures on Information Concepts Retrieval and Services","year":2021,"referenceCount":66,"citationCount":9,"influentialCitationCount":0,"publicationDate":"27/10/2021","authors":"Rishiraj Saha Roy,Avishek Anand","id":"50a00638b8fb2037bf8d06ef7b1f52ae4299f8a1","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/04052cfab34af874498726209225216bb3b89d3d","title":"GENIE: Toward Reproducible and Standardized Human Evaluation for Text Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":71,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/01/2021","authors":"Daniel Khashabi,Gabriel Stanovsky,Jonathan Bragg,Nicholas Lourie,Jungo Kasai,Yejin Choi,Noah A. Smith,Daniel S. Weld","id":"04052cfab34af874498726209225216bb3b89d3d","summary":"This work considers design choices for the annotation interface used to elicit human judgments and their impact on reproducibility, and develops an automated mechanism for maintaining annotator quality via a probabilistic model that detects and excludes noisy annotators.","score":3},{"url":"https://www.semanticscholar.org/paper/9b4d73e7087e2ddc26f0eb3a6412436aa50e4c76","title":"The Defeat of the Winograd Schema Challenge","venue":"ArXiv","year":2022,"referenceCount":97,"citationCount":5,"influentialCitationCount":0,"publicationDate":"07/01/2022","authors":"Vid Kocijan,E. Davis,Thomas Lukasiewicz,G. Marcus,L. Morgenstern","id":"9b4d73e7087e2ddc26f0eb3a6412436aa50e4c76","summary":"The history of the Winograd Schema Challenge is reviewed, and a number of AI systems, based on large pre-trained transformer-based language models and fine-tuned on these kinds of problems, achieved better than 90% accuracy.","score":3},{"url":"https://www.semanticscholar.org/paper/5b44101b2372a33ec06e15ce4d20ad9a15518325","title":"UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":19,"influentialCitationCount":6,"publicationDate":"23/02/2022","authors":"Daniel Khashabi,Yeganeh Kordi,Hannaneh Hajishirzi","id":"5b44101b2372a33ec06e15ce4d20ad9a15518325","summary":"This work presents UNIFIEDQA-v2, a QA model built with the same process as UNifiedQA, except that it utilizes more supervision – roughly 3× the number of datasets used for UNIFIED QA.","score":3},{"url":"https://www.semanticscholar.org/paper/79032fc08981bbb0ae9ae353f399f0df8bdd25ee","title":"Near-Negative Distinction: Giving a Second Life to Human Evaluation Datasets","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/05/2022","authors":"Philippe Laban,Chien-Sheng Wu,Wenhao Liu,Caiming Xiong","id":"79032fc08981bbb0ae9ae353f399f0df8bdd25ee","summary":"This paper proposes a new and simple automatic evaluation method for NLG called Near-Negative Distinction (NND) that repurposes prior human annotations into NND tests that can give a second life to human annotations and provide low-cost NLG evaluation.","score":3},{"url":"https://www.semanticscholar.org/paper/1f1b01cb09e557e6a1e6b0292cb84a67839c2e1a","title":"Lamarckian Platform: Pushing the Boundaries of Evolutionary Reinforcement Learning towards Asynchronous Commercial Games","venue":"IEEE Transactions on Games","year":2022,"referenceCount":61,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/09/2022","authors":"Hui Bai,R. Shen,Yue Lin,Bo Xu,Ran Cheng","id":"1f1b01cb09e557e6a1e6b0292cb84a67839c2e1a","summary":"An open-source platform featuring support for evolutionary reinforcement learning scalable to distributed computing resources and an asynchronous Markov Decision Process interface and an object-oriented software architecture with decoupled modules is introduced.","score":3},{"url":"https://www.semanticscholar.org/paper/15e5935c79433cc132e347c963a22ad86ee682b8","title":"Parallel Reinforcement Learning Simulation for Visual Quadrotor Navigation","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/09/2022","authors":"Jack D. Saunders,Sajad Saeedi,Wenbin Li","id":"15e5935c79433cc132e347c963a22ad86ee682b8","summary":"A simulation framework, built on AirSim, which provides efﬁcient parallel training of RL agents and Ape-X is modiﬂed to incorporate decentralised training of AirSim environments to make use of numerous networked computers.","score":3},{"url":"https://www.semanticscholar.org/paper/bd1173bfa8955222517cd8b6f6831dac4400cc67","title":"Open-Ended Diverse Solution Discovery with Regulated Behavior Patterns for Cross-Domain Adaptation","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/09/2022","authors":"Kang Xu,Yan Ma,Wei Li,Bingsheng Wei","id":"bd1173bfa8955222517cd8b6f6831dac4400cc67","summary":"Diversity in Regulation is proposed, training diverse policies with regulated behaviors to discover desired patterns that generalize to downstream environments with various discrepancies and indicates that this method attains improvements over other diversity-driven counterparts.","score":3},{"url":"https://www.semanticscholar.org/paper/35f8eb09776abebad8a963b59978d673ab97301a","title":"Understanding Hindsight Goal Relabeling from a Divergence Minimization Perspective","venue":"","year":2022,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/09/2022","authors":"Lunjun Zhang,Bradly C. Stadie","id":"35f8eb09776abebad8a963b59978d673ab97301a","summary":"This paper proposes a novel framework to understand hindsight goal relabeling from a divergence minimization perspective and explains the puz-zling phenomenon wherein a reward of {− 1 , 0 } results in signiﬁcantly better performance than a { 0, 1 } reward for goal reaching.","score":3},{"url":"https://www.semanticscholar.org/paper/d4087ce0cbf828f05ea5d7494eee3b0d36a13cb9","title":"Learning Low-Frequency Motion Control for Robust and Dynamic Robot Locomotion","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/09/2022","authors":"Siddhant Gangapurwala,Luigi Campanaro,I. Havoutis","id":"d4087ce0cbf828f05ea5d7494eee3b0d36a13cb9","summary":"It is shown that low-frequency policies are less sensitive to actuation latencies and variations in system dynamics, and to the extent that a successful sim-to-real transfer can be performed even without any dynamics randomization or actuation modeling.","score":3},{"url":"https://www.semanticscholar.org/paper/8ccf4d4e3a611113b6529ccaee33ca3ead7b256b","title":"Optimal Tasking of Ground-Based Sensors for Space Situational Awareness Using Deep Reinforcement Learning","venue":"Italian National Conference on Sensors","year":2022,"referenceCount":30,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/10/2022","authors":"Peng Mun Siew,R. Linares","id":"8ccf4d4e3a611113b6529ccaee33ca3ead7b256b","summary":"This work successfully applied deep reinforcement learning (DRL) to overcome the curse of dimensionality and optimally task a ground-based sensor and trained several DRL agents that outperformed myopic policies in both objective metrics of RSOs’ state uncertainties and the number of unique RSO's observed over a 90-min observation window.","score":3},{"url":"https://www.semanticscholar.org/paper/3827bf251a52cd80489a6578c8e05f1521b7a69e","title":"Resilient Mechanism Against Byzantine Failure for Distributed Deep Reinforcement Learning","venue":"IEEE International Symposium on Software Reliability Engineering","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/10/2022","authors":"Mingyue Zhang,Zhi Jin,Jian Hou,Renwei Luo","id":"3827bf251a52cd80489a6578c8e05f1521b7a69e","summary":"A resilient mechanism for mitigating the influence of Byzantine workers on DDRL-based systems is proposed and a resilient distributed A3C (ReD-A3C) is implemented, which outperforms available Byzantine tolerant approaches.","score":3},{"url":"https://www.semanticscholar.org/paper/d7fdbaa9483daf72a6c712f08efe3e04f0047509","title":"Probabilistic Safeguard for Reinforcement Learning Using Safety Index Guided Gaussian Process Models","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/10/2022","authors":"Weiye Zhao,Tairan He,Changliu Liu","id":"d7fdbaa9483daf72a6c712f08efe3e04f0047509","summary":"This paper presents an integrated model learning and safe control framework to safeguard any agent, where its dynamics are learned as Gaussian processes and guarantees almost zero safety violation on various continuous control tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/1c6035ba208d64d119b7cf6a895f9e71d662b90c","title":"Generative Augmented Flow Networks","venue":"ArXiv","year":2022,"referenceCount":24,"citationCount":6,"influentialCitationCount":0,"publicationDate":"07/10/2022","authors":"L. Pan,Dinghuai Zhang,Aaron C. Courville,Longbo Huang,Y. Bengio","id":"1c6035ba208d64d119b7cf6a895f9e71d662b90c","summary":"GAFlowNets is proposed, a novel learning framework to incorporate intermediate rewards by intrinsic motivation to tackle the exploration problem in sparse reward environments and is scalable to a more complex and large-scale molecule generation domain, where it achieves consistent and signiﬁcant performance improvement.","score":3},{"url":"https://www.semanticscholar.org/paper/db7d94055535a116c30e61dd7b83eb37ac0defed","title":"Towards a Theoretical Foundation of Policy Optimization for Learning Control Policies","venue":"ArXiv","year":2022,"referenceCount":168,"citationCount":6,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"B. Hu,K. Zhang,N. Li,M. Mesbahi,Maryam Fazel,Tamer Bacsar","id":"db7d94055535a116c30e61dd7b83eb37ac0defed","summary":"A number of recently-developed theoretical results on the optimization landscape, global convergence, and sample complexity of gradient-based methods for various continuous control problems such as the linear quadratic regulator (LQR), H ∞ control, risk-sensitive control, linear Quadratic Gaussian control, and output feedback synthesis are reviewed.","score":3},{"url":"https://www.semanticscholar.org/paper/4bbbb6ee03d35a7c032a21ed98d9c53609f8a348","title":"In-Hand Object Rotation via Rapid Motor Adaptation","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Haozhi Qi,Ashish Kumar,R. Calandra,Yinsong Ma,J. Malik","id":"4bbbb6ee03d35a7c032a21ed98d9c53609f8a348","summary":"This work designs and learns a simple adaptive controller that can be directly deployed to a real robot hand to rotate dozens of objects with diverse sizes, shapes, and weights over the z -axis via rapid online adaptation of the robot's controller to the object properties using only proprioception history.","score":3},{"url":"https://www.semanticscholar.org/paper/d27be7f60b256d5dc8facb1337aede953b8ca9ef","title":"Behavior policy learning: Learning multi-stage tasks via solution sketches and model-based controllers","venue":"Frontiers in Robotics and AI","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Konstantinos Tsinganos,Konstantinos Chatzilygeroudis,Denis Hadjivelichkov,Theodoros Komninos,E. Dermatas,D. Kanoulas","id":"d27be7f60b256d5dc8facb1337aede953b8ca9ef","summary":"This paper proposes Behavior Policy Learning (BPL) that effectively combines only few solution sketches, model-based controllers, and simulations to effectively solve multi-stage tasks without strong knowledge about the underlying task.","score":3},{"url":"https://www.semanticscholar.org/paper/c3355f238b34edfb4ad5d375801d3b61f631d5c5","title":"Real World Offline Reinforcement Learning with Realistic Data Source","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"G. Zhou,Liyiming Ke,S. Srinivasa,Abhi Gupta,A. Rajeswaran,Vikash Kumar","id":"c3355f238b34edfb4ad5d375801d3b61f631d5c5","summary":"It is posited that data collected from safe operations of closely related tasks are more practical data sources for real-world robot learning and that ORL algorithms can generalize from leveraging ofﬂine heterogeneous data sources and outperform imitation learning.","score":3},{"url":"https://www.semanticscholar.org/paper/e5f03282c562f3c51ae1b874a5d2e1071fe351d4","title":"Learning multi-agent cooperation","venue":"Frontiers in Neurorobotics","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Corban G. Rivera,Edward W. Staley,A. Llorens","id":"e5f03282c562f3c51ae1b874a5d2e1071fe351d4","summary":"The AI Arena is introduced: a scalable framework with flexible abstractions for associating agents with policies and policies with learning algorithms that enable AI research for complex operating environments that incorporate distributed, heterogeneous teams of agents.","score":3},{"url":"https://www.semanticscholar.org/paper/091a4fb606011b0ffa8a9a9cb64eee161bae4036","title":"Just Round: Quantized Observation Spaces Enable Memory Efficient Learning of Dynamic Locomotion","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Lev Grossman,B. Plancher","id":"091a4fb606011b0ffa8a9a9cb64eee161bae4036","summary":"This paper evaluates the approach using four simulated robot locomotion tasks and two state-of-the-art DRL algorithms, finding that observation space quantization reduces overall memory costs by as much as 4 .","score":3},{"url":"https://www.semanticscholar.org/paper/cdd2b92a54bda70ead0f95e2695f8ed77122724f","title":"Towards an Interpretable Hierarchical Agent Framework using Semantic Goals","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/10/2022","authors":"Bharat Prakash,Nicholas R. Waytowich,T. Oates,T. Mohsenin","id":"cdd2b92a54bda70ead0f95e2695f8ed77122724f","summary":"This work introduces an interpretable hierarchical agent framework by combining planning and semantic goal directed reinforce- ment learning, which assumes access to certain spatial and haptic predicates and construct a simple and powerful semantic goal space.","score":3},{"url":"https://www.semanticscholar.org/paper/db182e410abd8ac1bf6d569cda0f1193d36ef1e4","title":"Indoor Target-Driven Visual Navigation based on Spatial Semantic Information","venue":"International Conference on Information Photonics","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/10/2022","authors":"Jiaojie Yan,Qieshi Zhang,Jun Cheng,Ziliang Ren,Tianbo Li,Zhuo Yang","id":"db182e410abd8ac1bf6d569cda0f1193d36ef1e4","summary":"An end-to-end target-driven visual navigation method, which uses Spatial Semantic Information (SSI) to navigate the agent to the target, is presented and to fully integrate the spatial and semantic information in the scene, visual information is encoded into an 8-D spatial context vector.","score":3},{"url":"https://www.semanticscholar.org/paper/5d83896a08584a3018fcc8b4031cc4a39450e3f1","title":"CLUTR: Curriculum Learning via Unsupervised Task Representation Learning","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/10/2022","authors":"Abdus Salam Azad,Izzeddin Gur,Aleksandra Faust,P. Abbeel,I. Stoica","id":"5d83896a08584a3018fcc8b4031cc4a39450e3f1","summary":"This work introduces CLUTR: a novel curriculum learning algorithm that decouples task representation and curriculum learning into a two-stage optimization that outperforms PAIRED in terms of generalization and sample efficiency in the challenging CarRacing and navigation environments.","score":3},{"url":"https://www.semanticscholar.org/paper/6600be062e90ac6834dab1b33f5459087df14d2b","title":"RMBench: Benchmarking Deep Reinforcement Learning for Robotic Manipulator Control","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/10/2022","authors":"Yanfei Xiang,Xin Wang,Shu Hu,Bin Zhu,Xiao-lin Huang,Xi Wu,Siwei Lyu","id":"6600be062e90ac6834dab1b33f5459087df14d2b","summary":"RMBench is presented, the first benchmark for robotic manipulations, which have high-dimensional continuous action and state spaces, and soft Actor- Critic outperforms most algorithms in average reward and stability, and an algorithm combined with data augmentation may facilitate learning policies.","score":3},{"url":"https://www.semanticscholar.org/paper/5c2ee755a616e76241313659f9009a859889cda9","title":"Trust Region Policy Optimization with Optimal Transport Discrepancies: Duality and Algorithm for Continuous Actions","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/10/2022","authors":"Antonio Terpin,Nicolas Lanzetti,Batuhan Yardim,F. Dörfler,Giorgia Ramponi","id":"5c2ee755a616e76241313659f9009a859889cda9","summary":"Optimal transport discrepancies are explored to define trust regions, and a novel algorithm – Optimal Transport Trust Region Policy Optimization (OT-TRPO) – is proposed for continuous state-action spaces to show that optimal transport discrepancies can offer an advantage over state-of-the-art approaches.","score":3},{"url":"https://www.semanticscholar.org/paper/5c926e61729fb37fd4f635bd42760aa20faf07ec","title":"Task Decoupling in Preference-based Reinforcement Learning for Personalized Human-Robot Interaction","venue":"IEEE/RJS International Conference on Intelligent RObots and Systems","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"Mingjiang Liu,Chunlin Chen","id":"5c926e61729fb37fd4f635bd42760aa20faf07ec","summary":"This work decouple the task from preference in human-robot interaction, and utilizes a sketchy task reward derived from task priori to instruct robots to conduct more effective task exploration and incorporates prior knowledge of the task into preference-based RL.","score":3},{"url":"https://www.semanticscholar.org/paper/4f945ce66f8086acd68f02874c3d0cbc20f42480","title":"Multi-Agent Path Finding via Tree LSTM","venue":"ArXiv","year":2022,"referenceCount":15,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/10/2022","authors":"Yuhao Jiang,Kunjie Zhang,Qimai Li,Jiaxin Chen,Xiaolong Zhu","id":"4f945ce66f8086acd68f02874c3d0cbc20f42480","summary":"A novel network architecture, TreeLSTM, is creatively applied to MAPF and together with several other RL techniques, including reward shaping, multiple-phase training, and centralized control, the solution is comparable to the top 2-3 OR methods.","score":3},{"url":"https://www.semanticscholar.org/paper/4521c33ad2f84385498bcd8d21db420f36805523","title":"Avalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/10/2022","authors":"Joshua Albrecht,A. Fetterman,Bryden Fogelman,Ellie Kitanidis,Bartosz Wr'oblewski,Nicole Seo,Michael Rosenthal,Maksis Knutins,Zachary Polizzi,James B. Simon,Kanjun Qiu","id":"4521c33ad2f84385498bcd8d21db420f36805523","summary":"Avalon is a set of tasks in which embodied agents in highly diverse procedural 3D worlds must survive by navigating terrain, hunting or gathering food, and avoiding hazards, suggesting Avalon is challenging enough to advance the quest for generalizable RL.","score":3},{"url":"https://www.semanticscholar.org/paper/fb090d312f6f33b7f9bb44f04f81304fb7a11bac","title":"Learning Deep Sensorimotor Policies for Vision-based Autonomous Drone Racing","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Jia-Wei Fu,Yunlong Song,Y. Wu,F. Yu,D. Scaramuzza","id":"fb090d312f6f33b7f9bb44f04f81304fb7a11bac","summary":"This work uses contrastive learning to extract robust feature representations from the input images and leverage a two-stage learning-by- cheating framework for training a neural network policy that directly infers control commands with feature representations learned from raw images, forgoing the need for globally-consistent state estimation, trajectory planning, and handcrafted control design.","score":3},{"url":"https://www.semanticscholar.org/paper/1e1d13b81f8138276e97a5932c61e3e834d8665f","title":"Efficient Language-Guided Reinforcement Learning for Resource-Constrained Autonomous Systems","venue":"IEEE Micro","year":2022,"referenceCount":15,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Aidin Shiri,Mozhgan Navardi,Tejaswini Manjunath,Nicholas R. Waytowich,T. Mohsenin","id":"1e1d13b81f8138276e97a5932c61e3e834d8665f","summary":"An energy-efficient architecture, which is designed to receive both images and text inputs as a step toward designing reinforcement learning agents that can understand human language and act in real-world environments is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/f2d65fdc50df42aabae7491d2a8c25c75a35bf64","title":"Fair Virtual Network Function Mapping and Scheduling Using Proximal Policy Optimization","venue":"IEEE Transactions on Communications","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Zhenran Kuai,Tianyu Wang,Shaowei Wang","id":"f2d65fdc50df42aabae7491d2a8c25c75a35bf64","summary":"A deep reinforcement learning method based on offline proximal policy optimization, which dynamically determines the mapping and scheduling decision based on the state of unfinished service chains, is proposed, which is scalable to the number of service chains and can be enhanced by Monte Carlo tree search.","score":3},{"url":"https://www.semanticscholar.org/paper/c934669e8c0816b4ffbf8fe3cebabb6779e3ec80","title":"Regret Analysis for RL using Renewal Bandit Feedback","venue":"Information Theory Workshop","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Sujay Bhatt,Guanhua Fang,P. Li,G. Samorodnitsky","id":"c934669e8c0816b4ffbf8fe3cebabb6779e3ec80","summary":"A new perspective on model-based learning in MDPs using ideas from renewal theory is presented and a naive algorithm based on this reformulation of the problem of controlling a Markov chain to a renewal reward process with bandit feedback is provided.","score":3},{"url":"https://www.semanticscholar.org/paper/56211d8b1c94032312b3ce4fd1f06bf81db8d7a7","title":"HMDRL: Hierarchical Mixed Deep Reinforcement Learning to Balance Vehicle Supply and Demand","venue":"IEEE transactions on intelligent transportation systems (Print)","year":2022,"referenceCount":53,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Jinhao Xi,F. Zhu,Peijun Ye,Yisheng Lv,Haina Tang,Fei-yue Wang","id":"56211d8b1c94032312b3ce4fd1f06bf81db8d7a7","summary":"Experimental results demonstrate that HMDRL outperforms all the other methods to reposition idle vehicles and aims to improve the training effect by avoiding sparse rewards.","score":3},{"url":"https://www.semanticscholar.org/paper/e8a2929e2ba75768865feb4b172e15aec45b37c5","title":"Leveraging Fully Observable Policies for Learning under Partial Observability","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/11/2022","authors":"Hai Nguyen,Andrea Baisero,Dian Wang,Chris Amato,Robert W. Platt","id":"e8a2929e2ba75768865feb4b172e15aec45b37c5","summary":"This work proposes a method for partially observable reinforcement learning that uses a fully observable policy (which it is called a state expert) during ofﬂine training to improve online performance and outperforms pure imitation, pure reinforcement learning, the sequential or parallel combination of both types, and a recent state-of-the-art method in the same setting.","score":3},{"url":"https://www.semanticscholar.org/paper/1b6fd3b9be019af0747fe195a46941bbf116614c","title":"Optimal Behavior Prior: Data-Efficient Human Models for Improved Human-AI Collaboration","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/11/2022","authors":"Mesut Yang,Micah Carroll,A. Dragan","id":"1b6fd3b9be019af0747fe195a46941bbf116614c","summary":"It is shown that using optimal behavior as a prior for human models makes these models vastly more data-efﬁcient and able to generalize to new environments and leads to better human-AI collaboration performance compared to using models based on real human data alone.","score":3},{"url":"https://www.semanticscholar.org/paper/19ca582aea8819a25357342dae5efd367765e8c5","title":"Policy-Based Reinforcement Learning for Assortative Matching in Human Behavior Modeling","venue":"ArXiv","year":2022,"referenceCount":12,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/11/2022","authors":"O. Deng,Q. Jin","id":"19ca582aea8819a25357342dae5efd367765e8c5","summary":"A modeling approach based on Multi-Agent Reinforcement Learning (MARL) is proposed, adding a multi-head attention function to the A3C algorithm to enhance learning effectiveness, which simulates human behavior in certain scenarios through various environmental parameter settings and agent action strategies.","score":3},{"url":"https://www.semanticscholar.org/paper/c8a7a5af3dae60a6732316beecd602c880c985d4","title":"An efficient training strategy for multi-agent reinforcement learning in card games","venue":"Other Conferences","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Jiyuan Shen","id":"c8a7a5af3dae60a6732316beecd602c880c985d4","summary":"The experiment found that the gradual promotion training strategy can effectively improve the winning rate and average reward of the agent and is an extremely convenient and easy training strategy to implement.","score":3},{"url":"https://www.semanticscholar.org/paper/490b6db3ce7cf9743c5a7c3125ab9dca240ce907","title":"Deep Reinforcement Learning with Vector Quantized Encoding","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/11/2022","authors":"Liang Zhang,Justin Lieffers,A. Pyarelal","id":"490b6db3ce7cf9743c5a7c3125ab9dca240ce907","summary":"A novel method for clustering state features in deep reinforcement learning (RL) methods to prove their interpretability and its impact on robustness and generalization of deep RL is investigated.","score":3},{"url":"https://www.semanticscholar.org/paper/c25933713708bca9a7a2c1379e9783c88a3afdd3","title":"Legged Locomotion in Challenging Terrains using Egocentric Vision","venue":"ArXiv","year":2022,"referenceCount":91,"citationCount":6,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Ananye Agarwal,Ashish Kumar,Jitendra Malik,Deepak Pathak","id":"c25933713708bca9a7a2c1379e9783c88a3afdd3","summary":"This paper presents the first end-to-end locomotion system capable of traversing stairs, curbs, stepping stones, and gaps on a medium-sized quadruped robot using a single front-facing depth camera.","score":3},{"url":"https://www.semanticscholar.org/paper/ea14465601f45bf50a148563c73c4d6e1971dcb1","title":"Redeeming Intrinsic Rewards via Constrained Optimization","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"E. Chen,Zhang-Wei Hong,J. Pajarinen,Pulkit Agrawal","id":"ea14465601f45bf50a148563c73c4d6e1971dcb1","summary":"A principled constrained policy optimization procedure that automatically tunes the importance of the intrinsic reward is proposed, which results in superior exploration that does not require manual tuning to balance the intrinsic rewards against the task reward.","score":3},{"url":"https://www.semanticscholar.org/paper/022c285ba675ae5ef5f9b0c755a625dcb1aa8ca3","title":"Pandering in a Flexible Representative Democracy","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/11/2022","authors":"Xiaolin Sun,Jacob Masur,Ben Abramowitz,Nicholas Mattei,Zizhan Zheng","id":"022c285ba675ae5ef5f9b0c755a625dcb1aa8ca3","summary":"A novel formal model of pandering, or strategic preference reporting by candidates seeking to be elected, is introduced, and the resilience of two democratic voting systems to pandering within a single round and across multiple rounds is examined.","score":3},{"url":"https://www.semanticscholar.org/paper/87ed4d733690c38aa2287b53ad2d72f14946d0b1","title":"Automating Rigid Origami Design","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/11/2022","authors":"Jeremia Geiger,Karolis Martinkus,Oliver Richter,Roger Wattenhofer","id":"87ed4d733690c38aa2287b53ad2d72f14946d0b1","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/89294e0b8bc32c563291f261f1172fdc11214f4b","title":"Noisy Symbolic Abstractions for Deep RL: A case study with Reward Machines","venue":"ArXiv","year":2022,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/11/2022","authors":"Andrew C. Li,Zizhao Chen,Pashootan Vaezipoor,Toryn Q. Klassen,Rodrigo Toro Icarte,Sheila A. McIlraith","id":"89294e0b8bc32c563291f261f1172fdc11214f4b","summary":"This work forms the problem of policy learning in Reward Machines with noisy symbolic abstractions as a special class of POMDP optimization problem, and investigates several methods to address the problem, building on existing and new techniques.","score":3},{"url":"https://www.semanticscholar.org/paper/ec25b1b13c5e74c919abb0dd000cae964f13b0fe","title":"SafeLight: A Reinforcement Learning Method toward Collision-free Traffic Signal Control","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/11/2022","authors":"Wenlu Du,J. Ye,Jingyi Gu,Jing Li,Hua Wei,Gui-Liu Wang","id":"ec25b1b13c5e74c919abb0dd000cae964f13b0fe","summary":"This work incorporates road safety standards as enforcement to ensure the safety of existing reinforcement learning methods, aiming toward operating intersections with zero collisions, and proposes a safety-enhanced residual reinforcement learning method (SafeLight), which can significantly reduce collisions while increasing traffic mobility.","score":3},{"url":"https://www.semanticscholar.org/paper/aa13a24ce0f242fb328603c83d456eafba177dbe","title":"Examining Policy Entropy of Reinforcement Learning Agents for Personalization Tasks","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"A. Dereventsov,Andrew Starnes,C. Webster","id":"aa13a24ce0f242fb328603c83d456eafba177dbe","summary":"This effort is focused on examining the behavior of reinforce- ment learning systems in personalization environments and detailing the differences in policy entropy associated with the type of learning algorithm utilized, demonstrating that Policy Optimization agents often possess low-entropy policies during training and Q-Learning agents are far less susceptible to such behavior.","score":3},{"url":"https://www.semanticscholar.org/paper/8225ac1bec34a645d45d903642a8af59b3367cc2","title":"Tackling Visual Control via Multi-View Exploration Maximization","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":1,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"Mingqi Yuan,Xin Jin,Bo Li,Wenjun Zeng","id":"8225ac1bec34a645d45d903642a8af59b3367cc2","summary":"MEM can signiﬁcantly promote the sample-e-ciency and generalization ability of the RL agent, facilitating solving real-world problems with high-dimensional observations and spare-reward space and outperform the benchmarking schemes with simple architecture and higher eﬃciency.","score":3},{"url":"https://www.semanticscholar.org/paper/d8753089a4a57cc5b147d461d8e2c3585f0e8a38","title":"Dynamic Bipedal Turning through Sim-to-Real Reinforcement Learning","venue":"IEEE-RAS International Conference on Humanoid Robots","year":2022,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"Fangzhou Yu,Ryan Batke,Jeremy Dao,J. Hurst,Kevin R. Green,Alan Fern","id":"d8753089a4a57cc5b147d461d8e2c3585f0e8a38","summary":"This work applies a recurrent policy to execute four-step, 90° turns trained using reference data generated from optimized single rigid body model trajectories and presents a training framework using epilogue terminal rewards for learning specific behaviors from pre-computed trajectory data.","score":3},{"url":"https://www.semanticscholar.org/paper/90cd4aa3176e3f176a81c1fc834cb8352ffe2564","title":"End-to-End Mobile Robot Navigation using a Residual Deep Reinforcement Learning in Dynamic Human Environments","venue":"IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications","year":2022,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"Abdullah Ahmed,Yasser F. O. Mohammad,V. Parque,Haitham El-Hussieny,S. Ahmed","id":"90cd4aa3176e3f176a81c1fc834cb8352ffe2564","summary":"An architecture based on convolutional units and residual blocks being able to enhance adaptability to unseen and dynamic human environments is reported, which outperformed the state-of-the-art baselines SOADRL and NAVREP.","score":3},{"url":"https://www.semanticscholar.org/paper/d052b40fb39024c8858b28fd644d63c4cf2cb22f","title":"DeepADMR: A Deep Learning based Anomaly Detection for MANET Routing","venue":"IEEE Military Communications Conference","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"Alex Yahja,Saeed Kaviani,Bo Ryu,Jae H. Kim,Kevin Larson","id":"d052b40fb39024c8858b28fd644d63c4cf2cb22f","summary":"The DeepADMR performance in the presence of channel disruptions, high mobility levels, and network sizes beyond the training environments, which shows its effectiveness.","score":3},{"url":"https://www.semanticscholar.org/paper/5f76d20ce2fe6c655de9ddde525ac339d66190ca","title":"Enhancing Robot Task Completion Through Environment and Task Inference: A Survey from the Mobile Robot Perspective","venue":"Journal of Intelligent and Robotic Systems","year":2022,"referenceCount":182,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"A. H. Tan,G. Nejat","id":"5f76d20ce2fe6c655de9ddde525ac339d66190ca","summary":"The first extensive investigation of mobile robot inference problems in unknown environments with limited sensor and communication range is presented and a new taxonomy to classify the different environment and task inference methods for single- and multi-robot systems is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/c6209b74d296b3db01e41372976b044158f2d205","title":"Karolos: An Open-Source Reinforcement Learning Framework for Robot-Task Environments","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Christian Bitter,Timo Thun,Tobias Meisen","id":"c6209b74d296b3db01e41372976b044158f2d205","summary":"Karolos is introduced, a RL framework developed for robotic applications, with a particular focus on transfer scenarios with varying robot-task combinations reﬂected in a modular environment architecture that provides implementations of state-of-the-art RL algorithms along with common learning-facilitating enhancements.","score":3},{"url":"https://www.semanticscholar.org/paper/7595d53e249d9e6e43049b81ae02d347ac08b836","title":"Deep Reinforcement Learning With Part-Aware Exploration Bonus in Video Games","venue":"IEEE Transactions on Games","year":2022,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Pei Xu,Qiyue Yin,Junge Zhang,Kaiqi Huang","id":"7595d53e249d9e6e43049b81ae02d347ac08b836","summary":"The effectiveness of introducing prior learned features for existing prediction-based exploration methods is demonstrated and an attention map mechanism is designed to discretize learned features, thereby updating the learned feature and meanwhile reducing the impact of randomness on intrinsic rewards caused by the learning process of features.","score":3},{"url":"https://www.semanticscholar.org/paper/1e9a22488bd872b9f1ee71216e2d6848c266e15d","title":"DRAS: Deep Reinforcement Learning for Cluster Scheduling in High Performance Computing","venue":"IEEE Transactions on Parallel and Distributed Systems","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Yuping Fan,Boyang Li,Dustin Favorite,Naunidh Singh,T. Childers,P. Rich,W. Allcock,M. Papka,Z. Lan","id":"1e9a22488bd872b9f1ee71216e2d6848c266e15d","summary":"This work presents an automated HPC scheduling agent named DRAS (Deep Reinforcement Agent for Scheduling) by leveraging deep reinforcement learning and demonstrates that DRAS outperforms the existing heuristic and optimization approaches by up to 50%.","score":3},{"url":"https://www.semanticscholar.org/paper/4cb0fc6739f79b7dc06d448624e930e40a7064f4","title":"Policy Learning for Active Target Tracking over Continuous SE(3) Trajectories","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/12/2022","authors":"Pengzhi Yang,Shumon Koga,Arash Asgharivaskasi,Nikolay A. Atanasov","id":"4cb0fc6739f79b7dc06d448624e930e40a7064f4","summary":"A novel model-based policy gradient algorithm for tracking dynamic targets using a mobile robot, equipped with an onboard sensor with limited view, to obtain a continuous control policy for the mobile robot to collect sensor measurements that reduce uncertainty in the target states, measured by the target distribution entropy.","score":3},{"url":"https://www.semanticscholar.org/paper/0212f5f26397d64a9df4c532c912dbcc6df94dab","title":"Distributed Reinforcement Learning for Low-delay Uplink User Scheduling in Multicell Networks","venue":"Global Communications Conference","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/12/2022","authors":"A. Destounis,D. Tsilimantos","id":"0212f5f26397d64a9df4c532c912dbcc6df94dab","summary":"The results illustrate that the proposed algorithm outperforms standard schedulers, such as Proportional Fair and MaxWeight and that information exchange is crucial for challenging problem instances,such as topologies with many devices on the cell edge and/or high traffic demands.","score":3},{"url":"https://www.semanticscholar.org/paper/583f72e9fd06cf2a028a1f871b852410734ec4b9","title":"A Mobile Robot Experiment System with Lightweight Simulator Generator for Deep Reinforcement Learning Algorithm","venue":"IEEE International Conference on Robotics and Biomimetics","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Yunfei Xiang,Jiantao Qiu,Jincheng Yu,Jiahao Tang,Guangjun Ge,Yu Wang,Huazhong Yang","id":"583f72e9fd06cf2a028a1f871b852410734ec4b9","summary":"A fast simulator generation method using linear approximate kinematics model and bake-based lidar rendering methods to generate a fast approximate simulator used in the pre-experimental stage to solve the problem of data cost is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/74ac29cf04ea3b55ebf30fa34a23d86ea1f1b590","title":"Structure-Aware Policy to Improve Generalization among Various Robots and Environments","venue":"IEEE International Conference on Robotics and Biomimetics","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Wei-qing Xu,Yue Gao,Buqing Nie","id":"74ac29cf04ea3b55ebf30fa34a23d86ea1f1b590","summary":"A novel DRL generalization method called GNN-embedding is proposed, which incorporates the robot hardware and the environment simultaneously withGNN-based policy network and learnable embedding vectors of tasks and improves the generalization performance of existing DRL robot policies.","score":3},{"url":"https://www.semanticscholar.org/paper/776975b39a1672dad9203d9bc469d05cbe44d35e","title":"Multiple Subgoals-guided Hierarchical Learning in Robot Navigation","venue":"IEEE International Conference on Robotics and Biomimetics","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Shan Luo,L. Schomaker","id":"776975b39a1672dad9203d9bc469d05cbe44d35e","summary":"This work introduces multiple subgoals-guided navigation (MSGN) which consists of a high-level multiple subGoals Planner and a low-level goal-conditioned RL Controller which could alleviate the suboptimal subgoal problem by transferring the subgoal selection process to the RL agent.","score":3},{"url":"https://www.semanticscholar.org/paper/e150cfc540712807193f2ae10284338a8efeea58","title":"Cooperative Guidance Strategy for Active Defense Spacecraft with Imperfect Information via Deep Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Li Zhi,Haizhao Liang,Jinze Wu,Jianying Wang,Yu Zheng","id":"e150cfc540712807193f2ae10284338a8efeea58","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/fcab3c0dd4e03ce3e664e422a22f8309049e91f4","title":"Trajectory planning of a manipulator based on the DDPG algorithm","venue":"Other Conferences","year":2022,"referenceCount":18,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/12/2022","authors":"Xiangyu Jing,C. Zang,Yuqi Liu,Yu Liu,Changqing Xia","id":"fcab3c0dd4e03ce3e664e422a22f8309049e91f4","summary":"The findings demonstrate that the DDPG method converges easier than the AC algorithm, and that the directed reward function can help the manipulator avoid obstacles.","score":3},{"url":"https://www.semanticscholar.org/paper/a38167796e525c5c384050f5a3a281274beb0f78","title":"Reinforcement Learning and Mixed-Integer Programming for Power Plant Scheduling in Low Carbon Systems: Comparison and Hybridisation","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/12/2022","authors":"C. O'Malley,Patrick de Mars,Luis Badesa,G. Strbac","id":"a38167796e525c5c384050f5a3a281274beb0f78","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/ce0acbba6d2feb2870ab4a3ce72067429c358a40","title":"Deep Reinforcement Learning for Queue-Time Management in Semiconductor Manufacturing","venue":"Online World Conference on Soft Computing in Industrial Applications","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/12/2022","authors":"Harel Yedidsion,Prafulla Dawadi,David Norman,E. Zarifoglu","id":"ce0acbba6d2feb2870ab4a3ce72067429c358a40","summary":"This work proposes a deep Reinforcement Learning (RL) method to manage releasing lots into the queue time constraint and shows that the RL method outperforms the baselines in five performance metrics including the number of queue-time violations and makespan, while requiring negligible online compute time.","score":3},{"url":"https://www.semanticscholar.org/paper/5df105539a7185b6422586aae9c40e7144b9def9","title":"Behavior analysis of emergent rule discovery for cooperative automated driving using deep reinforcement learning","venue":"Artificial Life and Robotics","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/12/2022","authors":"Tomohiro Harada,Johei Matsuoka,K. Hattori","id":"5df105539a7185b6422586aae9c40e7144b9def9","summary":"The simulation results showed that reinforcement learning achieves rational control of the overtaking behavior and indicates that the proposed multi-agent learning method with an extended own-vehicle environment has the potential to learn automated vehicle control with cooperative behavior automatically.","score":3},{"url":"https://www.semanticscholar.org/paper/1d8c10f09a97b699706967d7315ce368b1047263","title":"Anticipatory Fictitious Play","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Alex Cloud,Albert Wang,W. Kerr","id":"1d8c10f09a97b699706967d7315ce368b1047263","summary":"This paper presents a simple modiﬁcation of ﬁctitious play which is a strict improvement over the original: it has the same theoretical worst-case convergence rate, is equally applicable in a machine learning context, and enjoys superior empirical performance.","score":3},{"url":"https://www.semanticscholar.org/paper/d04c8caaddd159ff6696babec6f09ec13b4e81b5","title":"Hyperparameters in Contextual RL are Highly Situational","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":2,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Theresa Eimer,C. Benjamins,M. Lindauer","id":"d04c8caaddd159ff6696babec6f09ec13b4e81b5","summary":"It is shown that agents in contextual RL require different hyperparameters if they are shown how environmental factors change, further highlighting the need for research into how hyperparameter learning and generalization in RL is handled.","score":3},{"url":"https://www.semanticscholar.org/paper/89bfce65591e1f26df76c90be0fc5cb2629dcba7","title":"Deep Reinforcement Learning for Heat Pump Control","venue":"ArXiv","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/12/2022","authors":"T. Rohrer,Lilli Frison,Lukas Kaupenjohann,K. Scharf,Elke Hergenröther","id":"89bfce65591e1f26df76c90be0fc5cb2629dcba7","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/e8cd8e1ee73ce6059c1a08acd5cc088fee2e7a29","title":"Stock Trading Optimization through Model-based Reinforcement Learning with Normalizing Fl (preprint)","venue":"","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Huifang Huang,Ting Gao,Pengbo Li,Jinqiu Guo,P. Zhang","id":"e8cd8e1ee73ce6059c1a08acd5cc088fee2e7a29","summary":"This paper demonstrates normalizing flows is adopted to simulated high-dimensional joint probability of the complex trading environment, and develops a novel model based reinforcement learning framework to better understand the intrinsic mechanisms of quantitative online trading.","score":3},{"url":"https://www.semanticscholar.org/paper/387aed1f28d352fffa04e6d18c0ec46de6f8133e","title":"Reinforcement Learning From Hierarchical Critics","venue":"IEEE Transactions on Neural Networks and Learning Systems","year":2019,"referenceCount":19,"citationCount":11,"influentialCitationCount":0,"publicationDate":"08/02/2019","authors":"Zehong Cao,Chin-Teng Lin","id":"387aed1f28d352fffa04e6d18c0ec46de6f8133e","summary":"The proposed RL from the hierarchical critics (RLHC) algorithm outperforms the benchmark on these four competitive tasks under four experimental scenarios consisting of tennis, soccer, banana collection, and crawler competitions within the Unity environment.","score":3},{"url":"https://www.semanticscholar.org/paper/f92baa1d3bb07c6972b5a712100205154079bbd8","title":"Development and Validation of an AI-Driven Model for the La Rance Tidal Barrage: A Generalisable Case Study","venue":"Applied Energy","year":2022,"referenceCount":84,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2022","authors":"Túlio Marcondes Moreira,Jackson Geraldo de Faria,Pedro O. S. Vaz de Melo,G. Medeiros-Ribeiro","id":"f92baa1d3bb07c6972b5a712100205154079bbd8","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/c2dd4b8436443c05033317b1a13f82f5e1a18ee5","title":"Machine Learning for Computer Systems and Networking: A Survey","venue":"ACM Computing Surveys","year":2022,"referenceCount":207,"citationCount":2,"influentialCitationCount":0,"publicationDate":"09/03/2022","authors":"Marios Evangelos Kanakis,R. Khalili,Lin Wang","id":"c2dd4b8436443c05033317b1a13f82f5e1a18ee5","summary":"This article attempts to shed light on recent literature that appeals for machine learning-based solutions to traditional problems in computer systems and networking, and introduces a taxonomy based on a set of major research problem domains.","score":3},{"url":"https://www.semanticscholar.org/paper/2c607661939027d81677df9aa86b78c44fce1bca","title":"Deep reinforcement learning for optimizing well settings in subsurface systems with uncertain geology","venue":"Journal of Computational Physics","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/03/2022","authors":"Y. Nasir,L. Durlofsky","id":"2c607661939027d81677df9aa86b78c44fce1bca","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/7881b5fef0489ba1404094e81a11983ed241aa7d","title":"Joint Power Allocation and Rate Control for Rate Splitting Multiple Access Networks with Covert Communications","venue":"IEEE Transactions on Communications","year":2022,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/03/2022","authors":"Nguyen Quang Hieu,D. Hoang,D. Niyato,Diep N. Nguyen,Dong In Kim,A. Jamalipour","id":"7881b5fef0489ba1404094e81a11983ed241aa7d","summary":"This work first proposes a stochastic optimization framework that allows the transmitter to adaptively adjust its power and transmission rates allocated to users, and thereby maximizing the sum-rate and fairness of the system under the presence of an adversary, and develops a highly effective learning algorithm that can help the transmitter find the optimal policy.","score":3},{"url":"https://www.semanticscholar.org/paper/761b0d169c18cde007a1557463576bcaf3cfc0b6","title":"When Virtual Reality Meets Rate Splitting Multiple Access: A Joint Communication and Computation Approach","venue":"IEEE Journal on Selected Areas in Communications","year":2022,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/07/2022","authors":"Nguyen Quang Hieu,Diep N. Nguyen,D. Hoang,E. Dutkiewicz","id":"761b0d169c18cde007a1557463576bcaf3cfc0b6","summary":"A novel multicast approach is proposed to cluster users into different groups based on a Field-of-View metric and transmit multicast streams in a hierarchical manner and a deep reinforcement learning approach is presented to obtain the solution for the optimization problem.","score":3},{"url":"https://www.semanticscholar.org/paper/28f8ac5fedd09594d4c28d2038bda46e44330cd4","title":"DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning","venue":"IEEE Transactions on Visualization and Computer Graphics","year":2022,"referenceCount":82,"citationCount":5,"influentialCitationCount":1,"publicationDate":"02/08/2022","authors":"Dazhen Deng,Aoyu Wu,Huamin Qu,Yingcai Wu","id":"28f8ac5fedd09594d4c28d2038bda46e44330cd4","summary":"This work proposes using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning, and uses visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network.","score":3},{"url":"https://www.semanticscholar.org/paper/9f7b9d7b294095d1a4f0153a7140ed05541e383a","title":"PPO-Based PDACB Traffic Control Scheme for Massive IoV Communications","venue":"IEEE transactions on intelligent transportation systems (Print)","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Haijun Zhang,Minghui Jiang,Xiangnan Liu,X. Wen,Ning Wang,Keping Long","id":"9f7b9d7b294095d1a4f0153a7140ed05541e383a","summary":"Proximal policy optimization (PPO) algorithm as a unique deep reinforcement learning (DRL) method is utilized in this paper, which can obtain continuous action space and solve for the optimal ACB factors without estimating backlog of nodes.","score":3},{"url":"https://www.semanticscholar.org/paper/700322cb828c52714749e1882291bf7b6397a92d","title":"S-MFRL: Spiking Mean Field Reinforcement Learning for Dynamic Resource Allocation of D2D Networks","venue":"IEEE Transactions on Vehicular Technology","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"P. Ye,Yuan-Gen Wang,Weixuan Tang","id":"700322cb828c52714749e1882291bf7b6397a92d","summary":"Results show that the designed S-MFAC and S-MFPPO outperform both AC and PPO in terms of convergence rate, access rate, time-averaged overall throughput, and collision probability and the optimization process of resource allocation becomes tractable as the number of D2D users increases.","score":3},{"url":"https://www.semanticscholar.org/paper/85d4cf8e1bacdc77aa45a901415a843d40b25192","title":"Deep Reinforcement Learning of Semi-Active Suspension Controller for Vehicle Ride Comfort","venue":"IEEE Transactions on Vehicular Technology","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Daekyun Lee,S. Jin,Chibum Lee","id":"85d4cf8e1bacdc77aa45a901415a843d40b25192","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/ca8511ea5d9b3ab954a578222a4b54d6d7644130","title":"Emergent behaviour and neural dynamics in artificial agents tracking odour plumes","venue":"Nature Machine Intelligence","year":2023,"referenceCount":100,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Satpreet H. Singh,Floris van Breugel,Rajesh P. N. Rao,Bingni W. Brunton","id":"ca8511ea5d9b3ab954a578222a4b54d6d7644130","summary":"This work trains artificial recurrent neural network agents using deep reinforcement learning to locate the source of simulated odour plumes that mimic features of plumes in a turbulent flow and produces trajectories that mimic real insect behaviours.","score":3},{"url":"https://www.semanticscholar.org/paper/73b4b216f03a2e507de2ffe7476d663eb063f9f3","title":"Multi-Agent Reinforcement Learning for Fast-Timescale Demand Response of Residential Loads","venue":"ArXiv","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/01/2023","authors":"Vincent Mai,Philippe Maisonneuve,Tianyu Zhang,Hadi Nekoei,L. Paull,Antoine Lesage-Landry","id":"73b4b216f03a2e507de2ffe7476d663eb063f9f3","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/73090396539710dc3a735a09b334ae8d8890170c","title":"SoK: Adversarial Machine Learning Attacks and Defences in Multi-Agent Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":117,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/01/2023","authors":"Maxwell Standen,Junae Kim,Claudia Szabo","id":"73090396539710dc3a735a09b334ae8d8890170c","summary":"A novel perspective to understand the manner of perpetrating an AML attack, by deﬁning Attack Vectors is proposed, and two new frameworks to address a gap in current modelling frameworks are developed.","score":3},{"url":"https://www.semanticscholar.org/paper/24f4a82476142c3d0b48da9855fe0698585485aa","title":"Effective Diversity in Unsupervised Environment Design","venue":"ArXiv","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/01/2023","authors":"WenJu Sun,Pradeep Varakantham,Dexun Li","id":"24f4a82476142c3d0b48da9855fe0698585485aa","summary":"A principled approach to adaptively identify diverse environments based on a novel distance measure relevant to environment design is provided and empirically demonstrate the versatility and effectiveness of the method in comparison to multiple leading approaches for unsupervised environment design on three distinct benchmark prob-lems used in literature.","score":3},{"url":"https://www.semanticscholar.org/paper/c35b61b07bdf6671e062c8c44b0e8939efb71436","title":"AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/01/2023","authors":"Tingting Yuan,Liang Mi,Weijun Wang,Haipeng Dai,Xiaoming Fu","id":"c35b61b07bdf6671e062c8c44b0e8939efb71436","summary":"AccDecoder is presented, a novel accelerated decoder for real-time and neural- enhanced video analytics that can select a few frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality by neural super-resolution and then up-scale the unselected frames that reference them, which leads to 6-21% accuracy improvement.","score":3},{"url":"https://www.semanticscholar.org/paper/6f3e605a6f9a6b26138c2b2f83d2bbbe4b972e5f","title":"Stock Trading Optimization through Model-based Reinforcement Learning with Normalizing Flows","venue":"","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2023","authors":"Huifang Huang,Ting Gao,Pengbo Li,Jinqiu Guo,P. Zhang","id":"6f3e605a6f9a6b26138c2b2f83d2bbbe4b972e5f","summary":"This paper demonstrates normalizing flows is adopted to simulated high-dimensional joint probability of the complex trading environment, and develops a novel model based reinforcement learning framework to better understand the intrinsic mechanisms of quantitative online trading.","score":3},{"url":"https://www.semanticscholar.org/paper/da5862ad8495e4622bbbb6413523216a5455d620","title":"Language-guided Task Adaptation for Imitation Learning","venue":"ArXiv","year":2023,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"Prasoon Goyal,R. Mooney,S. Niekum","id":"da5862ad8495e4622bbbb6413523216a5455d620","summary":"A novel setting, wherein an agent needs to learn a task from a demonstration of a related task with the difference between the tasks communicated in natural language is introduced, and a framework that uses a transformer-based model to reason about the entities in the tasks and their relationships is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/cee43aad6a4039b2d412bc27dadde41040e8f3e0","title":"Intrinsic Motivation in Model-based Reinforcement Learning: A Brief Review","venue":"ArXiv","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"A. Latyshev,A. Panov","id":"cee43aad6a4039b2d412bc27dadde41040e8f3e0","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/b65e9e7f500437a01848ac6fde6692e6b241de85","title":"AutoCost: Evolving Intrinsic Cost for Zero-violation Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":52,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"Tairan He,Weiye Zhao,Changliu Liu","id":"b65e9e7f500437a01848ac6fde6692e6b241de85","summary":"Inspired by the analysis, this paper proposes AutoCost, a simple yet effective framework that automatically searches for cost functions that help constrained RL to achieve zero- violation performance and compares the performance of augmented agents that use the authors' cost function to provide additive intrinsic costs with baseline Agents that use the same policy learners but with only extrinsic costs.","score":3},{"url":"https://www.semanticscholar.org/paper/89f6e05ed3bd4fe01ffd4e7290ea7111d51683e6","title":"Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/01/2023","authors":"Mingqi Yuan,Bo Li,Xin Jin,Wenjun Zeng","id":"89f6e05ed3bd4fe01ffd4e7290ea7111d51683e6","summary":"An intrinsic reward toolkit is developed that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL) and can outperform the benchmarking schemes and achieve superior performance with simple architecture.","score":3},{"url":"https://www.semanticscholar.org/paper/19f5b9767a5f4e30d3258cb52f080d7b29577e1b","title":"A Deep Reinforcement Learning Framework for Optimizing Congestion Control in Data Centers","venue":"ArXiv","year":2023,"referenceCount":18,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/01/2023","authors":"Shiva Ketabi,Hongkai Chen,Haiwei Dong,Y. Ganjali","id":"19f5b9767a5f4e30d3258cb52f080d7b29577e1b","summary":"This work uses multi- agent reinforcement learning to design a system for automatic and dynamic tuning of congestion control parameters at end- hosts in a data center that has the potential to mitigate the problems of static parameters.","score":3},{"url":"https://www.semanticscholar.org/paper/b39a47797df8fe3c6d21a2c0af4658aedd66fadb","title":"Sample Efficient Deep Reinforcement Learning via Local Planning","venue":"ArXiv","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/01/2023","authors":"Dong Yin,S. Thiagarajan,Nevena Lazic,Nived Rajaraman,Botao Hao,Csaba Szepesvari","id":"b39a47797df8fe3c6d21a2c0af4658aedd66fadb","summary":"An algorithmic framework that resets the environment to an observed state which has high uncertainty, instead of sampling according to the initial-state distribution, and can achieve super-human performance on the notoriously hard Atari game, Montezuma’s Revenge, with a simple (distributional) double DQN.","score":3},{"url":"https://www.semanticscholar.org/paper/34d2b456fd6045e13067b6126560f77e4b2a9738","title":"Enabling surrogate-assisted evolutionary reinforcement learning via policy embedding","venue":"ArXiv","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Lan Tang,Xiaxi Li,Jinyuan Zhang,Guiying Li,Peng Yang,Ke Tang","id":"34d2b456fd6045e13067b6126560f77e4b2a9738","summary":"A PE-SAERL Framework is proposed to at the first time enable surrogate-assisted evolutionary reinforcement learning via policy embedding (PE) and empirical results show that the proposed method can perform more efficiently than the four state-of-the-art algorithms.","score":3},{"url":"https://www.semanticscholar.org/paper/363fad5eddf482dba0a65a62eabfec81c6be58ec","title":"Two-Stream Fused Fuzzy Deep Neural Network for Multiagent Learning","venue":"IEEE transactions on fuzzy systems","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Baofu Fang,Caiming Zheng,Hao Wang,Tingting Yu","id":"363fad5eddf482dba0a65a62eabfec81c6be58ec","summary":"Fuzzy MA2C is presented which integrates 2s-FDNN into multiagent deep RL to deal with uncertain communication informations for improving the robustness and generalization under partially observed environments and can achieve superior performance against existing RL algorithms.","score":3},{"url":"https://www.semanticscholar.org/paper/f8bce9b9aa7900dfddf8c1ce3f2a71211ad82f0f","title":"Task Placement and Resource Allocation for Edge Machine Learning: A GNN-based Multi-Agent Reinforcement Learning Paradigm","venue":"ArXiv","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Yihong Li,Xiaoxi Zhang,Tian Zeng,Jingpu Duan,Chuanxi Wu,Di Wu,Xu Chen","id":"f8bce9b9aa7900dfddf8c1ce3f2a71211ad82f0f","summary":"TapFinger is proposed, a distributed scheduler for edge clusters that minimizes the total completion time of ML tasks through co-optimizing task placement and fine-grained multi-resource allocation and the integration of Bayes' theorem and masking schemes.","score":3},{"url":"https://www.semanticscholar.org/paper/037d5ef2da31b6e5f4d7e18a49ebd4c679b4b121","title":"Safe reinforcement learning under temporal logic with reward design and quantum action selection","venue":"Scientific Reports","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Mingyu Cai,Shaoping Xiao,Junchao Li,Z. Kan","id":"037d5ef2da31b6e5f4d7e18a49ebd4c679b4b121","summary":"An advanced Reinforcement Learning (RL) method, incorporating reward-shaping, safety value functions, and a quantum action selection algorithm is proposed, which can synthesize a finite policy that maximizes the probability of satisfying a complex task.","score":3},{"url":"https://www.semanticscholar.org/paper/5261f5a46ad372e15f755cd6df141d5840019c5b","title":"Average-Constrained Policy Optimization","venue":"ArXiv","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Akhil Agnihotri,R. Jain,Haipeng Luo","id":"5261f5a46ad372e15f755cd6df141d5840019c5b","summary":"The Average-Constrained Policy Optimization (ACPO) algorithm is inspired by the famed PPO-type algorithms based on trust region methods, and basic sensitivity theory for average MDPs is developed, and the corresponding bounds are used in the design of the algorithm.","score":3},{"url":"https://www.semanticscholar.org/paper/d99f23bd44f88d769a8892206581aea5d556975f","title":"Locally Constrained Policy Optimization for Online Reinforcement Learning in Non-Stationary Input-Driven Environments","venue":"ArXiv","year":2023,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/02/2023","authors":"Pouya Hamadanian,Arash Nasr-Esfahany,Siddartha Sen,Malte Schwarzkopf,MohammadIman Alizadeh","id":"d99f23bd44f88d769a8892206581aea5d556975f","summary":"Locally Constrained Policy Optimization (LCPO) is presented, an on-policy RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences.","score":3},{"url":"https://www.semanticscholar.org/paper/1d872bdcf8fd5bb8b033ece24eb31cba5d9476c9","title":"Adaptive Aggregation for Safety-Critical Control","venue":"ArXiv","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Huiliang Zhang,Di Wu,B. Boulet","id":"1d872bdcf8fd5bb8b033ece24eb31cba5d9476c9","summary":"This work proposes an adaptive aggregation framework for safety-critical control by aggregating the multiple source tasks and a target task through the attention network and utilizing a safeguard to achieve fewer safety violations.","score":3},{"url":"https://www.semanticscholar.org/paper/ef213d6543cd995ac6b1dfde7d08a7a120232391","title":"Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"R. Wang,Longtao Zheng,Wei Qiu,Bowei He,Bo An,Zinovi Rabinovich,Yujing Hu,Yingfeng Chen,Tangjie Lv,Changjie Fan","id":"ef213d6543cd995ac6b1dfde7d08a7a120232391","summary":"A novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), is introduced, which adapts curriculum learning to multi-agent coordination and endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents.","score":3},{"url":"https://www.semanticscholar.org/paper/18e8c09f269a9970b6a32c57aed979b96aec9535","title":"Zero-shot Sim2Real Adaptation Across Environments","venue":"ArXiv","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"B. L. Semage,Thommen George Karimpanal,S. Rana,S. Venkatesh","id":"18e8c09f269a9970b6a32c57aed979b96aec9535","summary":"A Reverse Action Transformation (RAT) policy is proposed which learns to imitate simulated policies in the real-world and can then be deployed on top of a Universal Policy Network to achieve zero-shot adaptation to new environments.","score":3},{"url":"https://www.semanticscholar.org/paper/e216f0a7cd4b87ee8d04c6eed3a58992021b8142","title":"Novel Model-free Optimal Active Vibration Control Strategy Based on Deep Reinforcement Learning","venue":"Structural Control & Health Monitoring","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Yi-Ang Zhang,Songye Zhu","id":"e216f0a7cd4b87ee8d04c6eed3a58992021b8142","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/5e2d38b3770025aa7b1d1c12f86d355c25bd8717","title":"An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/02/2023","authors":"Jaesik Yoon,Yi-Fu Wu,Heechul Bae,Sungjin Ahn","id":"5e2d38b3770025aa7b1d1c12f86d355c25bd8717","summary":"This paper investigates the effectiveness of OCR pre-training for image-based reinforcement learning via empirical experiments and examines the critical aspects of incorporating OCRpre-training in RL, including performance in a visually complex environment and the appropriate pooling layer to aggregate the object representations.","score":3},{"url":"https://www.semanticscholar.org/paper/4bafa3ede603e5d3ad107002e66d0daa23a89741","title":"Verifying Generalization in Deep Learning","venue":"ArXiv","year":2023,"referenceCount":117,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/02/2023","authors":"Guy Amir,O. Maayan,Tom Zelazny,Guy Katz,Michael Schapira","id":"4bafa3ede603e5d3ad107002e66d0daa23a89741","summary":"This work puts forth a novel objective for formal verification, with the potential for mitigating the risks associated with deploying DNN-based systems in the wild, and establishes the usefulness of the approach, and, in particular, its superiority over gradient-based methods.","score":3},{"url":"https://www.semanticscholar.org/paper/6a4f1c742e7851ed6c7cb73b3ecf82c460ad0ad2","title":"Cross-domain Random Pre-training with Prototypes for Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/02/2023","authors":"Xin Liu,Yaran Chen,Haoran Li,Boyu Li,Dong Zhao","id":"6a4f1c742e7851ed6c7cb73b3ecf82c460ad0ad2","summary":"CRPTpro is proposed, a Cross-domain self-supervised Random Pre-Training framework with prototypes for image-based RL that employs cross-domain random policy to easily and quickly sample diverse data from multiple domains, to improve pre-training efficiency.","score":3},{"url":"https://www.semanticscholar.org/paper/638f7e0c3d1a2cc2c57f2ebcfd7669c0e1993e5f","title":"Improving robot navigation in crowded environments using intrinsic rewards","venue":"","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Diego Martinez-Baselga,L. Riazuelo,L. Montano","id":"638f7e0c3d1a2cc2c57f2ebcfd7669c0e1993e5f","summary":"This work proposes using intrinsic rewards to balance between exploration and exploitation and explore depending on the uncertainty of the states instead of on the time the agent has been trained, encouraging the agent to get more curious about unknown states.","score":3},{"url":"https://www.semanticscholar.org/paper/adb68e8d5fa63618fe68127b8963eb0d5016543e","title":"Learning Hierarchical Resource Allocation and Multi-agent Coordination of 5G mobile IAB Nodes","venue":"","year":2023,"referenceCount":16,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Mohamed Sana,B. Miscopein","id":"adb68e8d5fa63618fe68127b8963eb0d5016543e","summary":"A hierarchical multi-agent reinforcement with a two-level structure is adopted on in-band relaying networks, which conduct access and backhaul links on the same frequency band with severe constraints on co-channel interference, to address the complex problem of dynamic relay node positioning, user association, andBackhaul capacity allocation.","score":3},{"url":"https://www.semanticscholar.org/paper/ff106b68484cc45a296390d14bb2cb88b75a1e0d","title":"CERiL: Continuous Event-based Reinforcement Learning","venue":"","year":2023,"referenceCount":16,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Celyn Walters,Simon Hadfield","id":"ff106b68484cc45a296390d14bb2cb88b75a1e0d","summary":"This paper presents a method to train on event streams derived from standard RL environments, thereby solving the proposed continuous time RL problem and shows the advantages of event streams over less-frequent RGB images.","score":3},{"url":"https://www.semanticscholar.org/paper/74dd51db773ea883d9804d1845345a46ab908ccd","title":"A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning","venue":"IEEE Transactions on Systems, Man, and Cybernetics: Systems","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/03/2023","authors":"Liangyu Huo,Zulin Wang,Mai Xu,Yuhang Song","id":"74dd51db773ea883d9804d1845345a46ab908ccd","summary":"A task-agnostic regularizer for learning diverse subpolicies in HRL is proposed that can improve upon the state-of-the-art performance on all three HRL domains without modifying any existing hyperparameters, indicating the wide applicability and robustness of the approach.","score":3},{"url":"https://www.semanticscholar.org/paper/03da1b6759f70c10e49b33a0ee914cb893d6f949","title":"Learning Algebraic Representation for Systematic Generalization in Abstract Reasoning","venue":"European Conference on Computer Vision","year":2021,"referenceCount":80,"citationCount":3,"influentialCitationCount":0,"publicationDate":"25/11/2021","authors":"Chi Zhang,Sirui Xie,Baoxiong Jia,Y. Wu,Song-Chun Zhu,Yixin Zhu","id":"03da1b6759f70c10e49b33a0ee914cb893d6f949","summary":"This work showcases a prototype with algebraic representation for the abstract spatial-temporal reasoning task of Raven’s Progressive Matrices and presents the ALgebra-Aware Neuro-Semi-Symbolic (ALANS) learner, a hybrid approach to improve systematic generalization in reasoning.","score":3},{"url":"https://www.semanticscholar.org/paper/6c1f6ede9b786e1ffdaab91bf346817f61706912","title":"Reinforcement-Learning-Guided Source Code Summarization Using Hierarchical Attention","venue":"IEEE Transactions on Software Engineering","year":2022,"referenceCount":97,"citationCount":50,"influentialCitationCount":4,"publicationDate":"01/01/2022","authors":"Wenhua Wang,Yuqun Zhang,Yulei Sui,Yao Wan,Zhou Zhao,Jian Wu,Philip S. Yu,Guandong Xu","id":"6c1f6ede9b786e1ffdaab91bf346817f61706912","summary":"This paper presents a new code summarization approach using hierarchical attention network by incorporating multiple code features, including type-augmented abstract syntax trees and program control flows into a deep reinforcement learning (DRL) framework for comment generation.","score":3},{"url":"https://www.semanticscholar.org/paper/f4159f7eadf44d4bdebff486f008797f5225bf14","title":"Beyond the Policy Gradient Theorem for Efficient Policy Updates in Actor-Critic Algorithms","venue":"International Conference on Artificial Intelligence and Statistics","year":2022,"referenceCount":45,"citationCount":2,"influentialCitationCount":0,"publicationDate":"15/02/2022","authors":"R. Laroche,R. Tachet","id":"f4159f7eadf44d4bdebff486f008797f5225bf14","summary":"This paper discovers that the policy gradient theorem prescribes policy updates that are slow to unlearn because of their structural symmetry with respect to the value target, and introduces a modiﬁed policy update devoid of that symmetry, and proves its guarantees of convergence to global optimality in O ( t − 1 ) under classic assumptions.","score":3},{"url":"https://www.semanticscholar.org/paper/06b10851b7a53316b3b6588017c9f3b9aae8c7cb","title":"Hierarchical Reinforcement Learning: A Survey and Open Research Challenges","venue":"Machine Learning and Knowledge Extraction","year":2022,"referenceCount":257,"citationCount":7,"influentialCitationCount":0,"publicationDate":"17/02/2022","authors":"Matthias Hutsebaut-Buysse,Kevin Mets,S. Latré","id":"06b10851b7a53316b3b6588017c9f3b9aae8c7cb","summary":"This survey paper introduces a selection of problem-specific approaches, which provided insight in how to utilize often handcrafted abstractions in specific task settings, and introduces the Options framework, which provides a more generic approach, allowing abstractions to be discovered and learned semi-automatically.","score":3},{"url":"https://www.semanticscholar.org/paper/583f317cca19be32a7dc0d5378ad6725ad43e91b","title":"A Survey on Reinforcement Learning Methods in Character Animation","venue":"Computer graphics forum (Print)","year":2022,"referenceCount":141,"citationCount":4,"influentialCitationCount":0,"publicationDate":"07/03/2022","authors":"Ariel Kwiatkowski,Eduardo Alvarado,Vicky S. Kalogeiton,C. K. Liu,Julien Pettr'e,M. V. D. Panne,Marie-Paule Cani","id":"583f317cca19be32a7dc0d5378ad6725ad43e91b","summary":"This paper surveys the modern Deep Reinforcement Learning methods and discusses their possible applications in Character Animation, from skeletal control of a single, physically‐based character to navigation controllers for individual agents and virtual crowds, and describes the practical side of training DRL systems.","score":3},{"url":"https://www.semanticscholar.org/paper/6274b8a642bde8ab24e71a26fcb361982f5b1543","title":"Strategic Maneuver and Disruption with Reinforcement Learning Approaches for Multi-Agent Coordination","venue":"The Journal of Defence Modeling and Simulation: Applications, Methodology, Technology","year":2022,"referenceCount":61,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/03/2022","authors":"Derrik E. Asher,Anjon Basak,Rolando Fernandez,P. Sharma,Erin G. Zaroukian,Christopher D. Hsu,M. Dorothy,Thomas Mahre,Gerardo Galindo,Luke Frerichs,J. Rogers,J. Fossaceca","id":"6274b8a642bde8ab24e71a26fcb361982f5b1543","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/4203aad01afe1dadab9d35845ae89b74a99fb2eb","title":"Efficient Bayesian Policy Reuse with a Scalable Observation Model in Deep Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/04/2022","authors":"Donghan Xie,Zhi Wang,Chunlin Chen,D. Dong","id":"4203aad01afe1dadab9d35845ae89b74a99fb2eb","summary":"An improved BPR method is proposed to achieve more efﬁcient policy transfer in deep reinforcement learning (DRL) by expanding the scalable observation model in a plug-and-play fashion, which can avoid negative transfer when faced with new unknown tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/1ee71262c4525bca44c6da5b8e1321a67a484953","title":"Dexterous Manipulation for Multi-Fingered Robotic Hands With Reinforcement Learning: A Review","venue":"Frontiers in Neurorobotics","year":2022,"referenceCount":130,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/04/2022","authors":"Chunmiao Yu,Peng Wang","id":"1ee71262c4525bca44c6da5b8e1321a67a484953","summary":"The purpose is to present a comprehensive review of the techniques for dexterous manipulation with multi-fingered robotic hands, such as the model-based approach without learning in early years, and the latest research and methodologies focused on the method based on reinforcement learning and its variations.","score":3},{"url":"https://www.semanticscholar.org/paper/447ad78823fbf3369e72e8d1c36467f76f8dc427","title":"RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":84,"citationCount":7,"influentialCitationCount":1,"publicationDate":"26/04/2022","authors":"Marc Rigter,Bruno Lacerda,N. Hawes","id":"447ad78823fbf3369e72e8d1c36467f76f8dc427","summary":"RamBO is presented, a novel approach to model-based oﬄine RL that addresses the problem as a two-player zero sum game against an adversarial environment model, resulting in a PAC performance guarantee and a pessimistic value function which lower bounds the value function in the true environment.","score":3},{"url":"https://www.semanticscholar.org/paper/ba27837c477884223d291da6df43abfced973d04","title":"Learning Mean Field Games: A Survey","venue":"ArXiv","year":2022,"referenceCount":227,"citationCount":7,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"M. Laurière,Sarah Perrin,M. Geist,O. Pietquin","id":"ba27837c477884223d291da6df43abfced973d04","summary":"A general framework for classical iterative methods (based on best-response computation or policy evaluation) to solve Mean Field Games in an exact way is presented and how RL can be used to learn MFG solutions in a model-free way is explained.","score":3},{"url":"https://www.semanticscholar.org/paper/6d846a7601c4be41034b9316d7c256f639085d9f","title":"Deep Hierarchical Planning from Pixels","venue":"ArXiv","year":2022,"referenceCount":59,"citationCount":9,"influentialCitationCount":0,"publicationDate":"08/06/2022","authors":"Danijar Hafner,Kuang-Huei Lee,Ian S. Fischer,P. Abbeel","id":"6d846a7601c4be41034b9316d7c256f639085d9f","summary":"Director is introduced, a practical method for learning hierarchical behaviors directly from pixels by planning inside the latent space of a learned world model, and the decisions are interpretable because the world model can decode goals into images for visualization.","score":3},{"url":"https://www.semanticscholar.org/paper/44f76721abb22837f73976c021a69714c76943e1","title":"Uncertainty Aware Model Integration on Reinforcement Learning","venue":"IEEE International Joint Conference on Neural Network","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/07/2022","authors":"Takashi Nagata,Jinwei Xing,Tsutomu Kumazawa,E. Neftci","id":"44f76721abb22837f73976c021a69714c76943e1","summary":"This paper proposes to use the confidence of the model simulations to the integrated learning process so that the agent avoids updating its policy based on uncertain simulations by the model, and applies the Monte Carlo dropout technique to the state transition model.","score":3},{"url":"https://www.semanticscholar.org/paper/2638b7afd079233c7dbce5f389a92edb1d8f3f31","title":"Interactive Imitation Learning in Robotics based on Simulations","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/07/2022","authors":"Xinyi Liu","id":"2638b7afd079233c7dbce5f389a92edb1d8f3f31","summary":"This thesis implements IIL algorithms in four simulation scenarios and conducts extensive experiments, aiming at providing exhaustive information about IIL methods both in action space and state space as well as comparison with RL methods.","score":3},{"url":"https://www.semanticscholar.org/paper/cb66cdc34a15ac2819b827a8368db8d0301f906d","title":"Reinforcement learning in spacecraft control applications: Advances, prospects, and challenges","venue":"Annual Reviews in Control","year":2022,"referenceCount":125,"citationCount":5,"influentialCitationCount":0,"publicationDate":"01/08/2022","authors":"M. Tipaldi,R. Iervolino,Paolo Roberto Massenio","id":"cb66cdc34a15ac2819b827a8368db8d0301f906d","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/c003663a61b3a210db77200202557805b3d326fa","title":"Stochastic cubic-regularized policy gradient method","venue":"Knowledge-Based Systems","year":2022,"referenceCount":47,"citationCount":1,"influentialCitationCount":1,"publicationDate":"01/08/2022","authors":"Pengfei Wang,Hongyu Wang,Nenggan Zheng","id":"c003663a61b3a210db77200202557805b3d326fa","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/728b8f104849432cc2763d0c1c9115e6e5b79b07","title":"Extracting Relevant Information from User's Utterances in Conversational Search and Recommendation","venue":"Knowledge Discovery and Data Mining","year":2022,"referenceCount":47,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/08/2022","authors":"Ali Montazeralghaem,James Allan","id":"728b8f104849432cc2763d0c1c9115e6e5b79b07","summary":"A model based on reinforcement learning, namely RelInCo, is proposed, which takes the user's utterances and the context of the conversation and classifies each word in the users' utterances as belonging to the relevant or non-relevant class.","score":3},{"url":"https://www.semanticscholar.org/paper/18a736a92ff5ff214fb91589326da4c1f55453f5","title":"Global Convergence of Two-timescale Actor-Critic for Solving Linear Quadratic Regulator","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/08/2022","authors":"Xuyang Chen,Jingliang Duan,Yingbin Liang,Lin Zhao","id":"18a736a92ff5ff214fb91589326da4c1f55453f5","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/a08752bbc4abef0e70e7f61a296990ce58609cd6","title":"Deep Anomaly Detection and Search via Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/08/2022","authors":"Chao Chen,Dawei Wang,Feng Mao,Zongzhang Zhang,Yang Yu","id":"a08752bbc4abef0e70e7f61a296990ce58609cd6","summary":"This paper proposes Deep Anomaly Detection and Search (DADS), which applies Reinforcement Learning (RL) to bal- ance exploitation and exploration of labeled data and under-exploration of unlabeled data and shows that DADS can efﬁciently and precisely search anomalies from unlabeling data and learn from them, thus achieving good performance.","score":3},{"url":"https://www.semanticscholar.org/paper/68e305ee1e80a4750c16b2cdd52658f005e09009","title":"Survey on Machine Learning for Traffic-Driven Service Provisioning in Optical Networks","venue":"ArXiv","year":2022,"referenceCount":180,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/09/2022","authors":"T. Panayiotou,M. Michalopoulou,G. Ellinas","id":"68e305ee1e80a4750c16b2cdd52658f005e09009","summary":"This survey provides a comprehensive review of the state of the art on machine learning (ML)-based techniques at the optical layer for trafﬁc-driven service provisioning.","score":3},{"url":"https://www.semanticscholar.org/paper/93ecc6fef75a3254bd800a97c0bb05515884d686","title":"An Adaptive Threshold for the Canny Edge Detection with Actor-Critic Algorithm","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/09/2022","authors":"Keong-Hun Choi,J. Ha","id":"93ecc6fef75a3254bd800a97c0bb05515884d686","summary":"A spatio-temporal fusion network (STFN) that could extract temporal and spatial information using a temporal network and a spatial network is proposed that shows excellent performance in an environment different from training and is shown through experiments with various public datasets.","score":3},{"url":"https://www.semanticscholar.org/paper/6590ff0c6ac7fcb51a55e19b13913dc31eaebcb5","title":"Deep reinforcement learning for automated search of model parameters: photo-fenton wastewater disinfection case study","venue":"Neural computing & applications (Print)","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/09/2022","authors":"S. Hernández-García,Alfredo Cuesta-Infante,José Moreno-SanSegundo,A. S. Montemayor","id":"6590ff0c6ac7fcb51a55e19b13913dc31eaebcb5","summary":"This work presents a proximal policy optimization agent that learns to optimize in a real case study such as the modeling of the photo-fenton disinfection process, which involves a number of parameters that have to be adjusted to minimize the error of the model with respect to the experimental data collected in several trials.","score":3},{"url":"https://www.semanticscholar.org/paper/3f50fd0db14f42da5a27c62bc29c8e14264981a0","title":"Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":2,"influentialCitationCount":1,"publicationDate":"28/09/2022","authors":"Filippos Christianos,Georgios Papoudakis,Stefano V. Albrecht","id":"3f50fd0db14f42da5a27c62bc29c8e14264981a0","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/84de39c1e7f3f2652b5704648ff06ca301342f71","title":"Reward Shaping for User Satisfaction in a REINFORCE Recommender","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/09/2022","authors":"Konstantina Christakopoulou,Can Xu,Sai Zhang,Sriraj Badam,Trevor Potter,Daniel Li,Hao Wan,Xinyang Yi,Ya Le,Chris Berg,E. B. Dixon,Ed H. Chi,Minmin Chen","id":"84de39c1e7f3f2652b5704648ff06ca301342f71","summary":"This work proposes to jointly learn a policy network and a satisfaction imputation network, postulate that reward shaping in RL recommender agents is powerful for driving satisfying user experiences, and uses both ofﬂine analysis and live experiments in an industrial large-scale recommendation platform to demonstrate the promise.","score":3},{"url":"https://www.semanticscholar.org/paper/93d628899dc7389c97f20c1214d9af963d28055f","title":"An End-to-End Curriculum Learning Approach for Autonomous Driving Scenarios","venue":"IEEE transactions on intelligent transportation systems (Print)","year":2022,"referenceCount":44,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/10/2022","authors":"Luca Anzalone,Paola Barra,Silvio Barra,Aniello Castiglione,M. Nappi","id":"93d628899dc7389c97f20c1214d9af963d28055f","summary":"This work combines Curriculum Learning with Deep Reinforcement Learning to learn without any prior domain knowledge, an end-to-end competitive driving policy for the CARLA autonomous driving simulator, and is the first to provide consistent results of the driving policy on all towns available in CARLA.","score":3},{"url":"https://www.semanticscholar.org/paper/2df437fe675c89a00dc0a7c3a31fa46ed3d94c9d","title":"A Video Summarization Model Based on Deep Reinforcement Learning with Long-Term Dependency","venue":"Italian National Conference on Sensors","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/10/2022","authors":"Xu Wang,Yujie Li,Haoyu Wang,Longzhao Huang,Shuxue Ding","id":"2df437fe675c89a00dc0a7c3a31fa46ed3d94c9d","summary":"This paper introduces an unsupervised auxiliary summarization loss module with LSTM and a swish activation function to capture the long-term dependencies for video summarization, which can be easily integrated with various networks.","score":3},{"url":"https://www.semanticscholar.org/paper/e0baf5c65a4913fc6e0b6b56be848def11867f47","title":"Deep Intrinsically Motivated Exploration in Continuous Control","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/10/2022","authors":"Baturay Saglam,S. Kozat","id":"e0baf5c65a4913fc6e0b6b56be848def11867f47","summary":"This paper adapt the existing theories on animal motivational systems into the reinforcement learning paradigm and introduces a novel and scalable directed exploration strategy that extends to larger and more diverse state spaces, dramatically improves the baselines, and outperforms the undirected strategies.","score":3},{"url":"https://www.semanticscholar.org/paper/57c04d33366a8c7d797cd413efa5d263131c2fb8","title":"Policy Gradients for Probabilistic Constrained Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/10/2022","authors":"Weiqin Chen,D. Subramanian,Santiago Paternain","id":"57c04d33366a8c7d797cd413efa5d263131c2fb8","summary":"This work relates this notion of safety to the notion of average safety often considered in the literature by providing theoretical bounds in terms of their safety and performance by considering a continuous navigation problem to empirically illustrate the advantages of working with probabilistic constraints as compared to average constraints.","score":3},{"url":"https://www.semanticscholar.org/paper/c2bd6bf1c2250561f954a04a7061f6d6bf9ff855","title":"Faster Last-iterate Convergence of Policy Optimization in Zero-Sum Markov Games","venue":"ArXiv","year":2022,"referenceCount":59,"citationCount":7,"influentialCitationCount":0,"publicationDate":"03/10/2022","authors":"Shicong Cen,Yuejie Chi,S. Du,Lin Xiao","id":"c2bd6bf1c2250561f954a04a7061f6d6bf9ff855","summary":"This paper focuses on the most basic setting of competitive multi-agent RL, namely two-player zero-sum Markov games, and proposes a single-loop policy optimization method with symmetric updates from both agents, which achieves a last-iterate linear convergence to the quantal response equilibrium of the regularized problem.","score":3},{"url":"https://www.semanticscholar.org/paper/61d56ece2d19f4bfeb322c92085fb28521e169da","title":"Neural-Symbolic Recursive Machine for Systematic Generalization","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/10/2022","authors":"Qing Li,Yixin Zhu,Yitao Liang,Y. Wu,Song-Chun Zhu,Siyuan Huang","id":"61d56ece2d19f4bfeb322c92085fb28521e169da","summary":"The proposed Neural-Symbolic Recursive Machine (NSR) demonstrates stronger generalization than pure neural networks due to its symbolic representation and inductive biases, and demonstrates better transferability than existing neural-symbolic approaches due to less domain-speciﬁc knowledge required.","score":3},{"url":"https://www.semanticscholar.org/paper/090657e0c4369380c762b2bdffe3c92449015c76","title":"Scaling up stochastic gradient descent for non-convex optimisation","venue":"Machine-mediated learning","year":2022,"referenceCount":80,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"S. Mohamad,Hamad Alamri,A. Bouchachia","id":"090657e0c4369380c762b2bdffe3c92449015c76","summary":"A unified distributed and parallel implementation of SGD (named DPSGD) that relies on both asynchronous distribution and lock-free parallelism is proposed by combining two strategies into a unified framework, able to strike a better trade-off between local computation and communication.","score":3},{"url":"https://www.semanticscholar.org/paper/0830ff278815709600f249ed488ffe3718e6f7a1","title":"Optimal Action Space Search: An Effective Deep Reinforcement Learning Method for Algorithmic Trading","venue":"International Conference on Information and Knowledge Management","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/10/2022","authors":"Zhongjie Duan,Cen Chen,Dawei Cheng,Yuqi Liang,Weining Qian","id":"0830ff278815709600f249ed488ffe3718e6f7a1","summary":"This paper proposes an end-to-end DRL method that explores solutions on the whole graph via a probabilistic dynamic programming algorithm and can generate stable trading strategies for both high-frequency and low-frequency trading, significantly outperforming the baseline DRL methods on annualized return and Sharpe ratio.","score":3},{"url":"https://www.semanticscholar.org/paper/3be669b4156e8247c3082ca2afb2c29041e0e930","title":"On the convergence of policy gradient methods to Nash equilibria in general stochastic games","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/10/2022","authors":"Angeliki Giannou,Kyriakos Lotidis,P. Mertikopoulos,Emmanouil-Vasileios Vlatakis-Gkaragkounis","id":"3be669b4156e8247c3082ca2afb2c29041e0e930","summary":"This work examines the long-run behavior of policy gradient methods with respect to Nash equilibrium policies that are second-order stationary and shows that SOS policies are locally attracting with high probability, and that policy gradient trajectories with gradient estimates provided by the Reinforce algorithm achieve an O (1 / √ n ) distance-squared convergence rate if the method’s step-size is chosen appropriately.","score":3},{"url":"https://www.semanticscholar.org/paper/44918fd3ddd5eb3b48b86a0f6bff4007957443ab","title":"Finite-time analysis of single-timescale actor-critic","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/10/2022","authors":"Xuyang Chen,Lin Zhao","id":"44918fd3ddd5eb3b48b86a0f6bff4007957443ab","summary":"This analysis develops a novel framework that evaluates and controls the error propagation between actor and critic in a systematic way and compares favorably to the existing literature on analyzing actor-critic in terms of considering the most practical settings and requiring weaker assumptions.","score":3},{"url":"https://www.semanticscholar.org/paper/df46d3985445ed5a0ed46782e191223796c37c58","title":"Krylov-Bellman boosting: Super-linear policy evaluation in general state spaces","venue":"ArXiv","year":2022,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/10/2022","authors":"Eric Xia,M. Wainwright","id":"df46d3985445ed5a0ed46782e191223796c37c58","summary":"The Krylov–Bellman Boosting algorithm alternates between boosting the Bellman residual using non-parametric regression, and estimating the value function via the least-squares temporal diﬀerence procedure applied with a feature set that grows adaptively over time.","score":3},{"url":"https://www.semanticscholar.org/paper/bfa97f55991cdf42f65e9607d47a34a86180b2e9","title":"A Bibliometric Analysis and Review on Reinforcement Learning for Transportation Applications","venue":"ArXiv","year":2022,"referenceCount":173,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Can Li,Lei Bai,L. Yao,S. Waller,Wei Liu","id":"bfa97f55991cdf42f65e9607d47a34a86180b2e9","summary":"A bibliometric analysis is conducted to identify the development of RL-based methods for transportation applications, typical journals/conferences, and leading topics in the industry in recent ten years by categorizing methods with respect to the application domains.","score":3},{"url":"https://www.semanticscholar.org/paper/339550fa989a2d7000fce319a3af68704fd6d289","title":"Towards High-Quality CGRA Mapping with Graph Neural Networks and Reinforcement Learning","venue":"International Conference on Computer Aided Design","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/10/2022","authors":"Yan Zhuang,Zhihao Zhang,Dajiang Liu","id":"339550fa989a2d7000fce319a3af68704fd6d289","summary":"This paper integrates the routing explorations into the mapping process and makes it have more opportunities to find a globally optimized solution, and introduces graph neural network based reinforcement learning to predict a placement distribution over different resource nodes for all operations in a DDG.","score":3},{"url":"https://www.semanticscholar.org/paper/f0d8b48343ed0a7b503899a8191bbe501641f5b1","title":"Solving non-permutation flow-shop scheduling problem via a novel deep reinforcement learning approach","venue":"Computers & Operations Research","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Zhenyu Wang,Bin Cai,Jun Yu Li,Deheng Yang,Yang Zhao,Huan Xie","id":"f0d8b48343ed0a7b503899a8191bbe501641f5b1","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/8faa5de0401ff49eda52f15c7b7d4da9bc78cf23","title":"Deep reinforcement learning-based critical element identification and demolition planning of frame structures","venue":"Frontiers of Structural and Civil Engineering","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Shaojun Zhu,M. Ohsaki,Kazuki Hayashi,Shaohan Zong,Xiao Guo","id":"8faa5de0401ff49eda52f15c7b7d4da9bc78cf23","summary":"It is proved that the Q values produced by the reinforcement learning agent can make up for the deficiencies of existing indices, and can be directly used as the quantitative index for the decision-making for determining the most expected collapse scenario, i.e., the sequence of element removals.","score":3},{"url":"https://www.semanticscholar.org/paper/dfdada90e09ca1b17b4c72362f833dc3f8d7f29a","title":"Connecting Stochastic Optimal Control and Reinforcement Learning","venue":"","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/11/2022","authors":"J. Quer,Enric Ribera Borrell","id":"dfdada90e09ca1b17b4c72362f833dc3f8d7f29a","summary":"This article connects the optimal control problem to reinforcement learning since both share the same underlying framework namely a Markov decision process (MDP) and shows how the MDP can be formulated for the optimal Control problem.","score":3},{"url":"https://www.semanticscholar.org/paper/65ca4c3768bc98ccf6a4676467fc443bad7ce7bf","title":"Designing mechanically tough graphene oxide materials using deep reinforcement learning","venue":"npj Computational Materials","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/11/2022","authors":"Bowen Zheng,Zeyu Zheng,Grace X. Gu","id":"65ca4c3768bc98ccf6a4676467fc443bad7ce7bf","summary":"The present research showcases the impact of functional group distribution on GO properties, and illustrates the effectiveness and data efficiency of the deep RL approach to design mechanically tough GOs.","score":3},{"url":"https://www.semanticscholar.org/paper/47d4c14cc0878841febeeed5b3ee46a4a44e566f","title":"Graph Reinforcement Learning Application to Co-operative Decision-Making in Mixed Autonomy Traffic: Framework, Survey, and Challenges","venue":"ArXiv","year":2022,"referenceCount":110,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/11/2022","authors":"Qi Liu,Xueyuan Li,Zirui Li,Jingda Wu,Guodong Du,Xinlu Gao,Fan Yang,Shihua Yuan","id":"47d4c14cc0878841febeeed5b3ee46a4a44e566f","summary":"Results show that the GRL methods can well optimize the performance of multi-agent decision-making for CAVs in mixed autonomy trafﬁc compared to the DRL methods.","score":3},{"url":"https://www.semanticscholar.org/paper/ede0f8aedce15acf87f207431268a17ba5bcb206","title":"GreenDRL: managing green datacenters using deep reinforcement learning","venue":"ACM Symposium on Cloud Computing","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Kuo Zhang,Peijian Wang,Ning Gu,Thu D. Nguyen","id":"ede0f8aedce15acf87f207431268a17ba5bcb206","summary":"This work design and evaluate GreenDRL, a system that combines a deep RL agent with simple heuristics to manage workload, energy consumption, and cooling in the presence of onsite generation of renewable energy to minimize brown energy consumption and cost and shows that deep RL is a promising technique for building efficient management systems for green datacenters.","score":3},{"url":"https://www.semanticscholar.org/paper/d49c110efd463d6096e0a6cf05b3ea3e49588a4b","title":"A Unified Mutual Supervision Framework for Referring Expression Segmentation and Generation","venue":"ArXiv","year":2022,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Shijia Huang,Feng Li,Hao Zhang,Siyi Liu,Lei Zhang,Liwei Wang","id":"d49c110efd463d6096e0a6cf05b3ea3e49588a4b","summary":"A unified mutual supervision framework that enables two tasks to improve each other by solving their bottleneck problems is proposed and significantly outperforms all existing methods on REG and RES tasks under the same setting.","score":3},{"url":"https://www.semanticscholar.org/paper/1e245b05a218fb0b555bc6883983b649b82310cc","title":"A Deep Reinforcement Learning Framework for Multi-Stage Optimized Object Detection","venue":"International Conference on Robotics and Mechatronics","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Sobhan Siamak,E. Mansoori","id":"1e245b05a218fb0b555bc6883983b649b82310cc","summary":"An optimized method upon a deep reinforcement learning framework along with multi-stage approaches for object detection as well as train two intelligent agents for a more accurate search of objects and localize them in images is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/1029f62fb8f895a62695c0c08812221527e5bc5f","title":"Debiasing Meta-Gradient Reinforcement Learning by Learning the Outer Value Function","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/11/2022","authors":"Clément Bonnet,L. Midgley,Alexandre Laterre","id":"1029f62fb8f895a62695c0c08812221527e5bc5f","summary":"This paper identifies a bias in the meta-gradient of current meta- gradient RL approaches, and proposes a simple solution to eliminate this bias by using an alternative, outer value function in the estimation of the outer loss.","score":3},{"url":"https://www.semanticscholar.org/paper/c59aaa7eccc125c82488243a71e7840cede38032","title":"Transfer RL via the Undo Maps Formalism","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/11/2022","authors":"Abhi Gupta,Theodore H. Moskovitz,David Alvarez-Melis,Aldo Pacchiano","id":"c59aaa7eccc125c82488243a71e7840cede38032","summary":"This work characterizes the discrepancy in environments by means of (potentially complex) transformation between their state spaces, and thus posing the problem of transfer as learning to undo this transformation, and introduces a novel optimization objective based on an optimal transport distance between two distributions over trajectories.","score":3},{"url":"https://www.semanticscholar.org/paper/6faf5f82a546a28f0575a473b01df4f758c350d7","title":"Reinforcement Learning for Multi-Truck Vehicle Routing Problems","venue":"ArXiv","year":2022,"referenceCount":15,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"R. Correll,Sean J. Weinberg,F. Sanches,T. Ide,Takafumi Suzuki","id":"6faf5f82a546a28f0575a473b01df4f758c350d7","summary":"This work develops new extensions to encoder-decoder models for vehicle routing that allow for complex supply chains and shows how the model, even if trained only for a small number of trucks, can be embedded into a large supply chain to yield viable solutions.","score":3},{"url":"https://www.semanticscholar.org/paper/61760432c8af40c57a4dea0e7e40e2d866140e01","title":"An Energy-aware, Fault-tolerant, and Robust Deep Reinforcement Learning based approach for Multi-agent Patrolling Problems","venue":"","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Chenhao Tong,A. Harwood,M. A. Rodriguez,R. Sinnott","id":"61760432c8af40c57a4dea0e7e40e2d866140e01","summary":"A distributed homogeneous multi-agent architecture is proposed, where all patrolling agents execute identical policies locally based on their local observations and shared information that provides a fault-tolerant and robust patrolling system that can tolerate agent failures and allow supplementary agents to be added to replace failed agents or to increase the overall patrol performance.","score":3},{"url":"https://www.semanticscholar.org/paper/2667a8960c1f07e57729cddfe91bca10ce6a4979","title":"Multi-Agent Patrolling with Battery Constraints through Deep Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Chenhao Tong,A. Harwood,Maria A. Rodriguez,R. Sinnott","id":"2667a8960c1f07e57729cddfe91bca10ce6a4979","summary":"This work proposes an approach based on a distributed, model-free deep reinforcement learning based multi-agent patrolling strategy that provides a robust patrolling system that can tolerate agent failures and allow supplementary agents to be added to replace failed agents or to increase the overall patrol performance.","score":3},{"url":"https://www.semanticscholar.org/paper/5b4919511da477630737174fa988101d598bf09d","title":"Risk-Sensitive Reinforcement Learning with Exponential Criteria","venue":"ArXiv","year":2022,"referenceCount":59,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Erfaun Noorani,Christos N. Mavridis,J. Baras","id":"5b4919511da477630737174fa988101d598bf09d","summary":"This work studies the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develops variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm.","score":3},{"url":"https://www.semanticscholar.org/paper/8a195693047aa27b58924241d2da706cf41d62a3","title":"Annotated History of Modern AI and Deep Learning","venue":"ArXiv","year":2022,"referenceCount":468,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Juergen Schmidhuber","id":"8a195693047aa27b58924241d2da706cf41d62a3","summary":"The history of modern artificial intelligence (AI) which is dominated by artificial neural networks (NNs) and deep learning, both conceptually closer to the old field of cybernetics than to what's been called AI since 1956 is focused on.","score":3},{"url":"https://www.semanticscholar.org/paper/dd4a3b4ec42de0a46d16a49a4375723996f5f6fd","title":"Stock Market Prediction via Deep Learning Techniques: A Survey","venue":"","year":2022,"referenceCount":162,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/12/2022","authors":"Jinan Zou,Qi Zhao,Yang Jiao,Hai Cao,Yanxi Liu,Qingsen Yan,Ehsan Abbasnejad,Lingqiao Liu,Javen Qinfeng Shi","id":"dd4a3b4ec42de0a46d16a49a4375723996f5f6fd","summary":"This poster presents a probabilistic procedure to identify the neurons in the brain that secrete During the Yangtze River blackout, a process known as “red-plating”.","score":3},{"url":"https://www.semanticscholar.org/paper/7cf43ee9ab155834aed3dc8e3429128783090705","title":"Variance Reduction for Score Functions Using Optimal Baselines","venue":"ArXiv","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/12/2022","authors":"Ronan Keane,H. Gao","id":"7cf43ee9ab155834aed3dc8e3429128783090705","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/aa118b8e6e31457bc2c3de895a1e298b381a3428","title":"Learning One Abstract Bit at a Time Through Self-Invented Experiments Encoded as Neural Networks","venue":"ArXiv","year":2022,"referenceCount":87,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/12/2022","authors":"Vincent Herrmann,Louis Kirsch,J. Schmidhuber","id":"aa118b8e6e31457bc2c3de895a1e298b381a3428","summary":"An empirical analysis of the automatic generation of interesting experiments is presented and it is shown that self-invented experiments in a reinforcement-providing environment and show that they lead to effective exploration.","score":3},{"url":"https://www.semanticscholar.org/paper/ea02c73891d267c21e937f4bcfaaef4f623f2e08","title":"MR-Selection: A Meta-Reinforcement Learning Approach for Zero-Shot Hyperspectral Band Selection","venue":"IEEE Transactions on Geoscience and Remote Sensing","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Jie Feng,Gaiqin Bai,Di Li,Xiangrong Zhang,Ronghua Shang,Licheng Jiao","id":"ea02c73891d267c21e937f4bcfaaef4f623f2e08","summary":"A novel zero-shot band selection method, called MR-Selection, is proposed for HSI classification, which formalizes zero- shot band selection as a metalearning problem, where advantage actor–critic algorithm-based reinforcement learning (A2C-RL) is designed to extract the metaknowledge in the band selection tasks of various seen hyperspectral datasets through a shared agent.","score":3},{"url":"https://www.semanticscholar.org/paper/4210f34f30ae38de65c199a6bd9d3875ba5bbd30","title":"A N E NERGY - AWARE , F AULT - TOLERANT , AND R OBUST D EEP R EINFORCEMENT L EARNING BASED APPROACH FOR M ULTI - AGENT P ATROLLING P ROBLEMS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"4210f34f30ae38de65c199a6bd9d3875ba5bbd30","summary":"A distributed homogeneous multi-agent architecture is proposed, where all patrolling agents execute identical policies locally based on their local observations and shared information that provides a fault-tolerant and robust patrolling system that can tolerate agent failures and allow supplementary agents to be added to replace failed agents or to increase the overall patrol performance.","score":3},{"url":"https://www.semanticscholar.org/paper/58ed7613df5b9f5a425e31bcbecb885e4d2d18c6","title":"Tensor networks for unsupervised machine learning","venue":"Physical Review E","year":2021,"referenceCount":48,"citationCount":7,"influentialCitationCount":0,"publicationDate":"24/06/2021","authors":"Jing Liu,Sujie Li,Jiang Zhang,Pan Zhang","id":"58ed7613df5b9f5a425e31bcbecb885e4d2d18c6","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/9f4f1b5559155e88d141f804770314e7098264f0","title":"Interpretable Learned Emergent Communication for Human-Agent Teams","venue":"IEEE Transactions on Cognitive and Developmental Systems","year":2022,"referenceCount":50,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/01/2022","authors":"Seth Karten,Mycal Tucker,Huao Li,Siva Kailas,Michael Lewis,K. Sycara","id":"9f4f1b5559155e88d141f804770314e7098264f0","summary":"This work develops agent-only teams that communicate sparsely via the scheme of Enforcers that enables high agent- only and human-agent team performance and develops a prototype-based method that produces meaningful discrete tokens that enable human partners to learn agent communication faster and better than a one-hot baseline.","score":3},{"url":"https://www.semanticscholar.org/paper/60d6e0ed45ce062bd6dddc7a5ce48718c075f9a5","title":"Opportunities for reinforcement learning in stochastic dynamic vehicle routing","venue":"Computers & Operations Research","year":2022,"referenceCount":79,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"F. D. Hildebrandt,Barrett W. Thomas,Marlin W. Ulmer","id":"60d6e0ed45ce062bd6dddc7a5ce48718c075f9a5","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/4100c4690bd90c46bd292d08699071b04cc5107b","title":"Deep reinforcement learning for irrigation scheduling using high-dimensional sensor feedback","venue":"ArXiv","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/01/2023","authors":"Yuji Saikai,A. Peake,K. Chenu","id":"4100c4690bd90c46bd292d08699071b04cc5107b","summary":"A general framework and actionable procedure is proposed that allow researchers to formulate their own optimisation problems and implement solution algorithms based on deep reinforcement learning and is applicable to a wide range of cropping systems with realistic Optimisation problems.","score":3},{"url":"https://www.semanticscholar.org/paper/3e224b785aae67e0bd4277adbec9bbf063b9f303","title":"DeepDLP: Deep Reinforcement Learning based Framework for Dynamic Liner Trade Pricing","venue":"International Conference on Ubiquitous Information Management and Communication","year":2023,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/01/2023","authors":"Xue Li,Yongyi Hu,Yumeng Bai,Xiaofeng Gao,Guihai Chen","id":"3e224b785aae67e0bd4277adbec9bbf063b9f303","summary":"A Deep Reinforcement Learning (RL) Framework for Liner Trade Problem (DeepDLP) to identify price strategy based on the characteristics of liner trade environment is designed and the outcome shows that DeepDLP outperforms baselines and validates the significance of the designs.","score":3},{"url":"https://www.semanticscholar.org/paper/35c4531a6466ccfb6505a6298c080d5260a93f04","title":"Learning to View: Decision Transformers for Active Object Detection","venue":"ArXiv","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2023","authors":"Wenhao Ding,Nathalie Majcherczyk,Mohit Deshpande,Xuewei Qi,Ding Zhao,R. Madhivanan,Arnie Sen","id":"35c4531a6466ccfb6505a6298c080d5260a93f04","summary":"This paper uses reinforcement learning (RL) methods to control the robot in order to obtain images that maximize the detection quality and provides exhaustive analyses of the reward distribution and observation space.","score":3},{"url":"https://www.semanticscholar.org/paper/967a9a1a70c81a3d2118c55b08d080776559db43","title":"Approximating Nash equilibrium for anti-UAV jamming Markov game using a novel event-triggered multi-agent reinforcement learning.","venue":"Neural Networks","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Zikai Feng,Mengxing Huang,Yuanyuan Wu,Di Wu,Jinde Cao,I. Korovin,Sergey Gorbachev,N. Gorbacheva","id":"967a9a1a70c81a3d2118c55b08d080776559db43","summary":"A novel event-triggered multi-agent proximal policy optimization algorithm with Beta strategy (ETMAPPO) is proposed in this paper, which aims to reduce the dimension of information transmission and improve the efficiency of policy convergence.","score":3},{"url":"https://www.semanticscholar.org/paper/6ca1057501f8c2ac06d068cf9f1a8a984f87f60f","title":"Two-Stage Constrained Actor-Critic for Short Video Recommendation","venue":"ArXiv","year":2023,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"Qingpeng Cai,Zhenghai Xue,Chi Zhang,Wanqi Xue,Shuchang Liu,Ruohan Zhan,Xueliang Wang,Tianyou Zuo,Wentao Xie,Dong Zheng,Peng Jiang,Kun Gai","id":"6ca1057501f8c2ac06d068cf9f1a8a984f87f60f","summary":"This paper formulate the problem of short video recommendation as a Constrained Markov Decision Process (CMDP) and proposes a novel two-stage constrained actor-critic method, which significantly outperforms other baselines in terms of both watch time and interactions.","score":3},{"url":"https://www.semanticscholar.org/paper/069553a928fb2e5c40200e8763b148ea35d00973","title":"A Comparison of Reinforcement Learning and Deep Trajectory Based Stochastic Control Agents for Stepwise Mean-Variance Hedging","venue":"","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Ali Fathi,B. Hientzsch","id":"069553a928fb2e5c40200e8763b148ea35d00973","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/faeb99b4ef2d6040ec4c7ee053a07918c1cead97","title":"BERT Embeddings Can Track Context in Conversational Search","venue":"ArXiv","year":2021,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/04/2021","authors":"Rafael Ferreira,David Semedo,João Magalhães","id":"faeb99b4ef2d6040ec4c7ee053a07918c1cead97","summary":"This work used a Transformer-based re-ranking method and expanded this architecture to use the conversational context and showed the advantages of using the context present in the natural language utterances and in the neural embeddings generated throughout the conversation.","score":3},{"url":"https://www.semanticscholar.org/paper/656165e23c99097cd61d23f660cab480b85fd20d","title":"Open-domain conversational search assistants: the Transformer is all you need","venue":"Information Retrieval Journal","year":2022,"referenceCount":64,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/03/2022","authors":"Rafael Ferreira,Mariana Leite,David Semedo,João Magalhães","id":"656165e23c99097cd61d23f660cab480b85fd20d","summary":"This work proposes a complete open-domain abstractive conversational search agent pipeline to address two major challenges: first, conversation context-aware search and second, abstractive search-answers generation.","score":3},{"url":"https://www.semanticscholar.org/paper/0d2c76843f43d6f7219c7c3ad69a17beac19205b","title":"Conversational Entity Linking: Problem Definition and Datasets","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2021,"referenceCount":63,"citationCount":8,"influentialCitationCount":0,"publicationDate":"11/05/2021","authors":"Hideaki Joko,Faegheh Hasibi,K. Balog,A. D. Vries","id":"0d2c76843f43d6f7219c7c3ad69a17beac19205b","summary":"This paper analyzes a large number of dialogues from existing conversational datasets and annotates references to concepts, named entities, and personal entities using crowdsourcing to identify the main characteristics of conversational entity linking.","score":3},{"url":"https://www.semanticscholar.org/paper/44be4baa91754c7e738d0bee828dd88d7042e2ee","title":"DialDoc 2022 Shared Task: Open-Book Document-grounded Dialogue Modeling","venue":"Workshop on Document-grounded Dialogue and Conversational Question Answering","year":2022,"referenceCount":24,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Song Feng,S. Patel,H. Wan","id":"44be4baa91754c7e738d0bee828dd88d7042e2ee","summary":"The paper presents the results of the Shared Task hosted by the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering co-located at ACL 2022, to build goal-oriented information-seeking conversation systems that are grounded in the domain documents.","score":3},{"url":"https://www.semanticscholar.org/paper/da402a19b509405c7c83f052a5bb7f86159cc586","title":"Embedding-based Zero-shot Retrieval through Query Generation","venue":"ArXiv","year":2020,"referenceCount":43,"citationCount":20,"influentialCitationCount":2,"publicationDate":"22/09/2020","authors":"Davis Liang,Peng Xu,Siamak Shakeri,C. D. Santos,Ramesh Nallapati,Zhiheng Huang,Bing Xiang","id":"da402a19b509405c7c83f052a5bb7f86159cc586","summary":"This work considers the embedding-based two-tower architecture as the neural retrieval model and proposes a novel method for generating synthetic training data for retrieval, which produces remarkable results, significantly outperforming BM25 on 5 out of 6 datasets tested.","score":3},{"url":"https://www.semanticscholar.org/paper/e30514dd14a6f150ee96f5d0f8e238bd5662168a","title":"Using the Hammer Only on Nails: A Hybrid Method for Evidence Retrieval for Question Answering","venue":"ArXiv","year":2020,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/09/2020","authors":"Zhengzhong Liang,Yiyun Zhao,M. Surdeanu","id":"e30514dd14a6f150ee96f5d0f8e238bd5662168a","summary":"This work uses a routing classifier that learns when to direct incoming questions to BM25 vs. USE-QA for evidence retrieval using very simple statistics, which can be efficiently extracted from the top candidate evidence sentences produced by a BM25 model.","score":3},{"url":"https://www.semanticscholar.org/paper/be68cd5d7f14e0b735cd5e9ab0c9e99c3ce290c9","title":"RECONSIDER: Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering","venue":"ArXiv","year":2020,"referenceCount":23,"citationCount":12,"influentialCitationCount":0,"publicationDate":"21/10/2020","authors":"Srini Iyer,Sewon Min,Yashar Mehdad,Wen-tau Yih","id":"be68cd5d7f14e0b735cd5e9ab0c9e99c3ce290c9","summary":"A simple and effective re-ranking approach (RECONSIDER) for span-extraction tasks, that improves upon the performance of large pre-trained MRC models, and achieves a new state of the art on four QA tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/15f6bf7a9eae8890e7cff47bbc2483a77a5d7e9f","title":"Neural Passage Retrieval with Improved Negative Contrast","venue":"ArXiv","year":2020,"referenceCount":32,"citationCount":19,"influentialCitationCount":3,"publicationDate":"23/10/2020","authors":"Jing Lu,Gustavo Hernández Ábrego,Ji Ma,Jianmo Ni,Yinfei Yang","id":"15f6bf7a9eae8890e7cff47bbc2483a77a5d7e9f","summary":"The effects of negative sampling in dual encoder models used to retrieve passages for automatic question answering are explored and a new state-of-the-art level of performance is established on two of the open-domain question answering datasets that are evaluated.","score":3},{"url":"https://www.semanticscholar.org/paper/68d6fcf840a4112914f808bdb721aefc0cabeb6b","title":"Context-Aware Answer Extraction in Question Answering","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":24,"citationCount":11,"influentialCitationCount":1,"publicationDate":"01/11/2020","authors":"Yeon Seonwoo,Ji-Hoon Kim,Jung-Woo Ha,Alice H. Oh","id":"68d6fcf840a4112914f808bdb721aefc0cabeb6b","summary":"With experiments on reading comprehension, it is shown that BLANC outperforms the state-of-the-art QA models, and the performance gap increases as the number of answer text occurrences increases.","score":3},{"url":"https://www.semanticscholar.org/paper/11adf5397466ecbec178f01ef644143d43138d09","title":"Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":53,"citationCount":20,"influentialCitationCount":7,"publicationDate":"01/11/2020","authors":"Dongfang Li,Baotian Hu,Qingcai Chen,Weihua Peng,Anqi Wang","id":"11adf5397466ecbec178f01ef644143d43138d09","summary":"A novel reading comprehension model KMQA is proposed, which can fully exploit the structural medical knowledge and the reference medical plain text and passes the exam with 61.8% accuracy rate on the test set.","score":3},{"url":"https://www.semanticscholar.org/paper/436364d3acec28652f6b48ccc65c21a61f7848f8","title":"Unified Open-Domain Question Answering with Structured and Unstructured Knowledge","venue":"ArXiv","year":2020,"referenceCount":45,"citationCount":29,"influentialCitationCount":4,"publicationDate":2020,"authors":"Barlas Oğuz,Xilun Chen,Vladimir Karpukhin,Stanislav Peshterliev,Dmytro Okhonko,M. Schlichtkrull,Sonal Gupta,Yashar Mehdad,S. Yih","id":"436364d3acec28652f6b48ccc65c21a61f7848f8","summary":"This work homogenizes all sources by reducing them to text, and applies recent, powerful retriever-reader models which have so far been limited to text sources only to show that knowledge-base QA can be greatly improved when reformulated in this way.","score":3},{"url":"https://www.semanticscholar.org/paper/8d48441fcf5dd900955036761972cecd18110813","title":"NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons Learned","venue":"Neural Information Processing Systems","year":2021,"referenceCount":64,"citationCount":43,"influentialCitationCount":0,"publicationDate":"01/01/2021","authors":"Sewon Min,Jordan L. Boyd-Graber,Chris Alberti,Danqi Chen,Eunsol Choi,Michael Collins,Kelvin Guu,Hannaneh Hajishirzi,Kenton Lee,Jennimaria Palomaki,Colin Raffel,Adam Roberts,T. Kwiatkowski,Patrick Lewis,Yuxiang Wu,Heinrich Kuttler,Linqing Liu,Pasquale Minervini,Pontus Stenetorp,Sebastian Riedel,Sohee Yang,Minjoon Seo,Gautier Izacard,Fabio Petroni,Lucas Hosseini,Nicola De Cao,Edouard Grave,Ikuya Yamada,Sonse Shimaoka,Masatoshi Suzuki,Shumpei Miyawaki,Shun Sato,Ryo Takahashi,Jun Suzuki,Martin Fajcik,Martin Docekal,Karel Ondrej,P. Smrz,Hao Cheng,Yelong Shen,Xiaodong Liu,Pengcheng He,Weizhu Chen,Jianfeng Gao,Barlas Oğuz,Xilun Chen,Vladimir Karpukhin,Stanislav Peshterliev,Dmytro Okhonko,M. Schlichtkrull,Sonal Gupta,Yashar Mehdad,Wen-tau Yih","id":"8d48441fcf5dd900955036761972cecd18110813","summary":"The motivation and organization of the competition is described, the best submissions are reviewed, and system predictions are analyzed to inform a discussion of evaluation for open-domain QA.","score":3},{"url":"https://www.semanticscholar.org/paper/0558e6575cbed16a63761a906bbaf91c7843a78d","title":"Reader-Guided Passage Reranking for Open-Domain Question Answering","venue":"Findings","year":2021,"referenceCount":27,"citationCount":16,"influentialCitationCount":2,"publicationDate":2021,"authors":"Yuning Mao,Pengcheng He,Xiaodong Liu,Yelong Shen,Jianfeng Gao,Jiawei Han,Weizhu Chen","id":"0558e6575cbed16a63761a906bbaf91c7843a78d","summary":"A simple and effective passage reranking method, named ReaderguIDEd Reranker (RIDER), which does not involve training and reranks the retrieved passages solely based on the top predictions of the reader before reranking, which outperforms state-of-the-art transformer-based supervised rerankers.","score":3},{"url":"https://www.semanticscholar.org/paper/2dd273342715da6742fc84b619bf4d798620f206","title":"Using the Hammer only on Nails: A Hybrid Method for Representation-Based Evidence Retrieval for Question Answering","venue":"European Conference on Information Retrieval","year":2021,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Zhengzhong Liang,Yiyun Zhao,M. Surdeanu","id":"2dd273342715da6742fc84b619bf4d798620f206","summary":"This work introduces a hybrid approach for representation-based evidence retrieval that combines the advantages of both IR directions, and shows that the proposed routing strategy is considerably faster than neural methods, with a runtime that is up to 5 times faster than USE-QA.","score":3},{"url":"https://www.semanticscholar.org/paper/cd2f3398727bf3d5ccef42b33ad9097aeb1c44f1","title":"xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":22,"citationCount":19,"influentialCitationCount":4,"publicationDate":2021,"authors":"Nan Yang,Furu Wei,Binxing Jiao,Daxin Jiang,Linjun Yang","id":"cd2f3398727bf3d5ccef42b33ad9097aeb1c44f1","summary":"This paper proposes a new contrastive learning method called Cross Momentum Contrastive learning (xMoCo), for learning a dual-encoder model for question-passage matching that efficiently maintains a large pool of negative samples like the original MoCo and enables using separate encoders for questions and passages.","score":3},{"url":"https://www.semanticscholar.org/paper/5be217c678916543884c354654263f27c0a6bd9f","title":"Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":36,"citationCount":32,"influentialCitationCount":10,"publicationDate":2021,"authors":"Shuqi Lu,Di He,Chenyan Xiong,Guolin Ke,Waleed Malik,Zhicheng Dou,Paul Bennett,Tie-Yan Liu,Arnold Overwijk","id":"5be217c678916543884c354654263f27c0a6bd9f","summary":"A new self-learning method that pre-trains the autoencoder using a weak decoder, with restricted capacity and attention flexibility to push the encoder to provide better text representations, which significantly boosts the effectiveness and few-shot ability of dense retrieval models.","score":3},{"url":"https://www.semanticscholar.org/paper/60e292b7cef1507cb779385080cfaeac8849a78e","title":"Cluster-Former: Clustering-based Sparse Transformer for Question Answering","venue":"Findings","year":2021,"referenceCount":40,"citationCount":9,"influentialCitationCount":3,"publicationDate":2021,"authors":"Shuohang Wang,Luowei Zhou,Zhe Gan,Yen-Chun Chen,Yuwei Fang,Siqi Sun,Yu Cheng,Jingjing Liu","id":"60e292b7cef1507cb779385080cfaeac8849a78e","summary":"Cluster-Former is proposed, a novel clusteringbased sparse Transformer to perform attention across chunked sequences that allows information integration beyond local windows, which is especially beneficial for question answering (QA) tasks that rely on long-range dependencies.","score":3},{"url":"https://www.semanticscholar.org/paper/c3a3665d0891abdc1cbdf244fcc3ba55acc67ea1","title":"Representing Long Documents with Contextualized Passage Embeddings","venue":"","year":2021,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"","id":"c3a3665d0891abdc1cbdf244fcc3ba55acc67ea1","summary":"In experiments, it is found that PE+BDE is competitive with token-level or sentence-level models and sometimes even better, and improves over a Longformer-based model by +14 accuracy points for plagiarism detection.","score":3},{"url":"https://www.semanticscholar.org/paper/d4b95b37bcec7f5b09f02c60e5e12bb13aa86866","title":"Preprint. Under review","venue":"","year":2021,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Hang Zhang,Yeyun Gong,Yelong Shen,Jiancheng Lv,Nan Duan,Weizhu Chen","id":"d4b95b37bcec7f5b09f02c60e5e12bb13aa86866","summary":"Experimental results show that AR2 consistently and significantly outperforms existing dense retriever methods and achieves new state-of-the-art results on all of them.","score":3},{"url":"https://www.semanticscholar.org/paper/196e1b5ee39219904427dbe636d154559d4f84b3","title":"Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":72,"citationCount":50,"influentialCitationCount":7,"publicationDate":2021,"authors":"Ji Ma,I. Korotkov,Yinfei Yang,K. Hall,Ryan T. McDonald","id":"196e1b5ee39219904427dbe636d154559d4f84b3","summary":"Empirically, it is shown that this is an effective strategy for building neural passage retrieval models in the absence of large training corpora and depending on the domain, this technique can even approach the accuracy of supervised models.","score":3},{"url":"https://www.semanticscholar.org/paper/6606f7b9938dcc0e03dba45359dd04cb15018f13","title":"Robust Question Answering Through Sub-part Alignment","venue":"North American Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":54,"citationCount":8,"influentialCitationCount":0,"publicationDate":"30/04/2020","authors":"Jifan Chen,Greg Durrett","id":"6606f7b9938dcc0e03dba45359dd04cb15018f13","summary":"This work model question answering as an alignment problem, decomposing both the question and context into smaller units based on off-the-shelf semantic representations, and align the question to a subgraph of the context in order to find the answer.","score":3},{"url":"https://www.semanticscholar.org/paper/30602e3382df3abedb5f225b55b7efce8580f74d","title":"ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":59,"citationCount":12,"influentialCitationCount":1,"publicationDate":"02/05/2020","authors":"Woojeong Jin,Suji Kim,Rahul Khanna,Dong-Ho Lee,Fred Morstatter,A. Galstyan,Xiang Ren","id":"30602e3382df3abedb5f225b55b7efce8580f74d","summary":"This work aims to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data, and introduces ForecastQA, a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts.","score":3},{"url":"https://www.semanticscholar.org/paper/030d7d7ae48a9f81700b2c1f7cf835235777b8e7","title":"Relevance-guided Supervision for OpenQA with ColBERT","venue":"Transactions of the Association for Computational Linguistics","year":2020,"referenceCount":43,"citationCount":51,"influentialCitationCount":11,"publicationDate":"01/07/2020","authors":"O. Khattab,Christopher Potts,M. Zaharia","id":"030d7d7ae48a9f81700b2c1f7cf835235777b8e7","summary":"This work proposes a weak supervision strategy that iteratively uses ColBERT to create its own training data, which greatly improves OpenQA retrieval on both Natural Questions and TriviaQA, and the resulting end-to-end Open QA system attains state-of-the-art performance on both of those datasets.","score":3},{"url":"https://www.semanticscholar.org/paper/cb58542c94ce83b09f5d3809e69518ba52709c92","title":"Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":30,"citationCount":124,"influentialCitationCount":23,"publicationDate":"06/08/2020","authors":"Patrick Lewis,Pontus Stenetorp,Sebastian Riedel","id":"cb58542c94ce83b09f5d3809e69518ba52709c92","summary":"A detailed study of the test sets of three popular open-domain benchmark datasets finds that 30% of test-set questions have a near-duplicate paraphrase in their corresponding train sets, and that simple nearest-neighbor models outperform a BART closed-book QA model.","score":3},{"url":"https://www.semanticscholar.org/paper/4b3de90c18b451f3929b143f11461d060df56e5d","title":"Rethinking the Objectives of Extractive Question Answering","venue":"Workshop on Machine Reading for Question Answering","year":2020,"referenceCount":41,"citationCount":7,"influentialCitationCount":0,"publicationDate":"28/08/2020","authors":"Martin Fajcik,Josef Jon,Santosh Kesiraju,P. Smrz","id":"4b3de90c18b451f3929b143f11461d060df56e5d","summary":"This work demonstrates that using the objective with independence assumption for modelling the span probability P (a_s , a_e) of span starting at position a_s and ending at positiona_e has adverse effects, and proposes a compound objective, composed from the joint probability while still keeping the objectivewith independence assumption as an auxiliary objective.","score":3},{"url":"https://www.semanticscholar.org/paper/08f501ea41cafa359c99269612579f5cac43c744","title":"XOR QA: Cross-lingual Open-Retrieval Question Answering","venue":"North American Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":48,"citationCount":61,"influentialCitationCount":20,"publicationDate":"22/10/2020","authors":"Akari Asai,Jungo Kasai,J. Clark,Kenton Lee,Eunsol Choi,Hannaneh Hajishirzi","id":"08f501ea41cafa359c99269612579f5cac43c744","summary":"This work constructs a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TyDi QA could not find same-language answers for and introduces a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving cross-lingually document retrieval from multilingual and English resources.","score":3},{"url":"https://www.semanticscholar.org/paper/6027ef3b4e5585b45db0b9d333956425d3972351","title":"Differentiable Open-Ended Commonsense Reasoning","venue":"North American Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":34,"citationCount":27,"influentialCitationCount":4,"publicationDate":"24/10/2020","authors":"Bill Yuchen Lin,Haitian Sun,Bhuwan Dhingra,M. Zaheer,Xiang Ren,William W. Cohen","id":"6027ef3b4e5585b45db0b9d333956425d3972351","summary":"DrFact is proposed, an efficient Differentiable model for multi-hop Reasoning over knowledge Facts, which outperforms strong baseline methods by a large margin and is evaluated to evaluate OpenCSR methods.","score":3},{"url":"https://www.semanticscholar.org/paper/17ccd81f77d8cf798f12608ade1d838ffb96b66c","title":"Answering Ambiguous Questions through Generative Evidence Fusion and Round-Trip Prediction","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":29,"citationCount":7,"influentialCitationCount":4,"publicationDate":"26/11/2020","authors":"Yifan Gao,Henghui Zhu,Patrick Ng,C. D. Santos,Zhiguo Wang,Feng Nan,Dejiao Zhang,Ramesh Nallapati,Andrew O. Arnold,Bing Xiang","id":"17ccd81f77d8cf798f12608ade1d838ffb96b66c","summary":"The proposed round-trip prediction is a model-agnostic general approach for answering ambiguous open-domain questions, which improves the state-of-the-art Refuel as well as several baseline models.","score":3},{"url":"https://www.semanticscholar.org/paper/1283ca87e6215b7393eba1653a4a2e4bf28d2868","title":"Learning Dense Representations of Phrases at Scale","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":52,"citationCount":58,"influentialCitationCount":9,"publicationDate":"23/12/2020","authors":"Jinhyuk Lee,Mujeen Sung,Jaewoo Kang,Danqi Chen","id":"1283ca87e6215b7393eba1653a4a2e4bf28d2868","summary":"This work shows for the first time that it can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA and proposes a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference.","score":3},{"url":"https://www.semanticscholar.org/paper/1c6f7579ca91da348273d8be200b3c9e3a77718e","title":"Rider: Reader-Guided Passage Reranking for Open-Domain Question Answering","venue":"","year":2021,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2021","authors":"Yuning Mao,Pengcheng He,Xiaodong Liu,Yelong Shen,Jianfeng Gao,Jiawei Han,Weizhu Chen","id":"1c6f7579ca91da348273d8be200b3c9e3a77718e","summary":"A simple and effective passage reranking method, named ReaderguIDEd Reranker (RIDER), which does not involve training and reranks the retrieved passages solely based on the top predictions of the reader before reranking, which outperforms state-of-the-art transformer-based supervised rerankers.","score":3},{"url":"https://www.semanticscholar.org/paper/346081161bdc8f18e2a4c4af7f51d35452b5cb01","title":"Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies","venue":"Transactions of the Association for Computational Linguistics","year":2021,"referenceCount":28,"citationCount":97,"influentialCitationCount":27,"publicationDate":"06/01/2021","authors":"Mor Geva,Daniel Khashabi,Elad Segal,Tushar Khot,D. Roth,Jonathan Berant","id":"346081161bdc8f18e2a4c4af7f51d35452b5cb01","summary":"This work introduces StrategyQA, a question answering benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy, and proposes a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts.","score":3},{"url":"https://www.semanticscholar.org/paper/789b5441743c2e38cf4c38749ed820c0671d81b1","title":"Muppet: Massive Multi-task Representations with Pre-Finetuning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":78,"citationCount":125,"influentialCitationCount":20,"publicationDate":"26/01/2021","authors":"Armen Aghajanyan,Anchit Gupta,Akshat Shrivastava,Xilun Chen,Luke Zettlemoyer,Sonal Gupta","id":"789b5441743c2e38cf4c38749ed820c0671d81b1","summary":"It is shown that pre-finetuning consistently improves performance for pretrained discriminators and generation models on a wide range of tasks while also significantly improving sample efficiency during fine-tuning, and that large-scale multi-tasking is crucial.","score":3},{"url":"https://www.semanticscholar.org/paper/87c7a320cefdfa9924052e945f6eab423c9b1446","title":"Do Question Answering Modeling Improvements Hold Across Benchmarks?","venue":"","year":2021,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2021","authors":"Nelson F. Liu,Tony Lee,Robin Jia,Percy Liang","id":"87c7a320cefdfa9924052e945f6eab423c9b1446","summary":"It is found that human-constructed benchmarks have high concurrence amongst themselves, even if their passage and question distributions are very different, which indicates that, despite years of intense community focus on a small number of benchmarks, the modeling improvements studied hold broadly.","score":3},{"url":"https://www.semanticscholar.org/paper/972a74968d2522908b06c5bd1e26266194c5a9ee","title":"Decontextualization: Making Sentences Stand-Alone","venue":"Transactions of the Association for Computational Linguistics","year":2021,"referenceCount":33,"citationCount":37,"influentialCitationCount":5,"publicationDate":"09/02/2021","authors":"Eunsol Choi,Jennimaria Palomaki,Matthew Lamm,T. Kwiatkowski,Dipanjan Das,Michael Collins","id":"972a74968d2522908b06c5bd1e26266194c5a9ee","summary":"It is argued that decontextualization is an important subtask in many downstream applications, and that the definitions and resources provided can benefit tasks that operate on sentences that occur in a richer context.","score":3},{"url":"https://www.semanticscholar.org/paper/1e9ff5f2e9aa3e6c01bc89c81ba16442f1b5938d","title":"Less is More: Pre-train a Strong Text Encoder for Dense Retrieval Using a Weak Decoder","venue":"","year":2021,"referenceCount":33,"citationCount":3,"influentialCitationCount":0,"publicationDate":"18/02/2021","authors":"Shuqi Lu,Di He,Chenyan Xiong,Guolin Ke,Waleed Malik,Zhicheng Dou,Paul Bennett,Tie-Yan Liu,Arnold Overwijk","id":"1e9ff5f2e9aa3e6c01bc89c81ba16442f1b5938d","summary":"A new selflearning method that pre-trains the autoencoder using a weak decoder, with restricted capacity and attention flexibility to push the encoder to provide better text representations, which significantly boosts the effectiveness and few-shot ability of dense retrieval models.","score":3},{"url":"https://www.semanticscholar.org/paper/facefd2fc4b718c6a0d8096b4eb02866028a04c2","title":"Weakly-Supervised Open-Retrieval Conversational Question Answering","venue":"European Conference on Information Retrieval","year":2021,"referenceCount":57,"citationCount":7,"influentialCitationCount":3,"publicationDate":"03/03/2021","authors":"Chen Qu,Liu Yang,Cen Chen,W. Bruce Croft,Kalpesh Krishna,Mohit Iyyer","id":"facefd2fc4b718c6a0d8096b4eb02866028a04c2","summary":"This work introduces a learned weak supervision approach that can identify a paraphrased span of the known answer in a passage of a passage in the open-retrieval ConvQA setting under a weak supervision setting.","score":3},{"url":"https://www.semanticscholar.org/paper/5b4dc3b099b9f500392b4fb0de86876feedc522b","title":"Open Domain Question Answering over Tables via Dense Retrieval","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":26,"citationCount":30,"influentialCitationCount":6,"publicationDate":"22/03/2021","authors":"Jonathan Herzig,Thomas Müller,Syrine Krichene,Julian Martin Eisenschlos","id":"5b4dc3b099b9f500392b4fb0de86876feedc522b","summary":"This work tackles open-domain QA over tables for the first time, and shows that retrieval can be improved by a retriever designed to handle tabular context, and presents an effective pre-training procedure for this retriever.","score":3},{"url":"https://www.semanticscholar.org/paper/443f889a2d4da14e930586e9416068f91c93a5bb","title":"Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":35,"citationCount":6,"influentialCitationCount":0,"publicationDate":"22/03/2021","authors":"Ansong Ni,Matt Gardner,Pradeep Dasigi","id":"443f889a2d4da14e930586e9416068f91c93a5bb","summary":"A new parameterization of set-valued retrieval that handles unanswerable queries is developed, and it is shown that marginalizing over this set during training allows a model to mitigate false negatives in supporting evidence annotations.","score":3},{"url":"https://www.semanticscholar.org/paper/681b2120803ca621620341ef3da2c041f8e0ea02","title":"Designing a Minimal Retrieve-and-Read System for Open-Domain Question Answering","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":26,"citationCount":5,"influentialCitationCount":1,"publicationDate":"15/04/2021","authors":"Sohee Yang,Minjoon Seo","id":"681b2120803ca621620341ef3da2c041f8e0ea02","summary":"The results indicate that retrieve-and-read can be a viable option even in a highly constrained serving environment such as edge devices, as it is shown that it can achieve better accuracy than a purely parametric model with comparable docker-level system size.","score":3},{"url":"https://www.semanticscholar.org/paper/cea1f8894cb73f6b1ece7f465bad18fb4b79168d","title":"Simple and Efficient ways to Improve REALM","venue":"Workshop on Machine Reading for Question Answering","year":2021,"referenceCount":27,"citationCount":4,"influentialCitationCount":0,"publicationDate":"18/04/2021","authors":"Vidhisha Balachandran,Ashish Vaswani,Yulia Tsvetkov,Niki Parmar","id":"cea1f8894cb73f6b1ece7f465bad18fb4b79168d","summary":"This work proposes REALM++, where it improves upon the training and inference setups and introduces better supervision signal for improving performance, without any architectural changes, and achieves ~5.5% absolute accuracy gains over the baseline while being faster to train.","score":3},{"url":"https://www.semanticscholar.org/paper/9668781de7fdb513cb1c7fef3f0cbf06c892c9cd","title":"Efficient Retrieval Optimized Multi-task Learning","venue":"ArXiv","year":2021,"referenceCount":19,"citationCount":3,"influentialCitationCount":0,"publicationDate":"20/04/2021","authors":"He Fun,S. Gandhi,Sujith Ravi","id":"9668781de7fdb513cb1c7fef3f0cbf06c892c9cd","summary":"This paper proposes a novel Retrieval Optimized Multi-task (ROM) framework for jointly training self-supervised tasks, knowledge retrieval, and extractive question answering that achieves comparable or better performance than recent methods on QA, while drastically reducing the number of parameters.","score":3},{"url":"https://www.semanticscholar.org/paper/c435fdeff9d3b30c3bece24429123d23da147992","title":"RECONSIDER: Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":23,"citationCount":9,"influentialCitationCount":1,"publicationDate":"01/06/2021","authors":"Srini Iyer,Sewon Min,Yashar Mehdad,Wen-tau Yih","id":"c435fdeff9d3b30c3bece24429123d23da147992","summary":"A successful re-ranking approach (RECONSIDER) for span-extraction tasks that improves upon the performance of MRC models, even beyond large-scale pre-training.","score":3},{"url":"https://www.semanticscholar.org/paper/60163eab80666401862f16c01a799368ddd8be8d","title":"Investigating the Effect of Background Knowledge on Natural Questions","venue":"Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out","year":2021,"referenceCount":20,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/06/2021","authors":"Vidhisha Balachandran,Bhuwan Dhingra,Haitian Sun,Michael Collins,W. Cohen","id":"60163eab80666401862f16c01a799368ddd8be8d","summary":"The results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets and propose and analyze a simple, model-agnostic approach for incorporating KB paths into text-based QA systems.","score":3},{"url":"https://www.semanticscholar.org/paper/5b2a13b85ef9d3ad61639c3b400e447f3217ec98","title":"Answer Generation for Retrieval-based Question Answering Systems","venue":"Findings","year":2021,"referenceCount":28,"citationCount":7,"influentialCitationCount":0,"publicationDate":"02/06/2021","authors":"Chao-Chun Hsu,Eric Lind,Luca Soldaini,Alessandro Moschitti","id":"5b2a13b85ef9d3ad61639c3b400e447f3217ec98","summary":"This work proposes to generate answers from a set of AS2 top candidates by training a sequence to sequence transformer model to generate an answer from a candidate set.","score":3},{"url":"https://www.semanticscholar.org/paper/15493bbb5387d60e7c77cee34528acd3acae7b65","title":"Efficient Passage Retrieval with Hashing for Open-domain Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":27,"citationCount":43,"influentialCitationCount":8,"publicationDate":"02/06/2021","authors":"Ikuya Yamada,Akari Asai,Hannaneh Hajishirzi","id":"15493bbb5387d60e7c77cee34528acd3acae7b65","summary":"BPR is a memory-efficient neural retrieval model that integrates a learning-to-hash technique into the state-of-the-art Dense Passage Retriever to represent the passage index using compact binary codes rather than continuous vectors.","score":3},{"url":"https://www.semanticscholar.org/paper/4291fe672cf6dc73e237ca0942fa49beb8c98711","title":"Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":44,"citationCount":18,"influentialCitationCount":4,"publicationDate":"12/06/2021","authors":"Anthony Chen,Pallavi Gudipati,S. Longpre,Xiao Ling,Sameer Singh","id":"4291fe672cf6dc73e237ca0942fa49beb8c98711","summary":"It is found that the retrievers exhibit popularity bias, significantly under-performing on rarer entities that share a name, e.g., they are twice as likely to retrieve erroneous documents on queries for the less popular entity under the same name.","score":3},{"url":"https://www.semanticscholar.org/paper/068eb6b3797d5807b744d9326e7ebb50769e4b4d","title":"A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":56,"citationCount":2,"influentialCitationCount":0,"publicationDate":"16/06/2021","authors":"Dimitris Pappas,Ion Androutsopoulos","id":"068eb6b3797d5807b744d9326e7ebb50769e4b4d","summary":"This work presents an architecture for joint document and snippet ranking, the two middle stages, which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents and can be used with any neural text relevance ranker.","score":3},{"url":"https://www.semanticscholar.org/paper/0c8ffa9e192a4f9a90dd57aefe47289a3357f51d","title":"Semantic Answer Similarity for Evaluating Question Answering Models","venue":"Workshop on Machine Reading for Question Answering","year":2021,"referenceCount":35,"citationCount":15,"influentialCitationCount":6,"publicationDate":"13/08/2021","authors":"Julian Risch,Timo Moller,Julian Gutsch,Malte Pietsch","id":"0c8ffa9e192a4f9a90dd57aefe47289a3357f51d","summary":"SAS, a cross-encoder-based metric for the estimation of semantic answer similarity, is presented and it is found that semantic similarity metrics based on recent transformer models correlate much better with human judgment than traditional lexical similarity metrics on two newly created datasets and one dataset from related work.","score":3},{"url":"https://www.semanticscholar.org/paper/ef76276f9ef929496f03282fa85ae1bbcdc69767","title":"Robust Retrieval Augmented Generation for Zero-shot Slot Filling","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":36,"citationCount":15,"influentialCitationCount":1,"publicationDate":"31/08/2021","authors":"Michael R. Glass,Gaetano Rossiello,Md. Faisal Mahbub Chowdhury,A. Gliozzo","id":"ef76276f9ef929496f03282fa85ae1bbcdc69767","summary":"A novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models and demonstrates the robustness of the system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling.","score":3},{"url":"https://www.semanticscholar.org/paper/1973f251069086d850bde83fabfb05ceaf7b8859","title":"Multi-Stage Conversational Passage Retrieval: An Approach to Fusing Term Importance Estimation and Neural Query Rewriting","venue":"ACM Trans. Inf. Syst.","year":2021,"referenceCount":127,"citationCount":19,"influentialCitationCount":3,"publicationDate":"31/08/2021","authors":"Sheng-Chieh Lin,Jheng-Hong Yang,Rodrigo Nogueira,Ming-Feng Tsai,Chuan-Ju Wang,Jimmy J. Lin","id":"1973f251069086d850bde83fabfb05ceaf7b8859","summary":"This article proposes two conversational query reformulation (CQR) methods: (1) term importance estimation and (2) neural query rewriting, which expand conversational queries using important terms extracted from the conversational context with frequency-based signals and reformulation into natural queries with a pretrained sequence-to-sequence model.","score":3},{"url":"https://www.semanticscholar.org/paper/e3480d9395e692833b722b2e957d51139985f310","title":"General-Purpose Question-Answering with Macaw","venue":"ArXiv","year":2021,"referenceCount":25,"citationCount":33,"influentialCitationCount":8,"publicationDate":"06/09/2021","authors":"Oyvind Tafjord,Peter Clark","id":"e3480d9395e692833b722b2e957d51139985f310","summary":"The M ACAW system is described, and a variety of question types where it produces surprisingly good answers are illustrated, well outside the training setup, offering insights into the limitations of pretrained language models.","score":3},{"url":"https://www.semanticscholar.org/paper/395aae6e7a79e5760457ca38e868acc970016230","title":"MATE: Multi-view Attention for Table Transformer Efficiency","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":37,"citationCount":32,"influentialCitationCount":9,"publicationDate":"09/09/2021","authors":"Julian Martin Eisenschlos,Maharshi Gor,Thomas Müller,William W. Cohen","id":"395aae6e7a79e5760457ca38e868acc970016230","summary":"MATE is proposed, a novel Transformer architecture designed to model the structure of web tables that uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table.","score":3},{"url":"https://www.semanticscholar.org/paper/1e8672dfcb2be6e371dc5ccbe3845a6ba9716955","title":"Extract, Integrate, Compete: Towards Verification Style Reading Comprehension","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":26,"citationCount":3,"influentialCitationCount":0,"publicationDate":"11/09/2021","authors":"Chen Zhang,Yuxuan Lai,Yansong Feng,Dongyan Zhao","id":"1e8672dfcb2be6e371dc5ccbe3845a6ba9716955","summary":"A novel Extract-Integrate-Compete approach is proposed, which iteratively selects complementary evidence with a novel query updating mechanism and adaptively distills supportive evidence, followed by a pairwise competition to push models to learn the subtle difference among similar text pieces.","score":3},{"url":"https://www.semanticscholar.org/paper/bde10a2d3dfd138ea6a83f14c17ea3c3f9ac6db0","title":"A Survey of Knowledge Enhanced Pre-trained Models","venue":"ArXiv","year":2021,"referenceCount":189,"citationCount":11,"influentialCitationCount":0,"publicationDate":"01/10/2021","authors":"Jian Yang,Gang Xiao,Yulong Shen,Wei Jiang,Xinyu Hu,Ying Zhang,Jinghui Peng","id":"bde10a2d3dfd138ea6a83f14c17ea3c3f9ac6db0","summary":"A comprehensive overview of KEPTMs in NLP and CV is provided and the progress of pre-trained models and knowledge representation learning is introduced.","score":3},{"url":"https://www.semanticscholar.org/paper/ef5a79368dddf07a368347c64c289bfaaa587fa5","title":"Perhaps PTLMs Should Go to School – A Task to Assess Open Book and Closed Book QA","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":36,"citationCount":2,"influentialCitationCount":0,"publicationDate":"04/10/2021","authors":"Manuel R. Ciosici,Joe Cecil,Alex Hedges,Dong-Ho Lee,Marjorie Freedman,R. Weischedel","id":"ef5a79368dddf07a368347c64c289bfaaa587fa5","summary":"This goal is to deliver a new task and leaderboard to stimulate research on question answering and pre-trained language models (PTLMs) to understand a significant instructional document, e.g., an introductory college textbook or a manual.","score":3},{"url":"https://www.semanticscholar.org/paper/0effb07e34ed93ec11b1f2371ec80781e0268561","title":"Encoder Adaptation of Dense Passage Retrieval for Open-Domain Question Answering","venue":"ArXiv","year":2021,"referenceCount":34,"citationCount":5,"influentialCitationCount":0,"publicationDate":"04/10/2021","authors":"Minghan Li,Jimmy J. Lin","id":"0effb07e34ed93ec11b1f2371ec80781e0268561","summary":"Different combinations of DPR’s question and passage encoder learned from five benchmark QA datasets on both indomain and out-of-domain questions are inspected to answer the question how an in-distribution question/passage encoder would generalize if paired with an OOD passage/question encoder from another domain.","score":3},{"url":"https://www.semanticscholar.org/paper/d835d95e252c315103b435cc21a350ebc8d52616","title":"Cross-Lingual G EN QA: Open-Domain Question Answering with Answer Sentence Generation","venue":"","year":2022,"referenceCount":64,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Benjamin Muller,Luca Soldaini,Rik Koncel-Kedziorski,Eric Lind,Alessandro Moschitti","id":"d835d95e252c315103b435cc21a350ebc8d52616","summary":"This paper introduces G EN -T Y D I QA, an extension of the TyDiQA dataset with well-formed and complete answers for Arabic, Bengali, English, Japanese, and Russian questions and presents the first Cross-Lingual answer sentence generation system (C ROSS -L INGUAL G EN QA).","score":3},{"url":"https://www.semanticscholar.org/paper/bce1bcccf00bb5790792aec7fcbfbf1b8485b759","title":"Representation Decoupling for Open-Domain Passage Retrieval","venue":"ArXiv","year":2021,"referenceCount":38,"citationCount":2,"influentialCitationCount":0,"publicationDate":2021,"authors":"Bohong Wu,Zhuosheng Zhang,Jinyuan Wang,Hai Zhao","id":"bce1bcccf00bb5790792aec7fcbfbf1b8485b759","summary":"This work proposes to solve the influence of conflicts in the widely used CL strategy in ODPR by decoupling the passage representations into contextual sentence-level ones, and design specific CL strategies to mediate these conflicts.","score":3},{"url":"https://www.semanticscholar.org/paper/375348cb132d8e1bc96e1302e025954f4e12d888","title":"Question Rewriting for Open-Domain Conversational QA: Best Practices and Limitations","venue":"International Conference on Information and Knowledge Management","year":2021,"referenceCount":35,"citationCount":5,"influentialCitationCount":0,"publicationDate":"26/10/2021","authors":"Marco Del Tredici,Gianni Barlacchi,Xiaoyu Shen,Weiwei Cheng,A. Gispert","id":"375348cb132d8e1bc96e1302e025954f4e12d888","summary":"While conversation history modeling with dense representations outperforms QR, it is shown the advantages to apply both jointly, as QR boosts the performance especially when limited history turns are considered.","score":3},{"url":"https://www.semanticscholar.org/paper/1f49d34277d6eb3503fad0608e0251b9861172b7","title":"DuReaderretrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine","venue":"","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yifu Qiu,Hongyu Li,Yingqi Qu,Ying Chen,Qiaoqiao She,Jing Liu,Huaqin Wu","id":"1f49d34277d6eb3503fad0608e0251b9861172b7","summary":"The experiment results demonstrate that DuReaderretrieval is challenging and there is still plenty of room for the community to improve, e.g. the generalization across domains, salient phrase and syntax mismatch between query and paragraph and robustness.","score":3},{"url":"https://www.semanticscholar.org/paper/6716b7ec2e5ccb8efbcbc22c9a640a6ae523a860","title":"R3 : Refined Retriever-Reader pipeline for Multidoc2dial","venue":"Workshop on Document-grounded Dialogue and Conversational Question Answering","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Srijan Bansal,Suraj Tripathi,Sumit Agarwal,Sireesh Gururaja,Aditya Srikanth Veerubhotla,Ritam Dutt,T. Mitamura,Eric Nyberg","id":"6716b7ec2e5ccb8efbcbc22c9a640a6ae523a860","summary":"This paper proposes several improvements over the baseline’s retriever-reader architecture to aid in modeling goal-oriented dialogues grounded in multiple documents in MultiDoc2Dial.","score":3},{"url":"https://www.semanticscholar.org/paper/3a6b2244ed6dcf68b213d584a274b61866f9726d","title":"Zero-shot cross-lingual open domain question answering","venue":"MIA","year":2022,"referenceCount":23,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sumit Agarwal,Suraj Tripathi,T. Mitamura,C. Rosé","id":"3a6b2244ed6dcf68b213d584a274b61866f9726d","summary":"This paper employs a passage reranker, the fusion-in-decoder technique for generation, and a wiki data entity-based post-processing system to tackle the inability to generate entities across all languages.","score":3},{"url":"https://www.semanticscholar.org/paper/699b440c004332b70b6aae5a2bd83b96bbb1e3a1","title":"Ask Me Anything in Your Native Language","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":29,"citationCount":1,"influentialCitationCount":1,"publicationDate":2022,"authors":"Nikita Sorokin,Dmitry Abulkhanov,Irina Piontkovskaya,Valentin Malykh","id":"699b440c004332b70b6aae5a2bd83b96bbb1e3a1","summary":"This work presents a novel approach based on single encoder for query and passage for retrieval from multi-lingual collection, together with cross-lingUAL generative reader that achieves a new state of the art in both retrieval and end-to-end tasks on the XOR TyDi dataset.","score":3},{"url":"https://www.semanticscholar.org/paper/8752a918fc1a772e15798dcf7f3a75891f545e95","title":"Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Tianhao Shen,Mingtong Liu,Ming Zhou,Deyi Xiong","id":"8752a918fc1a772e15798dcf7f3a75891f545e95","summary":"Experimental results show that mHFN outperforms strong sparse, dense and hybrid baselines and achieves new state-of-the-art performance on all languages.","score":3},{"url":"https://www.semanticscholar.org/paper/b9e68e3467cc7db36c740dd5cd53f338be311d21","title":"LinkBERT: Language Model Pretraining with Document Link Knowledge","venue":"","year":2022,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Michihiro Yasunaga,J. Leskovec,Percy Liang","id":"b9e68e3467cc7db36c740dd5cd53f338be311d21","summary":"LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks, outperforms BERT on diverse downstream tasks across two domains: a general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain ( pretrained on PubMed with citation links).","score":3},{"url":"https://www.semanticscholar.org/paper/d57c76963c5d134191b7bc7681f9c87513d97e2d","title":"Link-BERT: Pretraining a Language Model with Document Links","venue":"","year":2022,"referenceCount":83,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Michihiro Yasunaga,J. Leskovec,Percy Liang","id":"d57c76963c5d134191b7bc7681f9c87513d97e2d","summary":"This work proposes LinkBERT, an LM pretraining method that leverages links between documents that outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain ( pretrained on PubMed with citation links).","score":3},{"url":"https://www.semanticscholar.org/paper/c41f1f9621c6e2dc7ae44a3f72660ec27e5f8acd","title":"Towards Robust Neural Retrieval with Source Domain Synthetic Pre-Finetuning","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":23,"citationCount":2,"influentialCitationCount":1,"publicationDate":2022,"authors":"Revanth Gangi Reddy,Vikas Yadav,Md Arafat Sultan,M. Franz,Vittorio Castelli,Heng Ji,Avirup Sil","id":"c41f1f9621c6e2dc7ae44a3f72660ec27e5f8acd","summary":"This paper empirically shows that pre-finetuning DPR with additional synthetic data in its source domain (Wikipedia), which is generated using a fine-tuned sequence-to-sequence generator, can be a low-cost yet effective first step towards its generalization.","score":3},{"url":"https://www.semanticscholar.org/paper/59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?","venue":"Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out","year":2021,"referenceCount":54,"citationCount":155,"influentialCitationCount":32,"publicationDate":"17/01/2021","authors":"Jiachang Liu,Dinghan Shen,Yizhe Zhang,Bill Dolan,L. Carin,Weizhu Chen","id":"59641c10ed7431a3cf841f308367dc2dc0281b74","summary":"This work proposes to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt, and evaluates the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline.","score":3},{"url":"https://www.semanticscholar.org/paper/8a3ef7e6aea5083cef6fe13d7642714031547d38","title":"QAConv: Question Answering on Informative Conversations","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":49,"citationCount":12,"influentialCitationCount":1,"publicationDate":"14/05/2021","authors":"C. Wu,Andrea Madotto,Wenhao Liu,Pascale Fung,Caiming Xiong","id":"8a3ef7e6aea5083cef6fe13d7642714031547d38","summary":"A new question answering (QA) dataset that uses conversations as a knowledge source, focusing on informative conversations, including business emails, panel discussions, and work channels, that provides a new training and evaluation testbed to facilitate QA on conversations research.","score":3},{"url":"https://www.semanticscholar.org/paper/c132c485fe164f54dc8bdbe9a228500174badd5e","title":"Domain-matched Pre-training Tasks for Dense Retrieval","venue":"NAACL-HLT","year":2021,"referenceCount":49,"citationCount":45,"influentialCitationCount":4,"publicationDate":"28/07/2021","authors":"Barlas Oğuz,Kushal Lakhotia,Anchit Gupta,Patrick Lewis,Vladimir Karpukhin,Aleksandra Piktus,Xilun Chen,Sebastian Riedel,Wen-tau Yih,Sonal Gupta,Yashar Mehdad","id":"c132c485fe164f54dc8bdbe9a228500174badd5e","summary":"This work demonstrates that, with the right pre-training setup, large bi-encoder models on a recently released set of 65 million synthetically generated questions and 200 million post-comment pairs from a preexisting dataset of Reddit conversations can be overcome.","score":3},{"url":"https://www.semanticscholar.org/paper/29a3c6968888d8078d21ff5b7d9e6d78c470fdb0","title":"Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":37,"citationCount":107,"influentialCitationCount":23,"publicationDate":"12/08/2021","authors":"Luyu Gao,Jamie Callan","id":"29a3c6968888d8078d21ff5b7d9e6d78c470fdb0","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/e364737a7694141d1bc5024fc06983c648b676d1","title":"Slot Filling for Biomedical Information Extraction","venue":"Workshop on Biomedical Natural Language Processing","year":2021,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/09/2021","authors":"Yannis Papanikolaou,Francine Bennett","id":"e364737a7694141d1bc5024fc06983c648b676d1","summary":"This work presents a slot filling approach to the task of biomedical IE, effectively replacing the need for entity and relation-specific training data, allowing for zero-shot settings, and provides a fresh perspective on how to solve biomedical IE tasks, in the absence of relevant training data.","score":3},{"url":"https://www.semanticscholar.org/paper/606957402ee0741ed130c6a03ec7f20d30b7083f","title":"Adversarial Retriever-Ranker for dense text retrieval","venue":"International Conference on Learning Representations","year":2021,"referenceCount":54,"citationCount":44,"influentialCitationCount":11,"publicationDate":"07/10/2021","authors":"Hang Zhang,Yeyun Gong,Yelong Shen,Jiancheng Lv,Nan Duan,Weizhu Chen","id":"606957402ee0741ed130c6a03ec7f20d30b7083f","summary":"Experimental results show that AR2 consistently and signiﬁcantly outperforms existing dense retriever methods and achieves new state-of-the-art results on all of them.","score":3},{"url":"https://www.semanticscholar.org/paper/3f90c820417b4a770bbf74cd0bd7da0ded91f63b","title":"Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":43,"citationCount":30,"influentialCitationCount":10,"publicationDate":"13/10/2021","authors":"Xilun Chen,Kushal Lakhotia,Barlas Oğuz,Anchit Gupta,Patrick Lewis,Stanislav Peshterliev,Yashar Mehdad,Sonal Gupta,Wen-tau Yih","id":"3f90c820417b4a770bbf74cd0bd7da0ded91f63b","summary":"This work introduces the Salient Phrase Aware Retriever (SPAR), a dense retriever with the lexical matching capacity of a sparse model, and shows that a dense Lexical Model {\\Lambda} can be trained to imitate a sparse one.","score":3},{"url":"https://www.semanticscholar.org/paper/14edf1e81b2d8496c8bd88de94aa20d03efabaa2","title":"Cross-Lingual Open-Domain Question Answering with Answer Sentence Generation","venue":"AACL","year":2021,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/10/2021","authors":"Benjamin Muller,Luca Soldaini,Rik Koncel-Kedziorski,Eric Lind,Alessandro Moschitti","id":"14edf1e81b2d8496c8bd88de94aa20d03efabaa2","summary":"A cross-lingual generative model that produces full-sentence answers by exploiting passages written in multiple languages, including languages different from the question, outperforms answer sentence selection baselines for all 5 languages and monolingualGenerative pipelines for three out of five languages studied.","score":3},{"url":"https://www.semanticscholar.org/paper/2ab72b5bd8121664a64cdc0e95c3b2037ac4759d","title":"Sentence-aware Contrastive Learning for Open-Domain Passage Retrieval","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":37,"citationCount":4,"influentialCitationCount":0,"publicationDate":"14/10/2021","authors":"Bohong Wu,Zhuosheng Zhang,Jinyuan Wang,Hai Zhao","id":"2ab72b5bd8121664a64cdc0e95c3b2037ac4759d","summary":"This work introduces an in-passage negative sampling strategy to encourage a diverse generation of sentence representations within the same passage, and presents a refined model on the basis of a smaller granularity, contextual sentences, to alleviate the concerned conflicts.","score":3},{"url":"https://www.semanticscholar.org/paper/9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e","title":"Large Dual Encoders Are Generalizable Retrievers","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":40,"citationCount":47,"influentialCitationCount":19,"publicationDate":"15/12/2021","authors":"Jianmo Ni,Chen Qu,Jing Lu,Zhuyun Dai,Gustavo Hern'andez 'Abrego,Ji Ma,Vincent Zhao,Yi Luan,Keith B. Hall,Ming-Wei Chang,Yinfei Yang","id":"9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e","summary":"Experimental results show that the dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly and the ablation study finds that GTR is very data efficient, as it only needs 10% of MS Marco supervised data to match the out-of-domain performance of using all supervised data.","score":3},{"url":"https://www.semanticscholar.org/paper/56b30c6bd9dc4a2416ab3b74ad97dbb7a2904229","title":"WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":78,"citationCount":52,"influentialCitationCount":7,"publicationDate":"16/01/2022","authors":"Alisa Liu,Swabha Swayamdipta,Noah A. Smith,Yejin Choi","id":"56b30c6bd9dc4a2416ab3b74ad97dbb7a2904229","summary":"This work introduces a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluation strength of humans, and demonstrates the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process.","score":3},{"url":"https://www.semanticscholar.org/paper/15031e6a94f9d6d19f74740c224a5523ec64d975","title":"Improving Biomedical Information Retrieval with Neural Retrievers","venue":"AAAI Conference on Artificial Intelligence","year":2022,"referenceCount":49,"citationCount":9,"influentialCitationCount":0,"publicationDate":"19/01/2022","authors":"Man Luo,Arindam Mitra,Tejas Gokhale,Chitta Baral","id":"15031e6a94f9d6d19f74740c224a5523ec64d975","summary":"This paper proposes a template-based question generation method that can be leveraged to train neural retriever models, and introduces the ``Poly-DPR'' model which encodes each context into multiple context vectors.","score":3},{"url":"https://www.semanticscholar.org/paper/eef99f7811e987d57d9d0371facf1b0fd0fbc2ed","title":"LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval","venue":"Findings","year":2022,"referenceCount":70,"citationCount":15,"influentialCitationCount":3,"publicationDate":"11/03/2022","authors":"Canwen Xu,Daya Guo,Nan Duan,Julian McAuley","id":"eef99f7811e987d57d9d0371facf1b0fd0fbc2ed","summary":"This paper proposes LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for training, and proposes Lexicon-Enhanced Dense Retrieval (LEDR) as a simple yet effective way to enhance dense retrieval with lexical matching.","score":3},{"url":"https://www.semanticscholar.org/paper/4544a639fde8c1c387a2a01a3d7560163dbf6f99","title":"Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":30,"citationCount":7,"influentialCitationCount":0,"publicationDate":"14/03/2022","authors":"Jiawei Zhou,Xiaoguang Li,Lifeng Shang,Lan Luo,Ke Zhan,Enrui Hu,Xinyu Zhang,Hao Jiang,Zhao Cao,Fan Yu,Xin Jiang,Qun Liu,Lei Chen","id":"4544a639fde8c1c387a2a01a3d7560163dbf6f99","summary":"It is demonstrated that the hyperlink-based structures of dual-link and co-mention can provide effective relevance signals for large-scale pre-training that better facilitate downstream passage retrieval.","score":3},{"url":"https://www.semanticscholar.org/paper/e150dc483f7a23c5c5972cdf42f904982b055793","title":"Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering","venue":"SPANLP","year":2022,"referenceCount":32,"citationCount":6,"influentialCitationCount":0,"publicationDate":"14/03/2022","authors":"Man Luo,Kazuma Hashimoto,Semih Yavuz,Zhiwei Liu,Chitta Baral,Yingbo Zhou","id":"e150dc483f7a23c5c5972cdf42f904982b055793","summary":"This first attempt to systematically study the comparison of extractive and generative readers for question answering explores nine transformer-based large pre-trained language models (PrLMs) as backbone architectures and organizes the findings under two main categories: keeping the architecture invariant, and varying the underlying PrLMs.","score":3},{"url":"https://www.semanticscholar.org/paper/00b28e5f100aa35e83487b17013b8235cdf45c3c","title":"Multi-View Document Representation Learning for Open-Domain Dense Retrieval","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":39,"citationCount":13,"influentialCitationCount":2,"publicationDate":"16/03/2022","authors":"Shunyu Zhang,Yaobo Liang,Ming Gong,Daxin Jiang,Nan Duan","id":"00b28e5f100aa35e83487b17013b8235cdf45c3c","summary":"A simple yet effective method of generating multiple embeddings through viewers to represent documents and enforce them to align with different queries is proposed, and a global-local loss with annealed temperature is proposed to encourage the multiple viewers to better aligned with different potential queries.","score":3},{"url":"https://www.semanticscholar.org/paper/a83cdcc0135c58fddf89fc72f1b92b7a9d1e170f","title":"LinkBERT: Pretraining Language Models with Document Links","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":99,"citationCount":40,"influentialCitationCount":12,"publicationDate":"29/03/2022","authors":"Michihiro Yasunaga,J. Leskovec,Percy Liang","id":"a83cdcc0135c58fddf89fc72f1b92b7a9d1e170f","summary":"This work proposes LinkBERT, an LM pretraining method that leverages links between documents that outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain ( pretrained on PubMed with citation links).","score":3},{"url":"https://www.semanticscholar.org/paper/61116b638538bdf8ecf3d8accc50af6587e8c567","title":"Knowledge Base Index Compression via Dimensionality and Precision Reduction","venue":"SPANLP","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/04/2022","authors":"Vilém Zouhar,Marius Mosbach,Miaoran Zhang,D. Klakow","id":"61116b638538bdf8ecf3d8accc50af6587e8c567","summary":"This work systematically investigates reducing the size of the KB index by means of dimensionality (sparse random projections, PCA, autoencoders) and numerical precision reduction and shows that PCA is an easy solution that requires very little data and is only slightly worse than autoen coders, which are less stable.","score":3},{"url":"https://www.semanticscholar.org/paper/0e9e00c48b171ec13063bfcc6395809be91d5e8b","title":"GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Passage Retrieval","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":58,"citationCount":3,"influentialCitationCount":1,"publicationDate":"18/04/2022","authors":"Jiduan Liu,Jiahao Liu,Yang Yang,Jingang Wang,Wei Wu,Dongyan Zhao,Rui Yan","id":"0e9e00c48b171ec13063bfcc6395809be91d5e8b","summary":"A GNN-encoder model is proposed in which query (passage) information is fused into passage ( query) representations via graph neural networks that are constructed by queries and their top retrieved passages to achieve both efficiency and efficacy in passage retrieval.","score":3},{"url":"https://www.semanticscholar.org/paper/347dd049d9f8ab6cc2f72a8c872e2a5856dd2039","title":"Answer Consolidation: Formulation and Benchmarking","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":48,"citationCount":4,"influentialCitationCount":1,"publicationDate":"29/04/2022","authors":"Wenxuan Zhou,Qiang Ning,Heba Elfardy,Kevin Small,Muhao Chen","id":"347dd049d9f8ab6cc2f72a8c872e2a5856dd2039","summary":"This paper forms the problem of answer consolidation, where answers are partitioned into multiple groups, each representing different aspects of the answer set, and finds a comprehensive and non-redundant set of answers can be constructed by picking one answer from each group.","score":3},{"url":"https://www.semanticscholar.org/paper/68db9cd6374537f8c0b1dddc6e49e51ebeeb7304","title":"Better Retrieval May Not Lead to Better Question Answering","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/05/2022","authors":"Zhengzhong Liang,Tushar Khot,Steven Bethard,M. Surdeanu,Ashish Sabharwal","id":"68db9cd6374537f8c0b1dddc6e49e51ebeeb7304","summary":"This work shows that for StrategyQA, a challenging open-domain QA dataset that requires multi-hop reasoning, this common approach to improve the quality of the retrieved context from the IR stage is surprisingly ineffective.","score":3},{"url":"https://www.semanticscholar.org/paper/eea2129457fcd78c4071a9020355a2fe1da4d2fd","title":"SKILL: Structured Knowledge Infusion for Large Language Models","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":40,"citationCount":3,"influentialCitationCount":0,"publicationDate":"17/05/2022","authors":"Fedor Moiseev,Zhe Dong,Enrique Alfonseca,Martin Jaggi","id":"eea2129457fcd78c4071a9020355a2fe1da4d2fd","summary":"This work proposes a method to infuse structured knowledge into large language models, by directly training T5 models on factual triples of knowledge graphs (KGs), and shows that models pre-trained on Wikidata KG with this method outperform the T5 baselines on FreebaseQA and WikiHop, as well as theWikidata-answerable subset of TriviaQ a and NaturalQuestions.","score":3},{"url":"https://www.semanticscholar.org/paper/4010435fee6379f9edb53a8148ad1f6094ab2936","title":"ERNIE-Search: Bridging Cross-Encoder with Dual-Encoder via Self On-the-fly Distillation for Dense Passage Retrieval","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":18,"influentialCitationCount":3,"publicationDate":"18/05/2022","authors":"Yuxiang Lu,Yiding Liu,Jiaxiang Liu,Yunsheng Shi,Zhengjie Huang,Shi Feng,Yu Sun,Hao Tian,Hua Wu,Shuaiqiang Wang,Dawei Yin,Haifeng Wang","id":"4010435fee6379f9edb53a8148ad1f6094ab2936","summary":"This paper introduces a self on-the-ﬂy distillation method that can effectively distill late interaction and in-corporates a cascade distillation process to further improve the performance with a cross-encoder teacher.","score":3},{"url":"https://www.semanticscholar.org/paper/37ba5946527629cf538a4675476f824da99dfe35","title":"Table Retrieval May Not Necessitate Table-specific Model Design","venue":"SUKI","year":2022,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/05/2022","authors":"Zhiruo Wang,Zhengbao Jiang,Eric Nyberg,Graham Neubig","id":"37ba5946527629cf538a4675476f824da99dfe35","summary":"The task of table retrieval is focused on, and it is found that DPR performs well without any table-specific design and training, and even achieves superior results compared to DTR when fine-tuned on properly linearized tables.","score":3},{"url":"https://www.semanticscholar.org/paper/34ff09ddf6ae64299564cb673044055b4615fc3c","title":"Domain Adaptation for Memory-Efficient Dense Retrieval","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":7,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Nandan Thakur,Nils Reimers,Jimmy Lin","id":"34ff09ddf6ae64299564cb673044055b4615fc3c","summary":"It is shown that binary embedding models like BPR and JPQ can perform signif-icantly worse than baselines once there is a domain-shift involved, and a modi-cation to the training procedure is proposed and combined with a corpus speciﬁc generative procedure which allow the adaptation of BPRand JPQ to any corpus without requiring labeled training data.","score":3},{"url":"https://www.semanticscholar.org/paper/0a361ee0eb2fcefb048d2b03bfa1cf515bf24113","title":"Optimizing Test-Time Query Representations for Dense Retrieval","venue":"","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"Mujeen Sung,Jungsoo Park,Jaewoo Kang,Danqi Chen,Jinhyuk Lee","id":"0a361ee0eb2fcefb048d2b03bfa1cf515bf24113","summary":"T OU R is introduced, which further optimizes instance-level query representations guided by signals from test-time retrieval results and improves the end-to-end open-domain QA accuracy signiﬁcantly, as well as passage retrieval performance.","score":3},{"url":"https://www.semanticscholar.org/paper/3eada4005e2ee43205b8d39a80b8a2883f254a21","title":"A Neural Corpus Indexer for Document Retrieval","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":6,"influentialCitationCount":2,"publicationDate":"06/06/2022","authors":"Yujing Wang,Ying Hou,Hong Wang,Ziming Miao,Shibin Wu,Hao Sun,Qi Chen,Yuqing Xia,Chengmin Chi,Guoshuai Zhao,Zheng Liu,Xing Xie,Hao Sun,Weiwei Deng,Qi Zhang,Mao Yang","id":"3eada4005e2ee43205b8d39a80b8a2883f254a21","summary":"This paper proposes Neural Corpus Indexer (NCI), a sequence-to-sequence network that generates relevant document identifiers directly for a designated query and leverages tailored techniques including query generation, semantic document identifiers, and consistency-based regularization.","score":3},{"url":"https://www.semanticscholar.org/paper/b5c80cd17a3be2c2f9cb02974e140ac361f417ff","title":"QA Is the New KR: Question-Answer Pairs as Knowledge Bases","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/07/2022","authors":"Wenhu Chen,William W. Cohen,Michiel de Jong,Nitish Gupta,Alessandro Presta,Pat Verga,J. Wieting","id":"b5c80cd17a3be2c2f9cb02974e140ac361f417ff","summary":"It is argued that the proposed type of KB has many of the key advantages of a traditional symbolic KB: in particular, it consists of small modular components, which can be combined compositionally to answer complex queries, including relational queries and queries involving “multi-hop” inferences.","score":3},{"url":"https://www.semanticscholar.org/paper/c7ef24986470d06824d097d688fb0d942464d850","title":"MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages","venue":"MIA","year":2022,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"02/07/2022","authors":"Akari Asai,S. Longpre,Jungo Kasai,Chia-Hsuan Lee,Rui Zhang,Junjie Hu,Ikuya Yamada,J. Clark,Eunsol Choi","id":"c7ef24986470d06824d097d688fb0d942464d850","summary":"The results of the Workshop on Multilingual Information Access 2022 Shared Task, evaluating cross-lingual open-retrieval question answering (QA) systems in 16 typologically diverse languages are presented, with the best system obtains particularly significant improvements in Tamil.","score":3},{"url":"https://www.semanticscholar.org/paper/4ed96488b4cea6c40401d3ccbd4a9bc5d98237c4","title":"Cross-Lingual QA as a Stepping Stone for Monolingual Open QA in Icelandic","venue":"MIA","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/07/2022","authors":"Vésteinn Snæbjarnarson,H. Einarsson","id":"4ed96488b4cea6c40401d3ccbd4a9bc5d98237c4","summary":"A bilingual Icelandic/English language model is trained to embed English context and Icelandic questions following methodology introduced with DensePhrases, and the resulting system is an open domain cross-lingual QA system between Icelandic and English.","score":3},{"url":"https://www.semanticscholar.org/paper/c39b47fe533744437ee24116de5f5fe5c31c1b59","title":"MIA 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering","venue":"MIA","year":2022,"referenceCount":18,"citationCount":1,"influentialCitationCount":1,"publicationDate":"05/07/2022","authors":"Zhucheng Tu,Sarguna Padmanabhan","id":"c39b47fe533744437ee24116de5f5fe5c31c1b59","summary":"This work describes its two-stage system for the Multilingual Information Access (MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering and shows the efficacy of using entity representations, sparse retrieval signals to help dense retrieval, and Fusion-in-Decoder.","score":3},{"url":"https://www.semanticscholar.org/paper/5e7efbfb74e953b1e97f6f854e48475384ec87e6","title":"SpaceQA: Answering Questions about the Design of Space Missions and Space Craft Concepts","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2022,"referenceCount":31,"citationCount":3,"influentialCitationCount":0,"publicationDate":"06/07/2022","authors":"Andres Garcia-Silva,Cristian Berrio,José Manuél Gómez-Pérez,J. Martínez-Heras,A. Donati,Ilaria Roma","id":"5e7efbfb74e953b1e97f6f854e48475384ec87e6","summary":"This work presents SpaceQA, to the best of its knowledge the first open-domain QA system in Space mission design, and adopts a state-of-the-art architecture consisting of a dense retriever and a neural reader and opt for an approach based on transfer learning rather than fine-tuning due to the lack of domain-specific annotated data.","score":3},{"url":"https://www.semanticscholar.org/paper/75e59a0b5173c9e7bd6fb89e457884984dbc9ab1","title":"Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":3,"influentialCitationCount":0,"publicationDate":"07/07/2022","authors":"Sebastian Hofstätter,Jiecao Chen,K. Raman,Hamed Zamani","id":"75e59a0b5173c9e7bd6fb89e457884984dbc9ab1","summary":"A simple yet effective approach to clean the training set by utilizing a distinct property of knowledge-intensive generation: The connection of query-answer pairs to items in the knowledge base, which scales well with increased model capacity and achieves state-of-the-art results in seven KILT tasks.","score":3}]}