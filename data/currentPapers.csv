"id","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"f8baff7610d0483caec5dc6faf5bf486298eeb90","Large Language Models Are Reasoning Teachers","Fine-tune-CoT is proposed, a method that leverages the capabilities of very large LMs to generate reasoning samples and teach smaller models via ﬁne-tuning that enables substantial reasoning capability in small models, whereas previous prompt-based baselines exhibit near-random performance.","ArXiv",2022,"Namgyu Ho,Laura Schmid,Se-Young Yun",46,74,1,"https://www.semanticscholar.org/paper/f8baff7610d0483caec5dc6faf5bf486298eeb90"
"598d9b235f5ab148fc757240d9bc39a47b8eaf72","Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks","Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12% across all the evaluated datasets, and by combining PoT with self-consistency decoding, can achieve SoT performance on all math problem datasets and near-SoTA performance on ﬁnancial datasets.","ArXiv",2022,"Wenhu Chen,Xueguang Ma,Xinyi Wang,William W. Cohen",103,40,5,"https://www.semanticscholar.org/paper/598d9b235f5ab148fc757240d9bc39a47b8eaf72"
"6756fcd998caeb7b23702e08559e63710179334c","Reasoning with Language Model Prompting: A Survey","This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting and introduces research works with comparisons and summaries and provides systematic resources to help beginners.","ArXiv",2022,"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen",51,212,0,"https://www.semanticscholar.org/paper/6756fcd998caeb7b23702e08559e63710179334c"
"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","Multimodal Chain-of-Thought Reasoning in Language Models","This work proposes Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference so that answer inference can leverage better generated rationales that are based on multimodal information.","ArXiv",2023,"Zhuosheng Zhang,Aston Zhang,Mu Li,Hai Zhao,G. Karypis,Alexander J. Smola",67,41,0,"https://www.semanticscholar.org/paper/780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050"
"6b12de63704677cd205a4bec282c955b270a9066","Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning","This paper considers an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals.","ArXiv",2023,"Thomas Carta,Clément Romac,Thomas Wolf,S. Lamprier,Olivier Sigaud,P. Oudeyer",24,64,0,"https://www.semanticscholar.org/paper/6b12de63704677cd205a4bec282c955b270a9066"
"609fd0ffd05e6730287fcc603267c4cabcbe3e5a","Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback","This work collects deployment data of human interactions, collects various types of human feedback, and studies various algorithms for improving from such feedback in order to make recommendations on which type of feedback and algorithms work best.","ArXiv",2022,"Jing Xu,Megan Ung,M. Komeili,Kushal Arora,Y-Lan Boureau,J. Weston",18,37,0,"https://www.semanticscholar.org/paper/609fd0ffd05e6730287fcc603267c4cabcbe3e5a"
"102e4c860e39a2bfd7bf3f03b9ad69aac7bf3b5f","Collaborating with language models for embodied reasoning","This work investigates how to combine complementary abilities in a single system consisting of a Planner, an Actor, and a Reporter, and presents a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrates how components of this system can be trained with reinforcement-learning to improve performance.","ArXiv",2023,"I. Dasgupta,Christine Kaeser-Chen,Kenneth Marino,Arun Ahuja,Sheila Babayan,Felix Hill,R. Fergus",17,11,0,"https://www.semanticscholar.org/paper/102e4c860e39a2bfd7bf3f03b9ad69aac7bf3b5f"
"f9987cfce640d24037d2b27e03c4f1c9b5920c77","ALERT: Adapting Language Models to Reasoning Tasks","AlERT is introduced, a benchmark and suite of analyses for assessing language models’ reasoning ability comparing pre-trained and ﬁnetuned models on complex tasks that require reasoning skills to solve, and it is found that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during pretraining stage compared to pretraining state.","ArXiv",2022,"Ping Yu,Tianlu Wang,O. Yu. Golovneva,Badr AlKhamissi,Gargi Ghosh,Mona Diab,Asli Celikyilmaz",9,84,0,"https://www.semanticscholar.org/paper/f9987cfce640d24037d2b27e03c4f1c9b5920c77"
"53d128ea815bcc0526856eb5a9c42cc977cb36a7","Toolformer: Language Models Can Teach Themselves to Use Tools","This paper introduces Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction, which achieves substantially improved zero-shot performance across a variety of downstream tasks.","ArXiv",2023,"Timo Schick,Jane Dwivedi-Yu,Roberto Dessì,Roberta Raileanu,M. Lomeli,Luke Zettlemoyer,Nicola Cancedda,Thomas Scialom",234,62,1,"https://www.semanticscholar.org/paper/53d128ea815bcc0526856eb5a9c42cc977cb36a7"
"ada81a4de88a6ce474df2e2446ad11fea480616e","Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language","This work shows that model diversity is symbiotic, and can be leveraged to build AI systems with structured Socratic dialogue – in which new multimodal tasks are formulated as a guided language- based exchange between different pre-existing foundation models, without additional language-based exchange.","ArXiv",2022,"Andy Zeng,Adrian S. Wong,Stefan Welker,K. Choromanski,F. Tombari,Aveek Purohit,M. Ryoo,V. Sindhwani,Johnny Lee,Vincent Vanhoucke,Peter R. Florence",190,165,5,"https://www.semanticscholar.org/paper/ada81a4de88a6ce474df2e2446ad11fea480616e"
"c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58","A Maximum Likelihood Approach to Continuous Speech Recognition","This paper describes a number of statistical models for use in speech recognition, with special attention to determining the parameters for such models from sparse data, and describes two decoding methods appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks.","IEEE Transactions on Pattern Analysis and Machine Intelligence",1983,"L. Bahl,F. Jelinek,R. Mercer",1443,27,52,"https://www.semanticscholar.org/paper/c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58"
"3ee9c65366efbb17adf370c39f20dbef60d53670","Towards Reasoning in Large Language Models: A Survey","A comprehensive overview of the current state of knowledge on reasoning in large language models, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, and suggestions on future directions are provided.","ArXiv",2022,"Jie Huang,K. Chang",80,130,0,"https://www.semanticscholar.org/paper/3ee9c65366efbb17adf370c39f20dbef60d53670"
"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","Finetuned Language Models Are Zero-Shot Learners","It is shown that instruction tuning —ﬁnetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks and outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze.","International Conference on Learning Representations",2021,"Jason Wei,Maarten Bosma,Vincent Zhao,Kelvin Guu,A. Yu,Brian Lester,Nan Du,Andrew M. Dai,Quoc V. Le",980,168,82,"https://www.semanticscholar.org/paper/ff0b2681d7b05e16c46dfb71d980cc2f605907cd"
"deafbf7a1c8f588fc1ba19125faaf0c8c8743649","Languages are Rewards: Chain of Hindsight Finetuning using Human Feedback","This work proposes a novel technique called Hindsight Finetuning, which condition the model on a sequence of model generations paired with hindsight feedback, and finetune the model to predict the most preferred output so that models can learn to identify and correct negative attributes or errors.","ArXiv",2023,"Hao Liu,Carmelo Sferrazza,P. Abbeel",0,62,0,"https://www.semanticscholar.org/paper/deafbf7a1c8f588fc1ba19125faaf0c8c8743649"
"882ebe805c4d87929dbe4d930a1d52e0882d1d3e","Continual-T0: Progressively Instructing 50+ Tasks to Language Models Without Forgetting","The resulting model Continual-T0 (CT0) is able to learn diverse new tasks, while still maintaining good performance on previous tasks, spanning remarkably through 70 datasets in total.","ArXiv",2022,"Thomas Scialom,Tuhin Chakrabarty,S. Muresan",13,51,0,"https://www.semanticscholar.org/paper/882ebe805c4d87929dbe4d930a1d52e0882d1d3e"
"290732e9fb08a29af8892a7c1f73c9d2a1b9d7db","Language models show human-like content effects on reasoning","This work hypothesized that language models would show human-like content content on abstract reasoning problems, and explored this hypothesis across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task.","ArXiv",2022,"I. Dasgupta,Andrew Kyle Lampinen,Stephanie C. Y. Chan,Antonia Creswell,D. Kumaran,James L. McClelland,Felix Hill",59,113,0,"https://www.semanticscholar.org/paper/290732e9fb08a29af8892a7c1f73c9d2a1b9d7db"
"fa422a513a7f7c4011b8ad38c24246f6595d82fc","The alignment problem from a deep learning perspective","","ArXiv",2022,"Richard Ngo",36,170,0,"https://www.semanticscholar.org/paper/fa422a513a7f7c4011b8ad38c24246f6595d82fc"
"142ebbf4760145f591166bde2564ac70c001e927","Language Models (Mostly) Know What They Know","","ArXiv",2022,"Saurav Kadavath,Tom Conerly,Amanda Askell,T. Henighan,Dawn Drain,Ethan Perez,Nicholas Schiefer,Z. Dodds,Nova DasSarma,Eli Tran-Johnson,Scott Johnston,S. El-Showk,Andy Jones,Nelson Elhage,Tristan Hume,Anna Chen,Yuntao Bai,Sam Bowman,Stanislav Fort,Deep Ganguli,Danny Hernandez,Josh Jacobson,John Kernion,S. Kravec,Liane Lovitt,Kamal Ndousse,Catherine Olsson,Sam Ringer,Dario Amodei,Tom B. Brown,Jack Clark,Nicholas Joseph,Benjamin Mann,Sam McCandlish,C. Olah,Jared Kaplan",122,39,1,"https://www.semanticscholar.org/paper/142ebbf4760145f591166bde2564ac70c001e927"
"06b5090c00326183f7b3fe6e891586449e14650e","Ethics of artificial intelligence and robotics","","",2020,"V. C. Müller",149,0,12,"https://www.semanticscholar.org/paper/06b5090c00326183f7b3fe6e891586449e14650e"
"155108784e14747496af8ff5a1e2d40d72d2b7fb","Artificial intelligence. Fears of an AI pioneer.","One researcher now speaking up is Stuart Russell, a computer scientist at the University of California, Berkeley, who with Peter Norvig, director of research at Google, wrote the premier AI textbook, Artificial Intelligence: A Modern Approach, now in its third edition.","Science",2015,"Stuart J. Russell,J. Bohannon",66,0,2,"https://www.semanticscholar.org/paper/155108784e14747496af8ff5a1e2d40d72d2b7fb"
"8342b592fe238f3d230e4959b06fd10153c45db1","Training Compute-Optimal Large Language Models","This paper trains a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4 × more more data, and reaches a state-of-the-art average accuracy on the MMLU benchmark.","ArXiv",2022,"Jordan Hoffmann,Sebastian Borgeaud,A. Mensch,Elena Buchatskaya,Trevor Cai,Eliza Rutherford,Diego de Las Casas,Lisa Anne Hendricks,Johannes Welbl,Aidan Clark,Tom Hennigan,Eric Noland,Katie Millican,George van den Driessche,Bogdan Damoc,Aurelia Guy,Simon Osindero,K. Simonyan,Erich Elsen,Jack W. Rae,Oriol Vinyals,L. Sifre",624,78,35,"https://www.semanticscholar.org/paper/8342b592fe238f3d230e4959b06fd10153c45db1"
"dac3a172b504f4e33c029655e9befb3386e5f63a","Emergent Abilities of Large Language Models","","ArXiv",2022,"Jason Wei,Yi Tay,Rishi Bommasani,Colin Raffel,Barret Zoph,Sebastian Borgeaud,Dani Yogatama,Maarten Bosma,Denny Zhou,Donald Metzler,E. Chi,Tatsunori Hashimoto,Oriol Vinyals,P. Liang,J. Dean,W. Fedus",659,107,18,"https://www.semanticscholar.org/paper/dac3a172b504f4e33c029655e9befb3386e5f63a"
"8b293973061026d9d0eed90e71e30928e029171e","Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models","It is shown that larger models can memorize a larger portion of the data before over-ﬁtting and tend to forget less throughout the training process, and that larger language models memorize training data faster across all settings.","ArXiv",2022,"K. Tirumala,Aram H. Markosyan,Luke Zettlemoyer,Armen Aghajanyan",51,98,0,"https://www.semanticscholar.org/paper/8b293973061026d9d0eed90e71e30928e029171e"
"68f141724814839d556a989646194be88641b143","Scaling Language Models: Methods, Analysis & Insights from Training Gopher","This paper presents an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher.","ArXiv",2021,"Jack W. Rae,Sebastian Borgeaud,Trevor Cai,Katie Millican,Jordan Hoffmann,Francis Song,J. Aslanides,Sarah Henderson,Roman Ring,Susannah Young,Eliza Rutherford,Tom Hennigan,Jacob Menick,Albin Cassirer,Richard Powell,George van den Driessche,Lisa Anne Hendricks,M. Rauh,Po-Sen Huang,A. Glaese,Johannes Welbl,Sumanth Dathathri,Saffron Huang,J. Uesato,John F. J. Mellor,I. Higgins,Antonia Creswell,Nathan McAleese,Amy Wu,Erich Elsen,Siddhant M. Jayakumar,Elena Buchatskaya,D. Budden,Esme Sutherland,K. Simonyan,Michela Paganini,L. Sifre,Lena Martens,Xiang Lorraine Li,A. Kuncoro,Aida Nematzadeh,E. Gribovskaya,Domenic Donato,Angeliki Lazaridou,A. Mensch,J. Lespiau,Maria Tsimpoukelli,N. Grigorev,Doug Fritz,Thibault Sottiaux,Mantas Pajarskas,Tobias Pohlen,Z. Gong,Daniel Toyama,Cyprien de Masson d'Autume,Yujia Li,Tayfun Terzi,Vladimir Mikulik,I. Babuschkin,Aidan Clark,Diego de Las Casas,Aurelia Guy,Chris Jones,James Bradbury,Matthew G. Johnson,Blake A. Hechtman,Laura Weidinger,Iason Gabriel,William S. Isaac,Edward Lockhart,Simon Osindero,Laura Rimell,Chris Dyer,Oriol Vinyals,Kareem W. Ayoub,Jeff Stanway,L. Bennett,D. Hassabis,K. Kavukcuoglu,Geoffrey Irving",615,0,32,"https://www.semanticscholar.org/paper/68f141724814839d556a989646194be88641b143"
"53a77e8f73f2ca422d6e38fa9ecc490231ac044c","Neural Text Generation with Unlikelihood Training","It is shown that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution, thus providing a strong alternative to existing techniques.","International Conference on Learning Representations",2019,"S. Welleck,Ilia Kulikov,Stephen Roller,Emily Dinan,Kyunghyun Cho,J. Weston",360,32,50,"https://www.semanticscholar.org/paper/53a77e8f73f2ca422d6e38fa9ecc490231ac044c"
"856d2c0f7b3f80dcf1f68d1dc0dcbf5c6fe5679a","Interpreting docstrings without using common sense: the private science of very large language models∗","GPT-3, the natural language model on which Codex is built, and that services such as Copilot ultimately depend on, suffers from scientific deficiencies, and critical remarks on Copilot’s structure and underlying language model are presented.","",2022,"Darren Abramson,Ali Emami",0,39,0,"https://www.semanticscholar.org/paper/856d2c0f7b3f80dcf1f68d1dc0dcbf5c6fe5679a"
"b123a0d46ad917b79c43c5ae981e03ed2458ed11","Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems","Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.","Annual Meeting of the Association for Computational Linguistics",2017,"Wang Ling,Dani Yogatama,Chris Dyer,P. Blunsom",274,22,33,"https://www.semanticscholar.org/paper/b123a0d46ad917b79c43c5ae981e03ed2458ed11"
"2a7ae3e98357569c41424dacd60c62d3df78a0db","Limitations of Language Models in Arithmetic and Symbolic Induction","LMs with tutor is able to deliver 100% accuracy in situations of OOD and repeating symbols, shedding new insights on the boundary of large LMs in induction.","ArXiv",2022,"Jingu Qian,Hong Wang,Zekun Li,SHIYANG LI,Xifeng Yan",21,22,1,"https://www.semanticscholar.org/paper/2a7ae3e98357569c41424dacd60c62d3df78a0db"
"2cc3ab9fa41ba2804e301f7eae9598636e62422a","Investigating the Limitations of the Transformers with Simple Arithmetic Tasks","It is found that how a number is represented in its surface form has a strong influence on the model’s accuracy, and this result bolsters evidence that subword tokenizers and positional encodings are components in current transformer designs that might need improvement.","ArXiv",2021,"Rodrigo Nogueira,Zhiying Jiang,Jimmy J. Li",48,45,8,"https://www.semanticscholar.org/paper/2cc3ab9fa41ba2804e301f7eae9598636e62422a"
"e6770e3f5e74210c6863aaeed527ac4c1da419d7","A Survey on Retrieval-Augmented Text Generation","The generic paradigm of retrieval-augmented generation is highlighted, notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks are reviewed, and some important directions on top of recent methods are pointed out to facilitate future research.","ArXiv",2022,"Huayang Li,Yixuan Su,Deng Cai,Yan Wang,Lemao Liu",31,86,3,"https://www.semanticscholar.org/paper/e6770e3f5e74210c6863aaeed527ac4c1da419d7"
"78474604a192bb82c77288f46b521b0cf9ee14f5","Improving Multiple Documents Grounded Goal-Oriented Dialog Systems via Diverse Knowledge Enhanced Pretrained Language Model","This paper adopts model pretraining, fine-tuning, and multi-task learning to enhance the model’s coverage of pretrained knowledge for MultiDoc2Dial task, which aims to model the goal-oriented dialogues grounded in multiple documents.","Workshop on Document-grounded Dialogue and Conversational Question Answering",2022,"Yunah Jang,Dongryeol Lee,Hyung-joo Park,Taegwan Kang,Hwanhee Lee,Hyunkyung Bae,Kyomin Jung",1,19,0,"https://www.semanticscholar.org/paper/78474604a192bb82c77288f46b521b0cf9ee14f5"
"f8292d4ddf7a6dfe240eeaa9685f5d18eed9a3f6","Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion","It is shown that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness.","Conference on Empirical Methods in Natural Language Processing",2022,"Kurt Shuster,M. Komeili,Leonard Adolphs,Stephen Roller,Arthur D. Szlam,J. Weston",62,54,4,"https://www.semanticscholar.org/paper/f8292d4ddf7a6dfe240eeaa9685f5d18eed9a3f6"
"398e4061dde8f5c80606869cebfa2031de7b5b74","Few-shot Learning with Retrieval Augmented Language Models","Atlas is presented, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples, and the impact of the content of the document index is studied, showing that it can easily be updated.","ArXiv",2022,"Gautier Izacard,Patrick Lewis,M. Lomeli,Lucas Hosseini,Fabio Petroni,Timo Schick,Jane A. Yu,Armand Joulin,Sebastian Riedel,Edouard Grave",157,94,15,"https://www.semanticscholar.org/paper/398e4061dde8f5c80606869cebfa2031de7b5b74"
"1974b52e96bda50e4ff200be06d1b089c452550c","Some philosophical considerations over Yann LeCun ’ s position paper “ A Path Towards Autonomous Machine Intelligence ”","The overall effect of the paper’s thesis is to bring Machine Learning firmly back into the Artificial Intelligence field, correcting a drift which too many have embraced in sake of quick practical results.","",2022,"Giovanni Landi",52,0,3,"https://www.semanticscholar.org/paper/1974b52e96bda50e4ff200be06d1b089c452550c"
"60b05f32c32519a809f21642ef1eb3eaf3848008","ROUGE: A Package for Automatic Evaluation of Summaries","Four different RouGE measures are introduced: ROUGE-N, ROUge-L, R OUGE-W, and ROUAGE-S included in the Rouge summarization evaluation package and their evaluations.","Annual Meeting of the Association for Computational Linguistics",2004,"Chin-Yew Lin",9684,14,1945,"https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008"
"17bcb1edbe068e8fe6a97da552c70a77a15bbce7","Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned","It is found that the RLHF models are increasinglycult to red team as they scale, and a trend with scale for the other model types is found, which indicates that this transparency accelerates the ability to work together as a community in order to develop shared norms, practices, and technical standards.","ArXiv",2022,"Deep Ganguli,Liane Lovitt,John Kernion,Amanda Askell,Yuntao Bai,Saurav Kadavath,Benjamin Mann,Ethan Perez,Nicholas Schiefer,Kamal Ndousse,Andy Jones,Sam Bowman,Anna Chen,Tom Conerly,Nova DasSarma,Dawn Drain,Nelson Elhage,S. El-Showk,Stanislav Fort,Z. Dodds,T. Henighan,Danny Hernandez,Tristan Hume,Josh Jacobson,Scott Johnston,S. Kravec,Catherine Olsson,Sam Ringer,Eli Tran-Johnson,Dario Amodei,Tom B. Brown,Nicholas Joseph,Sam McCandlish,C. Olah,Jared Kaplan,Jack Clark",99,61,1,"https://www.semanticscholar.org/paper/17bcb1edbe068e8fe6a97da552c70a77a15bbce7"
"80e49004340d81b5fe250951b081a45bf7abc890","Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning","This work develops a real-time, open-ended dialogue system that uses reinforcement learning (RL) to power a bot’s conversational skill at scale and is able to substantially exceeds the (strong) baseline supervised model with respect to several metrics of interest in a live experiment with real users of the Google Assistant.","ArXiv",2022,"Deborah Cohen,M. Ryu,Yinlam Chow,Orgad Keller,Ido Greenberg,Avinatan Hassidim,Michael Fink,Yossi Matias,Idan Szpektor,Craig Boutilier,G. Elidan",5,48,0,"https://www.semanticscholar.org/paper/80e49004340d81b5fe250951b081a45bf7abc890"
"8249925cd04f9320d32b2f975dfde5e4d4b7b4bd","Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization","It is shown that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al., 2017)), based on both automatic and human evaluation.","ArXiv",2022,"Rajkumar Ramamurthy,Prithviraj Ammanabrolu,Kianté Brantley,Jack Hessel,R. Sifa,C. Bauckhage,Hannaneh Hajishirzi,Yejin Choi",58,90,2,"https://www.semanticscholar.org/paper/8249925cd04f9320d32b2f975dfde5e4d4b7b4bd"
"e00867c7108395d56b823bc5c75d2f4591e2878d","Open-vocabulary Queryable Scene Representations for Real World Planning","NLMap is developed, an open-vocabulary and queryable scene representation that allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods.","ArXiv",2022,"Boyuan Chen,F. Xia,Brian Ichter,Kanishka Rao,K. Gopalakrishnan,M. Ryoo,Austin Stone,Daniel Kappler",37,44,0,"https://www.semanticscholar.org/paper/e00867c7108395d56b823bc5c75d2f4591e2878d"
"4be0bf45a738067a1cd8b4eaf4a266eb1215404d","Offline RL for Natural Language Generation with Implicit Language Q Learning","This work proposes a novel ofﬂine RL motivated method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the ﬂexible utility optimization framework of traditional RL algorithms with supervised learning’s ability to leverage existing data and its simplicity and stability.","ArXiv",2022,"Charles Burton Snell,Ilya Kostrikov,Yi Su,Mengjiao Yang,S. Levine",27,74,0,"https://www.semanticscholar.org/paper/4be0bf45a738067a1cd8b4eaf4a266eb1215404d"
"429cb1fb913a37eeb913835909897ef8a3812b9e","NLP with BERT: Sentiment Analysis Using SAS® Deep Learning and DLPy","An overview of BERT is provided and how to create your own BERT model is shown by using SAS Deep Learning and the SAS DLPy Python package and is illustrated by performing sentiment analysis on unstructured product reviews submitted to Amazon.","",2020,"D. Cairns,Xian-yan Meng",0,11,0,"https://www.semanticscholar.org/paper/429cb1fb913a37eeb913835909897ef8a3812b9e"
"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","Evaluating Large Language Models Trained on Code","It is found that repeated sampling from the GPT language model is a surprisingly effective strategy for producing working solutions to difficult prompts, and the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics are discussed.","ArXiv",2021,"Mark Chen,Jerry Tworek,Heewoo Jun,Qiming Yuan,Henrique Ponde,Jared Kaplan,Harrison Edwards,Yura Burda,Nicholas Joseph,Greg Brockman,Alex Ray,Raul Puri,Gretchen Krueger,Michael Petrov,Heidy Khlaaf,Girish Sastry,Pamela Mishkin,Brooke Chan,Scott Gray,Nick Ryder,Mikhail Pavlov,Alethea Power,Lukasz Kaiser,Mohammad Bavarian,Clemens Winter,Philippe Tillet,F. Such,D. Cummings,Matthias Plappert,Fotios Chantzis,Elizabeth Barnes,Ariel Herbert-Voss,William H. Guss,Alex Nichol,I. Babuschkin,S. Balaji,Shantanu Jain,A. Carr,J. Leike,Joshua Achiam,Vedant Misra,Evan Morikawa,Alec Radford,M. Knight,Miles Brundage,Mira Murati,Katie Mayer,P. Welinder,Bob McGrew,Dario Amodei,Sam McCandlish,Ilya Sutskever,Wojciech Zaremba",1369,131,159,"https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269"
"51c2a0835fc565ad1fc7a58559ede9cbe8f6551e","Least-to-Most Prompting Enables Complex Reasoning in Large Language Models","Experiments on symbolic manipulation, compositional generalization and numerical reasoning demonstrate that least-to-most prompting can generalize to examples that are harder than those seen in the prompt context, outperforming other prompting-based approaches by a large margin.","ArXiv",2022,"Denny Zhou,Nathanael Scharli,Le Hou,Jason Wei,Nathan Scales,Xuezhi Wang,D. Schuurmans,O. Bousquet,Quoc Le,E. Chi",285,74,11,"https://www.semanticscholar.org/paper/51c2a0835fc565ad1fc7a58559ede9cbe8f6551e"
"92173d081b15824d22a9ef070e118744ceee8052","Show Your Work: Scratchpads for Intermediate Computation with Language Models","Surprisingly, large pre-trained language models are able to perform complex multistep computations—even in the few-shot regime—when asked to perform the operation “step by step”, showing the results of intermediate computations.","ArXiv",2021,"Maxwell Nye,Anders Andreassen,Guy Gur-Ari,H. Michalewski,Jacob Austin,David Bieber,David Dohan,Aitor Lewkowycz,Maarten Bosma,D. Luan,Charles Sutton,Augustus Odena",263,30,12,"https://www.semanticscholar.org/paper/92173d081b15824d22a9ef070e118744ceee8052"
"2d2ca2e54c54748557b8aac7d328ce32ebfe8944","ReAct: Synergizing Reasoning and Acting in Language Models","ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces.","ArXiv",2022,"Shunyu Yao,Jeffrey Zhao,Dian Yu,Nan Du,I. Shafran,Karthik Narasimhan,Yuan Cao",201,66,3,"https://www.semanticscholar.org/paper/2d2ca2e54c54748557b8aac7d328ce32ebfe8944"
"002c256d30d6be4b23d365a8de8ae0e67e4c9641","Improving language models by retrieving from trillions of tokens","With a 2 trillion token database, the Retrieval-Enhanced Transformer (R ETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 × fewer parameters.","International Conference on Machine Learning",2021,"Sebastian Borgeaud,A. Mensch,Jordan Hoffmann,Trevor Cai,Eliza Rutherford,Katie Millican,George van den Driessche,J. Lespiau,Bogdan Damoc,Aidan Clark,Diego de Las Casas,Aurelia Guy,Jacob Menick,Roman Ring,T. Hennigan,Saffron Huang,Lorenzo Maggiore,Chris Jones,Albin Cassirer,Andy Brock,Michela Paganini,Geoffrey Irving,Oriol Vinyals,Simon Osindero,K. Simonyan,Jack W. Rae,Erich Elsen,L. Sifre",334,64,28,"https://www.semanticscholar.org/paper/002c256d30d6be4b23d365a8de8ae0e67e4c9641"
"e94241c59b9caa3efe6950731a164fa787b4d737","Adversarial Environment Generation for Learning to Navigate the Web","This work proposes using Adversarial Environment Generation (AEG) to generate challenging web environments in which to train reinforcement learning (RL) agents, and provides a new benchmarking environment, gMiniWoB, which enables an RL adversary to use compositional primitives to learn to generate arbitrarily complex websites.","ArXiv",2021,"Izzeddin Gur,Natasha Jaques,Kevin Malta,Manoj Tiwari,Honglak Lee,Aleksandra Faust",14,18,0,"https://www.semanticscholar.org/paper/e94241c59b9caa3efe6950731a164fa787b4d737"
"74eae12620bd1c1393e268bddcb6f129a5025166","Improving alignment of dialogue agents via targeted human judgements","This research presents a state-of-the-art knowledge graph depicting the architecture of the connective tissue of the autonomic nervous system and some of the mechanisms responsible for seizure and depression are described.","ArXiv",2022,"A. Glaese,Nathan McAleese,Maja Trkebacz,J. Aslanides,Vlad Firoiu,Timo Ewalds,M. Rauh,Laura Weidinger,Martin Chadwick,Phoebe Thacker,Lucy Campbell-Gillingham,J. Uesato,Po-Sen Huang,R. Comanescu,Fan Yang,A. See,Sumanth Dathathri,Rory Greig,Charlie Chen,Doug Fritz,Jaume Sanchez Elias,Richard Green,Sovna Mokr'a,Nicholas Fernando,Boxi Wu,Rachel Foley,Susannah Young,Iason Gabriel,William S. Isaac,John F. J. Mellor,D. Hassabis,K. Kavukcuoglu,Lisa Anne Hendricks,Geoffrey Irving",149,105,4,"https://www.semanticscholar.org/paper/74eae12620bd1c1393e268bddcb6f129a5025166"
"0828722a8317a556c8753cfe1a8cf3a3eec0004f","Measuring and Narrowing the Compositionality Gap in Language Models","In the GPT-3 family of models, as model size increases, it is shown that the single-hop question answering performance improves faster than the multihop performance does, therefore the compositionality gap does not decrease.","ArXiv",2022,"Ofir Press,Muru Zhang,Sewon Min,Ludwig Schmidt,Noah A. Smith,M. Lewis",112,61,3,"https://www.semanticscholar.org/paper/0828722a8317a556c8753cfe1a8cf3a3eec0004f"
"e7ad08848d5d7c5c47673ffe0da06af443643bda","Large Language Models are Zero-Shot Reasoners","Experimental results demonstrate that the Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics, symbolic reasoning, and other logical reasoning tasks, without any hand-crafted few-shot examples.","ArXiv",2022,"Takeshi Kojima,S. Gu,Machel Reid,Yutaka Matsuo,Yusuke Iwasawa",669,60,33,"https://www.semanticscholar.org/paper/e7ad08848d5d7c5c47673ffe0da06af443643bda"
"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","PAL: Program-aided Language Models","Program-Aided Language models (P A L): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but ofﬂoads the solution step to a runtime such as a Python interpreter.","ArXiv",2022,"Luyu Gao,Aman Madaan,Shuyan Zhou,Uri Alon,Pengfei Liu,Yiming Yang,Jamie Callan,Graham Neubig",138,62,7,"https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7"
"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","Solving Quantitative Reasoning Problems with Language Models","","ArXiv",2022,"Aitor Lewkowycz,Anders Andreassen,David Dohan,Ethan Dyer,H. Michalewski,V. Ramasesh,Ambrose Slone,Cem Anil,Imanol Schlag,Theo Gutman-Solo,Yuhuai Wu,Behnam Neyshabur,Guy Gur-Ari,Vedant Misra",228,69,15,"https://www.semanticscholar.org/paper/ab0e3d3e4d42369de5933a3b4c237780b41c0d77"
"166b64f2ae8e52f5779682fab756cbd617a6e74b","A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level","This work solves university-level mathematics courses and improves upon state-of-the-art, increasing automatic accuracy on randomly sampled questions on a benchmark by order of magnitude.","Proceedings of the National Academy of Sciences of the United States of America",2021,"Iddo Drori,Sarah Zhang,Reece Shuttleworth,Leonard Tang,Albert Lu,Elizabeth Ke,Kevin Liu,Linda Chen,Sunny Tran,Newman Cheng,Roman Wang,Nikhil Singh,T. Patti,J. Lynch,A. Shporer,Nakul Verma,Eugene Wu,G. Strang",68,8,1,"https://www.semanticscholar.org/paper/166b64f2ae8e52f5779682fab756cbd617a6e74b"
"2aab6ca1a8dae3f3db6d248231ac3fa4e222b30a","Re3: Generating Longer Stories With Recursive Reprompting and Revision","The Recursive Reprompting and Revision framework (Re3) is proposed to address the problem of automatically generating longer stories of over two thousand words by prompting a general-purpose language model to construct a structured overarching plan, and generating story passages by repeatedly injecting contextual information from both the plan and current story state into a language model prompt.","Conference on Empirical Methods in Natural Language Processing",2022,"Kevin Yang,Nanyun Peng,Yuandong Tian,D. Klein",42,0,3,"https://www.semanticscholar.org/paper/2aab6ca1a8dae3f3db6d248231ac3fa4e222b30a"
"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","Unifying Language Learning Paradigms","UL2 achieves SOTA performance on 50 well-established supervised NLP tasks ranging from language generation, language understanding, text classiﬁcation, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval.","ArXiv",2022,"Yi Tay,M. Dehghani,V. Tran,Xavier García,Dara Bahri,Tal Schuster,Huaixiu Zheng,N. Houlsby,Donald Metzler",93,113,6,"https://www.semanticscholar.org/paper/f40aeae3e522ada1f6a9f326841b01ef5c8657b6"
"a6fdb277d0a4b09899f802bda3359f5c2021a156","Recursively Summarizing Books with Human Feedback","This method combines learning from human feedback with recursive task decomposition: it uses models trained on smaller parts of the task to assist humans in giving feedback on the broader task, and generates sensible summaries of entire books.","ArXiv",2021,"Jeff Wu,Long Ouyang,Daniel M. Ziegler,Nissan Stiennon,Ryan Lowe,J. Leike,P. Christiano",115,88,11,"https://www.semanticscholar.org/paper/a6fdb277d0a4b09899f802bda3359f5c2021a156"
"c84bb36ab145cf903c1ad404008e0250f688c162","Behavior Cloned Transformers are Neurosymbolic Reasoners","This experimental study indicates that injecting the actions from these symbolic modules into the action space of a behavior cloned transformer agent increases performance on four text game benchmarks that test arithmetic, navigation, sorting, and common sense reasoning by an average of 22%, allowing an agent to reach the highest possible performance on unseen games.","ArXiv",2022,"Ruoyao Wang,Peter Alexander Jansen,Marc-Alexandre Côté,Prithviraj Ammanabrolu",3,40,2,"https://www.semanticscholar.org/paper/c84bb36ab145cf903c1ad404008e0250f688c162"
"2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8","Mind's Eye: Grounded Language Model Reasoning through Simulation","Mind’s Eye is presented, a paradigm to ground language model reasoning in the physical world by using a computational physics engine to simulate the possible outcomes, and then using the simulation results as part of the input, which enables language models to perform reasoning.","ArXiv",2022,"Ruibo Liu,Jason Wei,S. Gu,Te-Yen Wu,Soroush Vosoughi,Claire Cui,Denny Zhou,Andrew M. Dai",26,82,4,"https://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8"
"3ee3f425482cf86989d809155cc8cf2bf8d8113e","Understanding HTML with Large Language Models","It is shown that LLMs pretrained on standard natural language corpora transfer re-markably well to HTML understanding tasks, and evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture is shown.","ArXiv",2022,"Izzeddin Gur,Ofir Nachum,Yingjie Miao,Mustafa Safdari,Austin Huang,Aakanksha Chowdhery,Sharan Narang,Noah Fiedel,Aleksandra Faust",15,34,0,"https://www.semanticscholar.org/paper/3ee3f425482cf86989d809155cc8cf2bf8d8113e"
"d6045d2ccc9c09ca1671348de86d07da6bc28eea","Training Verifiers to Solve Math Word Problems","It is demonstrated that verification significantly improves performance on GSM8K, and there is strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.","ArXiv",2021,"Karl Cobbe,V. Kosaraju,Mohammad Bavarian,Jacob Hilton,Reiichiro Nakano,Christopher Hesse,J. Schulman",445,31,48,"https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea"
"b3848d32f7294ec708627897833c4097eb4d8778","LaMDA: Language Models for Dialog Applications","It is demonstrated that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding.","ArXiv",2022,"Romal Thoppilan,Daniel De Freitas,Jamie Hall,Noam M. Shazeer,Apoorv Kulshreshtha,Heng-Tze Cheng,Alicia Jin,Taylor Bos,Leslie Baker,Yu Du,Yaguang Li,Hongrae Lee,Huaixiu Zheng,Amin Ghafouri,Marcelo Menegali,Yanping Huang,M. Krikun,Dmitry Lepikhin,James Qin,Dehao Chen,Yuanzhong Xu,Zhifeng Chen,Adam Roberts,Maarten Bosma,Yanqi Zhou,Chung-Ching Chang,I. Krivokon,W. Rusch,Marc Pickett,K. Meier-Hellstern,M. Morris,Tulsee Doshi,Renelito Delos Santos,Toju Duke,J. Søraker,Ben Zevenbergen,Vinodkumar Prabhakaran,Mark Díaz,B. Hutchinson,Kristen Olson,Alejandra Molina,Erin Hoffman-John,Josh Lee,Lora Aroyo,Ravindran Rajakumar,Alena Butryna,Matthew Lamm,V. Kuzmina,Joseph Fenton,Aaron Cohen,R. Bernstein,R. Kurzweil,Blaise Aguera-Arcas,Claire Cui,Marian Croak,E. Chi,Quoc Le",697,119,26,"https://www.semanticscholar.org/paper/b3848d32f7294ec708627897833c4097eb4d8778"
"fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b","Scaling Laws for Reward Model Overoptimization","This work studies how the gold reward model score changes as it optimize against the proxy reward model using either reinforcement learning or best-of- n sampling, and finds that this relationship follows a different functional form depending on the method of optimization.","ArXiv",2022,"Leo Gao,J. Schulman,Jacob Hilton",48,60,1,"https://www.semanticscholar.org/paper/fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b"
"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","PaLM: Scaling Language Modeling with Pathways","A 540-billion parameter, densely activated, Transformer language model, which is called PaLM achieves breakthrough performance, outperforming the state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark.","ArXiv",2022,"Aakanksha Chowdhery,Sharan Narang,Jacob Devlin,Maarten Bosma,Gaurav Mishra,Adam Roberts,P. Barham,Hyung Won Chung,Charles Sutton,Sebastian Gehrmann,Parker Schuh,Kensen Shi,Sasha Tsvyashchenko,Joshua Maynez,Abhishek Rao,Parker Barnes,Yi Tay,Noam M. Shazeer,Vinodkumar Prabhakaran,Emily Reif,Nan Du,B. Hutchinson,Reiner Pope,James Bradbury,Jacob Austin,M. Isard,Guy Gur-Ari,Pengcheng Yin,Toju Duke,Anselm Levskaya,S. Ghemawat,Sunipa Dev,H. Michalewski,Xavier García,Vedant Misra,Kevin Robinson,L. Fedus,Denny Zhou,Daphne Ippolito,D. Luan,Hyeontaek Lim,Barret Zoph,A. Spiridonov,Ryan Sepassi,David Dohan,Shivani Agrawal,Mark Omernick,Andrew M. Dai,T. S. Pillai,Marie Pellat,Aitor Lewkowycz,Erica Moreira,Rewon Child,Oleksandr Polozov,Katherine Lee,Zongwei Zhou,Xuezhi Wang,Brennan Saeta,Mark Díaz,Orhan Firat,Michele Catasta,Jason Wei,K. Meier-Hellstern,D. Eck,J. Dean,Slav Petrov,Noah Fiedel",1807,174,82,"https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb"
"355b66a65aee97822eb7404183ee72b18cb648de","Reordering Examples Helps during Priming-based Few-Shot Learning","This work introduces PERO (Prompting with Examples in the Right Order), where it is shown that PERO can learn to generalize efficiently using as few as 10 examples, in contrast to existing approaches.","Findings",2021,"Sawan Kumar,Partha P. Talukdar",33,36,0,"https://www.semanticscholar.org/paper/355b66a65aee97822eb7404183ee72b18cb648de"
"d766bffc357127e0dc86dd69561d5aeb520d6f4c","Training language models to follow instructions with human feedback","The results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent and showing improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets.","ArXiv",2022,"Long Ouyang,Jeff Wu,Xu Jiang,Diogo Almeida,Carroll L. Wainwright,Pamela Mishkin,Chong Zhang,Sandhini Agarwal,Katarina Slama,Alex Ray,J. Schulman,Jacob Hilton,Fraser Kelton,Luke E. Miller,Maddie Simens,Amanda Askell,P. Welinder,P. Christiano,J. Leike,Ryan J. Lowe",2281,80,82,"https://www.semanticscholar.org/paper/d766bffc357127e0dc86dd69561d5aeb520d6f4c"
"c2daf72d806295198502ef05fbc76d4690b532a0","The Book of Why: The New Science of Cause and Effect","","Journal of the American Statistical Association",2020,"P. Aronow,Fredrik Sävje",410,1,23,"https://www.semanticscholar.org/paper/c2daf72d806295198502ef05fbc76d4690b532a0"
"7d645a3fd276918374fd9483fd675c28e46506d1","Galactica: A Large Language Model for Science","Galactica is introduced: a large language model that can store, combine and reason about scientific knowledge, and sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%.","ArXiv",2022,"Ross Taylor,Marcin Kardas,Guillem Cucurull,Thomas Scialom,A. Hartshorn,Elvis Saravia,Andrew Poulton,Viktor Kerkez,Robert Stojnic",195,107,6,"https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1"
"65b4b25272c50dc376f5c018338931bfd349e532","HyperTree Proof Search for Neural Theorem Proving","This work proposes an online training procedure for a transformer-based automated theorem prover that leverages a new search algorithm, HyperTree Proof Search (HTPS), inspired by the recent success of AlphaZero, and improves the state of the art on the Lean-based miniF2F-curriculum dataset.","ArXiv",2022,"Guillaume Lample,Marie-Anne Lachaux,Thibaut Lavril,Xavier Martinet,Amaury Hayat,Gabriel Ebner,Aur'elien Rodriguez,Timothée Lacroix",21,69,0,"https://www.semanticscholar.org/paper/65b4b25272c50dc376f5c018338931bfd349e532"
"c28e95a06dfcf13fc65a1cac83722f53e34f12a5","Autoformalization with Large Language Models","The usefulness of this process is demonstrated by improving a previously introduced neural theorem prover via training on these autoformalized theorems, which results in a new state-of-the-art result on the MiniF2F theorem proving benchmark.","ArXiv",2022,"Yuhuai Wu,Albert Qiaochu Jiang,Wenda Li,Markus N. Rabe,Charles Staats,M. Jamnik,Christian Szegedy",47,69,5,"https://www.semanticscholar.org/paper/c28e95a06dfcf13fc65a1cac83722f53e34f12a5"
"d26f616699a122e5455a13189e276002ee4cf923","Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs","This work introduces Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems.","ArXiv",2022,"Albert Qiaochu Jiang,S. Welleck,J. Zhou,Wenda Li,Jiacheng Liu,M. Jamnik,Timothée Lacroix,Yuhuai Wu,Guillaume Lample",29,42,0,"https://www.semanticscholar.org/paper/d26f616699a122e5455a13189e276002ee4cf923"
"92575a3c554353a27b2c0263ad7f8487d9102301","Extracting Patterns and Relations from the World Wide Web","This paper presents a technique which exploits the duality between sets of patterns and relations to grow the target relation starting from a small sample and uses it to extract a relation of (author,title) pairs from the World Wide Web.","International Workshop on the Web and Databases",1998,"S. Brin",1258,7,95,"https://www.semanticscholar.org/paper/92575a3c554353a27b2c0263ad7f8487d9102301"
"72123a86eae2cb5c4eae8650f43524039d48875d","Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions","A knowledge distillation approach, that leverages the step-by-step CoT reasoning capabilities of larger models and distils these reasoning abilities into smaller models and boosts the performance of GPT-2 variants up to 35% when distilled with this approach compared to CoT.","ArXiv",2022,"K. Shridhar,Alessandro Stolfo,Mrinmaya Sachan",25,45,0,"https://www.semanticscholar.org/paper/72123a86eae2cb5c4eae8650f43524039d48875d"
"944cba683d10d8c1a902e05cd68e32a9f47b372e","Unsupervised Word Sense Disambiguation Rivaling Supervised Methods","An unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations.","Annual Meeting of the Association for Computational Linguistics",1995,"David Yarowsky",2724,26,190,"https://www.semanticscholar.org/paper/944cba683d10d8c1a902e05cd68e32a9f47b372e"
"7b486a6eac4b46cf39e41c97b25ea22c5d27a883","Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions","This work uses the existing NLP datasets and the instructions used to crowdsource them to create NATURALINSTRUCTIONS, a dataset of instructions and task-specific input/output data that indicates that the existing models indeed benefit from instructions and hence, show improved generalization to new tasks.","ArXiv",2021,"Swaroop Mishra,Daniel Khashabi,Chitta Baral,Hannaneh Hajishirzi",34,37,4,"https://www.semanticscholar.org/paper/7b486a6eac4b46cf39e41c97b25ea22c5d27a883"
"354bf043179e3e9f05df73e3f04517e53c326d1f","TALM: Tool Augmented Language Models","TALM is presented, combining a text-only approach to augment language models with non-differentiable tools, and an iterative “self-play” technique to bootstrap performance starting from few tool demonstrations, suggesting that Tool Augmented Language Models are a promising direction to enrich LMs’ capabilities, with less dependence on scale.","ArXiv",2022,"Aaron Parisi,Yao Zhao,Noah Fiedel",37,21,1,"https://www.semanticscholar.org/paper/354bf043179e3e9f05df73e3f04517e53c326d1f"
"06d7cb8c8816360feb33c3367073e0ef66d7d0b0","Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks","Tk-Instruct is built, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples) that outperforms existing instruction-following models such as InstructGPT by over 9% on the authors' benchmark despite being an order of magnitude smaller.","Conference on Empirical Methods in Natural Language Processing",2022,"Yizhong Wang,Swaroop Mishra,Pegah Alipoormolabashi,Yeganeh Kordi,Amirreza Mirzaei,Anjana Arunkumar,Arjun Ashok,Arut Selvan Dhanasekaran,Atharva Naik,David Stap,Eshaan Pathak,Giannis Karamanolakis,Haizhi Gary Lai,I. Purohit,Ishani Mondal,Jacob Anderson,Kirby Kuznia,Krima Doshi,Maitreya Patel,Kuntal Kumar Pal,M. Moradshahi,Mihir Parmar,Mirali Purohit,Neeraj Varshney,Phani Rohitha Kaza,Pulkit Verma,Ravsehaj Singh Puri,Rushang Karia,Shailaja Keyur Sampat,Savan Doshi,S. Mishra,Sujan Reddy,Sumanta Patro,Tanay Dixit,Xudong Shen,Chitta Baral,Yejin Choi,Noah A. Smith,Hanna Hajishirzi,Daniel Khashabi",180,52,3,"https://www.semanticscholar.org/paper/06d7cb8c8816360feb33c3367073e0ef66d7d0b0"
"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","Multitask Prompted Training Enables Zero-Shot Task Generalization","A system for easily mapping any natural language tasks into a human-readable prompted form and fine-tune a pretrained encoder-decoder model on this multitask mixture covering a wide variety of tasks.","International Conference on Learning Representations",2021,"Victor Sanh,Albert Webson,Colin Raffel,Stephen H. Bach,Lintang Sutawika,Zaid Alyafeai,Antoine Chaffin,Arnaud Stiegler,Teven Le Scao,Arun Raja,Manan Dey,M Saiful Bari,Canwen Xu,Urmish Thakker,Shanya Sharma,Eliza Szczechla,Taewoon Kim,Gunjan Chhablani,Nihal V. Nayak,Debajyoti Datta,Jonathan Chang,Mike Tian-Jian Jiang,Han Wang,Matteo Manica,Sheng Shen,Zheng Xin Yong,Harshit Pandey,Rachel Bawden,Thomas Wang,Trishala Neeraj,Jos Rozen,Abheesht Sharma,Andrea Santilli,Thibault Févry,Jason Alan Fries,Ryan Teehan,Stella Rose Biderman,Leo Gao,T. Bers,Thomas Wolf,Alexander M. Rush",803,0,45,"https://www.semanticscholar.org/paper/17dd3555fd1ccf1141cf984347fa1b3fd6b009ca"
"a3076ecfed0571fbbb5217a5cc6b4b6f24f6f7dd","BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage","The goal of this research program is to enable the community to study ever-improving responsible agents that learn through interaction in BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory.","ArXiv",2022,"Kurt Shuster,Jing Xu,M. Komeili,Da Ju,Eric Michael Smith,Stephen Roller,Megan Ung,Moya Chen,Kushal Arora,Joshua Lane,Morteza Behrooz,W.K.F. Ngan,Spencer Poff,Naman Goyal,Arthur D. Szlam,Y-Lan Boureau,Melanie Kambadur,J. Weston",111,111,6,"https://www.semanticscholar.org/paper/a3076ecfed0571fbbb5217a5cc6b4b6f24f6f7dd"
"81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85","How Can We Know What Language Models Know?","This paper proposes mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts to provide a tighter lower bound on what LMs know.","Transactions of the Association for Computational Linguistics",2019,"Zhengbao Jiang,Frank F. Xu,J. Araki,Graham Neubig",694,111,31,"https://www.semanticscholar.org/paper/81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85"
"0adec918885dff698acf359988ed79a543157f80","Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity","This work uses the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, it identifies performant prompts and yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.","Annual Meeting of the Association for Computational Linguistics",2021,"Yao Lu,Max Bartolo,Alastair Moore,S. Riedel,Pontus Stenetorp",334,33,22,"https://www.semanticscholar.org/paper/0adec918885dff698acf359988ed79a543157f80"
"87e02a265606f31e65986f3c1c448a3e3a3a066e","Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?","This paper shows that ground truth demonstrations are in fact not required and that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of the label space, the distribution of the input text, and the overall format of the sequence.","Conference on Empirical Methods in Natural Language Processing",2022,"Sewon Min,Xinxi Lyu,Ari Holtzman,Mikel Artetxe,M. Lewis,Hannaneh Hajishirzi,Luke Zettlemoyer",352,72,22,"https://www.semanticscholar.org/paper/87e02a265606f31e65986f3c1c448a3e3a3a066e"
"7dd8fc595afdd2097b43a5af9a8d9f5e97a65ec1","Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration","This work proposes to constrain exploration using demonstrations to train a novel neural policy designed to handle the semi-structured nature of websites, and shows that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 100x.","International Conference on Learning Representations",2018,"E. Liu,Kelvin Guu,Panupong Pasupat,Tianlin Shi,Percy Liang",68,58,13,"https://www.semanticscholar.org/paper/7dd8fc595afdd2097b43a5af9a8d9f5e97a65ec1"
"492de01a952b6c0df748bb401a921f9aa5cad28b","The cognitive niche: Coevolution of intelligence, sociality, and language","The second hypothesis is that humans possess an ability of metaphorical abstraction, which allows them to coopt faculties that originally evolved for physical problem-solving and social coordination, apply them to abstract subject matter, and combine them productively, which can help explain the emergence of abstract cognition without supernatural or exotic evolutionary forces.","Proceedings of the National Academy of Sciences",2010,"S. Pinker",407,80,12,"https://www.semanticscholar.org/paper/492de01a952b6c0df748bb401a921f9aa5cad28b"
"23525374cfd3af714f3ffb7a203b1ef3253333fe","WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents","It is shown that agents trained on WebShop exhibit non-trivial sim-to-real transfer when evaluated on amazon.com and ebay.com, indicating the potential value of WebShop in developing practical web-based agents that can operate in the wild.","ArXiv",2022,"Shunyu Yao,Howard Chen,John Yang,Karthik Narasimhan",34,65,1,"https://www.semanticscholar.org/paper/23525374cfd3af714f3ffb7a203b1ef3253333fe"
"0286b2736a114198b25fb5553c671c33aed5d477","Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback","An iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, and a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization is identified.","ArXiv",2022,"Yuntao Bai,Andy Jones,Kamal Ndousse,Amanda Askell,Anna Chen,Nova DasSarma,Dawn Drain,Stanislav Fort,Deep Ganguli,T. Henighan,Nicholas Joseph,Saurav Kadavath,John Kernion,Tom Conerly,S. El-Showk,Nelson Elhage,Zac Hatfield-Dodds,Danny Hernandez,Tristan Hume,Scott Johnston,S. Kravec,Liane Lovitt,Neel Nanda,Catherine Olsson,Dario Amodei,Tom B. Brown,Jack Clark,Sam McCandlish,C. Olah,Benjamin Mann,Jared Kaplan",280,72,13,"https://www.semanticscholar.org/paper/0286b2736a114198b25fb5553c671c33aed5d477"
"af61816bad6ee2c8583d7006ffd1670587e4bfb5","Towards an Enhanced, Faithful, and Adaptable Web Interaction Environment","The V2 reward function closes the gap between the scores of the WebShop’s automated reward function and human evaluation and reformulates the attribute tagging problem as a extractive short-phrase prediction task to enhance scalability.","",2022,"John Yang,Howard Chen,Karthik Narasimhan",0,14,0,"https://www.semanticscholar.org/paper/af61816bad6ee2c8583d7006ffd1670587e4bfb5"
"c9967dfb91441ee8fc383e3a35c136ae2b1ce8fb","ToCT: A Task Ontology to Manage Complex Templates","This work proposes a generalpurpose solution in the form of a novel model for templates, formalised as a task ontology in OWL, called ToCT, and uses it to develop an ontology-driven text generator for isiZulu, a morphologicallyrich language, to test its capabilities.","Joint Ontology Workshops",2021,"Zola Mahlaza,C. Keet",3,26,0,"https://www.semanticscholar.org/paper/c9967dfb91441ee8fc383e3a35c136ae2b1ce8fb"
"053b1d7b97eb2c91fc3921d589c160b0923c70b1","Learning to summarize from human feedback","This work shows that it is possible to significantly improve summary quality by training a model to optimize for human preferences, and establishes that the reward model generalizes to new datasets, and that optimizing the authors' reward model results in better summaries than optimizing ROUGE according to humans.","Neural Information Processing Systems",2020,"Nisan Stiennon,Long Ouyang,Jeff Wu,Daniel M. Ziegler,Ryan J. Lowe,Chelsea Voss,Alec Radford,Dario Amodei,Paul Christiano",485,85,27,"https://www.semanticscholar.org/paper/053b1d7b97eb2c91fc3921d589c160b0923c70b1"
"8666f9f379389a5dff31e72fb0f992a37763ba41","Teaching language models to support answers with verified quotes","This work uses reinforcement learning from human preferences to train “open-book” QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness.","ArXiv",2022,"Jacob Menick,Maja Trebacz,Vladimir Mikulik,J. Aslanides,Francis Song,Martin Chadwick,Mia Glaese,Susannah Young,Lucy Campbell-Gillingham,Geoffrey Irving,Nathan McAleese",79,72,4,"https://www.semanticscholar.org/paper/8666f9f379389a5dff31e72fb0f992a37763ba41"
"ebf59587f8f170ff4241c42263bbfb9da5bd2135","ELI5: Long Form Question Answering","This work introduces the first large-scale corpus for long form question answering, a task requiring elaborate and in-depth answers to open-ended questions, and shows that an abstractive model trained with a multi-task objective outperforms conventional Seq2Seq, language modeling, as well as a strong extractive baseline.","Annual Meeting of the Association for Computational Linguistics",2019,"Angela Fan,Yacine Jernite,Ethan Perez,David Grangier,J. Weston,Michael Auli",244,27,39,"https://www.semanticscholar.org/paper/ebf59587f8f170ff4241c42263bbfb9da5bd2135"
"77d956cdab4508d569ae5741549b78e715fd0749","TruthfulQA: Measuring How Models Mimic Human Falsehoods","It is suggested that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.","Annual Meeting of the Association for Computational Linguistics",2021,"Stephanie C. Lin,Jacob Hilton,Owain Evans",230,64,15,"https://www.semanticscholar.org/paper/77d956cdab4508d569ae5741549b78e715fd0749"
"8319c672980360ce38d853a6d674352f03943434","Boosting Search Engines with Interactive Agents","A novel way of generating synthetic search sessions is developed, which leverages the power of transformer-based language models through (self-)supervised learning and presents a reinforcement learning agent with dynamically constrained actions that learns interactive search strategies from scratch.","ArXiv",2021,"Leonard Adolphs,Benjamin Boerschinger,C. Buck,Michelle Chen Huebscher,Massimiliano Ciaramita,Lasse Espeholt,Thomas Hofmann,Yannic Kilcher",17,81,1,"https://www.semanticscholar.org/paper/8319c672980360ce38d853a6d674352f03943434"
"395de0bd3837fdf4b4b5e5f04835bcc69c279481","BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","BART is presented, a denoising autoencoder for pretraining sequence-to-sequence models, which matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.","Annual Meeting of the Association for Computational Linguistics",2019,"M. Lewis,Yinhan Liu,Naman Goyal,Marjan Ghazvininejad,Abdelrahman Mohamed,Omer Levy,Veselin Stoyanov,Luke Zettlemoyer",5848,35,992,"https://www.semanticscholar.org/paper/395de0bd3837fdf4b4b5e5f04835bcc69c279481"
"acbd6e09ad888e68472a5c1cb6ec83176c7969bd","Ask the Right Questions: Active Question Reformulation with Reinforcement Learning","This work proposes an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers, and finds that successful question reformulations look quite different from natural language paraphrases.","International Conference on Learning Representations",2017,"C. Buck,Jannis Bulian,Massimiliano Ciaramita,Andrea Gesmundo,N. Houlsby,Wojciech Gajewski,Wei Wang",143,50,8,"https://www.semanticscholar.org/paper/acbd6e09ad888e68472a5c1cb6ec83176c7969bd"
"3507bd62a14bd0e8ead28cdedb1c33ba83c39c6b","Mastering Atari, Go, chess and shogi by planning with a learned model","The MuZero algorithm is presented, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics.","Nature",2019,"Julian Schrittwieser,Ioannis Antonoglou,T. Hubert,K. Simonyan,L. Sifre,Simon Schmitt,A. Guez,Edward Lockhart,D. Hassabis,T. Graepel,T. Lillicrap,David Silver",1257,56,91,"https://www.semanticscholar.org/paper/3507bd62a14bd0e8ead28cdedb1c33ba83c39c6b"
"e75fb417b54a6eae589ff382874de09d7f58a3de","Open-Ended Learning Leads to Generally Capable Agents","The red player’s goal is to put both the purple cube and the black cube (its own cube) onto its base (the grey floor), while the blue player tries to put them on the blue floor – the cubes are used as flags.","ArXiv",2021,"Open-Ended Learning Team,Adam Stooke,Anuj Mahajan,Catarina Barros,Charlie Deck,Jakob Bauer,Jakub Sygnowski,Maja Trebacz,Max Jaderberg,Michaël Mathieu,Nathan McAleese,N. Bradley-Schmieg,Nathaniel Wong,Nicolas Porcel,Roberta Raileanu,Steph Hughes-Fitt,Valentin Dalibard,Wojciech M. Czarnecki",118,109,13,"https://www.semanticscholar.org/paper/e75fb417b54a6eae589ff382874de09d7f58a3de"
"946d51acd20d9acc649d0238628261b093ec572b","CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning","A query rewriting model CONQRR is developed that rewrites a conversational question in the context into a standalone question and is trained with a novel reward function to directly optimize towards retrieval using reinforcement learning and can be adapted to any off-the-shelf retriever.","Conference on Empirical Methods in Natural Language Processing",2021,"Zeqiu Wu,Yi Luan,Hannah Rashkin,D. Reitter,Gaurav Singh Tomar",24,53,4,"https://www.semanticscholar.org/paper/946d51acd20d9acc649d0238628261b093ec572b"
"17dbd7b72029181327732e4d11b52a08ed4630d0","Natural Questions: A Benchmark for Question Answering Research","The Natural Questions corpus, a question answering data set, is presented, introducing robust metrics for the purposes of evaluating question answering systems; demonstrating high human upper bounds on these metrics; and establishing baseline results using competitive methods drawn from related literature.","International Conference on Topology, Algebra and Categories in Logic",2019,"T. Kwiatkowski,Jennimaria Palomaki,Olivia Redfield,Michael Collins,Ankur P. Parikh,Chris Alberti,D. Epstein,Illia Polosukhin,Jacob Devlin,Kenton Lee,Kristina Toutanova,Llion Jones,Matthew Kelcey,Ming-Wei Chang,Andrew M. Dai,Jakob Uszkoreit,Quoc V. Le,Slav Petrov",1527,35,198,"https://www.semanticscholar.org/paper/17dbd7b72029181327732e4d11b52a08ed4630d0"
"32cb26814b320beb0c4a8fdea86a0065ad873ef3","Open-Domain Question Answering Goes Conversational via Question Rewriting","A strong baseline approach is introduced that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA, and the effectiveness of this approach is reported.","North American Chapter of the Association for Computational Linguistics",2020,"R. Anantha,Svitlana Vakulenko,Zhucheng Tu,S. Longpre,S. Pulman,Srinivas Chappidi",80,43,15,"https://www.semanticscholar.org/paper/32cb26814b320beb0c4a8fdea86a0065ad873ef3"
"1db81c2e030f37bc14a01c6e43171a8079e7cccd","Conversational Question Reformulation via Sequence-to-Sequence Architectures and Pretrained Language Models","Examining a variety of architectures with different numbers of parameters, it is demonstrated that the recent text-to-text transfer transformer (T5) achieves the best results both on CANARD and CAsT with fewer parameters, compared to similar transformer architectures.","ArXiv",2020,"Sheng-Chieh Lin,Jheng-Hong Yang,Rodrigo Nogueira,Ming-Feng Tsai,Chuan-Ju Wang,Jimmy J. Lin",41,26,9,"https://www.semanticscholar.org/paper/1db81c2e030f37bc14a01c6e43171a8079e7cccd"
"251bdaff7521e60fe81fc375acfd34951c7f13ea","ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models","This paper presents ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning to improve performance, and presents an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets.","Conference on Empirical Methods in Natural Language Processing",2021,"Pierre L. Dognin,Inkit Padhi,Igor Melnyk,Payel Das",10,41,1,"https://www.semanticscholar.org/paper/251bdaff7521e60fe81fc375acfd34951c7f13ea"
"2591c66c6006c9c275a3dc7108a487934bc1c06f","Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering","This work is the first to report that knowledge generated by models that are orders of magnitude smaller than GPT-3, even without direct supervision on the knowledge itself, can exceed the quality of commonsense knowledge elicited from G PT-3.","Conference on Empirical Methods in Natural Language Processing",2022,"Jiacheng Liu,Skyler Hallinan,Ximing Lu,Pengfei He,S. Welleck,Hannaneh Hajishirzi,Yejin Choi",18,48,2,"https://www.semanticscholar.org/paper/2591c66c6006c9c275a3dc7108a487934bc1c06f"
"ad5970584754cc7a1d91c95ab84a1e210258183a","UnifiedQA: Crossing Format Boundaries With a Single QA System","This work uses the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats, and results in a new state of the art on 10 factoid and commonsense question answering datasets.","Findings",2020,"Daniel Khashabi,Sewon Min,Tushar Khot,Ashish Sabharwal,Oyvind Tafjord,Peter Clark,Hannaneh Hajishirzi",483,58,70,"https://www.semanticscholar.org/paper/ad5970584754cc7a1d91c95ab84a1e210258183a"
"b360427d0991143013da6a208ccf28bcc8028fab","Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training","It is shown that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora and carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model.","North American Chapter of the Association for Computational Linguistics",2020,"Oshin Agarwal,Heming Ge,Siamak Shakeri,Rami Al-Rfou",120,45,14,"https://www.semanticscholar.org/paper/b360427d0991143013da6a208ccf28bcc8028fab"
"eadbe2e4f9de47dd357589cf59e3d1f0199e5075","Learning quadrupedal locomotion over challenging terrain","The presented work indicates that robust locomotion in natural environments can be achieved by training in simple domains.","Science Robotics",2020,"Joonho Lee,Jemin Hwangbo,Lorenz Wellhausen,V. Koltun,M. Hutter",532,56,22,"https://www.semanticscholar.org/paper/eadbe2e4f9de47dd357589cf59e3d1f0199e5075"
"e37b999f0c96d7136db07b0185b837d5decd599a","Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates","It is demonstrated that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots.","IEEE International Conference on Robotics and Automation",2016,"S. Gu,E. Holly,T. Lillicrap,S. Levine",1228,42,22,"https://www.semanticscholar.org/paper/e37b999f0c96d7136db07b0185b837d5decd599a"
"4c915c1eecb217c123a36dc6d3ce52d12c742614","Simple statistical gradient-following algorithms for connectionist reinforcement learning","This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units that are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reInforcement tasks, and they do this without explicitly computing gradient estimates.","Machine-mediated learning",1992,"Ronald J. Williams",7083,37,998,"https://www.semanticscholar.org/paper/4c915c1eecb217c123a36dc6d3ce52d12c742614"
"361c00b22e29d0816ca896513d2c165e26399821","Grandmaster level in StarCraft II using multi-agent reinforcement learning","The agent, AlphaStar, is evaluated, which uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ranking among the top 0.2% of human players for the real-time strategy game StarCraft II.","Nature",2019,"Oriol Vinyals,I. Babuschkin,Wojciech M. Czarnecki,Michaël Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,M. Kroiss,Ivo Danihelka,Aja Huang,L. Sifre,Trevor Cai,J. Agapiou,Max Jaderberg,A. Vezhnevets,Rémi Leblond,Tobias Pohlen,Valentin Dalibard,D. Budden,Yury Sulsky,James Molloy,T. Paine,Caglar Gulcehre,Ziyun Wang,T. Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario Wünsch,Katrina McKinney,Oliver Smith,T. Schaul,T. Lillicrap,K. Kavukcuoglu,D. Hassabis,C. Apps,David Silver",2517,59,106,"https://www.semanticscholar.org/paper/361c00b22e29d0816ca896513d2c165e26399821"
"e89ed6bb1864558e3889f5f2fb8931643c633479","Human-level play in the game of Diplomacy by combining language models with strategic reasoning","Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players, is introduced.","Science",2022,"A. Bakhtin,Noam Brown,Emily Dinan,Gabriele Farina,Colin Flaherty,Daniel Fried,Andrew Goff,Jonathan Gray,Hengyuan Hu,Athul Paul Jacob,Mojtaba Komeili,Karthik Konath,Minae Kwon,Adam Lerer,Mike Lewis,Alexander H. Miller,S. Mitts,Adithya Renduchintala,Stephen Roller,Dirk Rowe,Weiyan Shi,Joe Spisak,Alexander Wei,David J. Wu,Hugh Zhang,Markus Zijlstra",100,60,1,"https://www.semanticscholar.org/paper/e89ed6bb1864558e3889f5f2fb8931643c633479"
"320b227027030fc291de2896fc3c6da49d7614be","Solving Rubik's Cube with a Robot Hand","It is demonstrated that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot, made possible by a novel algorithm, which is called automatic domain randomization (ADR), and a robot platform built for machine learning.","ArXiv",2019,"OpenAI,Ilge Akkaya,Marcin Andrychowicz,Maciek Chociej,Mateusz Litwin,Bob McGrew,Arthur Petron,Alex Paino,Matthias Plappert,Glenn Powell,Raphael Ribas,Jonas Schneider,N. Tezak,Jerry Tworek,P. Welinder,Lilian Weng,Qiming Yuan,Wojciech Zaremba,Lei M. Zhang",847,127,34,"https://www.semanticscholar.org/paper/320b227027030fc291de2896fc3c6da49d7614be"
"0bce17fdfd4872b16c97a89516a24a092cdc3616","Blank Language Models","Experiments on style transfer and damaged ancient text restoration demonstrate the potential of the Blank Language Model, a model that generates sequences by dynamically creating and filling in blanks, for a wide range of applications.","Conference on Empirical Methods in Natural Language Processing",2020,"T. Shen,Victor Quach,R. Barzilay,T. Jaakkola",54,54,9,"https://www.semanticscholar.org/paper/0bce17fdfd4872b16c97a89516a24a092cdc3616"
"94551d326be51a57434659093904524c39b877cd","Enabling Language Models to Fill in the Blanks","It is shown that humans have difficulty identifying sentences infilled by the approach, which can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics.","Annual Meeting of the Association for Computational Linguistics",2020,"Chris Donahue,Mina Lee,Percy Liang",139,28,27,"https://www.semanticscholar.org/paper/94551d326be51a57434659093904524c39b877cd"
"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","Proximal Policy Optimization Algorithms","A new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ""surrogate"" objective function using stochastic gradient ascent, are proposed.","ArXiv",2017,"J. Schulman,F. Wolski,Prafulla Dhariwal,Alec Radford,Oleg Klimov",9987,14,2401,"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b"
"b4e644ef70f1df3c80d246730efa29069f70fd36","Environment Generation for Zero-Shot Compositional Reinforcement Learning","This work presents Compositional Design of Environments (CoDE), which trains a Generator agent to automatically build a series of compositional tasks tailored to the RL agent’s current skill level, and learns to generate environments composed of multiple pages or rooms, and trains RL agents capable of completing wide-range of complex tasks in those environments.","Neural Information Processing Systems",2022,"Izzeddin Gur,Natasha Jaques,Yingjie Miao,Jongwook Choi,Manoj Tiwari,Honglak Lee,Aleksandra Faust",22,46,0,"https://www.semanticscholar.org/paper/b4e644ef70f1df3c80d246730efa29069f70fd36"
"f914f0175f14d23d083939c5a6e4899f90716a33","Memory, reasoning, and categorization: parallels and common mechanisms","It is argued that there is broad scope for crossover between the methods and theories developed for each task, and progress in all three of these fields will be expedited by further investigation of the many commonalities between these tasks.","Frontiers in Psychology",2014,"B. Hayes,E. Heit,C. Rotello",20,76,0,"https://www.semanticscholar.org/paper/f914f0175f14d23d083939c5a6e4899f90716a33"
"982f5f3e9c21f252018bb6fe27930e4e7bda74f8","AndroidEnv: A Reinforcement Learning Platform for Android","AndroidEnv, a research platform built on top of the Android Operating System (OS) and open-source library, along with detailed technical documentation and a set of tasks are available on GitHub.","ArXiv",2021,"Daniel Toyama,P. Hamel,Anita Gergely,Gheorghe Comanici,A. Glaese,Zafarali Ahmed,Tyler Jackson,Shibl Mourad,Doina Precup",22,39,0,"https://www.semanticscholar.org/paper/982f5f3e9c21f252018bb6fe27930e4e7bda74f8"
"4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e","End-To-End Memory Networks","A neural network with a recurrent attention model over a possibly large external memory that is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings.","NIPS",2015,"Sainbayar Sukhbaatar,Arthur D. Szlam,J. Weston,R. Fergus",2318,26,249,"https://www.semanticscholar.org/paper/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e"
"c79c9a89eb682f90f41ebe6729288342affcc6c4","RID: A Unified Framework for Conversational Recommender Systems with Pretrained Language Models","A novel unified framework called RID is posed that integrates recommendation into the dialog gen018 eration by introducing a vocabulary pointer and significantly surpasses the state-of-the-art methods on the benchmark dataset ReDial.","",2022,"",0,22,0,"https://www.semanticscholar.org/paper/c79c9a89eb682f90f41ebe6729288342affcc6c4"
"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents","This paper investigates the possibility of grounding high-level tasks, expressed in natural language, to a chosen set of actionable steps and proposes a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions.","International Conference on Machine Learning",2022,"Wenlong Huang,P. Abbeel,Deepak Pathak,Igor Mordatch",267,55,13,"https://www.semanticscholar.org/paper/92a8f7f09f3705cb5a6009a42220a6f01ea084e8"
"a7038473320c50df76fa950aca486015c5659503","Learning to Navigate the Web","DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture is trained with the ability of the agent to generalize to new instructions on World of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions.","International Conference on Learning Representations",2018,"Izzeddin Gur,U. Rückert,Aleksandra Faust,Dilek Z. Hakkani-Tür",31,15,3,"https://www.semanticscholar.org/paper/a7038473320c50df76fa950aca486015c5659503"
"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","Do As I Can, Not As I Say: Grounding Language in Robotic Affordances","It is shown how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment.","ArXiv",2022,"Michael Ahn,Anthony Brohan,Noah Brown,Yevgen Chebotar,Omar Cortes,Byron David,Chelsea Finn,K. Gopalakrishnan,Karol Hausman,Alexander Herzog,Daniel Ho,Jasmine Hsu,Julian Ibarz,Brian Ichter,A. Irpan,Eric Jang,Rosario Jauregui Ruano,Kyle Jeffrey,Sally Jesmonth,N. Joshi,Ryan C. Julian,Dmitry Kalashnikov,Yuheng Kuang,Kuang-Huei Lee,S. Levine,Yao Lu,Linda Luu,Carolina Parada,P. Pastor,Jornell Quiambao,Kanishka Rao,Jarek Rettinghouse,D. Reyes,P. Sermanet,Nicolas Sievers,Clayton Tan,Alexander Toshev,Vincent Vanhoucke,Fei Xia,Ted Xiao,Peng Xu,Sichun Xu,Mengyuan Yan",472,113,14,"https://www.semanticscholar.org/paper/cb5e3f085caefd1f3d5e08637ab55d39e61234fc"
"f3cf71c51b882fe3111d71c4bf104297d38197f8","Inner Monologue: Embodied Reasoning through Planning with Language Models","This work proposes that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios, and finds that closed-loop language feedback significantly improves high-level instruction completion on three domains.","ArXiv",2022,"Wenlong Huang,F. Xia,Ted Xiao,Harris Chan,Jacky Liang,Peter R. Florence,Andy Zeng,Jonathan Tompson,Igor Mordatch,Yevgen Chebotar,P. Sermanet,Noah Brown,Tomas Jackson,Linda Luu,S. Levine,Karol Hausman,Brian Ichter",197,110,5,"https://www.semanticscholar.org/paper/f3cf71c51b882fe3111d71c4bf104297d38197f8"
"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","Learning Transferable Visual Models From Natural Language Supervision","It is demonstrated that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet.","International Conference on Machine Learning",2021,"Alec Radford,Jong Wook Kim,Chris Hallacy,A. Ramesh,Gabriel Goh,Sandhini Agarwal,Girish Sastry,Amanda Askell,Pamela Mishkin,Jack Clark,Gretchen Krueger,Ilya Sutskever",7349,222,1367,"https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4"
"91cac43160ca45e5a1a41e0c5b7e6ec5a74033b3","Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models","DIAL is introduced, which utilizes semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration data and then train language-conditioned policies on the augmented datasets, enabling cheaper acquisition of useful language descriptions compared to expensive human labels.","ArXiv",2022,"Ted Xiao,Harris Chan,P. Sermanet,Ayzaan Wahid,Anthony Brohan,Karol Hausman,S. Levine,Jonathan Tompson",20,56,0,"https://www.semanticscholar.org/paper/91cac43160ca45e5a1a41e0c5b7e6ec5a74033b3"
"1386b8a11929cf02da291c56aca353e33bbc22ed","Diffusion-LM Improves Controllable Text Generation","A new non-autoregressive language model based on continuous diffusions that iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequences of intermediate latent variables that enables a simple gradient-based algorithm to perform complex, controllable generation tasks.","ArXiv",2022,"Xiang Lisa Li,John Thickstun,Ishaan Gulrajani,Percy Liang,Tatsunori Hashimoto",237,59,19,"https://www.semanticscholar.org/paper/1386b8a11929cf02da291c56aca353e33bbc22ed"
"340b8d8f710459d809a3da1868cd3e011aeded67","Understanding Iterative Revision from Human-Written Text","This work describes IteraTeR: the first large-scale, multi-domain, edit-intention annotated corpus of iteratively revised text, collected based on a new framework to comprehensively model the iterative text revisions that generalizes to a variety of domains, edits, revision depths, and granularities.","Annual Meeting of the Association for Computational Linguistics",2022,"Wanyu Du,Vipul Raheja,Dhruv Kumar,Zae Myung Kim,Melissa Lopez,Dongyeop Kang",21,52,4,"https://www.semanticscholar.org/paper/340b8d8f710459d809a3da1868cd3e011aeded67"
"a938ff4539b09a785a66669844f1a35f76169218","PEER: A Collaborative Language Model","This work introduces PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add sugges-tions, propose edits and provide explanations for its actions.","ArXiv",2022,"Timo Schick,Jane Dwivedi-Yu,Zhengbao Jiang,Fabio Petroni,Patrick Lewis,Gautier Izacard,Qingfei You,Christoforos Nalmpantis,Edouard Grave,Sebastian Riedel",50,74,0,"https://www.semanticscholar.org/paper/a938ff4539b09a785a66669844f1a35f76169218"
"d3640eb3b542eaf36fee2261f037a6bf0d8eac9c","AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts","Chaining LLM steps together is introduced, where the output of one step becomes the input for the next, thus aggregating the gains per step, and found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration.","International Conference on Human Factors in Computing Systems",2021,"Tongshuang Sherry Wu,Michael Terry,Carrie J. Cai",115,102,2,"https://www.semanticscholar.org/paper/d3640eb3b542eaf36fee2261f037a6bf0d8eac9c"
"ef6c768f23f86c4aa59f7e859ca6ffc1392966ca","DOC: Improving Long Story Coherence With Detailed Outline Control","In human evaluations of automatically generated stories, DOC substantially outperforms a strong Re3 baseline on plot coherence, outline relevance, and interestingness and is judged to be much more controllable in an interactive generation setting.","ArXiv",2022,"Kevin Yang,D. Klein,Nanyun Peng,Yuandong Tian",9,0,1,"https://www.semanticscholar.org/paper/ef6c768f23f86c4aa59f7e859ca6ffc1392966ca"
"152d380c07a0206e74f1474d98579266cf7e9b47","One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval","This work presents Cross-lingual Open-Retrieval Answer Generation (CORA), the first unified many-to-many question answering (QA) model that can answer questions across many languages, even for ones without language-specific annotated data or knowledge sources, and introduces a new dense passage retrieval algorithm that is trained to retrieve documents across languages for a question.","Neural Information Processing Systems",2021,"Akari Asai,Xinyan Velocity Yu,Jungo Kasai,Hannaneh Hajishirzi",35,62,11,"https://www.semanticscholar.org/paper/152d380c07a0206e74f1474d98579266cf7e9b47"
"592a6691373f3936631bc4ac122f69df09c842bd","Scalable Zero-shot Entity Linking with Dense Entity Retrieval","This paper introduces a simple and effective two-stage approach for zero-shot linking, based on fine-tuned BERT architectures, and shows that it performs well in the non-zero-shot setting, obtaining the state-of-the-art result on TACKBP-2010.","Conference on Empirical Methods in Natural Language Processing",2019,"Ledell Yu Wu,Fabio Petroni,Martin Josifoski,Sebastian Riedel,Luke Zettlemoyer",325,23,66,"https://www.semanticscholar.org/paper/592a6691373f3936631bc4ac122f69df09c842bd"
"846aedd869a00c09b40f1f1f35673cb22bc87490","Mastering the game of Go with deep neural networks and tree search","Using this search algorithm, the program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0.5, the first time that a computer program has defeated a human professional player in the full-sized game of Go.","Nature",2016,"David Silver,Aja Huang,Chris J. Maddison,A. Guez,L. Sifre,George van den Driessche,Julian Schrittwieser,Ioannis Antonoglou,Vedavyas Panneershelvam,Marc Lanctot,S. Dieleman,Dominik Grewe,John Nham,Nal Kalchbrenner,Ilya Sutskever,T. Lillicrap,M. Leach,K. Kavukcuoglu,T. Graepel,D. Hassabis",13994,78,486,"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490"
"f77a8d77bb537dc2845a3f7266ad25c85d59f4f8","Unsupervised Context Aware Sentence Representation Pretraining for Multi-lingual Dense Retrieval","This work presents a simple but effective monolingual pretraining task called contrastive context prediction (CCP) to learn sentence representation by modeling sentence level contextual relation and achieves two SOTA results in both zero-shot and supervised setting among all pretraining models using bilingual data.","International Joint Conference on Artificial Intelligence",2022,"Ning Wu,Yaobo Liang,Houxing Ren,Linjun Shou,Nan Duan,Ming Gong,Daxin Jiang",5,26,2,"https://www.semanticscholar.org/paper/f77a8d77bb537dc2845a3f7266ad25c85d59f4f8"
"050050e30d0f162c4dd87c1aac8d37df266e4c93","Sparse, Dense, and Attentional Representations for Text Retrieval","A simple neural model is proposed that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and is explored to explore sparse-dense hybrids to capitalize on the precision of sparse retrieval.","Transactions of the Association for Computational Linguistics",2020,"Y. Luan,Jacob Eisenstein,Kristina Toutanova,M. Collins",262,66,48,"https://www.semanticscholar.org/paper/050050e30d0f162c4dd87c1aac8d37df266e4c93"
"47ced790a563344efae66588b5fb7fe6cca29ed3","The Probabilistic Relevance Framework: BM25 and Beyond","This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F.","Foundations and Trends in Information Retrieval",2009,"S. Robertson,H. Zaragoza",2157,60,373,"https://www.semanticscholar.org/paper/47ced790a563344efae66588b5fb7fe6cca29ed3"
"494aedf82da4755badc1fe74e4d21cf5fc029e9d","Programs with common sense","This paper discusses programs to manipulate in a suitable formal language (most likely a part of the predicate calculus) common instrumental statements, where the basic program will draw immediate conclusions from a list of premises.","",1960,"J. McCarthy",1201,11,55,"https://www.semanticscholar.org/paper/494aedf82da4755badc1fe74e4d21cf5fc029e9d"
"87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1","ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language","This work shows that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proofs that support them, and shows that generative techniques can perform a type of abduction with high precision.","Findings",2020,"Oyvind Tafjord,Bhavana Dalvi,Peter Clark",113,28,19,"https://www.semanticscholar.org/paper/87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1"
"128cb6b891aee1b5df099acb48e2efecfcff689f","The Winograd Schema Challenge","This paper presents an alternative to the Turing Test that has some conceptual and practical advantages, and English-speaking adults will have no difficulty with it, and the subject is not required to engage in a conversation and fool an interrogator into believing she is dealing with a person.","International Conference on Principles of Knowledge Representation and Reasoning",2011,"H. Levesque,E. Davis,L. Morgenstern",1004,26,185,"https://www.semanticscholar.org/paper/128cb6b891aee1b5df099acb48e2efecfcff689f"
"490d8006851b1562cfd9ec1f057471f2868289d1","Rethinking with Retrieval: Faithful Large Language Model Inference","A novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting, which can produce more faithful explanations and improve the performance of LLMs.","ArXiv",2022,"Hangfeng He,Hongming Zhang,D. Roth",31,58,0,"https://www.semanticscholar.org/paper/490d8006851b1562cfd9ec1f057471f2868289d1"
"586615b95d9942a79e711b043044a16b561dc8af","Training Language Models with Memory Augmentation","This work presents TRIME, a novel yet simple training approach designed for training LMs with memory augmentation that adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.","Conference on Empirical Methods in Natural Language Processing",2022,"Zexuan Zhong,Tao Lei,Danqi Chen",45,62,3,"https://www.semanticscholar.org/paper/586615b95d9942a79e711b043044a16b561dc8af"
"7be8c119dbe065c52125ee7716601751f3116844","Generalization through Memorization: Nearest Neighbor Language Models","It is suggested that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.","International Conference on Learning Representations",2019,"Urvashi Khandelwal,Omer Levy,Dan Jurafsky,Luke Zettlemoyer,M. Lewis",406,31,40,"https://www.semanticscholar.org/paper/7be8c119dbe065c52125ee7716601751f3116844"
"52fa450740913a6cdcb4d9395b45e203f46cab79","Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension","This work enables a BERT-based reading comprehension model to perform lightweight numerical reasoning by augmenting the model with a predefined set of executable ‘programs’ which encompass simple arithmetic as well as extraction.","Conference on Empirical Methods in Natural Language Processing",2019,"D. Andor,Luheng He,Kenton Lee,Emily Pitler",89,14,6,"https://www.semanticscholar.org/paper/52fa450740913a6cdcb4d9395b45e203f46cab79"
"34503c0b6a615124eaf82cb0e4a1dab2866e8980","Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models","Evaluation of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters finds that model performance and calibration both improve with scale, but are poor in absolute terms.","ArXiv",2022,"A. Srivastava,Abhinav Rastogi,Abhishek Rao,Abu Awal Md Shoeb,Abubakar Abid,Adam Fisch,Adam R. Brown,Adam Santoro,Aditya Gupta,Adrià Garriga-Alonso,Agnieszka Kluska,Aitor Lewkowycz,Akshat Agarwal,Alethea Power,Alex Ray,Alex Warstadt,Alexander W. Kocurek,Ali Safaya,Ali Tazarv,Alice Xiang,Alicia Parrish,Allen Nie,A. Hussain,Amanda Askell,Amanda Dsouza,A. Rahane,Anantharaman S. Iyer,Anders Andreassen,Andrea Santilli,Andreas Stuhlmuller,Andrew M. Dai,Andrew D. La,Andrew Kyle Lampinen,Andy Zou,Angela Jiang,Angelica Chen,Anh Vuong,Animesh Gupta,Anna Gottardi,Antonio Norelli,Anu Venkatesh,Arash Gholamidavoodi,Arfa Tabassum,Arul Menezes,Arun Kirubarajan,A. Mullokandov,Ashish Sabharwal,Austin Herrick,Avia Efrat,Aykut Erdem,Ayla Karakacs,B. R. Roberts,B. S. Loe,Barret Zoph,Bartlomiej Bojanowski,Batuhan Ozyurt,Behnam Hedayatnia,Behnam Neyshabur,Benjamin Inden,Benno Stein,Berk Ekmekci,Bill Yuchen Lin,B. Howald,Cameron Diao,Cameron Dour,Catherine Stinson,Cedrick Argueta,C'esar Ferri Ram'irez,Chandan Singh,Charles Rathkopf,Chenlin Meng,Chitta Baral,Chiyu Wu,Chris Callison-Burch,Chris Waites,Christian Voigt,Christopher D. Manning,Christopher Potts,C. Ramirez,Clara Rivera,Clemencia Siro,Colin Raffel,Courtney Ashcraft,Cristina Garbacea,Damien Sileo,Daniel H Garrette,Dan Hendrycks,D. Kilman,D. Roth,Daniel Freeman,Daniel Khashabi,Daniel Levy,Daniel Gonz'alez,Danny Hernandez,Danqi Chen,Daphne Ippolito,D. Gilboa,David Dohan,D. Drakard,David Jurgens,Debajyoti Datta,Deep Ganguli,Denis Emelin,D. Kleyko,Deniz Yuret,Derek Chen,Derek Tam,D. Hupkes,Diganta Misra,Dilyar Buzan,Dimitri Coelho Mollo,Diyi Yang,Dong-Ho Lee,Ekaterina Shutova,E. D. Cubuk,Elad Segal,Eleanor Hagerman,Elizabeth Barnes,Elizabeth P. Donoway,Ellie Pavlick,E. Rodolà,E. Lam,Eric Chu,Eric Tang,Erkut Erdem,Ernie Chang,Ethan A. Chi,Ethan Dyer,E. Jerzak,Ethan Kim,Eunice Engefu Manyasi,Evgenii Zheltonozhskii,Fan Xia,F. Siar,Fernando Mart'inez-Plumed,Francesca Happ'e,François Chollet,Frieda Rong,Gaurav Mishra,Genta Indra Winata,Gerard de Melo,Germán Kruszewski,Giambattista Parascandolo,G. Mariani,Gloria Wang,Gonzalo Jaimovitch-L'opez,Gregor Betz,Guy Gur-Ari,Hana Galijasevic,H. Kim,Hannah Rashkin,Hanna Hajishirzi,Harsh Mehta,H. Bogar,Henry Shevlin,Hinrich Schütze,Hiromu Yakura,Hongming Zhang,Hubert Wong,I. Ng,Isaac Noble,Jaap Jumelet,Jack Geissinger,John Kernion,Jacob Hilton,Jaehoon Lee,J. Fisac,J. B. Simon,James Koppel,James Zheng,James Zou,Jan Koco'n,Jana Thompson,Jared Kaplan,Jarema Radom,Jascha Narain Sohl-Dickstein,Jason Phang,Jason Wei,J. Yosinski,Jekaterina Novikova,Jelle Bosscher,Jenni Marsh,Jeremy Kim,Jeroen Taal,Jesse Engel,Jesujoba Oluwadara Alabi,Jiacheng Xu,Jiaming Song,Jillian Tang,Jane W Waweru,John Burden,John Miller,John U. Balis,Jonathan Berant,Jorg Frohberg,Jos Rozen,J. Hernández-Orallo,Joseph Boudeman,Joseph Jones,J. Tenenbaum,Joshua S. Rule,Joyce Chua,Kamil Kanclerz,Karen Livescu,K. Krauth,Karthik Gopalakrishnan,Katerina Ignatyeva,K. Markert,Kaustubh D. Dhole,Kevin Gimpel,K. Omondi,K. Mathewson,Kristen Chiafullo,Ksenia Shkaruta,K. Shridhar,Kyle McDonell,Kyle Richardson,Laria Reynolds,Leo Gao,Li Zhang,Liam Dugan,Lianhui Qin,Lidia Contreras-Ochando,Louis-Philippe Morency,Luca Moschella,Luca Lam,Lucy Noble,Ludwig Schmidt,Luheng He,Luis Oliveros Col'on,Luke Metz,Lutfi Kerem cSenel,Maarten Bosma,Maarten Sap,Maartje ter Hoeve,Madotto Andrea,M. Farooqi,Manaal Faruqui,Mantas Mazeika,Marco Baturan,M. Marelli,Marco Maru,M. Quintana,Marie Tolkiehn,Mario Giulianelli,Martha Lewis,Martin Potthast,Matthew Leavitt,Matthias Hagen,M. Schubert,Medina Baitemirova,M. Arnaud,M. McElrath,Michael A. Yee,Michael Cohen,Mi Gu,Michael I. Ivanitskiy,Michael Starritt,M. Strube,Michal Swkedrowski,Michele Bevilacqua,Michihiro Yasunaga,Mihir Kale,Mike Cain,Mimee Xu,Mirac Suzgun,Monica Tiwari,Mohit Bansal,Moin Aminnaseri,Mor Geva,Mozhdeh Gheini,T. MukundVarma,Nanyun Peng,Nathan Chi,Nayeon Lee,Neta Gur-Ari Krakover,Nicholas Cameron,Nicholas S. Roberts,Nicholas Doiron,Nikita Nangia,Niklas Deckers,Niklas Muennighoff,N. Keskar,Niveditha Iyer,Noah Constant,Noah Fiedel,Nuan Wen,Oliver Zhang,Omar Agha,Omar Elbaghdadi,Omer Levy,Owain Evans,Pablo Antonio Moreno Casares,P. Doshi,Pascale Fung,Paul Pu Liang,Paul Vicol,Pegah Alipoormolabashi,Peiyuan Liao,Percy Liang,Peter W. Chang,P. Eckersley,Phu Mon Htut,Pi-Bei Hwang,P. Milkowski,P. Patil,Pouya Pezeshkpour,P. Oli,Q. Mei,QING LYU,Qinlang Chen,Rabin Banjade,R. Rudolph,Raefer Gabriel,Rahel Habacker,R. Delgado,Raphaël Millière,Rhythm Garg,Richard Barnes,R. Saurous,Riku Arakawa,Robbe Raymaekers,R. Frank,Rohan Sikand,Roman Novak,Roman Sitelew,Ronan Le Bras,Rosanne Liu,Rowan Jacobs,Rui Zhang,R. Salakhutdinov,Ryan Chi,Ryan Lee,Ryan Stovall,Ryan Teehan,Rylan Yang,Sahib J. Singh,Saif M. Mohammad,Sajant Anand,Sam Dillavou,Sam Shleifer,Sam Wiseman,Samuel Gruetter,Sam Bowman,S. Schoenholz,Sanghyun Han,Sanjeev Kwatra,Sarah A. Rous,Sarik Ghazarian,Sayan Ghosh,S. Casey,Sebastian Bischoff,Sebastian Gehrmann,Sebastian Schuster,Sepideh Sadeghi,Shadi S. Hamdan,Sharon Zhou,Shashank Srivastava,Sherry Shi,Shikhar Singh,Shima Asaadi,S. Gu,Shubh Pachchigar,Shubham Toshniwal,Shyam Upadhyay,Shyamolima Debnath,Siamak Shakeri,Simon Thormeyer,S. Melzi,Siva Reddy,S. Makini,Soo-hwan Lee,Spencer Bradley Torene,Sriharsha Hatwar,S. Dehaene,Stefan Divic,S. Ermon,Stella Rose Biderman,Stephanie C. Lin,S. Prasad,S. Piantadosi,S. Shieber,Summer Misherghi,Svetlana Kiritchenko,Swaroop Mishra,Tal Linzen,Tal Schuster,Tao Li,Tao Yu,Tariq A. Ali,Tatsuo Hashimoto,Te-Lin Wu,T. Desbordes,Theodore Rothschild,Thomas Phan,Tianle Wang,Tiberius Nkinyili,Timo Schick,T. Kornev,Timothy Telleen-Lawton,T. Tunduny,Tobias Gerstenberg,T. Chang,Trishala Neeraj,Tushar Khot,T. Shultz,Uri Shaham,Vedant Misra,V. Demberg,Victoria Nyamai,Vikas Raunak,V. Ramasesh,Vinay Uday Prabhu,Vishakh Padmakumar,Vivek Srikumar,W. Fedus,W. Saunders,William Zhang,W. Vossen,Xiang Ren,Xiaoyu Tong,Xinyi Wu,Xudong Shen,Yadollah Yaghoobzadeh,Yair Lakretz,Yang Song,Yasaman Bahri,Y. Choi,Yichi Yang,Yiding Hao,Yifu Chen,Yonatan Belinkov,Yu Hou,Yu Hou,Yuntao Bai,Zachary Seid,Zhao Xinran,Zhuoye Zhao,Z. Wang,Zijie J. Wang,Zirui Wang,Ziyi Wu,Sahib Singh,Uri Shaham",480,0,17,"https://www.semanticscholar.org/paper/34503c0b6a615124eaf82cb0e4a1dab2866e8980"
"10bb7e2c54b947fa50e7bb65b0b5c700fe998044","Measuring Massive Multitask Language Understanding","While most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average, however, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy.","International Conference on Learning Representations",2020,"Dan Hendrycks,Collin Burns,Steven Basart,Andy Zou,Mantas Mazeika,D. Song,J. Steinhardt",321,33,30,"https://www.semanticscholar.org/paper/10bb7e2c54b947fa50e7bb65b0b5c700fe998044"
"2ff3b65e893afde3d98d0870129b1c7999fc1a43","Multi-Stage Prompting for Knowledgeable Dialogue Generation","This paper proposes a multi-stage prompting approach to generate knowledgeable responses from a single pretrained language model (LM) and shows that its knowledge generator outperforms the state-of-the-art retrieval-based model by 5.8% when combining knowledge relevance and correctness.","Findings",2022,"Zihan Liu,M. Patwary,R. Prenger,Shrimai Prabhumoye,Wei Ping,M. Shoeybi,Bryan Catanzaro",26,57,3,"https://www.semanticscholar.org/paper/2ff3b65e893afde3d98d0870129b1c7999fc1a43"
"fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f","TAMER: Training an Agent Manually via Evaluative Reinforcement","A general framework called Training an Agent Manually via Evaluative Reinforcement (TAMER) is proposed that allows a human to train a learning agent to perform a common class of complex tasks simply by giving scalar reward signals in response to the agentpsilas observed actions.","2008 7th IEEE International Conference on Development and Learning",2008,"W. B. Knox,P. Stone",133,20,11,"https://www.semanticscholar.org/paper/fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f"
"abcf11a9af3d83f85c5fbfffc5901d416ca7a73f","Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces","An extension of the TAMER framework that leverages the representational power of deep neural networks in order to learn complex tasks in just a short amount of time with a human trainer, and demonstrates its success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the Atari game of Bowling.","AAAI Conference on Artificial Intelligence",2017,"Garrett Warnell,Nicholas R. Waytowich,V. Lawhern,P. Stone",192,39,20,"https://www.semanticscholar.org/paper/abcf11a9af3d83f85c5fbfffc5901d416ca7a73f"
"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","Deep Reinforcement Learning from Human Preferences","This work explores goals defined in terms of (non-expert) human preferences between pairs of trajectory segments in order to effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion.","NIPS",2017,"P. Christiano,J. Leike,Tom B. Brown,Miljan Martic,S. Legg,Dario Amodei",1061,46,85,"https://www.semanticscholar.org/paper/5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd"
"630bf4c0b7e5abd276ec38468f490b7d6222f3a2","Interactive Learning from Policy-Dependent Human Feedback","Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum, is introduced and it is demonstrated that COACH can successfully learn multiple behaviors on a physical robot.","International Conference on Machine Learning",2017,"J. MacGlashan,Mark K. Ho,R. Loftin,Bei Peng,Guan Wang,David L. Roberts,Matthew E. Taylor,M. Littman",206,28,17,"https://www.semanticscholar.org/paper/630bf4c0b7e5abd276ec38468f490b7d6222f3a2"
"6b85b63579a916f705a8e10a49bd8d849d91b1fc","Language Models are Few-Shot Learners","GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic.","Neural Information Processing Systems",2020,"Tom B. Brown,Benjamin Mann,Nick Ryder,Melanie Subbiah,J. Kaplan,Prafulla Dhariwal,Arvind Neelakantan,Pranav Shyam,Girish Sastry,Amanda Askell,Sandhini Agarwal,Ariel Herbert-Voss,Gretchen Krueger,T. Henighan,Rewon Child,A. Ramesh,Daniel M. Ziegler,Jeff Wu,Clemens Winter,Christopher Hesse,Mark Chen,Eric Sigler,Mateusz Litwin,Scott Gray,Benjamin Chess,Jack Clark,Christopher Berner,Sam McCandlish,Alec Radford,Ilya Sutskever,Dario Amodei",14479,143,1004,"https://www.semanticscholar.org/paper/6b85b63579a916f705a8e10a49bd8d849d91b1fc"
"856fe866bcce5e7a540655bea6ecc7406bdcfcba","Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks","This paper introduces the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences, and tests the zero-shot generalization capabilities of a variety of recurrent neural networks trained on SCAN with sequence-to-sequence methods.","International Conference on Machine Learning",2017,"B. Lake,Marco Baroni",583,44,79,"https://www.semanticscholar.org/paper/856fe866bcce5e7a540655bea6ecc7406bdcfcba"
"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","Measuring Compositional Generalization: A Comprehensive Method on Realistic Data","A novel method to systematically construct compositional generalization benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets is introduced, and it is demonstrated how this method can be used to create new compositionality benchmarks on top of the existing SCAN dataset.","International Conference on Learning Representations",2019,"Daniel Keysers,Nathanael Schärli,Nathan Scales,Hylke Buisman,Daniel Furrer,S. Kashubin,Nikola Momchev,Danila Sinopalnikov,Lukasz Stafiniak,Tibor Tihon,D. Tsarkov,Xiao Wang,Marc van Zee,O. Bousquet",232,126,31,"https://www.semanticscholar.org/paper/5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b"
"40047a74b707743157051d38f76061ba5ff9aab4","Compositional Semantic Parsing with Large Language Models","The best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse.","ArXiv",2022,"Andrew Drozdov,Nathanael Scharli,Ekin Akyuurek,Nathan Scales,Xinying Song,Xinyun Chen,O. Bousquet,Denny Zhou",45,62,0,"https://www.semanticscholar.org/paper/40047a74b707743157051d38f76061ba5ff9aab4"
"55a3b36fd21dbbe9384ab3ba1bcf901235d95f47","Unsupervised Question Decomposition for Question Answering","An algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions, which is promising for shedding light on why a QA system makes a prediction.","Conference on Empirical Methods in Natural Language Processing",2020,"Ethan Perez,Patrick Lewis,Wen-tau Yih,Kyunghyun Cho,Douwe Kiela",117,78,11,"https://www.semanticscholar.org/paper/55a3b36fd21dbbe9384ab3ba1bcf901235d95f47"
"c90151f00b1ac4abf1cc353849b453aa21cc2df3","Successive Prompting for Decomposing Complex Questions","A way to generate synthetic dataset which can be used to bootstrap model’s ability to decompose and answer intermediate questions is introduced and achieves an improvement in F1 of ~5% when compared with a state-of-the-art model with synthetic augmentations and few-shot version of the DROP dataset.","Conference on Empirical Methods in Natural Language Processing",2022,"Dheeru Dua,Shivanshu Gupta,Sameer Singh,Matt Gardner",36,43,2,"https://www.semanticscholar.org/paper/c90151f00b1ac4abf1cc353849b453aa21cc2df3"
"b9372e972997c5056bb79c70526230baed2e372b","Multi-hop Reading Comprehension through Question Decomposition and Rescoring","A system that decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models is proposed and a new global rescoring approach is introduced that considers each decomposition to select the best final answer, greatly improving overall performance.","Annual Meeting of the Association for Computational Linguistics",2019,"Sewon Min,Victor Zhong,Luke Zettlemoyer,Hannaneh Hajishirzi",170,26,37,"https://www.semanticscholar.org/paper/b9372e972997c5056bb79c70526230baed2e372b"
"d48b29889241551e1ee6622fa78c3fa4159255dd","Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning","A Selection-Inference (SI) framework is proposed that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer.","ArXiv",2022,"Antonia Creswell,M. Shanahan,I. Higgins",109,46,5,"https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd"
"23dd78e424d32f6a48660dcd67ce994b8a7db8be","STaR: Bootstrapping Reasoning With Reasoning","A technique to iteratively leverage a small number of rationale examples and a large dataset without rationales to bootstrap the ability to perform successively more complex reasoning, called STaR, which lets a model improve itself by learning from its own generated reasoning.","ArXiv",2022,"E. Zelikman,Yuhuai Wu,Noah D. Goodman",16,50,13,"https://www.semanticscholar.org/paper/23dd78e424d32f6a48660dcd67ce994b8a7db8be"
"f602b23d0be759201ec31cd75fedc8c6fc4f6c09","Deep Reinforcement Learning with an Unbounded Action Space","The deep reinforcement relevance network (DRRN), a novel deep architecture, is proposed to design a better model for handling an unbounded action space with applications to language understanding for text-based games, showing superior performance over other deep Q-learning architectures.","ArXiv",2015,"Ji He,Jianshu Chen,Xiaodong He,Jianfeng Gao,Lihong Li,L. Deng,Mari Ostendorf",17,29,4,"https://www.semanticscholar.org/paper/f602b23d0be759201ec31cd75fedc8c6fc4f6c09"
"a0c40c5bf41c7bbb95014132d25f99ac15b2d0d9","Survey on reinforcement learning for language processing","The state of the art of RL methods for their possible use for different problems of NLP, focusing primarily on conversational systems, is reviewed, mainly due to their growing relevance.","Artificial Intelligence Review",2021,"Víctor Uc Cetina,Nicolás Navarro-Guerrero,A. Martín-González,C. Weber,S. Wermter",33,166,0,"https://www.semanticscholar.org/paper/a0c40c5bf41c7bbb95014132d25f99ac15b2d0d9"
"c27db32efa8137cbf654902f8f728f338e55cd1c","Mastering the game of Go without human knowledge","An algorithm based solely on reinforcement learning is introduced, without human data, guidance or domain knowledge beyond game rules, that achieves superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.","Nature",2017,"David Silver,Julian Schrittwieser,K. Simonyan,Ioannis Antonoglou,Aja Huang,A. Guez,T. Hubert,Lucas baker,Matthew Lai,A. Bolton,Yutian Chen,T. Lillicrap,Fan Hui,L. Sifre,George van den Driessche,T. Graepel,D. Hassabis",7556,67,322,"https://www.semanticscholar.org/paper/c27db32efa8137cbf654902f8f728f338e55cd1c"
"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7","METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments","METEOR is described, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations and can be easily extended to include more advanced matching strategies.","IEEvaluation@ACL",2005,"S. Banerjee,A. Lavie",4069,7,802,"https://www.semanticscholar.org/paper/0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7"
"e32f5d1dc33c0c393d3c3df58671ddfa96d58a72","Deep Reinforcement Learning with a Combinatorial Action Space for Predicting and Tracking Popular Discussion Threads","The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental configurations and domains, and it also generalizes well with varying numbers of recommendation requests.","ArXiv",2016,"Ji He,Mari Ostendorf,Xiaodong He,Jianshu Chen,Jianfeng Gao,Lihong Li,L. Deng",4,35,0,"https://www.semanticscholar.org/paper/e32f5d1dc33c0c393d3c3df58671ddfa96d58a72"
"2d7782c225e0fc123d6e227f2cb253e58279ac73","Improving Neural Language Models with a Continuous Cache","A simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation, which is very efficient and scales to very large memory sizes.","International Conference on Learning Representations",2016,"Edouard Grave,Armand Joulin,Nicolas Usunier",276,54,49,"https://www.semanticscholar.org/paper/2d7782c225e0fc123d6e227f2cb253e58279ac73"
"91fb9e6b3e3576188f8e886671c29a8cb602e738","Task-Oriented Query Reformulation with Reinforcement Learning","This work introduces a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned and trains this neural network with reinforcement learning.","Conference on Empirical Methods in Natural Language Processing",2017,"Rodrigo Nogueira,Kyunghyun Cho",158,24,13,"https://www.semanticscholar.org/paper/91fb9e6b3e3576188f8e886671c29a8cb602e738"
"69e76e16740ed69f4dc55361a3d319ac2f1293dd","Asynchronous Methods for Deep Reinforcement Learning","A conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers and shows that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.","International Conference on Machine Learning",2016,"Volodymyr Mnih,Adrià Puigdomènech Badia,Mehdi Mirza,A. Graves,T. Lillicrap,Tim Harley,David Silver,K. Kavukcuoglu",6952,43,1208,"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd"
"0f733817e82026f7c29909a51cb4df7d2685f0e7","PromptChainer: Chaining Large Language Model Prompts through Visual Programming","This work explores the LLM chain authoring process, and designs PromptChainer, an interactive interface for visually programming chains that supports building prototypes for a range of applications, as well as supporting low-fi chain prototyping.","CHI Extended Abstracts",2022,"Tongshuang Sherry Wu,Ellen Jiang,Aaron Donsbach,J. Gray,A. Molina,Michael Terry,Carrie J. Cai",63,23,1,"https://www.semanticscholar.org/paper/0f733817e82026f7c29909a51cb4df7d2685f0e7"
"f0a0e8b6e84207f50db4d24cc4016e40601214ef","Faithful Reasoning Using Large Language Models","The method carries out a beam search through the space of reasoning traces to improve reasoning quality, and generates humanly interpretable reasoning traces whose validity can be checked by the user.","ArXiv",2022,"Antonia Creswell,M. Shanahan",56,35,1,"https://www.semanticscholar.org/paper/f0a0e8b6e84207f50db4d24cc4016e40601214ef"
"116a877c774969b53399b1ccaa34d51bfb492ee4","Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning","This work proposes Seq2 SQL, a deep neural network for translating natural language questions to corresponding SQL queries, and releases WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables fromWikipedia that is an order of magnitude larger than comparable datasets.","ArXiv",2018,"Victor Zhong,Caiming Xiong,R. Socher",773,44,204,"https://www.semanticscholar.org/paper/116a877c774969b53399b1ccaa34d51bfb492ee4"
"bde0c85ed3d61de2a8874ddad70497b3d68bc8ad","Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering","Interestingly, it is observed that the performance of this method significantly improves when increasing the number of retrieved passages, evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.","Conference of the European Chapter of the Association for Computational Linguistics",2020,"Gautier Izacard,Edouard Grave",501,34,80,"https://www.semanticscholar.org/paper/bde0c85ed3d61de2a8874ddad70497b3d68bc8ad"
"efbd381493bb9636f489b965a2034d529cd56bcd","Pointer Sentinel Mixture Models","The pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank while using far fewer parameters than a standard softmax LSTM and the freely available WikiText corpus is introduced.","International Conference on Learning Representations",2016,"Stephen Merity,Caiming Xiong,James Bradbury,R. Socher",1492,30,296,"https://www.semanticscholar.org/paper/efbd381493bb9636f489b965a2034d529cd56bcd"
"a81874b4a651a740fffbfc47ef96515e8c7f782f","Latent Retrieval for Weakly Supervised Open Domain Question Answering","It is shown for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system, and outperforming BM25 by up to 19 points in exact match.","Annual Meeting of the Association for Computational Linguistics",2019,"Kenton Lee,Ming-Wei Chang,Kristina Toutanova",670,33,146,"https://www.semanticscholar.org/paper/a81874b4a651a740fffbfc47ef96515e8c7f782f"
"d4c60620570801a231a7756f931dda1740288fb9","Looped Transformers as Programmable Computers","It is demonstrated that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches, and using these building blocks to emulate a small instruction-set computer.","ArXiv",2023,"Angeliki Giannou,Shashank Rajput,Jy-yong Sohn,Kangwook Lee,Jason D. Lee,Dimitris Papailiopoulos",17,45,0,"https://www.semanticscholar.org/paper/d4c60620570801a231a7756f931dda1740288fb9"
"33ba6379835097ce3e450613bc22de7bfdb84395","Graph-based Recurrent Retriever Machine reading Reasoning Path reranking Hidden states of recurrent neural network Reasoning Path Retrieval","A new graphbased recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions and achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.","",2019,"",0,39,0,"https://www.semanticscholar.org/paper/33ba6379835097ce3e450613bc22de7bfdb84395"
"3c78c6df5eb1695b6a399e346dde880af27d1016","Simple and Effective Multi-Paragraph Reading Comprehension","It is shown that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs, which involves sampling multiple paragraphs from each document, and using an objective function that requires themodel to produce globally correct output.","Annual Meeting of the Association for Computational Linguistics",2017,"Christopher Clark,Matt Gardner",397,36,53,"https://www.semanticscholar.org/paper/3c78c6df5eb1695b6a399e346dde880af27d1016"
"9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3","Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents","This paper proposes ""Describe, Explain, Plan and Select""(DEPS), an interactive planning approach based on Large Language Models (LLMs) that helps with better error correction from the feedback during the long-haul planning, while also bringing the sense of proximity via goal Selector, a learnable module that ranks parallel sub-goals based on the estimated steps of completion and improves the original plan accordingly.","ArXiv",2023,"Zihao Wang,Shaofei Cai,Anji Liu,Xiaojian Ma,Yitao Liang",51,37,0,"https://www.semanticscholar.org/paper/9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3"