@string{nips={Advances in Neural Information Processing Systems (NeurIPS)}}
@string{icml={International Conference on Machine Learning (ICML)}}
@string{aistats={International Conference on Artificial Intelligence and Statistics (AISTATS)}}
@string{iclr={International Conference on Learning Representations (ICLR)}}
@string{jmlr={Journal of Machine Learning Research (JMLR)}}
@string{acl={Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)}}
@string{naacl={Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)}}
@string{emnlp={Conference on Empirical Methods in Natural Language Processing (EMNLP)}}
@string{cvpr={Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)}}
@string{iccv={Proceedings of the International Conference on Computer Vision (ICCV)}}
@string{ssvm={International Conference on Scale Space and Variational Methods in Computer Vision (SSVM)}}
@string{tmlr={Transactions on Machine Learning Research (TMLR)}}


@misc{ho2022large,
  author = {Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
  title = {Large Language Models Are Reasoning Teachers},
  publisher = {arXiv preprint arXiv:2212.10071}, 
  year = {2022},
}

@misc{chen2022program,
  author = {Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W.},
  title = {Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks},
  publisher = {arXiv preprint arXiv:2211.12588}, 
  year = {2022},
}

@misc{qiao2022reasoning,
  author = {Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  title = {Reasoning with Language Model Prompting: A Survey},
  publisher = {arXiv preprint arXiv:2212.09597},
  year = {2022},
}

@misc{zhang2023multimodal,
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex}, 
  title = {Multimodal Chain-of-Thought Reasoning in Language Models}, 
  publisher = {arXiv preprint arXiv:2302.00923},
  year = {2023},
}


@misc{carta2023grounding,
  author = {Carta, Thomas and Romac, Clément and Wolf, Thomas and Lamprier, Sylvain and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  title = {Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning},
  publisher = {arXiv preprint arXiv:2302.02662}, 
  year = {2023},
}

@misc{xu2022learning,
  author = {Xu, Jing and Ung, Megan and Komeili, Mojtaba and Arora, Kushal and Boureau, Y-Lan and Weston, Jason},
  title = {Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback},
  publisher = {arXiv preprint arXiv:2208.03270},
  year = {2022},
}


@misc{dasgupta2023collaborating,
  author = {Dasgupta, Ishita and Kaeser-Chen, Christine and Marino, Kenneth and Ahuja, Arun and Babayan, Sheila and Hill, Felix and Fergus, Rob},
  title = {Collaborating with language models for embodied reasoning},
  publisher = {arXiv preprint arXiv:2302.00763},
  year = {2023},
}

@article{yu2022alert,
  title={ALERT: Adapting Language Models to Reasoning Tasks},
  author={Yu, Ping and Wang, Tianlu and Golovneva, Olga and Alkhamissy, X and Ghosh, Gargi and Diab, Mona and Celikyilmaz, Asli},
  journal={arXiv preprint arXiv:2212.08286},
  year={2022}
}


@article{schick2023toolformer,
  title={Toolformer: Language Models Can Teach Themselves to Use Tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dessì†, Roberto and  Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023}
}

@article{zeng2022socratic,
  title={Socratic models: Composing zero-shot multimodal reasoning with language},
  author={Zeng, Andy and Wong, Adrian and Welker, Stefan and Choromanski, Krzysztof and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and others},
  journal={arXiv preprint arXiv:2204.00598},
  year={2022}
}

@ARTICLE{4767370,
  author={Bahl, Lalit R. and Jelinek, Frederick and Mercer, Robert L.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Maximum Likelihood Approach to Continuous Speech Recognition}, 
  year={1983},
  volume={PAMI-5},
  number={2},
  pages={179-190},
  }

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal=iclr,
  year={2022}
}

@article{huang2022towards,
  title={Towards Reasoning in Large Language Models: A Survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{dasgupta2022language,
  title={Language models show human-like content effects on reasoning},
  author={Dasgupta, Ishita and Lampinen, Andrew K and Chan, Stephanie CY and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L and Hill, Felix},
  journal={arXiv preprint arXiv:2207.07051},
  year={2022}
}



@article{scialom2022continual,
  title={Continual-T0: Progressively Instructing 50+ Tasks to Language Models Without Forgetting},
  author={Scialom, Thomas and Chakrabarty, Tuhin and Muresan, Smaranda},
  journal=emnlp,
  year={2022}
}

@article{ngo2022alignment,
  title={The alignment problem from a deep learning perspective},
  author={Ngo, Richard},
  journal={arXiv preprint arXiv:2209.00626},
  year={2022}
}

@article{krishna2021hurdles,
  title={Hurdles to progress in long-form question answering},
  author={Krishna, Kalpesh and Roy, Aurko and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2103.06332},
  year={2021}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@article{russell2015ethics,
  title={Ethics of artificial intelligence},
  author={Russell, Stuart and Hauert, Sabine and Altman, Russ and Veloso, Manuela},
  journal={Nature},
  volume={521},
  number={7553},
  pages={415--416},
  year={2015},
  publisher={Macmillan Publishers Ltd., London, England}
}

@article{russell2015artificial,
  title={Artificial intelligence. Fears of an AI pioneer.},
  author={Russell, Stuart and Bohannon, John},
  journal={Science (New York, NY)},
  volume={349},
  number={6245},
  pages={252--252},
  year={2015}
}

@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal=tmlr,
  year={2022}
}

@misc{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  year=2019
}

@inproceedings{tirumala2022memorization,
title={Memorization Without Overfitting:  Analyzing the Training Dynamics of Large Language Models},
author={Kushal Tirumala and Aram H. Markosyan and Luke Zettlemoyer and Armen Aghajanyan},
booktitle=nips,
year={2022},
}

@inproceedings{welleck2019neural,
  title={Neural text generation with unlikelihood training},
  author={Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
  booktitle=iclr,
  year={2020}
}

@misc{rae2021scaling,
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and Driessche, George van den and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and d'Autume, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  title = {Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
  publisher = {arXiv preprint arXiv:2112.11446},
  year = {2021},
}

@misc{goldberg2023some,
    author = {Goldberg, Yoav},
    title = {Some remarks on large language models},
    year = 2023,
    url = {https://gist.github.com/yoavg/59d174608e92e845c8994ac2e234c8a9},
    }

@inproceedings{ling2017program,
    title = "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
    author = "Ling, Wang  and
      Yogatama, Dani  and
      Dyer, Chris  and
      Blunsom, Phil",
    booktitle = acl,
    year = "2017",
}

@misc{qian2022limitations,
  author = {Qian, Jing and Wang, Hong and Li, Zekun and Li, Shiyang and Yan, Xifeng},
  title = {Limitations of Language Models in Arithmetic and Symbolic Induction},
  publisher = {arXiv preprint arXiv:2208.05051},
  year = {2022},
}


@misc{nogueira2021investigating,
  author = {Nogueira, Rodrigo and Jiang, Zhiying and Lin, Jimmy},
  title = {Investigating the Limitations of Transformers with Simple Arithmetic Tasks},
  publisher = {arXiv preprint arXiv:2102.13019},
  year = {2021},
}


@inproceedings{lewis2020retrieval,
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
  title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  booktitle = nips,
  year = {2020},
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle=icml,
  year={2020}
}

@article{nakano2021webgpt,
  title={WebGPT: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@article{shuster2022language,
  author = {Shuster, Kurt and Komeili, Mojtaba and Adolphs, Leonard and Roller, Stephen and Szlam, Arthur and Weston, Jason},
  title = {Language Models that Seek for Knowledge: Modular Search \& Generation for Dialogue and Prompt Completion},
  journal = {arXiv preprint arXiv:2203.13224},
  year = {2022},
}

@article{izacard2022atlas,
  author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  title = {Atlas: Few-shot Learning with Retrieval Augmented Language Models},
  journal = {arXiv preprint arXiv:2208.03299},
  year = {2022}
}

@misc{lecun2022a,
    author={LeCun, Yann},
    title={A Path Towards Autonomous Machine Intelligence},
    publisher={OpenReview},
    year={2022}
}

@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@article{cohen2022dynamic,
  title={Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning},
  author={Cohen, Deborah and Ryu, Moonkyung and Chow, Yinlam and Keller, Orgad and Greenberg, Ido and Hassidim, Avinatan and Fink, Michael and Matias, Yossi and Szpektor, Idan and Boutilier, Craig and others},
  journal={arXiv preprint arXiv:2208.02294},
  year={2022}
}

@article{ramamurthy2022reinforcement,
  title={Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization},
  author={Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kiant{\'e} and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin},
  journal={arXiv preprint arXiv:2210.01241},
  year={2022}
}

@article{chen2022open,
  title={Open-vocabulary queryable scene representations for real world planning},
  author={Chen, Boyuan and Xia, Fei and Ichter, Brian and Rao, Kanishka and Gopalakrishnan, Keerthana and Ryoo, Michael S and Stone, Austin and Kappler, Daniel},
  journal={arXiv preprint arXiv:2209.09874},
  year={2022}
}

@article{snell2022offline,
  title={Offline rl for natural language generation with implicit language q learning},
  author={Snell, Charlie and Kostrikov, Ilya and Su, Yi and Yang, Mengjiao and Levine, Sergey},
  journal={arXiv preprint arXiv:2206.11871},
  year={2022}
}

@misc{chen2021evaluating,
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  title = {Evaluating Large Language Models Trained on Code},
  publisher = {arXiv preprint arXiv:2107.03374},
  year = {2021}
}

@inproceedings{devlin2019bert,
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = naacl,
  year = {2019}
}

@article{zhou2022least,
  author = {Zhou, Denny and Schärli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  title = {Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  journal = {arXiv preprint arXiv:2205.10625},
  year = {2022},
}

@article{wang2022self,
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  journal = nips,
  year = {2022},
}

@article{nye2021show,
  author = {Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
  title = {Show Your Work: Scratchpads for Intermediate Computation with Language Models},
  journal = {arXiv preprint arXiv:2112.00114},
  year = {2021},
}

@article{yao2022react,
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  title = {ReAct: Synergizing Reasoning and Acting in Language Models},
  journal = {arXiv preprint arXiv:2210.03629},
  year = {2022},
}

@inproceedings{borgeaud2022improving,
  author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Driessche, George van den and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
  title = {Improving language models by retrieving from trillions of tokens},
  booktitle = icml,
  year = {2022},
}

@article{gur2021adversarial,
  title={Adversarial environment generation for learning to navigate the web},
  author={Gur, Izzeddin and Jaques, Natasha and Malta, Kevin and Tiwari, Manoj and Lee, Honglak and Faust, Aleksandra},
  journal={arXiv preprint arXiv:2103.01991},
  year={2021}
}

@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author = {Glaese, Amelia and McAleese, Nat and Trębacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and Campbell-Gillingham, Lucy and Uesato, Jonathan and Huang, Po-Sen and Comanescu, Ramona and Yang, Fan and See, Abigail and Dathathri, Sumanth and Greig, Rory and Chen, Charlie and Fritz, Doug and Elias, Jaume Sanchez and Green, Richard and Mokrá, Soňa and Fernando, Nicholas and Wu, Boxi and Foley, Rachel and Young, Susannah and Gabriel, Iason and Isaac, William and Mellor, John and Hassabis, Demis and Kavukcuoglu, Koray and Hendricks, Lisa Anne and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022}
}

@misc{press2022measuring,
  author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A. and Lewis, Mike},
  title = {Measuring and Narrowing the Compositionality Gap in Language Models},
  publisher = {arXiv preprint arXiv:2210.03350},
  year = {2022},
}

@inproceedings{kojima2022large,
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  title = {Large Language Models are Zero-Shot Reasoners},
  booktitle=nips,
  year = {2022},
}

@misc{lewkowycz2022solving,
  author = {Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and Wu, Yuhuai and Neyshabur, Behnam and Gur-Ari, Guy and Misra, Vedant},
  title = {Solving Quantitative Reasoning Problems with Language Models},
  publisher = {arXiv preprint arXiv:2206.14858},
  year = {2022},
}

@misc{gao2022pal,
  author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  title = {PAL: Program-aided Language Models},
  publisher = {arXiv preprint arXiv:2211.10435},
  year = {2022},
}

@article{drori2022neural,
  title={A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level},
  author={Drori, Iddo and Zhang, Sarah and Shuttleworth, Reece and Tang, Leonard and Lu, Albert and Ke, Elizabeth and Liu, Kevin and Chen, Linda and Tran, Sunny and Cheng, Newman and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={32},
  year={2022},
}

@article{yang2022re3,
  title={Re3: Generating Longer Stories With Recursive Reprompting and Revision},
  author={Yang, Kevin and Peng, Nanyun and Tian, Yuandong and Klein, Dan},
  journal=emnlp,
  year={2022}
}

@article{tay2022unifying,
  title={Unifying Language Learning Paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  journal={arXiv preprint arXiv:2205.05131},
  year={2022}
}

@article{wu2021recursively,
  title={Recursively summarizing books with human feedback},
  author={Wu, Jeff and Ouyang, Long and Ziegler, Daniel M and Stiennon, Nisan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
  journal={arXiv preprint arXiv:2109.10862},
  year={2021}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{wang2022behavior,
  author = {Wang, Ruoyao and Jansen, Peter and Côté, Marc-Alexandre and Ammanabrolu, Prithviraj},
  title = {Behavior Cloned Transformers are Neurosymbolic Reasoners},
  journal = {arXiv preprint arXiv:2210.07382},
  year = {2022},
}

@article{liu2022mind,
  title={Mind's Eye: Grounded Language Model Reasoning through Simulation},
  author={Liu, Ruibo and Wei, Jason and Gu, Shixiang Shane and Wu, Te-Yen and Vosoughi, Soroush and Cui, Claire and Zhou, Denny and Dai, Andrew M},
  journal={arXiv preprint arXiv:2210.05359},
  year={2022}
}

@article{gur2022understanding,
  title={Understanding HTML with Large Language Models},
  author={Gur, Izzeddin and Nachum, Ofir and Miao, Yingjie and Safdari, Mustafa and Huang, Austin and Chowdhery, Aakanksha and Narang, Sharan and Fiedel, Noah and Faust, Aleksandra},
  journal={arXiv preprint arXiv:2210.03945},
  year={2022}
}

@article{cobbe2021training,
  author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  title = {Training Verifiers to Solve Math Word Problems},
journal={arXiv preprint arXiv:2110.14168},
  year = {2021},
}

@article{thoppilan2022lamda,
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  title = {LaMDA: Language Models for Dialog Applications},
  journal = {arXiv preprint arXiv:2201.08239},
  year = {2022},
}

@article{chowdhery2022palm,
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  title = {PaLM: Scaling Language Modeling with Pathways},
  journal = {arXiv},
  year = {2022},
}

@article{gao2022scaling,
  title={Scaling Laws for Reward Model Overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  journal={arXiv preprint arXiv:2210.10760},
  year={2022}
}

@article{ouyang2022training,
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  title = {Training language models to follow instructions with human feedback},
  journal = {arXiv preprint arXiv:2203.02155},
  year = {2022},
}

@misc{brown2020language,
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  title = {Language Models are Few-Shot Learners},
  publisher = {arXiv},
  year = {2020},
}

@book{pearl2018the,
  author = {Pearl, Judea and Mackenzie, Dana},
  publisher = {Basic Books},
  subtitle = {The New Science of Cause and Effect},
  title = {The Book of Why},
  year = 2018
}

@article{taylor2022galactica,
  author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  title = {Galactica: A Large Language Model for Science},
  journal = {arXiv preprint arXiv:2211.09085},
  year = {2022},
}

%Theorem proving

@inproceedings{lample2022hypertree,
  author = {Lample, Guillaume and Lachaux, Marie-Anne and Lavril, Thibaut and Martinet, Xavier and Hayat, Amaury and Ebner, Gabriel and Rodriguez, Aurélien and Lacroix, Timothée},
  title = {HyperTree Proof Search for Neural Theorem Proving},
  booktitle = nips,
  year = {2022}
}

@article{wu2022autoformalization,
  title={Autoformalization with large language models},
  author={Wu, Yuhuai and Jiang, Albert Q and Li, Wenda and Rabe, Markus N and Staats, Charles and Jamnik, Mateja and Szegedy, Christian},
  journal=nips,
  year={2022}
} % using LLMs to automatically formalize mathematical competition problems in Isabelle/HOL

@article{jiang2022draft,
  title={Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs},
  author={Jiang, Albert Q and Welleck, Sean and Zhou, Jin Peng and Li, Wenda and Liu, Jiacheng and Jamnik, Mateja and Lacroix, Timoth{\'e}e and Wu, Yuhuai and Lample, Guillaume},
  journal={arXiv preprint arXiv:2210.12283},
  year={2022}
} % uses large language model to generate formal proof sketches, that are fed to a prover


% Supervised Learning

@inproceedings{yarowsky-1995-unsupervised,
    title = "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods",
    author = "Yarowsky, David",
    booktitle = acl,
    year = "1995",
}


@InProceedings{brin1999extracting,
author="Brin, Sergey",
title="Extracting Patterns and Relations from the World Wide Web",
booktitle="The World Wide Web and Databases",
year="1999",
publisher="Springer Berlin Heidelberg",
pages="172--183",
abstract="The World Wide Web is a vast resource for information. At the same time it is extremely distributed. A particular type of data such as restaurant lists may be scattered across thousands of independent information sources in many different formats. In this paper, we consider the problem of extracting a relation for such a data type from all of these sources automatically. We present a technique which exploits the duality between sets of patterns and relations to grow the target relation starting from a small sample. To test our technique we use it to extract a relation of (author,title) pairs from the World Wide Web.",
}


@article{shridhar2022distilling,
  author = {Shridhar, Kumar and Stolfo, Alessandro and Sachan, Mrinmaya},
  title = {Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions}, 
  journal = {arXiv preprint arXiv:2212.00193}, 
  year = {2022},
}

@article{mishra2021natural,
  title={Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  year={2021}
}


@article{parisi2022talm,
  author = {Parisi, Aaron and Zhao, Yao and Fiedel, Noah},
  title = {TALM: Tool Augmented Language Models},
  journal = {arXiv preprint arXiv:2205.12255},
  year = {2022},
}


@inproceedings{wang2022super,  
  author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi Gary and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Patel, Maitreya and Pal, Kuntal Kumar and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Sampat, Shailaja Keyur and Doshi, Savan and Mishra, Siddhartha and Reddy, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong and Baral, Chitta and Choi, Yejin and Smith, Noah A. and Hajishirzi, Hannaneh and Khashabi, Daniel}, 
  title = {Super-Natural Instructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},  
  booktitle = emnlp, 
  year = {2022},
}

@inproceedings{
sanh2022multitask,
title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
author={Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
booktitle=iclr,
year={2022},
}

@misc{suzgun2022challenging,
  doi = {10.48550/ARXIV.2210.09261},
  
  url = {https://arxiv.org/abs/2210.09261},
  
  author = {Suzgun, Mirac and Scales, Nathan and Schärli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Wei, Jason},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{lazaridou2022internet,
  doi = {10.48550/ARXIV.2203.05115},
  
  url = {https://arxiv.org/abs/2203.05115},
  
  author = {Lazaridou, Angeliki and Gribovskaya, Elena and Stokowiec, Wojciech and Grigorev, Nikolai},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Internet-augmented language models through few-shot prompting for open-domain question answering},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{shuster2022blenderbot,
  author = {Shuster, Kurt and Xu, Jing and Komeili, Mojtaba and Ju, Da and Smith, Eric Michael and Roller, Stephen and Ung, Megan and Chen, Moya and Arora, Kushal and Lane, Joshua and Behrooz, Morteza and Ngan, William and Poff, Spencer and Goyal, Naman and Szlam, Arthur and Boureau, Y-Lan and Kambadur, Melanie and Weston, Jason},
  title = {BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage},  
  journal = {arXiv preprint arXiv:2208.03188}, 
  year = {2022},
}


@article{jiang-etal-2020-know,
    title = "How Can We Know What Language Models Know?",
    author = "Jiang, Zhengbao  and
      Xu, Frank F.  and
      Araki, Jun  and
      Neubig, Graham",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
}

@inproceedings{lu-etal-2022-fantastically,
    title = "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
    author = "Lu, Yao  and
      Bartolo, Max  and
      Moore, Alastair  and
      Riedel, Sebastian  and
      Stenetorp, Pontus",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.556",
    doi = "10.18653/v1/2022.acl-long.556",
    pages = "8086--8098",
    abstract = "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are {``}fantastic{''} and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13{\%} relative improvement for GPT-family models across eleven different established text classification tasks.",
}



@misc{min2022rethinking,
  doi = {10.48550/ARXIV.2202.12837},
  
  url = {https://arxiv.org/abs/2202.12837},
  
  author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{kumar-talukdar-2021-reordering,
    title = "Reordering Examples Helps during Priming-based Few-Shot Learning",
    author = "Kumar, Sawan  and
      Talukdar, Partha",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.395",
    doi = "10.18653/v1/2021.findings-acl.395",
    pages = "4507--4518",
}

%  Reinforcement Learning

@article{pinker2010cognitive,
  title={The cognitive niche: Coevolution of intelligence, sociality, and language},
  author={Pinker, Steven},
  journal={Proceedings of the National Academy of Sciences},
  volume={107},
  number={supplement\_2},
  pages={8993--8999},
  year={2010},
  publisher={National Acad Sciences}
}


@article{liu2018reinforcement,
  title={Reinforcement learning on web interfaces using workflow-guided exploration},
  author={Liu, Evan Zheran and Guu, Kelvin and Pasupat, Panupong and Shi, Tianlin and Liang, Percy},
  journal={arXiv preprint arXiv:1802.08802},
  year={2018}
}

@article{yao2022webshop,
  title={Webshop: Towards scalable real-world web interaction with grounded language agents},
  author={Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  journal=nips,
  year={2022}
}

@inproceedings{yangtowards,
  title={Towards an Enhanced, Faithful, and Adaptable Web Interaction Environment},
  author={Yang, John and Chen, Howard and Narasimhan, Karthik R},
  booktitle={Second Workshop on Language and Reinforcement Learning},
  year={2022}
}


@article{bai2022training,
  title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{hao2022language,
  title={Language models are general-purpose interfaces},
  author={Hao, Yaru and Song, Haoyu and Dong, Li and Huang, Shaohan and Chi, Zewen and Wang, Wenhui and Ma, Shuming and Wei, Furu},
  journal={arXiv preprint arXiv:2206.06336},
  year={2022}
}

@inproceedings{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  booktitle=nips,
  year={2020}
}

@article{menick2022teaching,
  title={Teaching language models to support answers with verified quotes},
  author={Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and others},
  journal={arXiv preprint arXiv:2203.11147},
  year={2022}
}


% -------- Possible duplications -------------

@article{fan2019eli5,
  title={ELI5: Long form question answering},
  author={Fan, Angela and Jernite, Yacine and Perez, Ethan and Grangier, David and Weston, Jason and Auli, Michael},
  journal={arXiv preprint arXiv:1907.09190},
  year={2019}
}

@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}


%  Learning to Use Tools (RL)

@article{adolphs2021boosting,
  title={Boosting search engines with interactive agents},
  author={Adolphs, Leonard and Boerschinger, Benjamin and Buck, Christian and Huebscher, Michelle Chen and Ciaramita, Massimiliano and Espeholt, Lasse and Hofmann, Thomas and Kilcher, Yannic},
  journal=tmlr,
  year={2022}
}

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{buck2017ask,
  title={Ask the right questions: Active question reformulation with reinforcement learning},
  author={Buck, Christian and Bulian, Jannis and Ciaramita, Massimiliano and Gajewski, Wojciech and Gesmundo, Andrea and Houlsby, Neil and Wang, Wei},
  journal=iclr,
  year={2018}
}

@article{team2021open,
  title={Open-ended learning leads to generally capable agents},
  author={Team, Open Ended Learning and Stooke, Adam and Mahajan, Anuj and Barros, Catarina and Deck, Charlie and Bauer, Jakob and Sygnowski, Jakub and Trebacz, Maja and Jaderberg, Max and Mathieu, Michael and others},
  journal={arXiv preprint arXiv:2107.12808},
  year={2021}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press}
}

@article{wu2021conqrr,
  title={CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning},
  author={Wu, Zeqiu and Luan, Yi and Rashkin, Hannah and Reitter, David and Tomar, Gaurav Singh},
  journal=emnlp,
  year={2022}
}

@article{anantha2020open,
  title={Open-domain question answering goes conversational via question rewriting},
  author={Anantha, Raviteja and Vakulenko, Svitlana and Tu, Zhucheng and Longpre, Shayne and Pulman, Stephen and Chappidi, Srinivas},
  journal={arXiv preprint arXiv:2010.04898},
  year={2020}
}
@article{lin2020conversational,
  title={Conversational question reformulation via sequence-to-sequence architectures and pretrained language models},
  author={Lin, Sheng-Chieh and Yang, Jheng-Hong and Nogueira, Rodrigo and Tsai, Ming-Feng and Wang, Chuan-Ju and Lin, Jimmy},
  journal={arXiv preprint arXiv:2004.01909},
  year={2020}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{dognin2021regen,
  title={ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models},
  author={Dognin, Pierre L and Padhi, Inkit and Melnyk, Igor and Das, Payel},
  journal=emnlp,
  year={2021}
}

@article{liu2022rainier,
  title={Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering},
  author={Liu, Jiacheng and Hallinan, Skyler and Lu, Ximing and He, Pengfei and Welleck, Sean and Hajishirzi, Hannaneh and Choi, Yejin},
  journal={arXiv preprint arXiv:2210.03078},
  year={2022}
}

@article{raffel2020exploring,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
year = {2020},
journal = jmlr,
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{khashabi2020unifiedqa,
  title={Unifiedqa: Crossing format boundaries with a single qa system},
  author={Khashabi, Daniel and Min, Sewon and Khot, Tushar and Sabharwal, Ashish and Tafjord, Oyvind and Clark, Peter and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2005.00700},
  year={2020}
}

@inproceedings{ferreira2020proceedings,
  title={Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)},
  author={Ferreira, Thiago Castro and Gardent, Claire and Ilinykh, Nikolai and van der Lee, Chris and Mille, Simon and Moussallem, Diego and Shimorina, Anastasia},
  booktitle={Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)},
  year={2020}
}

@article{agarwal2020knowledge,
  title={Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training},
  author={Agarwal, Oshin and Ge, Heming and Shakeri, Siamak and Al-Rfou, Rami},
  journal={arXiv preprint arXiv:2010.12688},
  year={2020}
}

@article{lee2020learning,
  title={Learning quadrupedal locomotion over challenging terrain},
  author={Lee, Joonho and Hwangbo, Jemin and Wellhausen, Lorenz and Koltun, Vladlen and Hutter, Marco},
  journal={Science robotics},
  volume={5},
  number={47},
  pages={eabc5986},
  year={2020},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{gu2017deep,
  title={Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle={2017 IEEE international conference on robotics and automation (ICRA)},
  pages={3389--3396},
  year={2017},
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{kalashnikov2018qt,
  title={QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation. arXiv e-prints, page},
  author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  journal={arXiv preprint arXiv:1806.10293},
  year={2018}
}

@article{akkaya2019solving,
  title={Solving rubik's cube with a robot hand},
  author={Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}

@article{bakhtin2022human,
  title={Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
  author={Anton Bakhtin and Noam Brown and Emily Dinan and Gabriele Farina and Colin Flaherty and Daniel Fried and Andrew Goff and Jonathan Gray and Hengyuan Hu and Athul Paul Jacob and Mojtaba Komeili and Karthik Konath and Minae Kwon and Adam Lerer and Mike Lewis and Alexander H. Miller and Sandra Mitts and Adithya Renduchintala and Stephen Roller and Dirk Rowe and Weiyan Shi and Joe Spisak and Alexander Wei and David J. Wu and Hugh Zhang and Markus Zijlstra},
  journal={Science},
  year={2022},
  volume={378},
  pages={1067 - 1074}
}

@article{meta2022human,
  title={Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
  author={Meta Fundamental AI Research Diplomacy Team (FAIR)† and Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1067--1074},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{shen-etal-2020-blank,
    title = "Blank Language Models",
    author = "Shen, Tianxiao  and
      Quach, Victor  and
      Barzilay, Regina  and
      Jaakkola, Tommi",
    booktitle = emnlp,
    year = "2020",
}

@inproceedings{donahue-etal-2020-enabling,
    title = "Enabling Language Models to Fill in the Blanks",
    author = "Donahue, Chris  and
      Lee, Mina  and
      Liang, Percy",
    booktitle = acl,
    year = "2020",
}

@article{gur2021environment,
  title={Environment generation for zero-shot compositional reinforcement learning},
  author={Gur, Izzeddin and Jaques, Natasha and Miao, Yingjie and Choi, Jongwook and Tiwari, Manoj and Lee, Honglak and Faust, Aleksandra},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4157--4169},
  year={2021}
}

@inproceedings{humphreys2022data,
  title={A data-driven approach for learning to control computers},
  author={Humphreys, Peter C and Raposo, David and Pohlen, Tobias and Thornton, Gregory and Chhaparia, Rachita and Muldal, Alistair and Abramson, Josh and Georgiev, Petko and Santoro, Adam and Lillicrap, Timothy},
  booktitle=icml,
  year={2022},
}

@article{hayes2014memory,
  title={Memory, reasoning, and categorization: parallels and common mechanisms},
  author={Hayes, Brett K and Heit, Evan and Rotello, Caren M},
  journal={Frontiers in Psychology},
  volume={5},
  pages={529},
  year={2014},
  publisher={Frontiers Media SA}
}

@inproceedings{weston2014memory,
  title={Memory networks},
  author={Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  journal=iclr,
  year={2015}
}

@inproceedings{shi2017world,
  title={World of bits: An open-domain platform for web-based agents},
  author={Shi, Tianlin and Karpathy, Andrej and Fan, Linxi and Hernandez, Jonathan and Liang, Percy},
  booktitle=icml,
  year={2017},
}

@article{toyama2021androidenv,
  title={Androidenv: a reinforcement learning platform for android},
  author={Toyama, Daniel and Hamel, Philippe and Gergely, Anita and Comanici, Gheorghe and Glaese, Amelia and Ahmed, Zafarali and Jackson, Tyler and Mourad, Shibl and Precup, Doina},
  journal={arXiv preprint arXiv:2105.13231},
  year={2021}
}

@article{liang2022code,
  title={Code as policies: Language model programs for embodied control},
  author={Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  journal={arXiv preprint arXiv:2209.07753},
  year={2022}
}


@article{li2022pre,
  title={Pre-trained language models for interactive decision-making},
  author={Li, Shuang and Puig, Xavier and Du, Yilun and Wang, Clinton and Akyurek, Ekin and Torralba, Antonio and Andreas, Jacob and Mordatch, Igor},
  journal={arXiv preprint arXiv:2202.01771},
  year={2022}
}

@article{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  journal={arXiv preprint arXiv:2201.07207},
  year={2022}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{ahn2022can,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and others},
  journal={arXiv preprint arXiv:2204.01691},
  year={2022}
}

@article{gur2018learning,
  title={Learning to navigate the web},
  author={Gur, Izzeddin and Rueckert, Ulrich and Faust, Aleksandra and Hakkani-Tur, Dilek},
  journal=iclr,
  year={2019}
}

@article{huang2022inner,
  title={Inner monologue: Embodied reasoning through planning with language models},
  author={Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others},
  journal={arXiv preprint arXiv:2207.05608},
  year={2022}
}

@article{brohan2022rt,
  title={RT-1: Robotics Transformer for Real-World Control at Scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  journal={arXiv preprint arXiv:2212.06817},
  year={2022}
}

@article{xiao2022robotic,
  title={Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models},
  author={Xiao, Ted and Chan, Harris and Sermanet, Pierre and Wahid, Ayzaan and Brohan, Anthony and Hausman, Karol and Levine, Sergey and Tompson, Jonathan},
  journal={arXiv preprint arXiv:2211.11736},
  year={2022}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and others},
  journal=nips,
  year={2022}
}

@article{li2022diffusion,
  title={Diffusion-LM Improves Controllable Text Generation},
  author={Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2205.14217},
  year={2022}
}

@inproceedings{du2022understanding,
  title={Understanding Iterative Revision from Human-Written Text},
  author={Du, Wanyu and Raheja, Vipul and Kumar, Dhruv and Kim, Zae Myung and Lopez, Melissa and Kang, Dongyeop},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3573--3590},
  year={2022}
}

@article{schick2022peer,
  title={PEER: A Collaborative Language Model},
  author={Schick, Timo and Dwivedi-Yu, Jane and Jiang, Zhengbao and Petroni, Fabio and Lewis, Patrick and Izacard, Gautier and You, Qingfei and Nalmpantis, Christoforos and Grave, Edouard and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2208.11663},
  year={2022}
}


@article{wang2022shepherd,
  author = {Wang, Boshi and Deng, Xiang and Sun, Huan},
  title = {Iteratively Prompt Pre-trained Language Models for Chain of Thought}, 
  journal = emnlp,
  year = {2022},
}


@article{yang2022seqzero,
  title={SEQZERO: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models},
  author={Yang, Jingfeng and Jiang, Haoming and Yin, Qingyu and Zhang, Danqing and Yin, Bing and Yang, Diyi},
  journal=naacl,
  year={2022}
}

@inproceedings{wu2022ai,
  title={Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts},
  author={Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
  booktitle={CHI Conference on Human Factors in Computing Systems},
  pages={1--22},
  year={2022}
}


@article{yang2022doc,
  title={DOC: Improving Long Story Coherence With Detailed Outline Control},
  author={Yang, Kevin and Klein, Dan and Peng, Nanyun and Tian, Yuandong},
  journal={arXiv preprint arXiv:2212.10077},
  year={2022}
}


@article{asai2021cora,
  author    = {Akari Asai and
               Xinyan Yu and
               Jungo Kasai and
               Hannaneh Hajishirzi},
  title     = {One Question Answering Model for Many Languages with Cross-lingual
               Dense Passage Retrieval},
  journal   = nips,
  year      = {2021},
}

@book{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo},
  year={2009},
  publisher={Now Publishers Inc}
}

@article{wu2019zero,
	Author = {Wu, Ledell and Petroni, Fabio and Josifoski, Martin and Riedel, Sebastian and Zettlemoyer, Luke},
	Journal = {arXiv preprint arXiv:1911.03814},
	Title = {Zero-shot Entity Linking with Dense Entity Retrieval},
	Year = {2020}}

@inproceedings{maillard-etal-2021-multi,
    title = "Multi-Task Retrieval for Knowledge-Intensive Tasks",
    author = "Maillard, Jean  and
      Karpukhin, Vladimir  and
      Petroni, Fabio  and
      Yih, Wen-tau  and
      Oguz, Barlas  and
      Stoyanov, Veselin  and
      Ghosh, Gargi",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.89",
    doi = "10.18653/v1/2021.acl-long.89",
    pages = "1098--1111",
}

@article{oguz2021domainmatched,
      author={Barlas Oğuz and Kushal Lakhotia and Anchit Gupta and Patrick Lewis and Vladimir Karpukhin and Aleksandra Piktus and Xilun Chen and Sebastian Riedel and Wen-tau Yih and Sonal Gupta and Yashar Mehdad},
      title={Domain-matched Pre-training Tasks for Dense Retrieval},
      year={2021},
      journal={arXiv preprint arXiv:2107.13602},
}

@article{10.1162/tacl_a_00369,
    author = {Luan, Yi and Eisenstein, Jacob and Toutanova, Kristina and Collins, Michael},
    title = "{Sparse, Dense, and Attentional Representations for Text Retrieval}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {329-345},
    year = {2021},
    month = {04},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00369},
    url = {https://doi.org/10.1162/tacl\_a\_00369},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00369/1924040/tacl\_a\_00369.pdf},
}

@book{mccarthy1960programs,
  title={Programs with common sense},
  author={McCarthy, John and others},
  year={1960},
  publisher={RLE and MIT computation center Cambridge, MA, USA}
}

@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth international conference on the principles of knowledge representation and reasoning},
  year={2012}
}

@inproceedings{tafjord2021proofwriter,
  title={{ProofWriter}: Generating Implications, Proofs, and Abductive Statements over Natural Language},
  author={Tafjord, Oyvind and Dalvi, Bhavana and Clark, Peter},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={3621--3634},
  year={2021}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{he2022rethinking,
  title={Rethinking with Retrieval: Faithful Large Language Model Inference},
  author={He, Hangfeng and Zhang, Hongming and Roth, Dan},
  journal={arXiv preprint arXiv:2301.00303},
  year={2022}
}

@inproceedings{zhong2022training,
   title={Training Language Models with Memory Augmentation},
   author={Zhong, Zexuan and Lei, Tao and Chen, Danqi},
   booktitle=emnlp,
   year={2022}
}

@inproceedings{khandelwal20generalization,
  title={{Generalization through Memorization: Nearest Neighbor Language Models}},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@inproceedings{andor:etal:2019,
    title = "Giving {BERT} a Calculator: Finding Operations and Arguments with Reading Comprehension",
    author = "Andor, Daniel  and
      He, Luheng  and
      Lee, Kenton  and
      Pitler, Emily",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
}

@misc{carroll:etal:2019,
  doi = {10.48550/ARXIV.1910.05789},
  
  url = {https://arxiv.org/abs/1910.05789},
  
  author = {Carroll, Micah and Shah, Rohin and Ho, Mark K. and Griffiths, Thomas L. and Seshia, Sanjit A. and Abbeel, Pieter and Dragan, Anca},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Human-Computer Interaction (cs.HC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Utility of Learning about Humans for Human-AI Coordination},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{iyer2022opt,
  title={OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
  author={Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and Li, Xian and O'Horo, Brian and Pereyra, Gabriel and Wang, Jeff and Dewan, Christopher and Celikyilmaz, Asli and Zettlemoyer, Luke and Stoyanov, Ves},
  journal={arXiv preprint arXiv:2212.12017},
  year={2022}
}

@article{srivastava2022beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@inproceedings{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle=nips,
  year={2021}
}

@misc{zeng:etal:2022,
  doi = {10.48550/ARXIV.2204.00598},
  url = {https://arxiv.org/abs/2204.00598},
  author = {Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{sutton:barto:2018,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{fair:etal:2022,
author = {META FUNDAMENTAL AI RESEARCH DIPLOMACY TEAM FAIR and Anton Bakhtin  and Noam Brown  and Emily Dinan  and Gabriele Farina  and Colin Flaherty  and Daniel Fried  and Andrew Goff  and Jonathan Gray  and Hengyuan Hu  and Athul Paul Jacob  and Mojtaba Komeili  and Karthik Konath  and Minae Kwon  and Adam Lerer  and Mike Lewis  and Alexander H. Miller  and Sasha Mitts  and Adithya Renduchintala  and Stephen Roller  and Dirk Rowe  and Weiyan Shi  and Joe Spisak  and Alexander Wei  and David Wu  and Hugh Zhang  and Markus Zijlstra },
title = {Human-level play in the game of <i>Diplomacy</i> by combining language models with strategic reasoning},
journal = {Science},
volume = {378},
number = {6624},
pages = {1067-1074},
year = {2022},
doi = {10.1126/science.ade9097},
URL = {https://www.science.org/doi/abs/10.1126/science.ade9097},
}

@article{Komeili2021internet,
  title={Internet-Augmented Dialogue Generation},
  author={Mojtaba Komeili and Kurt Shuster and Jason Weston},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.07566}
}

@inproceedings{knox2008tamer,
  title={Tamer: Training an agent manually via evaluative reinforcement},
  author={Knox, W Bradley and Stone, Peter},
  booktitle={2008 7th IEEE international conference on development and learning},
  pages={292--297},
  year={2008},
  organization={IEEE}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal=nips,
  year={2017}
}

@inproceedings{warnell2018deep,
  title={Deep tamer: Interactive agent shaping in high-dimensional state spaces},
  author={Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32, 1},
  year={2018}
}

@inproceedings{macglashan2017interactive,
  title={Interactive learning from policy-dependent human feedback},
  author={MacGlashan, James and Ho, Mark K and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L and Taylor, Matthew E and Littman, Michael L},
  booktitle={International Conference on Machine Learning},
  pages={2285--2294},
  year={2017},
  organization={PMLR}
}

@inproceedings{lake2018generalization,
  title={Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author={Lake, Brenden and Baroni, Marco},
  booktitle={International conference on machine learning},
  pages={2873--2882},
  year={2018},
  organization={PMLR}
}

@inproceedings{keysers2019measuring,
  title={Measuring Compositional Generalization: A Comprehensive Method on Realistic Data},
  author={Keysers, Daniel and Sch{\"a}rli, Nathanael and Scales, Nathan and Buisman, Hylke and Furrer, Daniel and Kashubin, Sergii and Momchev, Nikola and Sinopalnikov, Danila and Stafiniak, Lukasz and Tihon, Tibor and others},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{li-etal-2022-quantifying,
    title = "Quantifying Adaptability in Pre-trained Language Models with 500 Tasks",
    author = "Li, Belinda  and
      Yu, Jane  and
      Khabsa, Madian  and
      Zettlemoyer, Luke  and
      Halevy, Alon  and
      Andreas, Jacob",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.346",
    doi = "10.18653/v1/2022.naacl-main.346",
    pages = "4696--4715",
    abstract = "When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks. These tasks combine core aspects of language processing, including lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge. Using TaskBench500, we evaluate three facets of adaptability, finding that: (1) adaptation procedures differ dramatically in their ability to memorize small datasets; (2) within a subset of task types, adaptation procedures exhibit compositional adaptability to complex tasks; and (3) failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels. Our experiments show that adaptability to new tasks, like generalization to new examples, can be systematically described and understood, and we conclude with a discussion of additional aspects of adaptability that could be studied using the new benchmark.",
}

@inproceedings{perez2020unsupervised,
  title={Unsupervised Question Decomposition for Question Answering},
  author={Perez, Ethan and Lewis, Patrick and Yih, Wen-tau and Cho, Kyunghyun and Kiela, Douwe},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2020}
}


@article{drozdov2022compositional,
  title={Compositional semantic parsing with large language models},
  author={Drozdov, Andrew and Sch{\"a}rli, Nathanael and Aky{\"u}rek, Ekin and Scales, Nathan and Song, Xinying and Chen, Xinyun and Bousquet, Olivier and Zhou, Denny},
  journal={arXiv preprint arXiv:2209.15003},
  year={2022}
}

@article{dua2022successive,
  title={Successive Prompting for Decomposing Complex Questions},
  author={Dua, Dheeru and Gupta, Shivanshu and Singh, Sameer and Gardner, Matt},
  journal=emnlp,
  year={2022}
}

@article{khot2022decomposed,
  title={Decomposed prompting: A modular approach for solving complex tasks},
  author={Khot, Tushar and Trivedi, Harsh and Finlayson, Matthew and Fu, Yao and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2210.02406},
  year={2022}
}

@inproceedings{talmor2018web,
  title={The Web as a Knowledge-Base for Answering Complex Questions},
  author={Talmor, Alon and Berant, Jonathan},
  booktitle=naacl,
  year={2018}
}

@inproceedings{min2019multi,
  title={Multi-hop Reading Comprehension through Question Decomposition and Rescoring},
  author={Min, Sewon and Zhong, Victor and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6097--6109},
  year={2019}
}

@article{creswell2022selection,
  title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
  author={Creswell, Antonia and Shanahan, Murray and Higgins, Irina},
  journal={arXiv preprint arXiv:2205.09712},
  year={2022}
}

@article{silver:etal:2017,
  title={Mastering the game of Go without human knowledge},
  author={David Silver and Julian Schrittwieser and Karen Simonyan and Ioannis Antonoglou and Aja Huang and Arthur Guez and Thomas Hubert and Lucas baker and Matthew Lai and Adrian Bolton and Yutian Chen and Timothy P. Lillicrap and Fan Hui and L. Sifre and George van den Driessche and Thore Graepel and Demis Hassabis},
  journal={Nature},
  year={2017},
  volume={550},
  pages={354-359}
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = acl,
    year = "2002",
}

@inproceedings{popovic-2017-chrf,
    title = "chr{F}++: words helping character n-grams",
    author = "Popovi{\'c}, Maja",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    year = "2017",
    publisher = "Association for Computational Linguistics",
    pages = "612--618",
}


@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    year = "2005",
    publisher = "Association for Computational Linguistics",
    pages = "65--72",
}

@inproceedings{vaswani:etal:2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{zelikman2022star,
  title={Star: Self-taught reasoner bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Mu, Jesse and Goodman, Noah D and Wu, Yuhuai Tony},
  journal=nips,
  year={2022}
}

@inproceedings{mnih:etal:2016,
author = {Mnih, Volodymyr and Badia, Adri\`{a} Puigdom\`{e}nech and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P. and Silver, David and Kavukcuoglu, Koray},
title = {Asynchronous Methods for Deep Reinforcement Learning},
year = {2016},
publisher = {JMLR.org},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1928–1937},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}




@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}


@article{uccetina:etal:2021,
  author    = {V{\'{\i}}ctor Uc{-}Cetina and
               Nicol{\'{a}}s Navarro{-}Guerrero and
               Anabel Mart{\'{\i}}n{-}Gonz{\'{a}}lez and
               Cornelius Weber and
               Stefan Wermter},
  title     = {Survey on reinforcement learning for language processing},
  journal   = {CoRR},
  volume    = {abs/2104.05565},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.05565},
  eprinttype = {arXiv},
  eprint    = {2104.05565},
  timestamp = {Sat, 09 Apr 2022 12:27:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-05565.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{he:etal:2015drl,
  author    = {Ji He and
               Jianshu Chen and
               Xiaodong He and
               Jianfeng Gao and
               Lihong Li and
               Li Deng and
               Mari Ostendorf},
  title     = {Deep Reinforcement Learning with an Unbounded Action Space},
  journal   = {CoRR},
  volume    = {abs/1511.04636},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.04636},
  eprinttype = {arXiv},
  eprint    = {1511.04636},
  timestamp = {Mon, 13 Aug 2018 16:47:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeCHGLDO15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{he:etal:2016drl,
  author    = {Ji He and
               Mari Ostendorf and
               Xiaodong He and
               Jianshu Chen and
               Jianfeng Gao and
               Lihong Li and
               Li Deng},
  title     = {Deep Reinforcement Learning with a Combinatorial Action Space for
               Predicting and Tracking Popular Discussion Threads},
  journal   = {CoRR},
  volume    = {abs/1606.03667},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.03667},
  eprinttype = {arXiv},
  eprint    = {1606.03667},
  timestamp = {Mon, 13 Aug 2018 16:48:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeOHCGLD16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
grave2017improving,
title={Improving Neural Language Models with a Continuous Cache},
author={Edouard Grave and Armand Joulin and Nicolas Usunier},
booktitle=iclr,
year={2017},
}

@inproceedings{nogueira:cho:2017,
    title = "Task-Oriented Query Reformulation with Reinforcement Learning",
    author = "Nogueira, Rodrigo  and
      Cho, Kyunghyun",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1061",
    doi = "10.18653/v1/D17-1061",
    pages = "574--583",
}

@article{trivedi2022interleaving,
  author = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  title = {Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions},
  journal = {arXiv preprint arXiv:2212.10509}, 
  year = {2022},
}

@article{asai2022task-aware,
  author = {Asai, Akari and Schick, Timo and Lewis, Patrick and Chen, Xilun and Izacard, Gautier and Riedel, Sebastian and Hajishirzi, Hannaneh and Yih, Wen-tau},
  title = {Task-aware Retrieval with Instructions},
  journal = {arXiv preprint arXiv:2211.09260},
  year = {2022},
}

@article{creswell2022faithful,
  title={Faithful reasoning using large language models},
  author={Creswell, Antonia and Shanahan, Murray},
  journal={arXiv preprint arXiv:2208.14271},
  year={2022}
}

@inproceedings{wu2022promptchainer,
  title={Promptchainer: Chaining large language model prompts through visual programming},
  author={Wu, Tongshuang and Jiang, Ellen and Donsbach, Aaron and Gray, Jeff and Molina, Alejandra and Terry, Michael and Cai, Carrie J},
  booktitle={CHI Conference on Human Factors in Computing Systems Extended Abstracts},
  pages={1--10},
  year={2022}
}

@misc{
zhong2018seqsql,
title={Seq2{SQL}: Generating Structured Queries From Natural Language Using Reinforcement Learning },
author={Victor Zhong and Caiming Xiong and Richard Socher},
year={2018},
url={https://openreview.net/forum?id=Syx6bz-Ab},
}

@inproceedings{merity2017pointer,
title={Pointer sentinel mixture models},
author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
booktitle=iclr,
year={2017},
}

@article{izacard2020leveraging,
  title={Leveraging passage retrieval with generative models for open domain question answering},
  author={Izacard, Gautier and Grave, Edouard},
  journal={arXiv preprint arXiv:2007.01282},
  year={2020}
}

@article{chen2017reading,
  title={Reading wikipedia to answer open-domain questions},
  author={Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
  journal={arXiv preprint arXiv:1704.00051},
  year={2017}
}

@article{lee2019latent,
  title={Latent retrieval for weakly supervised open domain question answering},
  author={Lee, Kenton and Chang, Ming-Wei and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1906.00300},
  year={2019}
}

@article{giannou2023looped,
  title={Looped Transformers as Programmable Computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2301.13196},
  year={2023}
}

@article{clark2017simple,
  title={Simple and effective multi-paragraph reading comprehension},
  author={Clark, Christopher and Gardner, Matt},
  journal={arXiv preprint arXiv:1710.10723},
  year={2017}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}


@misc{wang:etal:2023,
  doi = {10.48550/ARXIV.2302.01560},
  
  url = {https://arxiv.org/abs/2302.01560},
  
  author = {Wang, Zihao and Cai, Shaofei and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
  
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
